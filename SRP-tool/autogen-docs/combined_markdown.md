# AutoGen Consolidated Documentation

* [Getting Started](Getting-Started)
* [Installation](Installation)
* Use Cases
* Examples
* [Contributing](.docs/Contribute)
* [Research](.docs/Research)

## Getting Started {#getting-started}

FLAML is a lightweight Python library for efficient automation of machine
learning and AI operations. It automates workflow based on large language
models, machine learning models, etc. and optimizes their performance.

## Main Features​ {#main-features}

* FLAML enables building next-gen GPT-X applications based on multi-agent conversations with minimal effort. It simplifies the orchestration, automation and optimization of a complex GPT-X workflow. It maximizes the performance of GPT-X models and augments their weakness.
* For common machine learning tasks like classification and regression, it quickly finds quality models for user-provided data with low computational resources. It is easy to customize or extend.
* It supports fast and economical automatic tuning, capable of handling large search space with heterogeneous evaluation cost and complex constraints/guidance/early stopping.

FLAML is powered by a series of [research studies](.docs/Research) from
Microsoft Research and collaborators such as Penn State University, Stevens
Institute of Technology, University of Washington, and University of Waterloo.

### Quickstart​ {#quickstart}

Install FLAML from pip: `pip install flaml`. Find more options in
[Installation](.docs/Installation).

There are several ways of using flaml:

#### (New) [AutoGen](https://microsoft.github.io/autogen/)​ {#new-autogenhttpsmicrosoftgithubioautogen}

Autogen enables the next-gen GPT-X applications with a generic multi-agent
conversation framework. It offers customizable and conversable agents which
integrate LLMs, tools and human. By automating chat among multiple capable
agents, one can easily make them collectively perform tasks autonomously or
with human feedback, including tasks that require using tools via code. For
example,

  ```python
  from flaml import autogen  
  assistant = autogen.AssistantAgent("assistant")  
  user_proxy = autogen.UserProxyAgent("user_proxy")  
  user_proxy.initiate_chat(assistant, message="Show me the YTD gain of 10 largest technology companies as of today.")  
```

## This initiates an automated chat between the two agents to solve the task  

Autogen also helps maximize the utility out of the expensive LLMs such as
ChatGPT and GPT-4. It offers a drop-in replacement of `openai.Completion` or
`openai.ChatCompletion` with powerful functionalites like tuning, caching,
error handling, templating. For example, you can optimize generations by LLM
with your own tuning data, success metrics and budgets.

  ```python
      # perform tuning  
      config, analysis = autogen.Completion.tune(  
          data=tune_data,  
          metric="success",  
          mode="max",  
          eval_func=eval_func,  
          inference_budget=0.05,  
          optimization_budget=3,  
          num_samples=-1,  
      )  
      # perform inference for a test instance  
      response = autogen.Completion.create(context=test_instance, **config)
  ```

#### [Task-oriented AutoML](.docs/Use-Cases/task-oriented-automl)​ #taskoriented-automlflamldocsusecasestaskorientedautoml}

With three lines of code, you can start using this economical and fast AutoML engine as a scikit-learn style estimator.

  ```python
    from flaml import AutoML  
    automl = AutoML()  
    automl.fit(X_train, y_train, task="classification", time_budget=60)  
  ```

It automatically tunes the hyperparameters and selects the best model from default learners such as LightGBM, XGBoost, random forest etc. for the specified time budget 60 seconds. [Customizing](.docs/Use-Cases/task-
oriented-automl#customize-automlfit) the optimization metrics, learners and search spaces etc. is very easy. For example,

  ```python
    automl.add_learner("mylgbm", MyLGBMEstimator)  
    automl.fit(X_train, y_train, task="classification", metric=custom_metric, estimator_list=["mylgbm"], time_budget=60)  
  ```

#### [Tune user-defined function](.docs/Use-Cases/Tune-User-Defined- {#tune-userdefined-functionflamldocsusecasestuneuserdefined}Function)

  You can run generic hyperparameter tuning for a custom function (machine learning or beyond). For example,

  ```python
  from flaml import tune  
  from flaml.automl.model import LGBMEstimator  

  def train_lgbm(config: dict) -> dict:  
      # convert config dict to lgbm params  
      params = LGBMEstimator(**config).params  
      # train the model  
      train_set = lightgbm.Dataset(csv_file_name)  
      model = lightgbm.train(params, train_set)  
      # evaluate the model  
      pred = model.predict(X_test)  
      mse = mean_squared_error(y_test, pred)  
      # return eval results as a dictionary  
      return {"mse": mse}  
  # load a built-in search space from flaml  
  flaml_lgbm_search_space = LGBMEstimator.search_space(X_train.shape)  
  # specify the search space as a dict from hp name to domain; you can define your own search space same way  
  config_search_space = {hp: space["domain"] for hp, space in flaml_lgbm_search_space.items()}  
  # give guidance about hp values corresponding to low training cost, i.e., {"n_estimators": 4, "num_leaves": 4}  
  low_cost_partial_config = {  
      hp: space["low_cost_init_value"]  
      for hp, space in flaml_lgbm_search_space.items()  
      if "low_cost_init_value" in space  
  }  
  # run the tuning, minimizing mse, with total time budget 3 seconds  
  analysis = tune.run(  
      train_lgbm, metric="mse", mode="min", config=config_search_space,  
      low_cost_partial_config=low_cost_partial_config, time_budget_s=3, num_samples=-1,  
  )  
  ```

Please see this
[script](https://github.com/microsoft.blob/main/test/tune_example.py)
for the complete version of the above example.

#### [Zero-shot AutoML](.docs/Use-Cases/Zero-Shot-AutoML)​ {#zeroshot-automlflamldocsusecaseszeroshotautoml}

FLAML offers a unique, seamless and effortless way to leverage AutoML for the
commonly used classifiers and regressors such as LightGBM and XGBoost. For
example, if you are using `lightgbm.LGBMClassifier` as your current learner,
all you need to do is to replace `from lightgbm import LGBMClassifier` by:

  ```python
    from flaml.default import LGBMClassifier  
  ```

Then, you can use it just like you use the original `LGMBClassifier`. Your
other code can remain unchanged. When you call the `fit()` function from
`flaml.default.LGBMClassifier`, it will automatically instantiate a good data-
dependent hyperparameter configuration for your dataset, which is expected to
work better than the default configuration.

### Where to Go Next?​ {#where-to-go-next}

* Understand the use cases for [AutoGen](https://microsoft.github.io/autogen/), [Task-oriented AutoML](.docs/Use-Cases/Task-Oriented-Automl), [Tune user-defined function](.docs/Use-Cases/Tune-User-Defined-Function) and [Zero-shot AutoML](.docs/Use-Cases/Zero-Shot-AutoML).
* Find code examples under "Examples": from [AutoGen - AgentChat](.docs/Examples/AutoGen-AgentChat) to [Tune - PyTorch](.docs/Examples/Tune-PyTorch).
* Learn about [research](.docs/Research) around FLAML and check [blogposts](.blog).
* Chat on [Discord](https://discord.gg/Cppx2vSPVP).

[NextInstallation »](.docs/Installation)

## onlineml.trial_runner {#onlinemltrialrunner}

## OnlineTrialRunner Objects​ {#onlinetrialrunner-objects}

  ```python
    class OnlineTrialRunner()  
  ```

Class for the OnlineTrialRunner.

  ```python
  __init__​ {#init}

    def __init__(max_live_model_num: int, searcher=None, scheduler=None, champion_test_policy="loss_ucb", **kwargs) 
  ```

Constructor.

 **Arguments** :

* `max_live_model_num` \- The maximum number of 'live'/running models allowed.
* `searcher` \- A class for generating Trial objects progressively. The ConfigOracle is implemented in the searcher.
* `scheduler` \- A class for managing the 'live' trials and allocating the resources for the trials.
* `champion_test_policy` \- A string to specify what test policy to test for champion. Currently can choose from ['loss_ucb', 'loss_avg', 'loss_lcb', None].

#### champion_trial​ {#championtrial}

  ```python
    @property  
    def champion_trial() -> Trial  
  ```

The champion trial.

#### running_trials​ {#runningtrials}

  ```python
    @property  
    def running_trials()  
  ```

The running/'live' trials.

#### step​ {#step}
  
  ```python
    def step(data_sample=None, prediction_trial_tuple=None)  
  ```

Schedule one trial to run each time it is called.

 **Arguments** :

* `data_sample` \- One data example.
* `prediction_trial_tuple` \- A list of information containing (prediction_made, prediction_trial).

#### get_top_running_trials​ {#gettoprunningtrials}
```python
    def get_top_running_trials(top_ratio=None, top_metric="ucb") -> list  
  ```
Get a list of trial ids, whose performance is among the top running trials.

#### get_trials​ {#gettrials}
```python
    def get_trials() -> list  
  ```
Return the list of trials managed by this TrialRunner.

#### add_trial​ {#addtrial}
```python
    def add_trial(new_trial)  
  ```
Add a new trial to this TrialRunner. Trials may be added at any time.

 **Arguments** :

* `new_trial` _Trial_ \- Trial to queue.

#### stop_trial​ {#stoptrial}
```python
    def stop_trial(trial)  
  ```
Stop a trial: set the status of a trial to be Trial.TERMINATED and perform
other subsequent operations.

#### pause_trial​ {#pausetrial}
```python
    def pause_trial(trial)  
  ```
Pause a trial: set the status of a trial to be Trial.PAUSED and perform other
subsequent operations.

#### run_trial​ {#runtrial}

```python
    def run_trial(trial)  
  ```

Run a trial: set the status of a trial to be Trial.RUNNING and perform other
subsequent operations.


* Reference
  * agentchat
    * agentchat.contrib
      * agentchat.contrib.capabilities
      * [agent_builder](/autogen/docs/reference/agentchat/contrib/agent_builder)
      * [compressible_agent](/autogen/docs/reference/agentchat/contrib/compressible_agent)
      * [gpt_assistant_agent](/autogen/docs/reference/agentchat/contrib/gpt_assistant_agent)
      * [img_utils](/autogen/docs/reference/agentchat/contrib/img_utils)
      * [llava_agent](/autogen/docs/reference/agentchat/contrib/llava_agent)
      * [math_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/math_user_proxy_agent)
      * [multimodal_conversable_agent](/autogen/docs/reference/agentchat/contrib/multimodal_conversable_agent)
      * [qdrant_retrieve_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/qdrant_retrieve_user_proxy_agent)
      * [retrieve_assistant_agent](/autogen/docs/reference/agentchat/contrib/retrieve_assistant_agent)
      * [retrieve_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/retrieve_user_proxy_agent)
      * [text_analyzer_agent](/autogen/docs/reference/agentchat/contrib/text_analyzer_agent)
    * [agent](/autogen/docs/reference/agentchat/agent)
    * [assistant_agent](/autogen/docs/reference/agentchat/assistant_agent)
    * [conversable_agent](/autogen/docs/reference/agentchat/conversable_agent)
    * [groupchat](/autogen/docs/reference/agentchat/groupchat)
    * [user_proxy_agent](/autogen/docs/reference/agentchat/user_proxy_agent)
  * oai
  * [code_utils](/autogen/docs/reference/code_utils)
  * [function_utils](/autogen/docs/reference/function_utils)
  * [math_utils](/autogen/docs/reference/math_utils)
  * [retrieve_utils](/autogen/docs/reference/retrieve_utils)
  * [token_count_utils](/autogen/docs/reference/token_count_utils)

On this page
# agentchat.contrib.llava_agent {#agentchatcontribllavaagent}

## LLaVAAgent Objects​ {#llavaagent-objects}
```python 
    class LLaVAAgent(MultimodalConversableAgent)  

#### __init__​ {#init}
    def __init__(name: str,  
                 system_message: Optional[Tuple[str,  
                                                List]] = DEFAULT_LLAVA_SYS_MSG,  
                 *args,  
                 **kwargs)  
```

 **Arguments** :

* `name` _str_ \- agent name.
* `system_message` _str_ \- system message for the ChatCompletion inference. Please override this attribute if you want to reprogram the agent.
* `**kwargs` _dict_ \- Please refer to other kwargs in [ConversableAgent](/autogen/docs/reference/agentchat/conversable_agent#__init__).

#### llava_call​ {#llavacall}
```python
    def llava_call(prompt: str, llm_config: dict) -> str  

Makes a call to the LLaVA service to generate text based on a given prompt

[Edit this
page](https://github.com/microsoft/autogen/edit/main/website/docs/reference/agentchat/contrib/llava_agent.md)

[Previous« img_utils](/autogen/docs/reference/agentchat/contrib/img_utils)

[Nextmath_user_proxy_agent
»](/autogen/docs/reference/agentchat/contrib/math_user_proxy_agent)

* LLaVAAgent Objects

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* [Getting Started](.docs/Getting-Started)
* [Installation](.docs/Installation)
* Use Cases
* Examples
  * [AutoGen - Automated Multi Agent Chat](.docs/Examples/AutoGen-AgentChat)
  * [AutoGen - Tune GPT Models](.docs/Examples/AutoGen-OpenAI)
  * [AutoML - Classification](.docs/Examples/AutoML-Classification)
  * [AutoML - NLP](.docs/Examples/AutoML-NLP)
  * [AutoML - Rank](.docs/Examples/AutoML-Rank)
  * [AutoML - Regression](.docs/Examples/AutoML-Regression)
  * [AutoML - Time Series Forecast](.docs/Examples/AutoML-Time series forecast)
  * [AutoML for LightGBM](.docs/Examples/AutoML-for-LightGBM)
  * [AutoML for XGBoost](.docs/Examples/AutoML-for-XGBoost)
  * [Default - Flamlized Estimator](.docs/Examples/Default-Flamlized)
  * [Integrate - AzureML](.docs/Examples/Integrate - AzureML)
  * [Integrate - Scikit-learn Pipeline](.docs/Examples/Integrate - Scikit-learn Pipeline)
  * [Integrate - Spark](.docs/Examples/Integrate - Spark)
  * [Tune - AzureML pipeline](.docs/Examples/Tune-AzureML-pipeline)
  * [Tune - HuggingFace](.docs/Examples/Tune-HuggingFace)
  * [Tune - Lexicographic Objectives](.docs/Examples/Tune-Lexicographic-objectives)
  * [Tune - PyTorch](.docs/Examples/Tune-PyTorch)
* [Contributing](.docs/Contribute)
* [Research](.docs/Research)

On this page

# AutoML - Classification {#automl-classification}

### Prerequisites​ {#prerequisites}

Install the [automl] option.

    pip install "flaml[automl]"  

### A basic classification example​ {#a-basic-classification-example}

    from flaml import AutoML  
    from sklearn.datasets import load_iris  
      
    # Initialize an AutoML instance  
    automl = AutoML()  
    # Specify automl goal and constraint  
    automl_settings = {  
        "time_budget": 1,  # in seconds  
        "metric": 'accuracy',  
        "task": 'classification',  
        "log_file_name": "iris.log",  
    }  
    X_train, y_train = load_iris(return_X_y=True)  
    # Train with labeled input data  
    automl.fit(X_train=X_train, y_train=y_train,  
               **automl_settings)  
    # Predict  
    print(automl.predict_proba(X_train))  
    # Print the best model  
    print(automl.model.estimator)  

#### Sample of output​ {#sample-of-output}

    [flaml.automl: 11-12 18:21:44] {1485} INFO - Data split method: stratified  
    [flaml.automl: 11-12 18:21:44] {1489} INFO - Evaluation method: cv  
    [flaml.automl: 11-12 18:21:44] {1540} INFO - Minimizing error metric: 1-accuracy  
    [flaml.automl: 11-12 18:21:44] {1577} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'catboost', 'xgboost', 'extra_tree', 'lrl1']  
    [flaml.automl: 11-12 18:21:44] {1826} INFO - iteration 0, current learner lgbm  
    [flaml.automl: 11-12 18:21:44] {1944} INFO - Estimated sufficient time budget=1285s. Estimated necessary time budget=23s.  
    [flaml.automl: 11-12 18:21:44] {2029} INFO -  at 0.2s,  estimator lgbm's best error=0.0733, best estimator lgbm's best error=0.0733  
    [flaml.automl: 11-12 18:21:44] {1826} INFO - iteration 1, current learner lgbm  
    [flaml.automl: 11-12 18:21:44] {2029} INFO -  at 0.3s,  estimator lgbm's best error=0.0733, best estimator lgbm's best error=0.0733  
    [flaml.automl: 11-12 18:21:44] {1826} INFO - iteration 2, current learner lgbm  
    [flaml.automl: 11-12 18:21:44] {2029} INFO -  at 0.4s,  estimator lgbm's best error=0.0533, best estimator lgbm's best error=0.0533  
    [flaml.automl: 11-12 18:21:44] {1826} INFO - iteration 3, current learner lgbm  
    [flaml.automl: 11-12 18:21:44] {2029} INFO -  at 0.6s,  estimator lgbm's best error=0.0533, best estimator lgbm's best error=0.0533  
    [flaml.automl: 11-12 18:21:44] {1826} INFO - iteration 4, current learner lgbm  
    [flaml.automl: 11-12 18:21:44] {2029} INFO -  at 0.6s,  estimator lgbm's best error=0.0533, best estimator lgbm's best error=0.0533  
    [flaml.automl: 11-12 18:21:44] {1826} INFO - iteration 5, current learner xgboost  
    [flaml.automl: 11-12 18:21:45] {2029} INFO -  at 0.9s,  estimator xgboost's best error=0.0600,  best estimator lgbm's best error=0.0533  
    [flaml.automl: 11-12 18:21:45] {1826} INFO - iteration 6, current learner lgbm  
    [flaml.automl: 11-12 18:21:45] {2029} INFO -  at 1.0s,  estimator lgbm's best error=0.0533, best estimator lgbm's best error=0.0533  
    [flaml.automl: 11-12 18:21:45] {1826} INFO - iteration 7, current learner extra_tree  
    [flaml.automl: 11-12 18:21:45] {2029} INFO -  at 1.1s,  estimator extra_tree's best error=0.0667,   best estimator lgbm's best error=0.0533  
    [flaml.automl: 11-12 18:21:45] {2242} INFO - retrain lgbm for 0.0s  
    [flaml.automl: 11-12 18:21:45] {2247} INFO - retrained model: LGBMClassifier(learning_rate=0.2677050123105203, max_bin=127,  
                   min_child_samples=12, n_estimators=4, num_leaves=4,  
                   reg_alpha=0.001348364934537134, reg_lambda=1.4442580148221913,  
                   verbose=-1)  
    [flaml.automl: 11-12 18:21:45] {1608} INFO - fit succeeded  
    [flaml.automl: 11-12 18:21:45] {1610} INFO - Time taken to find the best model: 0.3756711483001709  

### A more advanced example including custom learner and metric​ {#a-more-advanced-example-including-custom-learner-and-metric}

[Link to
notebook](https://github.com/microsoft.blob/main/notebook/automl_classification.ipynb)
| [Open in
colab](https://colab.research.google.com/github/microsoft.blob/main/notebook/automl_classification.ipynb)

[Edit this
page](<https://github.com/microsoft.edit/main/website/docs/Examples/AutoML->
Classification.md)

[Previous« AutoGen - Tune GPT Models](.docs/Examples/AutoGen-OpenAI)

[NextAutoML - NLP »](.docs/Examples/AutoML-NLP)

* Prerequisites
* A basic classification example
* A more advanced example including custom learner and metric

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
    * autogen.agentchat
      * [agent](.docs/reference/autogen/agentchat/agent)
      * [assistant_agent](.docs/reference/autogen/agentchat/assistant_agent)
      * [conversable_agent](.docs/reference/autogen/agentchat/conversable_agent)
      * [groupchat](.docs/reference/autogen/agentchat/groupchat)
      * [user_proxy_agent](.docs/reference/autogen/agentchat/user_proxy_agent)
    * autogen.oai
    * [code_utils](.docs/reference/autogen/code_utils)
    * [math_utils](.docs/reference/autogen/math_utils)
    * [retrieve_utils](.docs/reference/autogen/retrieve_utils)
  * automl
  * default
  * onlineml
  * tune

On this page

# autogen.agentchat.agent {#autogenagentchatagent}

## Agent Objects​ {#agent-objects}

    class Agent()  

(In preview) An abstract class for AI agent.

An agent can communicate with other agents and perform actions. Different
agents can differ in what actions they perform in the `receive` method.

#### __init__​ {#init}
```python
    def __init__(name: str)  

 **Arguments** :

* `name` _str_ \- name of the agent.

#### name​ {#name}

    @property  
    def name()  

Get the name of the agent.

#### send​ {#send}
```python
    def send(message: Union[Dict, str], recipient: "Agent", request_reply: Optional[bool] = None)  

(Aabstract method) Send a message to another agent.

#### a_send​ {#asend}

    async def a_send(message: Union[Dict, str], recipient: "Agent", request_reply: Optional[bool] = None)  

(Aabstract async method) Send a message to another agent.

#### receive​ {#receive}

    def receive(message: Union[Dict, str], sender: "Agent", request_reply: Optional[bool] = None)  

(Abstract method) Receive a message from another agent.

#### a_receive​ {#areceive}

    async def a_receive(message: Union[Dict, str], sender: "Agent", request_reply: Optional[bool] = None)  

(Abstract async method) Receive a message from another agent.

#### reset​ {#reset}
```python
    def reset()  

(Abstract method) Reset the agent.

#### generate_reply​ {#generatereply}
```python
    def generate_reply(messages: Optional[List[Dict]] = None, sender: Optional["Agent"] = None, **kwargs, ,) -> Union[str, Dict, None]  

(Abstract method) Generate a reply based on the received messages.

 **Arguments** :

* `messages` _list [dict]_ \- a list of messages received.
* `sender` \- sender of an Agent instance.

 **Returns** :

str or dict or None: the generated reply. If None, no reply is generated.

#### a_generate_reply​ {#ageneratereply}

    async def a_generate_reply(messages: Optional[List[Dict]] = None, sender: Optional["Agent"] = None, **kwargs, ,) -> Union[str, Dict, None]  

(Abstract async method) Generate a reply based on the received messages.

 **Arguments** :

* `messages` _list [dict]_ \- a list of messages received.
* `sender` \- sender of an Agent instance.

 **Returns** :

str or dict or None: the generated reply. If None, no reply is generated.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/autogen/agentchat/agent.md)

[Nextassistant_agent
»](.docs/reference/autogen/agentchat/assistant_agent)

* Agent Objects

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

* Reference
  * agentchat
    * agentchat.contrib
      * agentchat.contrib.capabilities
      * [agent_builder](/autogen/docs/reference/agentchat/contrib/agent_builder)
      * [compressible_agent](/autogen/docs/reference/agentchat/contrib/compressible_agent)
      * [gpt_assistant_agent](/autogen/docs/reference/agentchat/contrib/gpt_assistant_agent)
      * [img_utils](/autogen/docs/reference/agentchat/contrib/img_utils)
      * [llava_agent](/autogen/docs/reference/agentchat/contrib/llava_agent)
      * [math_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/math_user_proxy_agent)
      * [multimodal_conversable_agent](/autogen/docs/reference/agentchat/contrib/multimodal_conversable_agent)
      * [qdrant_retrieve_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/qdrant_retrieve_user_proxy_agent)
      * [retrieve_assistant_agent](/autogen/docs/reference/agentchat/contrib/retrieve_assistant_agent)
      * [retrieve_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/retrieve_user_proxy_agent)
      * [text_analyzer_agent](/autogen/docs/reference/agentchat/contrib/text_analyzer_agent)
    * [agent](/autogen/docs/reference/agentchat/agent)
    * [assistant_agent](/autogen/docs/reference/agentchat/assistant_agent)
    * [conversable_agent](/autogen/docs/reference/agentchat/conversable_agent)
    * [groupchat](/autogen/docs/reference/agentchat/groupchat)
    * [user_proxy_agent](/autogen/docs/reference/agentchat/user_proxy_agent)
  * oai
  * [code_utils](/autogen/docs/reference/code_utils)
  * [function_utils](/autogen/docs/reference/function_utils)
  * [math_utils](/autogen/docs/reference/math_utils)
  * [retrieve_utils](/autogen/docs/reference/retrieve_utils)
  * [token_count_utils](/autogen/docs/reference/token_count_utils)

On this page

# agentchat.contrib.retrieve_assistant_agent {#agentchatcontribretrieveassistantagent}

## RetrieveAssistantAgent Objects​ {#retrieveassistantagent-objects}

    class RetrieveAssistantAgent(AssistantAgent)  

(Experimental) Retrieve Assistant agent, designed to solve a task with LLM.

RetrieveAssistantAgent is a subclass of AssistantAgent configured with a
default system message. The default system message is designed to solve a task
with LLM, including suggesting python code blocks and debugging.
`human_input_mode` is default to "NEVER" and `code_execution_config` is
default to False. This agent doesn't execute code by default, and expects the
user to execute the code.

[Edit this
page](https://github.com/microsoft/autogen/edit/main/website/docs/reference/agentchat/contrib/retrieve_assistant_agent.md)

[Previous«
qdrant_retrieve_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/qdrant_retrieve_user_proxy_agent)

[Nextretrieve_user_proxy_agent
»](/autogen/docs/reference/agentchat/contrib/retrieve_user_proxy_agent)

* RetrieveAssistantAgent Objects

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

Recent posts

* [All About Agent Descriptions](/autogen/blog/2023/12/29/AgentDescriptions)
* [AgentOptimizer - An Agentic Way to Train Your LLM Agent](/autogen/blog/2023/12/23/AgentOptimizer)
* [AutoGen Studio: Interactively Explore Multi-Agent Workflows](/autogen/blog/2023/12/01/AutoGenStudio)
* [Agent AutoBuild - Automatically Building Multi-agent Systems](/autogen/blog/2023/11/26/Agent-AutoBuild)
* [How to Assess Utility of LLM-powered Applications?](/autogen/blog/2023/11/20/AgentEval)
* [AutoGen Meets GPTs](/autogen/blog/2023/11/13/OAI-assistants)
* [EcoAssistant - Using LLM Assistants More Accurately and Affordably](/autogen/blog/2023/11/09/EcoAssistant)
* [Multimodal with GPT-4V and LLaVA](/autogen/blog/2023/11/06/LMM-Agent)
* [AutoGen's Teachable Agents](/autogen/blog/2023/10/26/TeachableAgent)
* [Retrieval-Augmented Generation (RAG) Applications with AutoGen](/autogen/blog/2023/10/18/RetrieveChat)
* [Use AutoGen for Local LLMs](/autogen/blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](/autogen/blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](/autogen/blog/2023/05/18/GPT-adaptive-humaneval)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](/autogen/blog/2023/04/21/LLM-tuning-math)

# One post tagged with "LMM" {#one-post-tagged-with-lmm}

[View All Tags](/autogen/blog/tags)

## [Multimodal with GPT-4V and LLaVA](/autogen/blog/2023/11/06/LMM-Agent) {#multimodal-with-gpt4v-and-llavaautogenblog20231106lmmagent}

November 6, 2023 · 3 min read

[![Beibin Li](https://github.com/beibinli.png)](https://github.com/beibinli)

[Beibin Li](https://github.com/beibinli)

Senior Research Engineer at Microsoft

![LMM
Teaser](/autogen/assets/images/teaser-380bdaa90a1c02ad009520bf289776c9.png)

 **In Brief:**

* Introducing the **Multimodal Conversable Agent** and the **LLaVA Agent** to enhance LMM functionalities.
* Users can input text and images simultaneously using the `<img img_path>` tag to specify image loading.
* Demonstrated through the [GPT-4V notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_gpt-4v.ipynb).
* Demonstrated through the [LLaVA notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_llava.ipynb).

## Introduction​ {#introduction}

Large multimodal models (LMMs) augment large language models (LLMs) with the
ability to process multi-sensory data.

This blog post and the latest AutoGen update concentrate on visual
comprehension. Users can input images, pose questions about them, and receive
text-based responses from these LMMs. We support the `gpt-4-vision-preview`
model from OpenAI and `LLaVA` model from Microsoft now.

Here, we emphasize the **Multimodal Conversable Agent** and the **LLaVA
Agent** due to their growing popularity. GPT-4V represents the forefront in
image comprehension, while LLaVA is an efficient model, fine-tuned from
LLama-2.

## Installation​ {#installation}

Incorporate the `lmm` feature during AutoGen installation:

    pip install "pyautogen[lmm]"  

Subsequently, import the **Multimodal Conversable Agent** or **LLaVA Agent**
from AutoGen:

    from autogen.agentchat.contrib.multimodal_conversable_agent import MultimodalConversableAgent  # for GPT-4V  
    from autogen.agentchat.contrib.llava_agent import LLaVAAgent  # for LLaVA  

## Usage​ {#usage}

A simple syntax has been defined to incorporate both messages and images
within a single string.

Example of an in-context learning prompt:

    prompt = """You are now an image classifier for facial expressions. Here are  
    some examples.  
      
    <img happy.jpg> depicts a happy expression.  
    <img http://some_location.com/sad.jpg> represents a sad expression.  
    <img obama.jpg> portrays a neutral expression.  
      
    Now, identify the facial expression of this individual: <img unknown.png>  
    """  
      
    agent = MultimodalConversableAgent()  
    user = UserProxyAgent()  
    user.initiate_chat(agent, message=prompt)  

The `MultimodalConversableAgent` interprets the input prompt, extracting
images from local or internet sources.

## Advanced Usage​ {#advanced-usage}

Similar to other AutoGen agents, multimodal agents support multi-round
dialogues with other agents, code generation, factual queries, and management
via a GroupChat interface.

For example, the `FigureCreator` in our [GPT-4V
notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_gpt-4v.ipynb)
and [LLaVA
notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_llava.ipynb)
integrates two agents: a coder (an AssistantAgent) and critics (a multimodal
agent). The coder drafts Python code for visualizations, while the critics
provide insights for enhancement. Collaboratively, these agents aim to refine
visual outputs. With `human_input_mode=ALWAYS`, you can also contribute
suggestions for better visualizations.

## Reference​ {#reference}

* [GPT-4V System Card](https://openai.com/research/gpt-4v-system-card)
* [LLaVA GitHub](https://github.com/haotian-liu/LLaVA)

## Future Enhancements​ {#future-enhancements}

For further inquiries or suggestions, please open an issue in the [AutoGen
repository](https://github.com/microsoft/autogen/) or contact me directly at
[beibin.li@microsoft.com](mailto:beibin.li@microsoft.com).

AutoGen will continue to evolve, incorporating more multimodal functionalities
such as DALLE model integration, audio interaction, and video comprehension.
Stay tuned for these exciting developments.

 **Tags:**

* [LMM](/autogen/blog/tags/lmm)
* [multimodal](/autogen/blog/tags/multimodal)

[**Read More**](/autogen/blog/2023/11/06/LMM-Agent)

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
  * automl
    * automl.contrib
    * automl.nlp
    * automl.spark
    * automl.task
    * automl.time_series
      * [sklearn](.docs/reference/automl/time_series/sklearn)
      * [tft](.docs/reference/automl/time_series/tft)
      * [ts_data](.docs/reference/automl/time_series/ts_data)
      * [ts_model](.docs/reference/automl/time_series/ts_model)
    * [automl](.docs/reference/automl/automl)
    * [data](.docs/reference/automl/data)
    * [ml](.docs/reference/automl/ml)
    * [model](.docs/reference/automl/model)
    * [state](.docs/reference/automl/state)
  * default
  * onlineml
  * tune

On this page

# automl.time_series.sklearn {#automltimeseriessklearn}

#### make_lag_features​ {#makelagfeatures}
```python
    def make_lag_features(X: pd.DataFrame, y: pd.Series, lags: int)  

Transform input data X, y into autoregressive form - shift them appropriately
based on horizon and create `lags` columns.

## Parameters​ {#parameters}

X : pandas.DataFrame Input features.

y : array_like, (1d) Target vector.

horizon : int length of X for `predict` method

## Returns​ {#returns}

pandas.DataFrame shifted dataframe with `lags` columns

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/automl/time_series/sklearn.md)

[Previous«
time_series_task](.docs/reference/automl/task/time_series_task)

[Nexttft »](.docs/reference/automl/time_series/tft)

* Parameters
* Returns

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* [Getting Started](.docs/Getting-Started)
* [Installation](.docs/Installation)
* Use Cases
* Examples
  * [AutoGen - Automated Multi Agent Chat](.docs/Examples/AutoGen-AgentChat)
  * [AutoGen - Tune GPT Models](.docs/Examples/AutoGen-OpenAI)
  * [AutoML - Classification](.docs/Examples/AutoML-Classification)
  * [AutoML - NLP](.docs/Examples/AutoML-NLP)
  * [AutoML - Rank](.docs/Examples/AutoML-Rank)
  * [AutoML - Regression](.docs/Examples/AutoML-Regression)
  * [AutoML - Time Series Forecast](.docs/Examples/AutoML-Time series forecast)
  * [AutoML for LightGBM](.docs/Examples/AutoML-for-LightGBM)
  * [AutoML for XGBoost](.docs/Examples/AutoML-for-XGBoost)
  * [Default - Flamlized Estimator](.docs/Examples/Default-Flamlized)
  * [Integrate - AzureML](.docs/Examples/Integrate - AzureML)
  * [Integrate - Scikit-learn Pipeline](.docs/Examples/Integrate - Scikit-learn Pipeline)
  * [Integrate - Spark](.docs/Examples/Integrate - Spark)
  * [Tune - AzureML pipeline](.docs/Examples/Tune-AzureML-pipeline)
  * [Tune - HuggingFace](.docs/Examples/Tune-HuggingFace)
  * [Tune - Lexicographic Objectives](.docs/Examples/Tune-Lexicographic-objectives)
  * [Tune - PyTorch](.docs/Examples/Tune-PyTorch)
* [Contributing](.docs/Contribute)
* [Research](.docs/Research)

On this page

# Default - Flamlized Estimator {#default-flamlized-estimator}

Flamlized estimators automatically use data-dependent default hyperparameter
configurations for each estimator, offering a unique zero-shot AutoML
capability, or "no tuning" AutoML.

## Flamlized LGBMRegressor​ {#flamlized-lgbmregressor}

### Prerequisites​ {#prerequisites} {#prerequisites}

This example requires the [autozero] option.

    pip install flaml[autozero] lightgbm openml  

### Zero-shot AutoML​ {#zeroshot-automl} {#zeroshot-automl}

    from flaml.automl.data import load_openml_dataset  
    from flaml.default import LGBMRegressor  
    from flaml.automl.ml import sklearn_metric_loss_score  
      
    X_train, X_test, y_train, y_test = load_openml_dataset(dataset_id=537, data_dir="./")  
    lgbm = LGBMRegressor()  
    lgbm.fit(X_train, y_train)  
    y_pred = lgbm.predict(X_test)  
    print("flamlized lgbm r2", "=", 1 - sklearn_metric_loss_score("r2", y_pred, y_test))  
    print(lgbm)  

#### Sample output​ {#sample-output} {#sample-output} {#sample-output}

    load dataset from ./openml_ds537.pkl  
    Dataset name: houses  
    X_train.shape: (15480, 8), y_train.shape: (15480,);  
    X_test.shape: (5160, 8), y_test.shape: (5160,)  
    flamlized lgbm r2 = 0.8537444671194614  
    LGBMRegressor(colsample_bytree=0.7019911744574896,  
                  learning_rate=0.022635758411078528, max_bin=511,  
                  min_child_samples=2, n_estimators=4797, num_leaves=122,  
                  reg_alpha=0.004252223402511765, reg_lambda=0.11288241427227624,  
                  verbose=-1)  

### Suggest hyperparameters without training​ {#suggest-hyperparameters-without-training}

    from flaml.automl.data import load_openml_dataset  
    from flaml.default import LGBMRegressor  
    from flaml.ml import sklearn_metric_loss_score  
      
    X_train, X_test, y_train, y_test = load_openml_dataset(dataset_id=537, data_dir="./")  
    lgbm = LGBMRegressor()  
    hyperparams, estimator_name, X_transformed, y_transformed = lgbm.suggest_hyperparams(X_train, y_train)  
    print(hyperparams)  

#### Sample output​ {#sample-output} {#sample-output} {#sample-output}

    load dataset from ./openml_ds537.pkl  
    Dataset name: houses  
    X_train.shape: (15480, 8), y_train.shape: (15480,);  
    X_test.shape: (5160, 8), y_test.shape: (5160,)  
    {'n_estimators': 4797, 'num_leaves': 122, 'min_child_samples': 2, 'learning_rate': 0.022635758411078528, 'colsample_bytree': 0.7019911744574896, 'reg_alpha': 0.004252223402511765, 'reg_lambda': 0.11288241427227624, 'max_bin': 511, 'verbose': -1}  

[Link to
notebook](https://github.com/microsoft.blob/main/notebook/zeroshot_lightgbm.ipynb)
| [Open in
colab](https://colab.research.google.com/github/microsoft.blob/main/notebook/zeroshot_lightgbm.ipynb)

## Flamlized XGBClassifier​ {#flamlized-xgbclassifier}

### Prerequisites​ {#prerequisites} {#prerequisites}

This example requires xgboost, sklearn, openml==0.10.2.

### Zero-shot AutoML​ {#zeroshot-automl} {#zeroshot-automl}

    from flaml.automl.data import load_openml_dataset  
    from flaml.default import XGBClassifier  
    from flaml.automl.ml import sklearn_metric_loss_score  
      
    X_train, X_test, y_train, y_test = load_openml_dataset(dataset_id=1169, data_dir="./")  
    xgb = XGBClassifier()  
    xgb.fit(X_train, y_train)  
    y_pred = xgb.predict(X_test)  
    print("flamlized xgb accuracy", "=", 1 - sklearn_metric_loss_score("accuracy", y_pred, y_test))  
    print(xgb)  

#### Sample output​ {#sample-output} {#sample-output} {#sample-output}

    load dataset from ./openml_ds1169.pkl  
    Dataset name: airlines  
    X_train.shape: (404537, 7), y_train.shape: (404537,);  
    X_test.shape: (134846, 7), y_test.shape: (134846,)  
    flamlized xgb accuracy = 0.6729009388487608  
    XGBClassifier(base_score=0.5, booster='gbtree',  
                  colsample_bylevel=0.4601573737792679, colsample_bynode=1,  
                  colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',  
                  importance_type='gain', interaction_constraints='',  
                  learning_rate=0.04039771837785377, max_delta_step=0, max_depth=0,  
                  max_leaves=159, min_child_weight=0.3396294979905001, missing=nan,  
                  monotone_constraints='()', n_estimators=540, n_jobs=4,  
                  num_parallel_tree=1, random_state=0,  
                  reg_alpha=0.0012362430984376035, reg_lambda=3.093428791531145,  
                  scale_pos_weight=1, subsample=1.0, tree_method='hist',  
                  use_label_encoder=False, validate_parameters=1, verbosity=0)  

[Edit this
page](<https://github.com/microsoft.edit/main/website/docs/Examples/Default->
Flamlized.md)

[Previous« AutoML for XGBoost](.docs/Examples/AutoML-for-XGBoost)

[NextIntegrate - AzureML »](.docs/Examples/Integrate - AzureML)

* Flamlized LGBMRegressor
  * Prerequisites
  * Zero-shot AutoML
  * Suggest hyperparameters without training
* Flamlized XGBClassifier
  * Prerequisites
  * Zero-shot AutoML

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* [Getting Started](.docs/Getting-Started)
* [Installation](.docs/Installation)
* Use Cases
* Examples
* [Contributing](.docs/Contribute)
* [Research](.docs/Research)

On this page

# Installation {#installation}

## Python​ {#python}

FLAML requires **Python version >= 3.7**. It can be installed from pip:

    pip install flaml  

or conda:

    conda install flaml -c conda-forge  

### Optional Dependencies​ {#optional-dependencies}

#### [Autogen](.docs/Use-Cases/Autogen)​ {#autogenflamldocsusecasesautogen}

    pip install "flaml[autogen]"  

#### [Task-oriented AutoML](.docs/Use-Cases/Task-Oriented-AutoML)​ {#taskoriented-automlflamldocsusecasestaskorientedautoml}

    pip install "flaml[automl]"  

#### Extra learners/models​ {#extra-learnersmodels}

* openai models

    pip install "flaml[openai]"  

* catboost

    pip install "flaml[catboost]"  

* vowpal wabbit

    pip install "flaml[vw]"  

* time series forecaster: prophet, statsmodels

    pip install "flaml[forecast]"  

* huggingface transformers

    pip install "flaml[hf]"  

#### Notebook​ {#notebook}

To run the [notebook
examples](https://github.com/microsoft.tree/main/notebook), install
flaml with the [notebook] option:

    pip install "flaml[notebook]"  

#### Distributed tuning​ {#distributed-tuning}

* ray

    pip install "flaml[ray]"  

* spark

> _Spark support is added in v1.1.0_

     pip install "flaml[spark]>=1.1.0"  

For cloud platforms such as [Azure Synapse](<https://azure.microsoft.com/en->
us/products/synapse-analytics/), Spark clusters are provided. But you may also
need to install `Spark` manually when setting up your own environment. For
latest Ubuntu system, you can install Spark 3.3.0 standalone version with
below script. For more details of installing Spark, please refer to [Spark
Doc](https://spark.apache.org/docs/latest/api/python/getting_started/install.html).

    sudo apt-get update && sudo apt-get install -y --allow-downgrades --allow-change-held-packages --no-install-recommends \  
        ca-certificates-java ca-certificates openjdk-17-jdk-headless \  
        && sudo apt-get clean && sudo rm -rf /var/lib/apt/lists/*  
    wget --progress=dot:giga "https://www.apache.org/dyn/closer.lua/spark/spark-3.3.0/spark-3.3.0-bin-hadoop2.tgz?action=download" \  
        -O - | tar -xzC /tmp; archive=$(basename "spark-3.3.0/spark-3.3.0-bin-hadoop2.tgz") \  
        bash -c "sudo mv -v /tmp/\${archive/%.tgz/} /spark"  
    export SPARK_HOME=/spark  
    export PYTHONPATH=/spark/python/lib/py4j-0.10.9.5-src.zip:/spark/python  
    export PATH=$PATH:$SPARK_HOME/bin  

* nni

    pip install "flaml[nni]"  

* blendsearch

    pip install "flaml[blendsearch]"  

* synapse

> _To install flaml in Azure Synapse and similar cloud platform_

     pip install flaml[synapse]  

#### Test and Benchmark​ {#test-and-benchmark}

* test

    pip install flaml[test]  

* benchmark

    pip install flaml[benchmark]  

## .NET​ {#net}

FLAML has a .NET implementation in [ML.NET](http://dot.net/ml), an open-
source, cross-platform machine learning framework for .NET.

You can use FLAML in .NET in the following ways:

 **Low-code**

* [_Model Builder_](https://dotnet.microsoft.com/apps/machinelearning-ai/ml-dotnet/model-builder) \- A Visual Studio extension for training ML models using FLAML. For more information on how to install, see the [install Model Builder](https://docs.microsoft.com/dotnet/machine-learning/how-to-guides/install-model-builder?tabs=visual-studio-2022) guide.
* [_ML.NET CLI_](https://docs.microsoft.com/dotnet/machine-learning/automate-training-with-cli) \- A dotnet CLI tool for training machine learning models using FLAML on Windows, MacOS, and Linux. For more information on how to install the ML.NET CLI, see the [install the ML.NET CLI](https://docs.microsoft.com/dotnet/machine-learning/how-to-guides/install-ml-net-cli?tabs=windows) guide.

 **Code-first**

* [_Microsoft.ML.AutoML_](https://www.nuget.org/packages/Microsoft.ML.AutoML/0.20.0-preview.22313.1) \- NuGet package that provides direct access to the FLAML AutoML APIs that power low-code solutions like Model Builder and the ML.NET CLI. For more information on installing NuGet packages, see the install and use a NuGet package in [Visual Studio](https://docs.microsoft.com/nuget/quickstart/install-and-use-a-package-in-visual-studio) or [dotnet CLI](https://docs.microsoft.com/nuget/quickstart/install-and-use-a-package-using-the-dotnet-cli) guides.

To get started with the ML.NET API and AutoML, see the [csharp-
notebooks](https://github.com/dotnet/csharp-notebooks#machine-learning).

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/Installation.md)

[Previous« Getting Started](.docs/Getting-Started)

[NextAutoGen for Large Language Models »](.docs/Use-Cases/Autogen)

* Python
  * Optional Dependencies
* .NET

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
  * automl
  * default
    * [estimator](.docs/reference/default/estimator)
    * [greedy](.docs/reference/default/greedy)
    * [portfolio](.docs/reference/default/portfolio)
    * [suggest](.docs/reference/default/suggest)
  * onlineml
  * tune

On this page

# default.estimator {#defaultestimator}

#### flamlize_estimator​ {#flamlizeestimator}
```python
    def flamlize_estimator(super_class, name: str, task: str, alternatives=None)  

Enhance an estimator class with flaml's data-dependent default hyperparameter
settings.

 **Example** :

    import sklearn.ensemble as ensemble  
    RandomForestRegressor = flamlize_estimator(  
        ensemble.RandomForestRegressor, "rf", "regression"  
    )  

 **Arguments** :

* `super_class` \- an scikit-learn compatible estimator class.
* `name` \- a str of the estimator's name.
* `task` \- a str of the task type.
* `alternatives` \- (Optional) a list for alternative estimator names. For example, `[("max_depth", 0, "xgboost")]` means if the "max_depth" is set to 0 in the constructor, then look for the learned defaults for estimator "xgboost".

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/default/estimator.md)

[Previous« state](.docs/reference/automl/state)

[Nextgreedy »](.docs/reference/default/greedy)

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
  * automl
  * default
  * onlineml
    * [autovw](.docs/reference/onlineml/autovw)
    * [trial](.docs/reference/onlineml/trial)
    * [trial_runner](.docs/reference/onlineml/trial_runner)
  * tune

On this page

# onlineml.autovw {#onlinemlautovw}

## AutoVW Objects​ {#autovw-objects}

    class AutoVW()  

Class for the AutoVW algorithm.

#### __init__​ {#init}
```python
    def __init__(max_live_model_num: int, search_space: dict, init_config: Optional[dict] = {}, min_resource_lease: Optional[Union[str, float]] = "auto", automl_runner_args: Optional[dict] = {}, scheduler_args: Optional[dict] = {}, model_select_policy: Optional[str] = "threshold_loss_ucb", metric: Optional[str] = "mae_clipped", random_seed: Optional[int] = None, model_selection_mode: Optional[str] = "min", cb_coef: Optional[float] = None)  

Constructor.

 **Arguments** :

* `max_live_model_num` \- An int to specify the maximum number of 'live' models, which, in other words, is the maximum number of models allowed to update in each learning iteraction.
* `search_space` \- A dictionary of the search space. This search space includes both hyperparameters we want to tune and fixed hyperparameters. In the latter case, the value is a fixed value.
* `init_config` \- A dictionary of a partial or full initial config, e.g. {'interactions': set(), 'learning_rate': 0.5}
* `min_resource_lease` \- string or float | The minimum resource lease assigned to a particular model/trial. If set as 'auto', it will be calculated automatically.
* `automl_runner_args` \- A dictionary of configuration for the OnlineTrialRunner. If set {}, default values will be used, which is equivalent to using the following configs.

 **Example** :

    automl_runner_args = {  
        "champion_test_policy": 'loss_ucb', # the statistic test for a better champion  
        "remove_worse": False,              # whether to do worse than test  
    }  

* `scheduler_args` \- A dictionary of configuration for the scheduler. If set {}, default values will be used, which is equivalent to using the following config.

 **Example** :

    scheduler_args = {  
        "keep_challenger_metric": 'ucb',  # what metric to use when deciding the top performing challengers  
        "keep_challenger_ratio": 0.5,     # denotes the ratio of top performing challengers to keep live  
        "keep_champion": True,            # specifcies whether to keep the champion always running  
    }  

* `model_select_policy` \- A string in ['threshold_loss_ucb', 'threshold_loss_lcb', 'threshold_loss_avg', 'loss_ucb', 'loss_lcb', 'loss_avg'] to specify how to select one model to do prediction from the live model pool. Default value is 'threshold_loss_ucb'.
* `metric` \- A string in ['mae_clipped', 'mae', 'mse', 'absolute_clipped', 'absolute', 'squared'] to specify the name of the loss function used for calculating the progressive validation loss in ChaCha.
* `random_seed` \- An integer of the random seed used in the searcher (more specifically this the random seed for ConfigOracle).
* `model_selection_mode` \- A string in ['min', 'max'] to specify the objective as minimization or maximization.
* `cb_coef` \- A float coefficient (optional) used in the sample complexity bound.

#### predict​ {#predict}
```python
    def predict(data_sample)  

Predict on the input data sample.

 **Arguments** :

* `data_sample` \- one data example in vw format.

#### learn​ {#learn}
```python
    def learn(data_sample)  

Perform one online learning step with the given data sample.

 **Arguments** :

* `data_sample` \- one data example in vw format. It will be used to update the vw model.

#### get_ns_feature_dim_from_vw_example​ {#getnsfeaturedimfromvwexample}

    @staticmethod  
    def get_ns_feature_dim_from_vw_example(vw_example) -> dict  

Get a dictionary of feature dimensionality for each namespace singleton.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/onlineml/autovw.md)

[Previous« suggest](.docs/reference/default/suggest)

[Nexttrial »](.docs/reference/onlineml/trial)

* AutoVW Objects

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

Recent posts

* [Use flaml.autogen for Local LLMs](.blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](.blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](.blog/2023/05/18/GPT-adaptive-humaneval)
* [Surpassing 1 Million Downloads - A Retrospective and a Look into the Future](.blog/2023/05/07/1M-milestone)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](.blog/2023/04/21/LLM-tuning-math)

# 5 posts tagged with "LLM" {#5-posts-tagged-with-llm}

[View All Tags](.blog/tags)

## [Use flaml.autogen for Local LLMs](.blog/2023/07/14/Local-LLMs) {#use-flamlautogen-for-local-llmsflamlblog20230714localllms}

July 14, 2023 · 3 min read

[![Jiale
Liu](https://github.com/LeoLjl/leoljl.github.io/blob/main/profile.jpg?raw=true)](https://leoljl.github.io)

[Jiale Liu](https://leoljl.github.io)

Undergraduate student at Xidian University

 **TL;DR:** We demonstrate how to use flaml.autogen for local LLM application.
As an example, we will initiate an endpoint using
[FastChat](https://github.com/lm-sys/FastChat) and perform inference on
[ChatGLMv2-6b](https://github.com/THUDM/ChatGLM2-6B).

## Preparations​ {#preparations}

### Clone FastChat​ {#clone-fastchat}

FastChat provides OpenAI-compatible APIs for its supported models, so you can
use FastChat as a local drop-in replacement for OpenAI APIs. However, its code
needs minor modification in order to function properly.

    git clone https://github.com/lm-sys/FastChat.git  
    cd FastChat  

### Download checkpoint​ {#download-checkpoint}

ChatGLM-6B is an open bilingual language model based on General Language Model
(GLM) framework, with 6.2 billion parameters. ChatGLM2-6B is its second-
generation version.

Before downloading from HuggingFace Hub, you need to have Git LFS
[installed](<https://docs.github.com/en/repositories/working-with->
files/managing-large-files/installing-git-large-file-storage).

    git clone https://huggingface.co/THUDM/chatglm2-6b  

## Initiate server​ {#initiate-server}

First, launch the controller

    python -m fastchat.serve.controller  

Then, launch the model worker(s)

    python -m fastchat.serve.model_worker --model-path chatglm2-6b  

Finally, launch the RESTful API server

    python -m fastchat.serve.openai_api_server --host localhost --port 8000  

Normally this will work. However, if you encounter error like
[this](https://github.com/lm-sys/FastChat/issues/1641), commenting out all the
lines containing `finish_reason` in `fastchat/protocol/api_protocal.py` and
`fastchat/protocol/openai_api_protocol.py` will fix the problem. The modified
code looks like:

    class CompletionResponseChoice(BaseModel):  
        index: int  
        text: str  
        logprobs: Optional[int] = None  
        # finish_reason: Optional[Literal["stop", "length"]]  
      
    class CompletionResponseStreamChoice(BaseModel):  
        index: int  
        text: str  
        logprobs: Optional[float] = None  
        # finish_reason: Optional[Literal["stop", "length"]] = None  

## Interact with model using `oai.Completion`​ {#interact-with-model-using-oaicompletion}

Now the models can be directly accessed through openai-python library as well
as `flaml.oai.Completion` and `flaml.oai.ChatCompletion`.

    from flaml import oai  
      
    # create a text completion request  
    response = oai.Completion.create(  
        config_list=[  
            {  
                "model": "chatglm2-6b",  
                "api_base": "http://localhost:8000/v1",  
                "api_type": "open_ai",  
                "api_key": "NULL", # just a placeholder  
            }  
        ],  
        prompt="Hi",  
    )  
    print(response)  
      
    # create a chat completion request  
    response = oai.ChatCompletion.create(  
        config_list=[  
            {  
                "model": "chatglm2-6b",  
                "api_base": "http://localhost:8000/v1",  
                "api_type": "open_ai",  
                "api_key": "NULL",  
            }  
        ],  
        messages=[{"role": "user", "content": "Hi"}]  
    )  
    print(response)  

If you would like to switch to different models, download their checkpoints
and specify model path when launching model worker(s).

## interacting with multiple local LLMs​ {#interacting-with-multiple-local-llms}

If you would like to interact with multiple LLMs on your local machine,
replace the `model_worker` step above with a multi model variant:

    python -m fastchat.serve.multi_model_worker \  
        --model-path lmsys/vicuna-7b-v1.3 \  
        --model-names vicuna-7b-v1.3 \  
        --model-path chatglm2-6b \  
        --model-names chatglm2-6b  

The inference code would be:

    from flaml import oai  
      
    # create a chat completion request  
    response = oai.ChatCompletion.create(  
        config_list=[  
            {  
                "model": "chatglm2-6b",  
                "api_base": "http://localhost:8000/v1",  
                "api_type": "open_ai",  
                "api_key": "NULL",  
            },  
            {  
                "model": "vicuna-7b-v1.3",  
                "api_base": "http://localhost:8000/v1",  
                "api_type": "open_ai",  
                "api_key": "NULL",  
            }  
        ],  
        messages=[{"role": "user", "content": "Hi"}]  
    )  
    print(response)  

## For Further Reading​ {#for-further-reading} {#for-further-reading} {#for-further-reading} {#for-further-reading} {#for-further-reading}

* [Documentation](https://microsoft.github.io/autogen/) about `flaml.autogen`
* [Documentation](https://github.com/lm-sys/FastChat) about FastChat.

 **Tags:**

* [LLM](.blog/tags/llm)
* [FLAMLv2](.blog/tags/flam-lv-2)

[**Read More**](.blog/2023/07/14/Local-LLMs)

## [MathChat - An Conversational Framework to Solve Math {#mathchat-an-conversational-framework-to-solve-math}

Problems](.blog/2023/06/28/MathChat)

June 28, 2023 · 8 min read

[![Yiran
Wu](https://github.com/kevin666aa.png)](https://github.com/kevin666aa)

[Yiran Wu](https://github.com/kevin666aa)

PhD student at Pennsylvania State University

![MathChat
WorkFlow](.assets/images/mathchatflow-926a8ed1975a114ab76c69996942c23a.png)
**TL;DR:**

* **We introduce MathChat, a conversational framework leveraging Large Language Models (LLMs), specifically GPT-4, to solve advanced mathematical problems.**
* **MathChat improves LLM 's performance on challenging math problem-solving, outperforming basic prompting and other strategies by about 6%. The improvement was especially notable in the Algebra category, with a 15% increase in accuracy.**
* **Despite the advancement, GPT-4 still struggles to solve very challenging math problems, even with effective prompting strategies. Further improvements are needed, such as the development of more specific assistant models or the integration of new tools and prompts.**

Recent Large Language Models (LLMs) like GTP-3.5 and GPT-4 have demonstrated
astonishing abilities over previous models on various tasks, such as text
generation, question answering, and code generation. Moreover, these models
can communicate with humans through conversations and remember previous
contexts, making it easier for humans to interact with them. These models play
an increasingly important role in our daily lives assisting people with
different tasks, such as writing emails, summarizing documents, and writing
code.

In this blog post, we probe into the problem-solving capabilities of LLMs.
Specifically, we are interested in their capabilities to solve advanced math
problems, which could be representative of a broader class of problems that
require precise reasoning and also have deterministic solutions.

We introduce MathChat, a conversational framework designed for solving
challenging math problems with LLMs. This framework takes advantage of the
chat-optimized feature of state-of-the-art LLMs, where a user proxy agent and
an LLM assistant work together to tackle math problems. We also test previous
prompting techniques for comparison.

## The MathChat Framework​ {#the-mathchat-framework}

MathChat simulates a conversation between the LLM assistant and a user proxy
agent. As the name indicates, the user proxy agent acts as a proxy for the
user, which is responsible for communicating with the LLM assistant and
continuing the conversation in a desired manner.

The proxy agent first presents a math problem to the LLM assistant, framed by
an initial prompt that guides the assistant in how it should work
collaboratively to solve the problem. We incorporate existing prompting
methods such as CoT (chain-of-thought) and tool-using are incorporated in the
initial prompt. Key components of the prompt include:

* **Tool-using Prompt:** This guides the LLM assistant to use Python code in the correct format to address the problem, allowing the user proxy agent to parse the code and provide the appropriate results.

* **Problem-Solving Strategy Selection Prompt:** The assistant is instructed to choose one of three potential problem-solving strategies, including:

    1. Writing a Python program to solve the problem directly.
    2. Solving the problem directly without using Python, which exercises LLMs' inherent reasoning capabilities.
    3. Solving the problem step by step with Python. This should be used when the first two strategies aren't suitable.

* **Final Answer Encapsulation Prompt:** This part instructs the assistant to put the final answer in _\boxed_ {}.

The prompt design in MathChat is flexible and can easily be refined to include
other tools, such as Wolfram Alpha.

Let's take a look at an example between the **User Proxy Agent** and the **LLM
Assistant** (GPT-4). The conversation focuses on how to solve inequality using
Python. (The conversation is modified for readability.)

Participant| Dialogue  
---|---  
User Proxy Agent| (Initial Prompt: Let's solve a math problem with Python...)
Problem: Find all x that satisfy the inequality (2x+10)(x+3) < (3x+9)(x+8).
Express your answer in interval notation.  
LLM assistant| The key idea to solve this problem is to find the critical
points, which are the points where the inequality is equal. Then, test the
intervals between the critical points to find the interval for which the
inequality holds.  
LLM assistant (continued)| Here's Python code to find the critical points:
`from sympy import symbols, Eq, solve; x = symbols("x"); lhs = (2*x + 10)*(x +
3); rhs = (3*x + 9)*(x + 8); equation = Eq(lhs, rhs); critical_points =
solve(equation, x); critical_points`  
User Proxy Agent| The critical points are: [-14, -3]  
LLM assistant| We found the critical points to be x = -14 and x = -3. Now
we'll test the intervals between these critical points as well as points
before and after the critical points. The intervals are: x < -14, -14 < x <
-3, x > -3.  
LLM assistant (continued)| Here's Python code to test the inequality for each
interval: `from sympy import symbols; x = symbols("x"); inequality = (2*x +
10)*(x + 3) < (3*x + 9)*(x + 8); test_points = [-15, -5, 0]; intervals = [];
for point in test_points: if inequality.subs(x, point):
intervals.append(point); intervals`  
User Proxy Agent| The intervals where the inequality holds true are for test
points: [-15, 0]  
LLM assistant| We have tested the intervals and found that the inequality
holds for: 1. x < -14 (test point: -15) 2. x > -3 (test point: 0). Therefore,
the solution to the inequality (2x+10)(x+3) < (3x+9)(x+8) is x ∈ (-∞, -14) ∪
(-3, ∞)  
  
## Experiment Setup​ {#experiment-setup} {#experiment-setup}

We evaluate the improvement brought by MathChat.

For the experiment, we focus on the level-5 problems from the MATH dataset,
which are composed of high school competition problems. These problems include
the application of theorems and complex equation derivation and are
challenging even for undergraduate students. We evaluate 6 of 7 categories
from the dataset (excluding Geometry): Prealgebra, Algebra, Number Theory,
Counting and Probability, Intermediate Algebra, and Precalculus.

We evaluate GPT-4 and use the default configuration of the OpenAI API. To
access the final performance, we manually compare the final answer with the
correct answer. For the vanilla prompt, Program Synthesis, and MathChat, we
have GPT-4 enclose the final answer in _\boxed_ {}, and we take the return of
the function in PoT as the final answer.

We also evaluate the following methods for comparison:

  1. **Vanilla prompting:** Evaluates GPT-4's direct problem-solving capability. The prompt used is: _" Solve the problem carefully. Put the final answer in \boxed{}"_.

  2. **Program of Thoughts (PoT):** Uses a zero-shot PoT prompt that requests the model to create a _Solver_ function to solve the problem and return the final answer.

  3. **Program Synthesis (PS) prompting:** Like PoT, it prompts the model to write a program to solve the problem. The prompt used is: _" Write a program that answers the following question: {Problem}"_.

## Experiment Results​ {#experiment-results} {#experiment-results} {#experiment-results}

The accuracy on all the problems with difficulty level-5 from different
categories of the MATH dataset with different methods is shown below:

![Result](.assets/images/result-514251f4c387d5b6e11656e1fa33de08.png)

We found that compared to basic prompting, which demonstrates the innate
capabilities of GPT-4, utilizing Python within the context of PoT or PS
strategy improved the overall accuracy by about 10%. This increase was mostly
seen in categories involving more numerical manipulations, such as Counting &
Probability and Number Theory, and in more complex categories like
Intermediate Algebra and Precalculus.

For categories like Algebra and Prealgebra, PoT and PS showed little
improvement, and in some instances, even led to a decrease in accuracy.
However, MathChat was able to enhance total accuracy by around 6% compared to
PoT and PS, showing competitive performance across all categories. Remarkably,
MathChat improved accuracy in the Algebra category by about 15% over other
methods. Note that categories like Intermediate Algebra and Precalculus
remained challenging for all methods, with only about 20% of problems solved
accurately.

The code for experiments can be found at this
[repository](https://github.com/kevin666aa.tree/gpt_math_solver/flaml/autogen/math).
We now provide an implementation of MathChat using the interactive agents in
FLAML. See this
[notebook](https://github.com/microsoft.blob/main/notebook/autogen_agentchat_MathChat.ipynb)
for example usage.

## Future Directions​ {#future-directions}

Despite MathChat's improvements over previous methods, the results show that
complex math problem is still challenging for recent powerful LLMs, like
GPT-4, even with help from external tools.

Further work can be done to enhance this framework or math problem-solving in
general:

* Although enabling the model to use tools like Python can reduce calculation errors, LLMs are still prone to logic errors. Methods like self-consistency (Sample several solutions and take a major vote on the final answer), or self-verification (use another LLM instance to check whether an answer is correct) might improve the performance.
* Sometimes, whether the LLM can solve the problem depends on the plan it uses. Some plans require less computation and logical reasoning, leaving less room for mistakes.
* MathChat has the potential to be adapted into a copilot system, which could assist users with math problems. This system could allow users to be more involved in the problem-solving process, potentially enhancing learning.

## For Further Reading​ {#for-further-reading} {#for-further-reading} {#for-further-reading} {#for-further-reading} {#for-further-reading}

* [Research paper of MathChat](https://arxiv.org/abs/2306.01337)
* [Documentation about `flaml.autogen`](https://microsoft.github.io/autogen/)

 _Are you working on applications that involve math problem-solving? Would you
appreciate additional research or support on the application of LLM-based
agents for math problem-solving? Please join
our[Discord](https://discord.gg/Cppx2vSPVP) server for discussion._

 **Tags:**

* [LLM](.blog/tags/llm)
* [GPT](.blog/tags/gpt)
* [research](.blog/tags/research)

[**Read More**](.blog/2023/06/28/MathChat)

## [Achieve More, Pay Less - Use GPT-4 Smartly](.blog/2023/05/18/GPT- {#achieve-more-pay-less-use-gpt4-smartlyflamlblog20230518gpt}

adaptive-humaneval)

May 18, 2023 · 8 min read

[![Chi Wang](https://github.com/sonichi.png)](<https://www.linkedin.com/in/chi->
wang-49b15b16/)

[Chi Wang](https://www.linkedin.com/in/chi-wang-49b15b16/)

Principal Researcher at Microsoft Research

![An adaptive way of using GPT-3.5 and GPT-4 outperforms GPT-4 in both coding
success rate and inference
cost](.assets/images/humaneval-b2718b0b15d76b8fc59a4094e1bd684c.png)

 **TL;DR:**

* **A case study using the HumanEval benchmark shows that an adaptive way of using multiple GPT models can achieve both much higher accuracy (from 68% to 90%) and lower inference cost (by 18%) than using GPT-4 for coding.**

GPT-4 is a big upgrade of foundation model capability, e.g., in code and math,
accompanied by a much higher (more than 10x) price per token to use over
GPT-3.5-Turbo. On a code completion benchmark,
[HumanEval](https://huggingface.co/datasets/openai_humaneval), developed by
OpenAI, GPT-4 can successfully solve 68% tasks while GPT-3.5-Turbo does 46%.
It is possible to increase the success rate of GPT-4 further by generating
multiple responses or making multiple calls. However, that will further
increase the cost, which is already nearly 20 times of using GPT-3.5-Turbo and
with more restricted API call rate limit. Can we achieve more with less?

In this blog post, we will explore a creative, adaptive way of using GPT
models which leads to a big leap forward.

## Observations​ {#observations}

* GPT-3.5-Turbo can alrady solve 40%-50% tasks. For these tasks if we never use GPT-4, we can save nearly 40-50% cost.
* If we use the saved cost to generate more responses with GPT-4 for the remaining unsolved tasks, it is possible to solve some more of them while keeping the amortized cost down.

The obstacle of leveraging these observations is that we do not know _a
priori_ which tasks can be solved by the cheaper model, which tasks can be
solved by the expensive model, and which tasks can be solved by paying even
more to the expensive model.

To overcome that obstacle, one may want to predict which task requires what
model to solve and how many responses are required for each task. Let's look
at one example code completion task:
```python
    def vowels_count(s):  
        """Write a function vowels_count which takes a string representing  
        a word as input and returns the number of vowels in the string.  
        Vowels in this case are 'a', 'e', 'i', 'o', 'u'. Here, 'y' is also a  
        vowel, but only when it is at the end of the given word.  
      
        Example:  
        >>> vowels_count("abcde")  
        2  
        >>> vowels_count("ACEDY")  
        3  
        """  

Can we predict whether GPT-3.5-Turbo can solve this task or do we need to use
GPT-4? My first guess is that GPT-3.5-Turbo can get it right because the
instruction is fairly straightforward. Yet, it turns out that GPT-3.5-Turbo
does not consistently get it right, if we only give it one chance. It's not
obvious (but an interesting research question!) how to predict the performance
without actually trying.

What else can we do? We notice that: **It 's "easier" to verify a given
solution than finding a correct solution from scratch.**

Some simple example test cases are provided in the docstr. If we already have
a response generated by a model, we can use those test cases to filter wrong
implementations, and either use a more powerful model or generate more
responses, until the result passes the example test cases. Moreover, this step
can be automated by asking GPT-3.5-Turbo to generate assertion statements from
the examples given in the docstr (a simpler task where we can place our bet)
and executing the code.

## Solution​ {#solution}

Combining these observations, we can design a solution with two intuitive
ideas:

* Make use of auto-generated feedback, i.e., code execution results, to filter responses.
* Try inference configurations one by one, until one response can pass the filter.

![Design](.assets/images/design-88630f6d8d95f1e31ef5ce79e81aab54.png)

This solution works adaptively without knowing or predicting which task fits
which configuration. It simply tries multiple configurations one by one,
starting from the cheapest configuration. Note that one configuration can
generate multiple responses (by setting the inference parameter n larger than
1). And different configurations can use the same model and different
inference parameters such as n and temperature. Only one response is returned
and evaluated per task.

An implementation of this solution is provided in
[flaml.autogen](.docs/reference/autogen/code_utils#implement). It uses
the following sequence of configurations:

  1. GPT-3.5-Turbo, n=1, temperature=0
  2. GPT-3.5-Turbo, n=7, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]
  3. GPT-4, n=1, temperature=0
  4. GPT-4, n=2, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]
  5. GPT-4, n=1, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]

## Experiment Results​ {#experiment-results} {#experiment-results} {#experiment-results}

The first figure in this blog post shows the success rate and average
inference cost of the adaptive solution compared with default GPT-4. The
inference cost includes the cost for generating the assertions in our
solution. The generated assertions are not always correct, and programs that
pass/fail the generated assertions are not always right/wrong. Despite of
that, the adaptive solution can increase the success rate (referred to as
pass@1 in the literature) from 68% to 90%, while reducing the cost by 18%.

Here are a few examples of function definitions which are solved by different
configurations in the portfolio.

  1. Solved by GPT-3.5-Turbo, n=1, temperature=0
```python
    def compare(game,guess):  
        """I think we all remember that feeling when the result of some long-awaited  
        event is finally known. The feelings and thoughts you have at that moment are  
        definitely worth noting down and comparing.  
        Your task is to determine if a person correctly guessed the results of a number of matches.  
        You are given two arrays of scores and guesses of equal length, where each index shows a match.  
        Return an array of the same length denoting how far off each guess was. If they have guessed correctly,  
        the value is 0, and if not, the value is the absolute difference between the guess and the score.  
      
      
        example:  
      
        compare([1,2,3,4,5,1],[1,2,3,4,2,-2]) -> [0,0,0,0,3,3]  
        compare([0,5,0,0,0,4],[4,1,1,0,0,-2]) -> [4,4,1,0,0,6]  
        """  

  2. Solved by GPT-3.5-Turbo, n=7, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]: the `vowels_count` function presented earlier.
  3. Solved by GPT-4, n=1, temperature=0:
```python
    def string_xor(a: str, b: str) -> str:  
        """ Input are two strings a and b consisting only of 1s and 0s.  
        Perform binary XOR on these inputs and return result also as a string.  
        >>> string_xor('010', '110')  
        '100'  
        """  

  4. Solved by GPT-4, n=2, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]:
```python
    def is_palindrome(string: str) -> bool:  
        """ Test if given string is a palindrome """  
        return string == string[::-1]  
      
      
    def make_palindrome(string: str) -> str:  
        """ Find the shortest palindrome that begins with a supplied string.  
        Algorithm idea is simple:  
        - Find the longest postfix of supplied string that is a palindrome.  
        - Append to the end of the string reverse of a string prefix that comes before the palindromic suffix.  
        >>> make_palindrome('')  
        ''  
        >>> make_palindrome('cat')  
        'catac'  
        >>> make_palindrome('cata')  
        'catac'  
        """  

  5. Solved by GPT-4, n=1, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]:
```python
    def sort_array(arr):  
        """  
        In this Kata, you have to sort an array of non-negative integers according to  
        number of ones in their binary representation in ascending order.  
        For similar number of ones, sort based on decimal value.  
      
        It must be implemented like this:  
        >>> sort_array([1, 5, 2, 3, 4]) == [1, 2, 3, 4, 5]  
        >>> sort_array([-2, -3, -4, -5, -6]) == [-6, -5, -4, -3, -2]  
        >>> sort_array([1, 0, 2, 3, 4]) [0, 1, 2, 3, 4]  
        """  

The last problem is an example with wrong example test cases in the original
definition. It misleads the adaptive solution because a correct implementation
is regarded as wrong and more trials are made. The last configuration in the
sequence returns the right implementation, even though it does not pass the
auto-generated assertions. This example demonstrates that:

* Our adaptive solution has a certain degree of fault tolerance.
* The success rate and inference cost for the adaptive solution can be further improved if correct example test cases are used.

It is worth noting that the reduced inference cost is the amortized cost over
all the tasks. For each individual task, the cost can be either larger or
smaller than directly using GPT-4. This is the nature of the adaptive
solution: The cost is in general larger for difficult tasks than that for easy
tasks.

An example notebook to run this experiment can be found at:
<https://github.com/microsoft.blob/v1.2.1/notebook/research/autogen_code.ipynb>

## Discussion​ {#discussion}

Our solution is quite simple to implement using a generic interface offered in
[`flaml.autogen`](<https://microsoft.github.io/autogen/docs/Use->
Cases/enhanced_inference#logic-error), yet the result is quite encouraging.

While the specific way of generating assertions is application-specific, the
main ideas are general in LLM operations:

* Generate multiple responses to select - especially useful when selecting a good response is relatively easier than generating a good response at one shot.
* Consider multiple configurations to generate responses - especially useful when:
  * Model and other inference parameter choice affect the utility-cost tradeoff; or
  * Different configurations have complementary effect.

A [previous blog post](.blog/2023/04/21/LLM-tuning-math) provides
evidence that these ideas are relevant in solving math problems too.
`flaml.autogen` uses a technique
[EcoOptiGen](https://arxiv.org/abs/2303.04673) to support inference parameter
tuning and model selection.

There are many directions of extensions in research and development:

* Generalize the way to provide feedback.
* Automate the process of optimizing the configurations.
* Build adaptive agents for different applications.

 _Do you find this approach applicable to your use case? Do you have any other
challenge to share about LLM applications? Do you like to see more support or
research of LLM optimization or automation? Please join
our[Discord](https://discord.gg/Cppx2vSPVP) server for discussion._

## For Further Reading​ {#for-further-reading} {#for-further-reading} {#for-further-reading} {#for-further-reading} {#for-further-reading}

* [Documentation](https://microsoft.github.io/autogen/) about `flaml.autogen` and [Research paper](https://arxiv.org/abs/2303.04673).
* [Blog post](.blog/2023/04/21/LLM-tuning-math) about a related study for math.

 **Tags:**

* [LLM](.blog/tags/llm)
* [GPT](.blog/tags/gpt)
* [research](.blog/tags/research)

[**Read More**](.blog/2023/05/18/GPT-adaptive-humaneval)

## [Surpassing 1 Million Downloads - A Retrospective and a Look into the {#surpassing-1-million-downloads-a-retrospective-and-a-look-into-the}

Future](.blog/2023/05/07/1M-milestone)

May 7, 2023 · 4 min read

[![Qingyun Wu](https://github.com/qingyun-wu.png)](<https://qingyun->
wu.github.io/)

[Qingyun Wu](https://qingyun-wu.github.io/)

Assistant Professor at the Pennsylvania State University

 **TL;DR:**

* **Celebrating FLAML 's milestone: 1 million downloads**
* **Introducing Large Language Model (LLM) support in the upcoming FLAML v2**

This week, FLAML has reached a significant milestone: 1 million downloads.
Originating as an intern research project within Microsoft Research, FLAML has
grown into an open-source library used widely across the industry and
supported by an active community. As we celebrate this milestone, we want to
recognize the passionate contributors and users who have played an essential
role in molding FLAML into the flourishing project it is today. Our heartfelt
gratitude goes out to each of you for your unwavering support, constructive
feedback, and innovative contributions that have driven FLAML to new heights.
A big shoutout to our industrial collaborators from Azure Core, Azure Machine
Learning, Azure Synapse Analytics, Microsoft 365, ML.NET, Vowpal Wabbit,
Anyscale, Databricks, and Wise; and academic collaborators from MIT, Penn
State University, Stevens Institute of Technology, Tel Aviv University, Texas
A & M University, University of Manchester, University of Washington, and The
Chinese University of Hong Kong etc.

We'd also like to take the opportunity to reflect on FLAML's past achievements
and its future roadmap, with a particular focus on large language models (LLM)
and LLMOps.

## FLAML's Journey: Past Achievements and Milestones​ {#flamls-journey-past-achievements-and-milestones}

### Bring AutoML to One's Fingertips​ {#bring-automl-to-ones-fingertips}

FLAML offers an off-the-shelf AutoML solution that enables users to quickly
discover high-quality models or configurations for common ML/AI tasks. By
automatically selecting models and hyperparameters for training or inference,
FLAML saves users time and effort. FLAML has significantly reduced development
time for developers and data scientists alike, while also providing a
convenient way to integrate new algorithms into the pipeline, enabling easy
extensions and large-scale parallel tuning. These features make FLAML a
valuable tool in R&D efforts for many enterprise users. FLAML is capable of
handling a variety of common ML tasks, such as
[classification](<https://microsoft.github.io.docs/Examples/AutoML->
Classification),
[regression](<https://microsoft.github.io.docs/Examples/AutoML->
Regression), [time series
forecasting](<https://microsoft.github.io.docs/Examples/AutoML->
Time%20series%20forecast), [NLP
tasks](https://microsoft.github.io.docs/Examples/AutoML-Rank), and
[generative tasks](https://microsoft.github.io/autogen/), providing a
comprehensive solution for various applications.

### Speed and Efficiency: The FLAML Advantage​ {#speed-and-efficiency-the-flaml-advantage}

What sets FLAML apart from other AutoML libraries is its exceptional
efficiency, thanks to the economical and efficient hyperparameter optimization
and model selection methods developed in our
[research](https://microsoft.github.io.docs/Research). FLAML is also
capable of handling large search spaces with heterogeneous evaluation costs,
complex constraints, guidance, and early stopping. The [zero-shot
AutoML](https://microsoft.github.io.docs/Use-Cases/Zero-Shot-AutoML)
option further reduces the cost of AutoML, making FLAML an even more
attractive solution for a wide range of applications with low resources.

### Easy Customization and Extensibility​ {#easy-customization-and-extensibility}

FLAML is designed for easy extensibility and customization, allowing users to
add custom learners, metrics, search space, etc. For example, the support of
hierarchical search spaces allows one to first choose an ML learner and then
sampling from the hyperparameter space specific to that learner. The level of
customization ranges from minimal (providing only training data and task type
as input) to full (tuning a user-defined function). This flexibility and
support for easy customization have led to FLAML's adoption in various
domains, including security, finance, marketing, engineering, supply chain,
insurance, and healthcare, delivering highly accurate results.

## Embracing Large Language Models in FLAML v2​ {#embracing-large-language-models-in-flaml-v2}

As large language models continue to reshape the AI ecosystem, FLAML is poised
to adapt and grow alongside these advancements. Recognizing the importance of
large language models, we have recently incorporated an autogen package into
FLAML, and are committed to focusing our collective efforts on addressing the
unique challenges that arise in LLMOps (Large Language Model Operations).

In its current iteration, FLAML offers support for model selection and
inference parameter tuning for large language models. We are actively working
on the development of new features, such as low-level inference API with
caching, templating, filtering, and higher-level components like LLM-based
coding and interactive agents, to enable more effective and economical usage
of LLM.

We are eagerly preparing for the launch of FLAML v2, where we will place
special emphasis on incorporating and enhancing features specifically tailored
for large language models (LLMs), further expanding FLAML's capabilities. We
invite contributions from anyone interested in this topic and look forward to
collaborating with the community as we shape the future of FLAML and LLMOps
together.

## For Further Reading​ {#for-further-reading} {#for-further-reading} {#for-further-reading} {#for-further-reading} {#for-further-reading}

* [Documentation about `flaml.autogen`](https://microsoft.github.io/autogen/)
* [Code Example: Tune chatGPT for Math Problem Solving with FLAML](https://github.com/microsoft.blob/main/notebook/autogen_chatgpt_gpt4.ipynb)

 _Do you have any experience to share about LLM applications? Do you like to
see more support or research of LLMOps? Please join
our[Discord](https://discord.gg/Cppx2vSPVP) server for discussion._

 **Tags:**

* [LLM](.blog/tags/llm)
* [LLMOps](.blog/tags/llm-ops)
* [FLAMLv2](.blog/tags/flam-lv-2)

[**Read More**](.blog/2023/05/07/1M-milestone)

## [Does Model and Inference Parameter Matter in LLM Applications? - A Case {#does-model-and-inference-parameter-matter-in-llm-applications-a-case}

Study for MATH](.blog/2023/04/21/LLM-tuning-math)

April 21, 2023 · 5 min read

[![Chi Wang](https://github.com/sonichi.png)](<https://www.linkedin.com/in/chi->
wang-49b15b16/)

[Chi Wang](https://www.linkedin.com/in/chi-wang-49b15b16/)

Principal Researcher at Microsoft Research

![level 2
algebra](.assets/images/level2algebra-659ba95286432d9945fc89e84d606797.png)

 **TL;DR:**

* **Just by tuning the inference parameters like model, number of responses, temperature etc. without changing any model weights or prompt, the baseline accuracy of untuned gpt-4 can be improved by 20% in high school math competition problems.**
* **For easy problems, the tuned gpt-3.5-turbo model vastly outperformed untuned gpt-4 in accuracy (e.g., 90% vs. 70%) and cost efficiency. For hard problems, the tuned gpt-4 is much more accurate (e.g., 35% vs. 20%) and less expensive than untuned gpt-4.**
* **FLAML can help with model selection, parameter tuning, and cost-saving in LLM applications.**

Large language models (LLMs) are powerful tools that can generate natural
language texts for various applications, such as chatbots, summarization,
translation, and more. GPT-4 is currently the state of the art LLM in the
world. Is model selection irrelevant? What about inference parameters?

In this blog post, we will explore how model and inference parameter matter in
LLM applications, using a case study for [MATH](<https://datasets-benchmarks->
proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-
round2.html), a benchmark for evaluating LLMs on advanced mathematical problem
solving. MATH consists of 12K math competition problems from AMC-10, AMC-12
and AIME. Each problem is accompanied by a step-by-step solution.

We will use the new subpackage [`flaml.autogen`](.blog/tags/docs/Use-
Cases/Autogen) to automatically find the best model and inference parameter
for LLMs on a given task and dataset given an inference budget, using a novel
low-cost search & pruning strategy. FLAML currently supports all the LLMs from
OpenAI, such as GPT-3.5 and GPT-4.

We will use FLAML to perform model selection and inference parameter tuning.
Then we compare the performance and inference cost on solving algebra problems
with the untuned gpt-4. We will also analyze how different difficulty levels
affect the results.

## Experiment Setup​ {#experiment-setup} {#experiment-setup}

We use FLAML to select between the following models with a target inference
budget $0.02 per instance:

* gpt-3.5-turbo, a relatively cheap model that powers the popular ChatGPT app
* gpt-4, the state of the art LLM that costs more than 10 times of gpt-3.5-turbo

We adapt the models using 20 examples in the train set, using the problem
statement as the input and generating the solution as the output. We use the
following inference parameters:

* temperature: The parameter that controls the randomness of the output text. A higher temperature means more diversity but less coherence. We search for the optimal temperature in the range of [0, 1].
* top_p: The parameter that controls the probability mass of the output tokens. Only tokens with a cumulative probability less than or equal to top-p are considered. A lower top-p means more diversity but less coherence. We search for the optimal top-p in the range of [0, 1].
* max_tokens: The maximum number of tokens that can be generated for each output. We search for the optimal max length in the range of [50, 1000].
* n: The number of responses to generate. We search for the optimal n in the range of [1, 100].
* prompt: We use the template: "{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \boxed{{}}." where {problem} will be replaced by the math problem instance.

In this experiment, when n > 1, we find the answer with highest votes among
all the responses and then select it as the final answer to compare with the
ground truth. For example, if n = 5 and 3 of the responses contain a final
answer 301 while 2 of the responses contain a final answer 159, we choose 301
as the final answer. This can help with resolving potential errors due to
randomness. We use the average accuracy and average inference cost as the
metric to evaluate the performance over a dataset. The inference cost of a
particular instance is measured by the price per 1K tokens and the number of
tokens consumed.

## Experiment Results​ {#experiment-results} {#experiment-results} {#experiment-results}

The first figure in this blog post shows the average accuracy and average
inference cost of each configuration on the level 2 Algebra test set.

Surprisingly, the tuned gpt-3.5-turbo model is selected as a better model and
it vastly outperforms untuned gpt-4 in accuracy (92% vs. 70%) with equal or
2.5 times higher inference budget. The same observation can be obtained on the
level 3 Algebra test set.

![level 3
algebra](.assets/images/level3algebra-94e87a683ac8832ac7ae6f41f30131a4.png)

However, the selected model changes on level 4 Algebra.

![level 4
algebra](.assets/images/level4algebra-492beb22490df30d6cc258f061912dcd.png)

This time gpt-4 is selected as the best model. The tuned gpt-4 achieves much
higher accuracy (56% vs. 44%) and lower cost than the untuned gpt-4. On level
5 the result is similar.

![level 5
algebra](.assets/images/level5algebra-8fba701551334296d08580b4b489fe56.png)

We can see that FLAML has found different optimal model and inference
parameters for each subset of a particular level, which shows that these
parameters matter in cost-sensitive LLM applications and need to be carefully
tuned or adapted.

An example notebook to run these experiments can be found at:
<https://github.com/microsoft.blob/v1.2.1/notebook/autogen_chatgpt.ipynb>

## Analysis and Discussion​ {#analysis-and-discussion}

While gpt-3.5-turbo demonstrates competitive accuracy with voted answers in
relatively easy algebra problems under the same inference budget, gpt-4 is a
better choice for the most difficult problems. In general, through parameter
tuning and model selection, we can identify the opportunity to save the
expensive model for more challenging tasks, and improve the overall
effectiveness of a budget-constrained system.

There are many other alternative ways of solving math problems, which we have
not covered in this blog post. When there are choices beyond the inference
parameters, they can be generally tuned via
[`flaml.tune`](.blog/tags/docs/Use-Cases/Tune-User-Defined-Function).

The need for model selection, parameter tuning and cost saving is not specific
to the math problems. The [Auto-GPT](<https://github.com/Significant->
Gravitas/Auto-GPT) project is an example where high cost can easily prevent a
generic complex task to be accomplished as it needs many LLM inference calls.

## For Further Reading​ {#for-further-reading} {#for-further-reading} {#for-further-reading} {#for-further-reading} {#for-further-reading}

* [Research paper about the tuning technique](https://arxiv.org/abs/2303.04673)
* [Documentation about `flaml.autogen`](https://microsoft.github.io/autogen/)

 _Do you have any experience to share about LLM applications? Do you like to
see more support or research of LLM optimization or automation? Please join
our[Discord](https://discord.gg/Cppx2vSPVP) server for discussion._

 **Tags:**

* [LLM](.blog/tags/llm)
* [GPT](.blog/tags/gpt)
* [research](.blog/tags/research)

[**Read More**](.blog/2023/04/21/LLM-tuning-math)

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

Recent posts

* [All About Agent Descriptions](/autogen/blog/2023/12/29/AgentDescriptions)
* [AgentOptimizer - An Agentic Way to Train Your LLM Agent](/autogen/blog/2023/12/23/AgentOptimizer)
* [AutoGen Studio: Interactively Explore Multi-Agent Workflows](/autogen/blog/2023/12/01/AutoGenStudio)
* [Agent AutoBuild - Automatically Building Multi-agent Systems](/autogen/blog/2023/11/26/Agent-AutoBuild)
* [How to Assess Utility of LLM-powered Applications?](/autogen/blog/2023/11/20/AgentEval)
* [AutoGen Meets GPTs](/autogen/blog/2023/11/13/OAI-assistants)
* [EcoAssistant - Using LLM Assistants More Accurately and Affordably](/autogen/blog/2023/11/09/EcoAssistant)
* [Multimodal with GPT-4V and LLaVA](/autogen/blog/2023/11/06/LMM-Agent)
* [AutoGen's Teachable Agents](/autogen/blog/2023/10/26/TeachableAgent)
* [Retrieval-Augmented Generation (RAG) Applications with AutoGen](/autogen/blog/2023/10/18/RetrieveChat)
* [Use AutoGen for Local LLMs](/autogen/blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](/autogen/blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](/autogen/blog/2023/05/18/GPT-adaptive-humaneval)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](/autogen/blog/2023/04/21/LLM-tuning-math)

# One post tagged with "UX" {#one-post-tagged-with-ux}

[View All Tags](/autogen/blog/tags)

## [AutoGen Studio: Interactively Explore Multi-Agent {#autogen-studio-interactively-explore-multiagent}

Workflows](/autogen/blog/2023/12/01/AutoGenStudio)

December 1, 2023 · 8 min read

[![Victor
Dibia](https://github.com/victordibia.png)](https://github.com/victordibia)

[Victor Dibia](https://github.com/victordibia)

Principal RSDE at Microsoft Research

[![Gagan
Bansal](https://github.com/gagb.png)](<https://www.linkedin.com/in/gagan->
bansal/)

[Gagan Bansal](https://www.linkedin.com/in/gagan-bansal/)

Senior Researcher at Microsoft Research

[![Saleema
Amershi](https://github.com/samershi.png)](https://github.com/samershi)

[Saleema Amershi](https://github.com/samershi)

Senior Principal Research Manager at Microsoft Research

![AutoGen Studio Playground View: Solving a task with multiple agents that
generate a pdf document with
images.](/autogen/assets/images/autogenstudio_home-
cce78dc150d1bb0073620754df73d863.png)

 _AutoGen Studio: Solving a task with multiple agents that generate a pdf
document with images._

## TLDR​ {#tldr}

To help you rapidly prototype multi-agent solutions for your tasks, we are
introducing AutoGen Studio, an interface powered by
[AutoGen](https://github.com/microsoft/autogen/tree/main/autogen). It allows
you to:

* Declaratively define and modify agents and multi-agent workflows through a point and click, drag and drop interface (e.g., you can select the parameters of two agents that will communicate to solve your task).
* Use our UI to create chat sessions with the specified agents and view results (e.g., view chat history, generated files, and time taken).
* Explicitly add skills to your agents and accomplish more tasks.
* Publish your sessions to a local gallery.

AutoGen Studio is open source [code
here](<https://github.com/microsoft/autogen/tree/main/samples/apps/autogen->
studio), and can be installed via pip. Give it a try!

    pip install autogenstudio  

## Introduction​ {#introduction}

The accelerating pace of technology has ushered us into an era where digital
assistants (or agents) are becoming integral to our lives.
[AutoGen](https://github.com/microsoft/autogen/tree/main/autogen) has emerged
as a leading framework for orchestrating the power of agents. In the spirit of
expanding this frontier and democratizing this capability, we are thrilled to
introduce a new user-friendly interface: **AutoGen Studio**.

With AutoGen Studio, users can rapidly create, manage, and interact with
agents that can learn, adapt, and collaborate. As we release this interface
into the open-source community, our ambition is not only to enhance
productivity but to inspire a level of personalized interaction between humans
and agents.

> **Note** : AutoGen Studio is meant to help you rapidly prototype multi-
> agent workflows and demonstrate an example of end user interfaces built with
> AutoGen. It is not meant to be a production-ready app.

## Getting Started with AutoGen Studio​ {#getting-started-with-autogen-studio}

The following guide will help you get AutoGen Studio up and running on your
system.

### Configuring an LLM Provider​ {#configuring-an-llm-provider}

To get started, you need access to a language model. You can get this set up
by following the steps in the AutoGen documentation
[here](https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints).
Configure your environment with either `OPENAI_API_KEY` or
`AZURE_OPENAI_API_KEY`.

For example, in your terminal, you would set the API key like this:

    export OPENAI_API_KEY=<your_api_key>  

You can also specify the model directly in the agent's configuration as shown
below.

    llm_config = LLMConfig(  
        config_list=[{  
            "model": "gpt-4",  
            "api_key": "<azure_api_key>",  
            "base_url": "<azure api base url>",  
            "api_type": "azure",  
            "api_version": "2023-06-01-preview"  
        }],  
        temperature=0,  
    )  

### Installation​ {#installation}

  1. **Install from PyPi**

We recommend using a virtual environment (e.g., conda) to avoid conflicts with
existing Python packages. With Python 3.10 or newer active in your virtual
environment, use pip to install AutoGen Studio:

        pip install autogenstudio  

  2. **Install from Source**

> Note: This approach requires some familiarity with building interfaces in
> React.

If you prefer to install from source, ensure you have Python 3.10+ and Node.js
(version above 14.15.0) installed. Here's how you get started:

    * Clone the AutoGen Studio repository and install its Python dependencies:
        
                pip install -e .  

    * Navigate to the `samples/apps/autogen-studio/frontend` directory, install dependencies, and build the UI:
        
                npm install -g gatsby-cli  
        npm install --global yarn  
        cd frontend  
        yarn install  
        yarn build  

For Windows users, to build the frontend, you may need alternative commands
provided in the [autogen studio
readme](<https://github.com/microsoft/autogen/tree/main/samples/apps/autogen->
studio).

### Running the Application​ {#running-the-application}

Once installed, run the web UI by entering the following in your terminal:

    autogenstudio ui --port 8081  

This will start the application on the specified port. Open your web browser
and go to `http://localhost:8081/` to begin using AutoGen Studio.

Now that you have AutoGen Studio installed and running, you are ready to
explore its capabilities, including defining and modifying agent workflows,
interacting with agents and sessions, and expanding agent skills.

## What Can You Do with AutoGen Studio?​ {#what-can-you-do-with-autogen-studio}

The AutoGen Studio UI is organized into 3 high level sections - **Build** ,
**Playground** , and **Gallery**.

### Build​ {#build}

![Specify
Agents.](/autogen/assets/images/autogenstudio_config-5e0107b565503d4952c9b00dd3d5c10e.png)

This section focuses on defining the properties of agents and agent workflows.
It includes the following concepts:

 **Skills** : Skills are functions (e.g., Python functions) that describe how
to solve a task. In general, a good skill has a descriptive name (e.g.
`generate_images`), extensive docstrings and good defaults (e.g., writing out
files to disk for persistence and reuse). You can add new skills to AutoGen
Studio via the provided UI. At inference time, these skills are made available
to the assistant agent as they address your tasks.

![View and add
skills.](/autogen/assets/images/autogenstudio_skills-7376f0a5aec5917fda817a4793f2b89a.png)

 _AutoGen Studio Build View: View, add or edit skills that an agent can
leverage in addressing tasks._

 **Agents** : This provides an interface to declaratively specify properties
for an AutoGen agent (mirrors most of the members of a base [AutoGen
conversable
agent](https://github.com/microsoft/autogen/blob/main/autogen/agentchat/conversable_agent.py)
class).

 **Agent Workflows** : An agent workflow is a specification of a set of agents
that can work together to accomplish a task. The simplest version of this is a
setup with two agents – a user proxy agent (that represents a user i.e. it
compiles code and prints result) and an assistant that can address task
requests (e.g., generating plans, writing code, evaluating responses,
proposing error recovery steps, etc.). A more complex flow could be a group
chat where even more agents work towards a solution.

### Playground​ {#playground}

![AutoGen Studio Playground View: Solving a task with multiple agents that
generate a pdf document with
images.](/autogen/assets/images/autogenstudio_home-
cce78dc150d1bb0073620754df73d863.png)

 _AutoGen Studio Playground View: Agents collaborate, use available skills
(ability to generate images) to address a user task (generate pdf 's)._

The playground section is focused on interacting with agent workflows defined
in the previous build section. It includes the following concepts:

 **Session** : A session refers to a period of continuous interaction or
engagement with an agent workflow, typically characterized by a sequence of
activities or operations aimed at achieving specific objectives. It includes
the agent workflow configuration, the interactions between the user and the
agents. A session can be “published” to a “gallery”.

 **Chat View** : A chat is a sequence of interactions between a user and an
agent. It is a part of a session.

### Gallery​ {#gallery}

This section is focused on sharing and reusing artifacts (e.g., workflow
configurations, sessions, etc.).

AutoGen Studio comes with 3 example skills: `fetch_profile`, `find_papers`,
`generate_images`. Please feel free to review the repo to learn more about how
they work.

## The AutoGen Studio API​ {#the-autogen-studio-api}

While AutoGen Studio is a web interface, it is powered by an underlying python
API that is reusable and modular. Importantly, we have implemented an API
where agent workflows can be declaratively specified (in JSON), loaded and
run. An example of the current API is shown below. Please consult the [AutoGen
Studio repo](https://microsoft.github.io/autogen/docs/autogenstudio) for more
details.

    import json  
    from autogenstudio import AutoGenWorkFlowManager, AgentWorkFlowConfig  
      
    # load an agent specification in JSON  
    agent_spec = json.load(open('agent_spec.json'))  
      
    # Create an AutoGen Workflow Configuration from the agent specification  
    agent_work_flow_config = FlowConfig(**agent_spec)  
      
    # Create a Workflow from the configuration  
    agent_work_flow = AutoGenWorkFlowManager(agent_work_flow_config)  
      
    # Run the workflow on a task  
    task_query = "What is the height of the Eiffel Tower?"  
    agent_work_flow.run(message=task_query)  

## Road Map and Next Steps​ {#road-map-and-next-steps}

As we continue to develop and refine AutoGen Studio, the road map below
outlines an array of enhancements and new features planned for future
releases. Here's what users can look forward to:

* **Complex Agent Workflows** : We're working on integrating support for more sophisticated agent workflows, such as `GroupChat`, allowing for richer interaction between multiple agents or dynamic topologies.
* **Improved User Experience** : This includes features like streaming intermediate model output for real-time feedback, better summarization of agent responses, information on costs of each interaction. We will also invest in improving the workflow for composing and reusing agents. We will also explore support for more interactive human in the loop feedback to agents.
* **Expansion of Agent Skills** : We will work towards improving the workflow for authoring, composing and reusing agent skills.
* **Community Features** : Facilitation of sharing and collaboration within AutoGen Studio user community is a key goal. We're exploring options for sharing sessions and results more easily among users and contributing to a shared repository of skills, agents, and agent workflows.

### FAQ​ {#faq}

 **Q: Where can I adjust the default skills, agent and workflow
configurations?** A: You can modify agent configurations directly from the UI
or by editing the `autogentstudio/utils/dbdefaults.json` file which is used to
initialize the database.

 **Q: If I want to reset the entire conversation with an agent, how do I go
about it?** A: To reset your conversation history, you can delete the
`database.sqlite` file. If you need to clear user-specific data, remove the
relevant `autogenstudio/web/files/user/<user_id_md5hash>` folder.

 **Q: Is it possible to view the output and messages generated by the agents
during interactions?** A: Yes, you can view the generated messages in the
debug console of the web UI, providing insights into the agent interactions.
Alternatively, you can inspect the `database.sqlite` file for a comprehensive
record of messages.

 **Q: Where can I find documentation and support for AutoGen Studio?** A: We
are constantly working to improve AutoGen Studio. For the latest updates,
please refer to the [AutoGen Studio
Readme](<https://github.com/microsoft/autogen/tree/main/samples/apps/autogen->
studio). For additional support, please open an issue on
[GitHub](https://github.com/microsoft/autogen) or ask questions on
[Discord](https://discord.gg/pAbnFJrkgZ).

 **Tags:**

* [AutoGen](/autogen/blog/tags/auto-gen)
* [UI](/autogen/blog/tags/ui)
* [web](/autogen/blog/tags/web)
* [UX](/autogen/blog/tags/ux)

[**Read More**](/autogen/blog/2023/12/01/AutoGenStudio)

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

* Reference
  * agentchat
    * agentchat.contrib
      * agentchat.contrib.capabilities
      * [agent_builder](/autogen/docs/reference/agentchat/contrib/agent_builder)
      * [compressible_agent](/autogen/docs/reference/agentchat/contrib/compressible_agent)
      * [gpt_assistant_agent](/autogen/docs/reference/agentchat/contrib/gpt_assistant_agent)
      * [img_utils](/autogen/docs/reference/agentchat/contrib/img_utils)
      * [llava_agent](/autogen/docs/reference/agentchat/contrib/llava_agent)
      * [math_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/math_user_proxy_agent)
      * [multimodal_conversable_agent](/autogen/docs/reference/agentchat/contrib/multimodal_conversable_agent)
      * [qdrant_retrieve_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/qdrant_retrieve_user_proxy_agent)
      * [retrieve_assistant_agent](/autogen/docs/reference/agentchat/contrib/retrieve_assistant_agent)
      * [retrieve_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/retrieve_user_proxy_agent)
      * [text_analyzer_agent](/autogen/docs/reference/agentchat/contrib/text_analyzer_agent)
    * [agent](/autogen/docs/reference/agentchat/agent)
    * [assistant_agent](/autogen/docs/reference/agentchat/assistant_agent)
    * [conversable_agent](/autogen/docs/reference/agentchat/conversable_agent)
    * [groupchat](/autogen/docs/reference/agentchat/groupchat)
    * [user_proxy_agent](/autogen/docs/reference/agentchat/user_proxy_agent)
  * oai
  * [code_utils](/autogen/docs/reference/code_utils)
  * [function_utils](/autogen/docs/reference/function_utils)
  * [math_utils](/autogen/docs/reference/math_utils)
  * [retrieve_utils](/autogen/docs/reference/retrieve_utils)
  * [token_count_utils](/autogen/docs/reference/token_count_utils)

On this page

# agentchat.contrib.img_utils {#agentchatcontribimgutils}

#### llava_formatter​ {#llavaformatter}
```python
    def llava_formatter(prompt: str,  
                        order_image_tokens: bool = False) -> Tuple[str, List[str]]  

Formats the input prompt by replacing image tags and returns the new prompt
along with image locations.

 **Arguments** :

* prompt (str): The input string that may contain image tags like <img ...>.
* order_image_tokens (bool, optional): Whether to order the image tokens with numbers. It will be useful for GPT-4V. Defaults to False.

 **Returns** :

* Tuple[str, List[str]]: A tuple containing the formatted string and a list of images (loaded in b64 format).

#### gpt4v_formatter​ {#gpt4vformatter}
```python
    def gpt4v_formatter(prompt: str) -> List[Union[str, dict]]  

Formats the input prompt by replacing image tags and returns a list of text
and images.

 **Arguments** :

* prompt (str): The input string that may contain image tags like <img ...>.

 **Returns** :

* List[Union[str, dict]]: A list of alternating text and image dictionary items.

#### extract_img_paths​ {#extractimgpaths}
```python
    def extract_img_paths(paragraph: str) -> list  

Extract image paths (URLs or local paths) from a text paragraph.

 **Arguments** :

* `paragraph` _str_ \- The input text paragraph.

 **Returns** :

* `list` \- A list of extracted image paths.

[Edit this
page](https://github.com/microsoft/autogen/edit/main/website/docs/reference/agentchat/contrib/img_utils.md)

[Previous«
gpt_assistant_agent](/autogen/docs/reference/agentchat/contrib/gpt_assistant_agent)

[Nextllava_agent »](/autogen/docs/reference/agentchat/contrib/llava_agent)

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

* Reference
  * agentchat
    * agentchat.contrib
      * agentchat.contrib.capabilities
      * [agent_builder](/autogen/docs/reference/agentchat/contrib/agent_builder)
      * [compressible_agent](/autogen/docs/reference/agentchat/contrib/compressible_agent)
      * [gpt_assistant_agent](/autogen/docs/reference/agentchat/contrib/gpt_assistant_agent)
      * [img_utils](/autogen/docs/reference/agentchat/contrib/img_utils)
      * [llava_agent](/autogen/docs/reference/agentchat/contrib/llava_agent)
      * [math_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/math_user_proxy_agent)
      * [multimodal_conversable_agent](/autogen/docs/reference/agentchat/contrib/multimodal_conversable_agent)
      * [qdrant_retrieve_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/qdrant_retrieve_user_proxy_agent)
      * [retrieve_assistant_agent](/autogen/docs/reference/agentchat/contrib/retrieve_assistant_agent)
      * [retrieve_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/retrieve_user_proxy_agent)
      * [text_analyzer_agent](/autogen/docs/reference/agentchat/contrib/text_analyzer_agent)
    * [agent](/autogen/docs/reference/agentchat/agent)
    * [assistant_agent](/autogen/docs/reference/agentchat/assistant_agent)
    * [conversable_agent](/autogen/docs/reference/agentchat/conversable_agent)
    * [groupchat](/autogen/docs/reference/agentchat/groupchat)
    * [user_proxy_agent](/autogen/docs/reference/agentchat/user_proxy_agent)
  * oai
  * [code_utils](/autogen/docs/reference/code_utils)
  * [function_utils](/autogen/docs/reference/function_utils)
  * [math_utils](/autogen/docs/reference/math_utils)
  * [retrieve_utils](/autogen/docs/reference/retrieve_utils)
  * [token_count_utils](/autogen/docs/reference/token_count_utils)

On this page

# agentchat.contrib.math_user_proxy_agent {#agentchatcontribmathuserproxyagent}

## MathUserProxyAgent Objects​ {#mathuserproxyagent-objects}

    class MathUserProxyAgent(UserProxyAgent)  

(Experimental) A MathChat agent that can handle math problems.

#### MAX_CONSECUTIVE_AUTO_REPLY​ {#maxconsecutiveautoreply}

maximum number of consecutive auto replies (subject to future change)

#### __init__​ {#init}
```python
    def __init__(name: Optional[str] = "MathChatAgent",  
                 is_termination_msg: Optional[Callable[  
                     [Dict], bool]] = _is_termination_msg_mathchat,  
                 human_input_mode: Optional[str] = "NEVER",  
                 default_auto_reply: Optional[Union[str, Dict,  
                                                    None]] = DEFAULT_REPLY,  
                 max_invalid_q_per_step=3,  
                 **kwargs)  

 **Arguments** :

* `name` _str_ \- name of the agent
* `is_termination_msg` _function_ \- a function that takes a message in the form of a dictionary and returns a boolean value indicating if this received message is a termination message. The dict can contain the following keys: "content", "role", "name", "function_call".
* `human_input_mode` _str_ \- whether to ask for human inputs every time a message is received. Possible values are "ALWAYS", "TERMINATE", "NEVER". (1) When "ALWAYS", the agent prompts for human input every time a message is received. Under this mode, the conversation stops when the human input is "exit", or when is_termination_msg is True and there is no human input. (2) When "TERMINATE", the agent only prompts for human input only when a termination message is received or the number of auto reply reaches the max_consecutive_auto_reply. (3) (Default) When "NEVER", the agent will never prompt for human input. Under this mode, the conversation stops when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.
* `default_auto_reply` _str or dict or None_ \- the default auto reply message when no code execution or llm based reply is generated.
* `max_invalid_q_per_step` _int_ \- (ADDED) the maximum number of invalid queries per step.
* `**kwargs` _dict_ \- other kwargs in [UserProxyAgent](/autogen/docs/reference/agentchat/user_proxy_agent#__init__).

#### generate_init_message​ {#generateinitmessage}
```python
    def generate_init_message(problem,  
                              prompt_type="default",  
                              customized_prompt=None)  

Generate a prompt for the assistant agent with the given problem and prompt.

 **Arguments** :

* `problem` _str_ \- the problem to be solved.
* `prompt_type` _str_ \- the type of the prompt. Possible values are "default", "python", "wolfram". (1) "default": the prompt that allows the agent to choose between 3 ways to solve a problem:
    1. write a python program to solve it directly.
    2. solve it directly without python.
    3. solve it step by step with python. (2) "python": a simplified prompt from the third way of the "default" prompt, that asks the assistant to solve the problem step by step with python. (3) "two_tools": a simplified prompt similar to the "python" prompt, but allows the model to choose between Python and Wolfram Alpha to solve the problem.
* `customized_prompt` _str_ \- a customized prompt to be used. If it is not None, the prompt_type will be ignored.

 **Returns** :

* `str` \- the generated prompt ready to be sent to the assistant agent.

#### execute_one_python_code​ {#executeonepythoncode}
```python
    def execute_one_python_code(pycode)  

Execute python code blocks.

Previous python code will be saved and executed together with the new code.
the "print" function will also be added to the last line of the code if needed

#### execute_one_wolfram_query​ {#executeonewolframquery}
```python
    def execute_one_wolfram_query(query: str)  

Run one wolfram query and return the output.

 **Arguments** :

* `query` \- string of the query.

 **Returns** :

* `output` \- string with the output of the query.
* `is_success` \- boolean indicating whether the query was successful.

#### get_from_dict_or_env​ {#getfromdictorenv}
```python
    def get_from_dict_or_env(data: Dict[str, Any],  
                             key: str,  
                             env_key: str,  
                             default: Optional[str] = None) -> str  

Get a value from a dictionary or an environment variable.

## WolframAlphaAPIWrapper Objects​ {#wolframalphaapiwrapper-objects}

    class WolframAlphaAPIWrapper(BaseModel)  

Wrapper for Wolfram Alpha.

Docs for using:

  1. Go to wolfram alpha and sign up for a developer account
  2. Create an app and get your APP ID
  3. Save your APP ID into WOLFRAM_ALPHA_APPID env variable
  4. pip install wolframalpha

#### wolfram_client​ {#wolframclient}

:meta private:

## Config Objects​ {#config-objects}

    class Config()  

Configuration for this pydantic object.

#### validate_environment​ {#validateenvironment}

    @root_validator(skip_on_failure=True)  ```python
    def validate_environment(cls, values: Dict) -> Dict  

Validate that api key and python package exists in environment.

#### run​ {#run}
```python
    def run(query: str) -> Tuple[str, bool]  

Run query through WolframAlpha and parse result.

[Edit this
page](https://github.com/microsoft/autogen/edit/main/website/docs/reference/agentchat/contrib/math_user_proxy_agent.md)

[Previous« llava_agent](/autogen/docs/reference/agentchat/contrib/llava_agent)

[Nextmultimodal_conversable_agent
»](/autogen/docs/reference/agentchat/contrib/multimodal_conversable_agent)

* MathUserProxyAgent Objects
* WolframAlphaAPIWrapper Objects
* Config Objects

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* [Getting Started](.docs/Getting-Started)
* [Installation](.docs/Installation)
* Use Cases
* Examples
  * [AutoGen - Automated Multi Agent Chat](.docs/Examples/AutoGen-AgentChat)
  * [AutoGen - Tune GPT Models](.docs/Examples/AutoGen-OpenAI)
  * [AutoML - Classification](.docs/Examples/AutoML-Classification)
  * [AutoML - NLP](.docs/Examples/AutoML-NLP)
  * [AutoML - Rank](.docs/Examples/AutoML-Rank)
  * [AutoML - Regression](.docs/Examples/AutoML-Regression)
  * [AutoML - Time Series Forecast](.docs/Examples/AutoML-Time series forecast)
  * [AutoML for LightGBM](.docs/Examples/AutoML-for-LightGBM)
  * [AutoML for XGBoost](.docs/Examples/AutoML-for-XGBoost)
  * [Default - Flamlized Estimator](.docs/Examples/Default-Flamlized)
  * [Integrate - AzureML](.docs/Examples/Integrate - AzureML)
  * [Integrate - Scikit-learn Pipeline](.docs/Examples/Integrate - Scikit-learn Pipeline)
  * [Integrate - Spark](.docs/Examples/Integrate - Spark)
  * [Tune - AzureML pipeline](.docs/Examples/Tune-AzureML-pipeline)
  * [Tune - HuggingFace](.docs/Examples/Tune-HuggingFace)
  * [Tune - Lexicographic Objectives](.docs/Examples/Tune-Lexicographic-objectives)
  * [Tune - PyTorch](.docs/Examples/Tune-PyTorch)
* [Contributing](.docs/Contribute)
* [Research](.docs/Research)

On this page

# AutoML for XGBoost {#automl-for-xgboost}

### Prerequisites for this example​ {#prerequisites-for-this-example}

Install the [automl] option.

    pip install "flaml[automl] matplotlib openml"  

### Use built-in XGBoostSklearnEstimator​ {#use-builtin-xgboostsklearnestimator}

    from flaml import AutoML  
    from flaml.automl.data import load_openml_dataset  
      
    # Download [houses dataset](https://www.openml.org/d/537) from OpenML. The task is to predict median price of the house in the region based on demographic composition and a state of housing market in the region.  
    X_train, X_test, y_train, y_test = load_openml_dataset(dataset_id=537, data_dir='./')  
      
    automl = AutoML()  
    settings = {  
        "time_budget": 60,  # total running time in seconds  
        "metric": 'r2',  # primary metrics for regression can be chosen from: ['mae','mse','r2']  
        "estimator_list": ['xgboost'],  # list of ML learners; we tune XGBoost in this example  
        "task": 'regression',  # task type  
        "log_file_name": 'houses_experiment.log',  # flaml log file  
        "seed": 7654321,  # random seed  
    }  
    automl.fit(X_train=X_train, y_train=y_train, **settings)  

#### Sample output​ {#sample-output}

    [flaml.automl: 09-29 23:06:46] {1446} INFO - Data split method: uniform  
    [flaml.automl: 09-29 23:06:46] {1450} INFO - Evaluation method: cv  
    [flaml.automl: 09-29 23:06:46] {1496} INFO - Minimizing error metric: 1-r2  
    [flaml.automl: 09-29 23:06:46] {1533} INFO - List of ML learners in AutoML Run: ['xgboost']  
    [flaml.automl: 09-29 23:06:46] {1763} INFO - iteration 0, current learner xgboost  
    [flaml.automl: 09-29 23:06:47] {1880} INFO - Estimated sufficient time budget=2621s. Estimated necessary time budget=3s.  
    [flaml.automl: 09-29 23:06:47] {1952} INFO -  at 0.3s,  estimator xgboost's best error=2.1267,  best estimator xgboost's best error=2.1267  
    [flaml.automl: 09-29 23:06:47] {1763} INFO - iteration 1, current learner xgboost  
    [flaml.automl: 09-29 23:06:47] {1952} INFO -  at 0.5s,  estimator xgboost's best error=2.1267,  best estimator xgboost's best error=2.1267  
    [flaml.automl: 09-29 23:06:47] {1763} INFO - iteration 2, current learner xgboost  
    [flaml.automl: 09-29 23:06:47] {1952} INFO -  at 0.6s,  estimator xgboost's best error=0.8485,  best estimator xgboost's best error=0.8485  
    [flaml.automl: 09-29 23:06:47] {1763} INFO - iteration 3, current learner xgboost  
    [flaml.automl: 09-29 23:06:47] {1952} INFO -  at 0.8s,  estimator xgboost's best error=0.3799,  best estimator xgboost's best error=0.3799  
    [flaml.automl: 09-29 23:06:47] {1763} INFO - iteration 4, current learner xgboost  
    [flaml.automl: 09-29 23:06:47] {1952} INFO -  at 1.0s,  estimator xgboost's best error=0.3799,  best estimator xgboost's best error=0.3799  
    [flaml.automl: 09-29 23:06:47] {1763} INFO - iteration 5, current learner xgboost  
    [flaml.automl: 09-29 23:06:47] {1952} INFO -  at 1.2s,  estimator xgboost's best error=0.3799,  best estimator xgboost's best error=0.3799  
    [flaml.automl: 09-29 23:06:47] {1763} INFO - iteration 6, current learner xgboost  
    [flaml.automl: 09-29 23:06:48] {1952} INFO -  at 1.5s,  estimator xgboost's best error=0.2992,  best estimator xgboost's best error=0.2992  
    [flaml.automl: 09-29 23:06:48] {1763} INFO - iteration 7, current learner xgboost  
    [flaml.automl: 09-29 23:06:48] {1952} INFO -  at 1.9s,  estimator xgboost's best error=0.2992,  best estimator xgboost's best error=0.2992  
    [flaml.automl: 09-29 23:06:48] {1763} INFO - iteration 8, current learner xgboost  
    [flaml.automl: 09-29 23:06:49] {1952} INFO -  at 2.2s,  estimator xgboost's best error=0.2992,  best estimator xgboost's best error=0.2992  
    [flaml.automl: 09-29 23:06:49] {1763} INFO - iteration 9, current learner xgboost  
    [flaml.automl: 09-29 23:06:49] {1952} INFO -  at 2.5s,  estimator xgboost's best error=0.2513,  best estimator xgboost's best error=0.2513  
    [flaml.automl: 09-29 23:06:49] {1763} INFO - iteration 10, current learner xgboost  
    [flaml.automl: 09-29 23:06:49] {1952} INFO -  at 2.8s,  estimator xgboost's best error=0.2513,  best estimator xgboost's best error=0.2513  
    [flaml.automl: 09-29 23:06:49] {1763} INFO - iteration 11, current learner xgboost  
    [flaml.automl: 09-29 23:06:49] {1952} INFO -  at 3.0s,  estimator xgboost's best error=0.2513,  best estimator xgboost's best error=0.2513  
    [flaml.automl: 09-29 23:06:49] {1763} INFO - iteration 12, current learner xgboost  
    [flaml.automl: 09-29 23:06:50] {1952} INFO -  at 3.3s,  estimator xgboost's best error=0.2113,  best estimator xgboost's best error=0.2113  
    [flaml.automl: 09-29 23:06:50] {1763} INFO - iteration 13, current learner xgboost  
    [flaml.automl: 09-29 23:06:50] {1952} INFO -  at 3.5s,  estimator xgboost's best error=0.2113,  best estimator xgboost's best error=0.2113  
    [flaml.automl: 09-29 23:06:50] {1763} INFO - iteration 14, current learner xgboost  
    [flaml.automl: 09-29 23:06:50] {1952} INFO -  at 4.0s,  estimator xgboost's best error=0.2090,  best estimator xgboost's best error=0.2090  
    [flaml.automl: 09-29 23:06:50] {1763} INFO - iteration 15, current learner xgboost  
    [flaml.automl: 09-29 23:06:51] {1952} INFO -  at 4.5s,  estimator xgboost's best error=0.2090,  best estimator xgboost's best error=0.2090  
    [flaml.automl: 09-29 23:06:51] {1763} INFO - iteration 16, current learner xgboost  
    [flaml.automl: 09-29 23:06:51] {1952} INFO -  at 5.2s,  estimator xgboost's best error=0.1919,  best estimator xgboost's best error=0.1919  
    [flaml.automl: 09-29 23:06:51] {1763} INFO - iteration 17, current learner xgboost  
    [flaml.automl: 09-29 23:06:52] {1952} INFO -  at 5.5s,  estimator xgboost's best error=0.1919,  best estimator xgboost's best error=0.1919  
    [flaml.automl: 09-29 23:06:52] {1763} INFO - iteration 18, current learner xgboost  
    [flaml.automl: 09-29 23:06:54] {1952} INFO -  at 8.0s,  estimator xgboost's best error=0.1797,  best estimator xgboost's best error=0.1797  
    [flaml.automl: 09-29 23:06:54] {1763} INFO - iteration 19, current learner xgboost  
    [flaml.automl: 09-29 23:06:55] {1952} INFO -  at 9.0s,  estimator xgboost's best error=0.1797,  best estimator xgboost's best error=0.1797  
    [flaml.automl: 09-29 23:06:55] {1763} INFO - iteration 20, current learner xgboost  
    [flaml.automl: 09-29 23:07:08] {1952} INFO -  at 21.8s, estimator xgboost's best error=0.1797,  best estimator xgboost's best error=0.1797  
    [flaml.automl: 09-29 23:07:08] {1763} INFO - iteration 21, current learner xgboost  
    [flaml.automl: 09-29 23:07:11] {1952} INFO -  at 24.4s, estimator xgboost's best error=0.1797,  best estimator xgboost's best error=0.1797  
    [flaml.automl: 09-29 23:07:11] {1763} INFO - iteration 22, current learner xgboost  
    [flaml.automl: 09-29 23:07:16] {1952} INFO -  at 30.0s, estimator xgboost's best error=0.1782,  best estimator xgboost's best error=0.1782  
    [flaml.automl: 09-29 23:07:16] {1763} INFO - iteration 23, current learner xgboost  
    [flaml.automl: 09-29 23:07:20] {1952} INFO -  at 33.5s, estimator xgboost's best error=0.1782,  best estimator xgboost's best error=0.1782  
    [flaml.automl: 09-29 23:07:20] {1763} INFO - iteration 24, current learner xgboost  
    [flaml.automl: 09-29 23:07:29] {1952} INFO -  at 42.3s, estimator xgboost's best error=0.1782,  best estimator xgboost's best error=0.1782  
    [flaml.automl: 09-29 23:07:29] {1763} INFO - iteration 25, current learner xgboost  
    [flaml.automl: 09-29 23:07:30] {1952} INFO -  at 43.2s, estimator xgboost's best error=0.1782,  best estimator xgboost's best error=0.1782  
    [flaml.automl: 09-29 23:07:30] {1763} INFO - iteration 26, current learner xgboost  
    [flaml.automl: 09-29 23:07:50] {1952} INFO -  at 63.4s, estimator xgboost's best error=0.1663,  best estimator xgboost's best error=0.1663  
    [flaml.automl: 09-29 23:07:50] {2059} INFO - selected model: <xgboost.core.Booster object at 0x7f6399005910>  
    [flaml.automl: 09-29 23:07:55] {2122} INFO - retrain xgboost for 5.4s  
    [flaml.automl: 09-29 23:07:55] {2128} INFO - retrained model: <xgboost.core.Booster object at 0x7f6398fc0eb0>  
    [flaml.automl: 09-29 23:07:55] {1557} INFO - fit succeeded  
    [flaml.automl: 09-29 23:07:55] {1558} INFO - Time taken to find the best model: 63.427649974823  
    [flaml.automl: 09-29 23:07:55] {1569} WARNING - Time taken to find the best model is 106% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.  

#### Retrieve best config​ {#retrieve-best-config}

    print('Best hyperparmeter config:', automl.best_config)  
    print('Best r2 on validation data: {0:.4g}'.format(1-automl.best_loss))  
    print('Training duration of best run: {0:.4g} s'.format(automl.best_config_train_time))  
    print(automl.model.estimator)  
    # Best hyperparmeter config: {'n_estimators': 473, 'max_leaves': 35, 'max_depth': 0, 'min_child_weight': 0.001, 'learning_rate': 0.26865031351923346, 'subsample': 0.9718245679598786, 'colsample_bylevel': 0.7421362469066445, 'colsample_bytree': 1.0, 'reg_alpha': 0.06824336834995245, 'reg_lambda': 250.9654222583276}  
    # Best r2 on validation data: 0.8384  
    # Training duration of best run: 2.194 s  
    # XGBRegressor(base_score=0.5, booster='gbtree',  
    #              colsample_bylevel=0.7421362469066445, colsample_bynode=1,  
    #              colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',  
    #              importance_type='gain', interaction_constraints='',  
    #              learning_rate=0.26865031351923346, max_delta_step=0, max_depth=0,  
    #              max_leaves=35, min_child_weight=0.001, missing=nan,  
    #              monotone_constraints='()', n_estimators=473, n_jobs=-1,  
    #              num_parallel_tree=1, random_state=0, reg_alpha=0.06824336834995245,  
    #              reg_lambda=250.9654222583276, scale_pos_weight=1,  
    #              subsample=0.9718245679598786, tree_method='hist',  
    #              use_label_encoder=False, validate_parameters=1, verbosity=0)  

#### Plot feature importance​ {#plot-feature-importance}

    import matplotlib.pyplot as plt  
      
    plt.barh(automl.feature_names_in_, automl.feature_importances_)  

![png](data:image/png;base64,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)

#### Compute predictions of testing dataset​ {#compute-predictions-of-testing-dataset}

    y_pred = automl.predict(X_test)  
    print('Predicted labels', y_pred)  
    # Predicted labels [139062.95 237622.   140522.03 ... 182125.5  252156.36 264884.5 ]  

#### Compute different metric values on testing dataset​ {#compute-different-metric-values-on-testing-dataset}

    from flaml.automl.ml import sklearn_metric_loss_score  
      
    print('r2', '=', 1 - sklearn_metric_loss_score('r2', y_pred, y_test))  
    print('mse', '=', sklearn_metric_loss_score('mse', y_pred, y_test))  
    print('mae', '=', sklearn_metric_loss_score('mae', y_pred, y_test))  
    # r2 = 0.8456494234135888  
    # mse = 2040284106.2781258  
    # mae = 30212.830996680445  

#### Compare with untuned XGBoost​ {#compare-with-untuned-xgboost}

    from xgboost import XGBRegressor  
      
    xgb = XGBRegressor()  
    xgb.fit(X_train, y_train)  
    y_pred = xgb.predict(X_test)  
    from flaml.automl.ml import sklearn_metric_loss_score  
      
    print('default xgboost r2', '=', 1 - sklearn_metric_loss_score('r2', y_pred, y_test))  
    # default xgboost r2 = 0.8265451174596482  

#### Plot learning curve​ {#plot-learning-curve}

How does the model accuracy improve as we search for different hyperparameter
configurations?

    from flaml.automl.data import get_output_from_log  
    import numpy as np  
      
    time_history, best_valid_loss_history, valid_loss_history, config_history, metric_history =  
        get_output_from_log(filename=settings['log_file_name'], time_budget=60)  
    plt.title('Learning Curve')  
    plt.xlabel('Wall Clock Time (s)')  
    plt.ylabel('Validation r2')  
    plt.step(time_history, 1 - np.array(best_valid_loss_history), where='post')  
    plt.show()  

![png](data:image/png;base64,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)

### Use a customized XGBoost learner​ {#use-a-customized-xgboost-learner}

You can easily enable a custom objective function by adding a customized
XGBoost learner (inherit XGBoostEstimator or XGBoostSklearnEstimator) in
FLAML. In the following example, we show how to add such a customized XGBoost
learner with a custom objective function.

```python
  import numpy as np  
      
      
    # define your customized objective function  
    def logregobj(preds, dtrain):  
        labels = dtrain.get_label()  
        preds = 1.0 / (1.0 + np.exp(-preds))  # transform raw leaf weight  
        grad = preds - labels  
        hess = preds * (1.0 - preds)  
        return grad, hess  
      
      
    from flaml.automl.model import XGBoostEstimator  
      
      
    class MyXGB1(XGBoostEstimator):  
        '''XGBoostEstimator with the logregobj function as the objective function  
        '''  
      
        def __init__(self, **config):  
            super().__init__(objective=logregobj, **config)  
      
      
    class MyXGB2(XGBoostEstimator):  
        '''XGBoostEstimator with 'reg:squarederror' as the objective function  
        '''  
      
        def __init__(self, **config):  
            super().__init__(objective='reg:gamma', **config)  

#### Add the customized learners and tune them​ {#add-the-customized-learners-and-tune-them}

    automl = AutoML()  
    automl.add_learner(learner_name='my_xgb1', learner_class=MyXGB1)  
    automl.add_learner(learner_name='my_xgb2', learner_class=MyXGB2)  
    settings["estimator_list"] = ['my_xgb1', 'my_xgb2']  # change the estimator list  
    automl.fit(X_train=X_train, y_train=y_train, **settings)  

[Link to
notebook](https://github.com/microsoft.blob/main/notebook/automl_xgboost.ipynb)
| [Open in
colab](https://colab.research.google.com/github/microsoft.blob/main/notebook/automl_xgboost.ipynb)

[Edit this
page](<https://github.com/microsoft.edit/main/website/docs/Examples/AutoML->
for-XGBoost.md)

[Previous« AutoML for LightGBM](.docs/Examples/AutoML-for-LightGBM)

[NextDefault - Flamlized Estimator »](.docs/Examples/Default-Flamlized)

* Prerequisites for this example
* Use built-in XGBoostSklearnEstimator
* Use a customized XGBoost learner

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* [Getting Started](.docs/Getting-Started)
* [Installation](.docs/Installation)
* Use Cases
* Examples
  * [AutoGen - Automated Multi Agent Chat](.docs/Examples/AutoGen-AgentChat)
  * [AutoGen - Tune GPT Models](.docs/Examples/AutoGen-OpenAI)
  * [AutoML - Classification](.docs/Examples/AutoML-Classification)
  * [AutoML - NLP](.docs/Examples/AutoML-NLP)
  * [AutoML - Rank](.docs/Examples/AutoML-Rank)
  * [AutoML - Regression](.docs/Examples/AutoML-Regression)
  * [AutoML - Time Series Forecast](.docs/Examples/AutoML-Time series forecast)
  * [AutoML for LightGBM](.docs/Examples/AutoML-for-LightGBM)
  * [AutoML for XGBoost](.docs/Examples/AutoML-for-XGBoost)
  * [Default - Flamlized Estimator](.docs/Examples/Default-Flamlized)
  * [Integrate - AzureML](.docs/Examples/Integrate - AzureML)
  * [Integrate - Scikit-learn Pipeline](.docs/Examples/Integrate - Scikit-learn Pipeline)
  * [Integrate - Spark](.docs/Examples/Integrate - Spark)
  * [Tune - AzureML pipeline](.docs/Examples/Tune-AzureML-pipeline)
  * [Tune - HuggingFace](.docs/Examples/Tune-HuggingFace)
  * [Tune - Lexicographic Objectives](.docs/Examples/Tune-Lexicographic-objectives)
  * [Tune - PyTorch](.docs/Examples/Tune-PyTorch)
* [Contributing](.docs/Contribute)
* [Research](.docs/Research)

# AutoGen - Automated Multi Agent Chat {#autogen-automated-multi-agent-chat}

Please refer to <<https://microsoft.github.io/autogen/docs/Examples/AutoGen->
AgentChat>.

[Edit this
page](<https://github.com/microsoft.edit/main/website/docs/Examples/AutoGen->
AgentChat.md)

[Previous« Zero Shot AutoML](.docs/Use-Cases/Zero-Shot-AutoML)

[NextAutoGen - Tune GPT Models »](.docs/Examples/AutoGen-OpenAI)

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

On this page

# Frequently Asked Questions {#frequently-asked-questions}

### [Guidelines on how to set a hyperparameter search space](.docs/Use- {#guidelines-on-how-to-set-a-hyperparameter-search-spaceflamldocsuse}

Cases/Tune-User-Defined-Function#details-and-guidelines-on-hyperparameter-
search-space)​

### [Guidelines on parallel vs seqential tuning](.docs/Use-Cases/Task- {#guidelines-on-parallel-vs-seqential-tuningflamldocsusecasestask}

Oriented-AutoML#guidelines-on-parallel-vs-sequential-tuning)​

### [Guidelines on creating and tuning a custom estimator](.docs/Use- {#guidelines-on-creating-and-tuning-a-custom-estimatorflamldocsuse}

Cases/Task-Oriented-AutoML#guidelines-on-tuning-a-custom-estimator)​

### About `low_cost_partial_config` in `tune`.​ {#about-lowcostpartialconfig-in-tune}

* Definition and purpose: The `low_cost_partial_config` is a dictionary of subset of the hyperparameter coordinates whose value corresponds to a configuration with known low-cost (i.e., low computation cost for training the corresponding model). The concept of low/high-cost is meaningful in the case where a subset of the hyperparameters to tune directly affects the computation cost for training the model. For example, `n_estimators` and `max_leaves` are known to affect the training cost of tree-based learners. We call this subset of hyperparameters, _cost-related hyperparameters_. In such scenarios, if you are aware of low-cost configurations for the cost-related hyperparameters, you are recommended to set them as the `low_cost_partial_config`. Using the tree-based method example again, since we know that small `n_estimators` and `max_leaves` generally correspond to simpler models and thus lower cost, we set `{'n_estimators': 4, 'max_leaves': 4}` as the `low_cost_partial_config` by default (note that `4` is the lower bound of search space for these two hyperparameters), e.g., in [LGBM](https://github.com/microsoft.blob/main/flaml/model.py#L215). Configuring `low_cost_partial_config` helps the search algorithms make more cost-efficient choices. In AutoML, the `low_cost_init_value` in `search_space()` function for each estimator serves the same role.

* Usage in practice: It is recommended to configure it if there are cost-related hyperparameters in your tuning task and you happen to know the low-cost values for them, but it is not required (It is fine to leave it the default value, i.e., `None`).

* How does it work: `low_cost_partial_config` if configured, will be used as an initial point of the search. It also affects the search trajectory. For more details about how does it play a role in the search algorithms, please refer to the papers about the search algorithms used: Section 2 of [Frugal Optimization for Cost-related Hyperparameters (CFO)](https://arxiv.org/pdf/2005.01571.pdf) and Section 3 of [Economical Hyperparameter Optimization with Blended Search Strategy (BlendSearch)](https://openreview.net/pdf?id=VbLH04pRA3).

### How does FLAML handle imbalanced data (unequal distribution of target {#how-does-flaml-handle-imbalanced-data-unequal-distribution-of-target}

classes in classification task)?​

Currently FLAML does several things for imbalanced data.

  1. When a class contains fewer than 20 examples, we repeatedly add these examples to the training data until the count is at least 20.
  2. We use stratified sampling when doing holdout and kf.
  3. We make sure no class is empty in both training and holdout data.
  4. We allow users to pass `sample_weight` to `AutoML.fit()`.
  5. User can customize the weight of each class by setting the `custom_hp` or `fit_kwargs_by_estimator` arguments. For example, the following code sets the weight for pos vs. neg as 2:1 for the RandomForest estimator:

    from flaml import AutoML  
    from sklearn.datasets import load_iris  
      
    X_train, y_train = load_iris(return_X_y=True)  
    automl = AutoML()  
    automl_settings = {  
        "time_budget": 2,  
        "task": "classification",  
        "log_file_name": "test/iris.log",  
        "estimator_list": ["rf", "xgboost"],  
    }  
      
    automl_settings["custom_hp"] = {  
        "xgboost": {  
            "scale_pos_weight": {  
                "domain": 0.5,  
                "init_value": 0.5,  
            }  
        },  
        "rf": {  
            "class_weight": {  
                "domain": "balanced",  
                "init_value": "balanced"  
            }  
        }  
    }  
    print(automl.model)  

### How to interpret model performance? Is it possible for me to visualize {#how-to-interpret-model-performance-is-it-possible-for-me-to-visualize}

feature importance, SHAP values, optimization history?​

You can use `automl.model.estimator.feature_importances_` to get the
`feature_importances_` for the best model found by automl. See an
[example](.docs/Examples/AutoML-for-XGBoost#plot-feature-importance).

Packages such as `azureml-interpret` and
`sklearn.inspection.permutation_importance` can be used on
`automl.model.estimator` to explain the selected model. Model explanation is
frequently asked and adding a native support may be a good feature.
Suggestions/contributions are welcome.

Optimization history can be checked from the [log](.docs/Use-Cases/Task-
Oriented-AutoML#log-the-trials). You can also [retrieve the log and plot the
learning curve](.docs/Use-Cases/Task-Oriented-AutoML#plot-learning-
curve).

### How to resolve out-of-memory error in `AutoML.fit()`​ {#how-to-resolve-outofmemory-error-in-automlfit}

* Set `free_mem_ratio` a float between 0 and 1. For example, 0.2 means try to keep free memory above 20% of total memory. Training may be early stopped for memory consumption reason when this is set.
* Set `model_history` False.
* If your data are already preprocessed, set `skip_transform` False. If you can preprocess the data before the fit starts, this setting can save memory needed for preprocessing in `fit`.
* If the OOM error only happens for some particular trials:
  * set `use_ray` True. This will increase the overhead per trial but can keep the AutoML process running when a single trial fails due to OOM error.
  * provide a more accurate [`size`](.docs/reference/automl/model#size) function for the memory bytes consumption of each config for the estimator causing this error.
  * modify the [search space](.docs/Use-Cases/Task-Oriented-AutoML#a-shortcut-to-override-the-search-space) for the estimators causing this error.
  * or remove this estimator from the `estimator_list`.
* If the OOM error happens when ensembling, consider disabling ensemble, or use a cheaper ensemble option. ([Example](.docs/Use-Cases/Task-Oriented-AutoML#ensemble)).

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/FAQ.md)

* Guidelines on how to set a hyperparameter search space
* Guidelines on parallel vs seqential tuning
* Guidelines on creating and tuning a custom estimator
* About `low_cost_partial_config` in `tune`.
* How does FLAML handle imbalanced data (unequal distribution of target classes in classification task)?
* How to interpret model performance? Is it possible for me to visualize feature importance, SHAP values, optimization history?
* How to resolve out-of-memory error in `AutoML.fit()`

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* [Getting Started](.docs/Getting-Started)
* [Installation](.docs/Installation)
* Use Cases
* Examples
* [Contributing](.docs/Contribute)
* [Research](.docs/Research)

On this page

# Contributing {#contributing}

This project welcomes and encourages all forms of contributions, including but
not limited to:

* Pushing patches.
* Code review of pull requests.
* Documentation, examples and test cases.
* Readability improvement, e.g., improvement on docstr and comments.
* Community participation in [issues](https://github.com/microsoft.issues), [discussions](https://github.com/microsoft.discussions), and [discord](https://discord.gg/7ZVfhbTQZ5).
* Tutorials, blog posts, talks that promote the project.
* Sharing application scenarios and/or related research.

You can take a look at the [Roadmap for Upcoming
Features](<https://github.com/microsoft.wiki/Roadmap-for-Upcoming->
Features) to identify potential things to work on.

Most contributions require you to agree to a Contributor License Agreement
(CLA) declaring that you have the right to, and actually do, grant us the
rights to use your contribution. For details, visit
<https://cla.opensource.microsoft.com>.

If you are new to GitHub
[here](<https://help.github.com/categories/collaborating-with-issues-and-pull->
requests/) is a detailed help source on getting involved with development on
GitHub.

When you submit a pull request, a CLA bot will automatically determine whether
you need to provide a CLA and decorate the PR appropriately (e.g., status
check, comment). Simply follow the instructions provided by the bot. You will
only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of
Conduct](https://opensource.microsoft.com/codeofconduct/). For more
information see the [Code of Conduct
FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact
[opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional
questions or comments.

## How to make a good bug report​ {#how-to-make-a-good-bug-report}

When you submit an issue to
[GitHub](https://github.com/microsoft.issues), please do your best to
follow these guidelines! This will make it a lot easier to provide you with
good feedback:

* The ideal bug report contains a short reproducible code snippet. This way anyone can try to reproduce the bug easily (see [this](https://stackoverflow.com/help/mcve) for more details). If your snippet is longer than around 50 lines, please link to a [gist](https://gist.github.com) or a GitHub repo.

* If an exception is raised, please **provide the full traceback**.

* Please include your **operating system type and version number** , as well as your **Python, flaml, scikit-learn versions**. The version of flaml can be found by running the following code snippet:

    import flaml  
    print(flaml.**version**)  

* Please ensure all **code snippets and error messages are formatted in appropriate code blocks**. See [Creating and highlighting code blocks](https://help.github.com/articles/creating-and-highlighting-code-blocks) for more details.

## Becoming a Reviewer​ {#becoming-a-reviewer}

There is currently no formal reviewer solicitation process. Current reviewers
identify reviewers from active contributors. If you are willing to become a
reviewer, you are welcome to let us know on discord.

## Developing​ {#developing}

### Setup​ {#setup}

    git clone https://github.com/microsoft/FLAML.git  
    pip install -e FLAML[notebook,autogen]  

In case the `pip install` command fails, try escaping the brackets such as
`pip install -e FLAML\[notebook,autogen\]`.

### Docker​ {#docker}

We provide a simple
[Dockerfile](https://github.com/microsoft.blob/main/Dockerfile).

    docker build https://github.com/microsoft/FLAML.git#main -t flaml-dev  
    docker run -it flaml-dev  

### Develop in Remote Container​ {#develop-in-remote-container}

If you use vscode, you can open the FLAML folder in a
[Container](https://code.visualstudio.com/docs/remote/containers). We have
provided the configuration in
[devcontainer](https://github.com/microsoft.blob/main/.devcontainer).

### Pre-commit​ {#precommit}

Run `pre-commit install` to install pre-commit into your git hooks. Before you
commit, run `pre-commit run` to check if you meet the pre-commit requirements.
If you use Windows (without WSL) and can't commit after installing pre-commit,
you can run `pre-commit uninstall` to uninstall the hook. In WSL or Linux this
is supposed to work.

### Coverage​ {#coverage}

Any code you commit should not decrease coverage. To run all unit tests,
install the [test] option under FLAML/:

    pip install -e."[test]"  
    coverage run -m pytest test  

Then you can see the coverage report by `coverage report -m` or `coverage
html`.

### Documentation​ {#documentation}

To build and test documentation locally, install
[Node.js](https://nodejs.org/en/download/). For example,

    nvm install --lts  

Then:

    npm install --global yarn  # skip if you use the dev container we provided  
    pip install pydoc-markdown==4.5.0  # skip if you use the dev container we provided  
    cd website  
    yarn install --frozen-lockfile --ignore-engines  
    pydoc-markdown  
    yarn start  

The last command starts a local development server and opens up a browser
window. Most changes are reflected live without having to restart the server.

Note: some tips in this guide are based off the contributor guide from
[ray](https://docs.ray.io/en/latest/ray-contribute/getting-involved.html),
[scikit-learn](https://scikit-learn.org/stable/developers/contributing.html),
or
[hummingbird](https://github.com/microsoft/hummingbird/blob/main/CONTRIBUTING.md).

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/Contribute.md)

[Previous« Tune - PyTorch](.docs/Examples/Tune-PyTorch)

[NextResearch »](.docs/Research)

* How to make a good bug report
* Becoming a Reviewer
* Developing
  * Setup
  * Docker
  * Develop in Remote Container
  * Pre-commit
  * Coverage
  * Documentation

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* [Getting Started](.docs/Getting-Started)
* [Installation](.docs/Installation)
* Use Cases
* Examples
  * [AutoGen - Automated Multi Agent Chat](.docs/Examples/AutoGen-AgentChat)
  * [AutoGen - Tune GPT Models](.docs/Examples/AutoGen-OpenAI)
  * [AutoML - Classification](.docs/Examples/AutoML-Classification)
  * [AutoML - NLP](.docs/Examples/AutoML-NLP)
  * [AutoML - Rank](.docs/Examples/AutoML-Rank)
  * [AutoML - Regression](.docs/Examples/AutoML-Regression)
  * [AutoML - Time Series Forecast](.docs/Examples/AutoML-Time series forecast)
  * [AutoML for LightGBM](.docs/Examples/AutoML-for-LightGBM)
  * [AutoML for XGBoost](.docs/Examples/AutoML-for-XGBoost)
  * [Default - Flamlized Estimator](.docs/Examples/Default-Flamlized)
  * [Integrate - AzureML](.docs/Examples/Integrate - AzureML)
  * [Integrate - Scikit-learn Pipeline](.docs/Examples/Integrate - Scikit-learn Pipeline)
  * [Integrate - Spark](.docs/Examples/Integrate - Spark)
  * [Tune - AzureML pipeline](.docs/Examples/Tune-AzureML-pipeline)
  * [Tune - HuggingFace](.docs/Examples/Tune-HuggingFace)
  * [Tune - Lexicographic Objectives](.docs/Examples/Tune-Lexicographic-objectives)
  * [Tune - PyTorch](.docs/Examples/Tune-PyTorch)
* [Contributing](.docs/Contribute)
* [Research](.docs/Research)

On this page

# Tune - AzureML pipeline {#tune-azureml-pipeline}

This example uses flaml to tune an Azure ML pipeline that fits a lightgbm
classifier on the [sklearn breast cancer
dataset](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+\(Diagnostic\)).
If you already have an Azure ML pipeline, you can use the approach to tune
your pipeline with flaml.

## Prepare for tuning​ {#prepare-for-tuning}

### Requirements​ {#requirements}

We recommend using conda or venv to create a virtual env to install the
dependencies.

    # set up new conda environment  
    conda create -n pipeline_tune python=3.8 pip=20.2 -y  
    conda activate pipeline_tune  
      
    # install azureml packages for runnig AzureML pipelines  
    pip install azureml-core==1.39.0  
    pip install azure-ml-component[notebooks]==0.9.10.post1  
    pip install azureml-dataset-runtime==1.39.0  
      
    # install hydra-core for passing AzureML pipeline parameters  
    pip install hydra-core==1.1.1  
      
    # install flaml  
    pip install flaml[blendsearch,ray]==1.0.9  

### Azure ML training pipeline​ {#azure-ml-training-pipeline}

Before we are ready for tuning, we must first have an Azure ML pipeline. In
this example, we use the following toy pipeline for illustration. The pipeline
consists of two steps: (1) data preparation and (2) model training.

![png](.assets/images/AzureML_train_pipeline-77c000b130274a1795f005eb627cc616.png).

The [code
example](https://github.com/microsoft.tree/main/test/pipeline_tuning_example)
discussed in the page is included in `test/pipeline_tuning_example/`. We will
use the relative path in the rest of the page.

### Data​ {#data}

The example data exsits in `data/data.csv`. It will be uploaded to AzureML
workspace to be consumed by the training pipeline using the following code.

    Dataset.File.upload_directory(  
        src_dir=to_absolute_path(LOCAL_DIR / "data"),  
        target=(datastore, "classification_data"),  
        overwrite=True,  
    )  
      
    dataset = Dataset.File.from_files(path=(datastore, 'classification_data'))  

### Configurations for the pipeline​ {#configurations-for-the-pipeline}

The pipeline configuration is defined in `configs/train_config.yaml`.

    hydra:  
      searchpath:  
        - file://.  
      
    aml_config:  
      workspace_name: your_workspace_name  
      resource_group: your_resource_group  
      subscription_id: your_subscription_id  
      cpu_target: cpucluster  
      
    train_config:  
      exp_name: sklearn_breast_cancer_classification  
      test_train_ratio: 0.4  
      learning_rate: 0.05  
      n_estimators: 50  

### Define and submit the pipeline​ {#define-and-submit-the-pipeline}

The pipeline was defined in `submit_train_pipeline.py`.

To submit the pipeline, please specify your AzureML resources in the
`configs/train_config.yaml` and run

    cd test/pipeline_tuning_example  
    python submit_train_pipeline.py  

To get the pipeline ready for HPO, in the training step, we need to log the
metrics of interest to AzureML using

    run.log(f"{data_name}_{eval_name}", result)  

## Hyperparameter Optimization​ {#hyperparameter-optimization}

We are now ready to set up the HPO job for the AzureML pipeline, including:

* config the HPO job,
* set up the interaction between the HPO job and the training job.

These two steps are done in `tuner/tuner_func.py`.

### Set up the tune job​ {#set-up-the-tune-job}

`tuner_func.tune_pipeline` sets up the search space, metric to optimize, mode,
etc.
```python
    def tune_pipeline(concurrent_run=1):  
        start_time = time.time()  
      
        # config the HPO job  
        search_space = {  
            "train_config.n_estimators": flaml.tune.randint(50, 200),  
            "train_config.learning_rate": flaml.tune.uniform(0.01, 0.5),  
        }  
      
        hp_metric = "eval_binary_error"  
        mode = "max"  
        num_samples = 2  
      
      
        if concurrent_run > 1:  
            import ray  # For parallel tuning  
      
            ray.init(num_cpus=concurrent_run)  
            use_ray = True  
        else:  
            use_ray = False  
      
        # launch the HPO job  
        analysis = flaml.tune.run(  
            run_with_config,  
            config=search_space,  
            metric=hp_metric,  
            mode=mode,  
            num_samples=num_samples,  # number of trials  
            use_ray=use_ray,  
        )  
      
        # get the best config  
        best_trial = analysis.get_best_trial(hp_metric, mode, "all")  
        metric = best_trial.metric_analysis[hp_metric][mode]  
        print(f"n_trials={len(analysis.trials)}")  
        print(f"time={time.time()-start_time}")  
        print(f"Best {hp_metric}: {metric:.4f}")  
        print(f"Best coonfiguration: {best_trial.config}")  

### Interact with AzureML pipeline jobs​ {#interact-with-azureml-pipeline-jobs}

The interaction between FLAML and AzureML pipeline jobs is in
`tuner_func.run_with_config`.
```python
    def run_with_config(config: dict):  
        """Run the pipeline with a given config dict  
        """  
      
        # pass the hyperparameters to AzureML jobs by overwriting the config file.  
        overrides = [f"{key}={value}" for key, value in config.items()]  
      
        print(overrides)  
        run = submit_train_pipeline.build_and_submit_aml_pipeline(overrides)  
      
        print(run.get_portal_url())  
      
        # retrieving the metrics to optimize before the job completes.  
        stop = False  
        while not stop:  
            # get status  
            status = run._core_run.get_status()  
            print(f'status: {status}')  
      
            # get metrics  
            metrics = run._core_run.get_metrics(recursive=True)  
            if metrics:  
                run_metrics = list(metrics.values())  
      
                new_metric = run_metrics[0]['eval_binary_error']  
      
                if type(new_metric) == list:  
                    new_metric = new_metric[-1]  
      
                print(f'eval_binary_error: {new_metric}')  
      
                tune.report(eval_binary_error=new_metric)  
      
            time.sleep(5)  
      
            if status == 'FAILED' or status == 'Completed':  
                stop = True  
      
        print("The run is terminated.")  
        print(status)  
      
        return  

Overall, to tune the hyperparameters of the AzureML pipeline, run:

    # the training job will run remotely as an AzureML job in both choices  
    # run the tuning job locally  
    python submit_tune.py --local  
    # run the tuning job remotely  
    python submit_tune.py --remote --subscription_id <your subscription_id> --resource_group <your resource_group> --workspace <your workspace>  

The local option runs the `tuner/tuner_func.py` in your local machine. The
remote option wraps up the `tuner/tuner_func.py` as an AzureML component and
starts another AzureML job to tune the AzureML pipeline.

[Edit this
page](<https://github.com/microsoft.edit/main/website/docs/Examples/Tune->
AzureML-pipeline.md)

[Previous« Integrate - Spark](.docs/Examples/Integrate - Spark)

[NextTune - HuggingFace »](.docs/Examples/Tune-HuggingFace)

* Prepare for tuning
  * Requirements
  * Azure ML training pipeline
  * Data
  * Configurations for the pipeline
  * Define and submit the pipeline
* Hyperparameter Optimization
  * Set up the tune job
  * Interact with AzureML pipeline jobs

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
    * autogen.agentchat
    * autogen.oai
    * [code_utils](.docs/reference/autogen/code_utils)
    * [math_utils](.docs/reference/autogen/math_utils)
    * [retrieve_utils](.docs/reference/autogen/retrieve_utils)
  * automl
  * default
  * onlineml
  * tune

On this page

# autogen.code_utils {#autogencodeutils}

#### infer_lang​ {#inferlang}
```python
    def infer_lang(code)  

infer the language for the code. TODO: make it robust.

#### extract_code​ {#extractcode}
```python
    def extract_code(text: str, pattern: str = CODE_BLOCK_PATTERN) -> List[Tuple[str, str]]  

Extract code from a text.

 **Arguments** :

* `text` _str_ \- The text to extract code from.
* `pattern` _Optional, str_ \- The regular expression pattern for finding the code block.

 **Returns** :

* `list` \- A list of tuples, each containing the language and the code.

#### generate_code​ {#generatecode}
```python
    def generate_code(pattern: str = CODE_BLOCK_PATTERN, **config) -> Tuple[str, float]  

Generate code.

 **Arguments** :

* `pattern` _Optional, str_ \- The regular expression pattern for finding the code block. The default pattern is for finding a code block in a markdown file.
* `config` _Optional, dict_ \- The configuration for the API call.

 **Returns** :

* `str` \- The generated code.
* `float` \- The cost of the generation.

#### improve_function​ {#improvefunction}
```python
    def improve_function(file_name, func_name, objective, **config)  

(work in progress) Improve the function to achieve the objective.

#### improve_code​ {#improvecode}
```python
    def improve_code(files, objective, suggest_only=True, **config)  

Improve the code to achieve a given objective.

 **Arguments** :

* `files` _list_ \- A list of file names containing the source code.
* `objective` _str_ \- The objective to achieve.
* `suggest_only` _bool_ \- Whether to return only the suggestions or the improved code.
* `config` _Optional, dict_ \- The configuration for the API call.

 **Returns** :

* `str` \- The improved code if suggest_only=False; a list of suggestions if suggest_only=True (default).
* `float` \- The cost of the generation.

#### execute_code​ {#executecode}
```python
    def execute_code(code: Optional[str] = None, timeout: Optional[int] = None, filename: Optional[str] = None, work_dir: Optional[str] = None, use_docker: Optional[Union[List[str], str, bool]] = docker is not None, lang: Optional[str] = "python") -> Tuple[int, str, str]  

Execute code in a docker container. This function is not tested on MacOS.

 **Arguments** :

* `code` _Optional, str_ \- The code to execute. If None, the code from the file specified by filename will be executed. Either code or filename must be provided.
* `timeout` _Optional, int_ \- The maximum execution time in seconds. If None, a default timeout will be used. The default timeout is 600 seconds. On Windows, the timeout is not enforced when use_docker=False.
* `filename` _Optional, str_ \- The file name to save the code or where the code is stored when `code` is None. If None, a file with a randomly generated name will be created. The randomly generated file will be deleted after execution. The file name must be a relative path. Relative paths are relative to the working directory.
* `work_dir` _Optional, str_ \- The working directory for the code execution. If None, a default working directory will be used. The default working directory is the "extensions" directory under "path_to_flaml/autogen".
* `use_docker` _Optional, list, str or bool_ \- The docker image to use for code execution. If a list or a str of image name(s) is provided, the code will be executed in a docker container with the first image successfully pulled. If None, False or empty, the code will be executed in the current environment. Default is True, which will be converted into a list. If the code is executed in the current environment, the code must be trusted.
* `lang` _Optional, str_ \- The language of the code. Default is "python".

 **Returns** :

* `int` \- 0 if the code executes successfully.
* `str` \- The error message if the code fails to execute; the stdout otherwise.
* `image` \- The docker image name after container run when docker is used.

#### generate_assertions​ {#generateassertions}
```python
    def generate_assertions(definition: str, **config) -> Tuple[str, float]  

Generate assertions for a function.

 **Arguments** :

* `definition` _str_ \- The function definition, including the signature and docstr.
* `config` _Optional, dict_ \- The configuration for the API call.

 **Returns** :

* `str` \- The generated assertions.
* `float` \- The cost of the generation.

#### eval_function_completions​ {#evalfunctioncompletions}
```python
    def eval_function_completions(responses: List[str], definition: str, test: Optional[str] = None, entry_point: Optional[str] = None, assertions: Optional[Union[str, Callable[[str], Tuple[str, float]]]] = None, timeout: Optional[float] = 3, use_docker: Optional[bool] = True) -> Dict  

Select a response from a list of responses for the function completion task
(using generated assertions), and/or evaluate if the task is successful using
a gold test.

 **Arguments** :

* `responses` _list_ \- The list of responses.
* `definition` _str_ \- The input definition.
* `test` _Optional, str_ \- The test code.
* `entry_point` _Optional, str_ \- The name of the function.
* `assertions` _Optional, str or Callable_ \- The assertion code which serves as a filter of the responses, or an assertion generator. When provided, only the responses that pass the assertions will be considered for the actual test (if provided).
* `timeout` _Optional, float_ \- The timeout for executing the code.

 **Returns** :

* `dict` \- The success metrics.

## PassAssertionFilter Objects​ {#passassertionfilter-objects}

    class PassAssertionFilter()  

#### pass_assertions​ {#passassertions}
```python
    def pass_assertions(context, response, **_)  

Check if the response passes the assertions.

#### implement​ {#implement}
```python
    def implement(definition: str, configs: Optional[List[Dict]] = None, assertions: Optional[Union[str, Callable[[str], Tuple[str, float]]]] = generate_assertions) -> Tuple[str, float]  

Implement a function from a definition.

 **Arguments** :

* `definition` _str_ \- The function definition, including the signature and docstr.
* `configs` _list_ \- The list of configurations for completion.
* `assertions` _Optional, str or Callable_ \- The assertion code which serves as a filter of the responses, or an assertion generator.

 **Returns** :

* `str` \- The implementation.
* `float` \- The cost of the implementation.
* `int` \- The index of the configuration which generates the implementation.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/autogen/code_utils.md)

[Previous« openai_utils](.docs/reference/autogen/oai/openai_utils)

[Nextmath_utils »](.docs/reference/autogen/math_utils)

* PassAssertionFilter Objects

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* [Getting Started](.docs/Getting-Started)
* [Installation](.docs/Installation)
* Use Cases
* Examples
  * [AutoGen - Automated Multi Agent Chat](.docs/Examples/AutoGen-AgentChat)
  * [AutoGen - Tune GPT Models](.docs/Examples/AutoGen-OpenAI)
  * [AutoML - Classification](.docs/Examples/AutoML-Classification)
  * [AutoML - NLP](.docs/Examples/AutoML-NLP)
  * [AutoML - Rank](.docs/Examples/AutoML-Rank)
  * [AutoML - Regression](.docs/Examples/AutoML-Regression)
  * [AutoML - Time Series Forecast](.docs/Examples/AutoML-Time series forecast)
  * [AutoML for LightGBM](.docs/Examples/AutoML-for-LightGBM)
  * [AutoML for XGBoost](.docs/Examples/AutoML-for-XGBoost)
  * [Default - Flamlized Estimator](.docs/Examples/Default-Flamlized)
  * [Integrate - AzureML](.docs/Examples/Integrate - AzureML)
  * [Integrate - Scikit-learn Pipeline](.docs/Examples/Integrate - Scikit-learn Pipeline)
  * [Integrate - Spark](.docs/Examples/Integrate - Spark)
  * [Tune - AzureML pipeline](.docs/Examples/Tune-AzureML-pipeline)
  * [Tune - HuggingFace](.docs/Examples/Tune-HuggingFace)
  * [Tune - Lexicographic Objectives](.docs/Examples/Tune-Lexicographic-objectives)
  * [Tune - PyTorch](.docs/Examples/Tune-PyTorch)
* [Contributing](.docs/Contribute)
* [Research](.docs/Research)

On this page

# Integrate - Spark {#integrate-spark}

FLAML has integrated Spark for distributed training. There are two main
aspects of integration with Spark:

* Use Spark ML estimators for AutoML.
* Use Spark to run training in parallel spark jobs.

## Spark ML Estimators​ {#spark-ml-estimators}

FLAML integrates estimators based on Spark ML models. These models are trained
in parallel using Spark, so we called them Spark estimators. To use these
models, you first need to organize your data in the required format.

### Data​ {#data}

For Spark estimators, AutoML only consumes Spark data. FLAML provides a
convenient function `to_pandas_on_spark` in the `flaml.automl.spark.utils`
module to convert your data into a pandas-on-spark (`pyspark.pandas`)
dataframe/series, which Spark estimators require.

This utility function takes data in the form of a `pandas.Dataframe` or
`pyspark.sql.Dataframe` and converts it into a pandas-on-spark dataframe. It
also takes `pandas.Series` or `pyspark.sql.Dataframe` and converts it into a
[pandas-on-
spark](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html)
series. If you pass in a `pyspark.pandas.Dataframe`, it will not make any
changes.

This function also accepts optional arguments `index_col` and
`default_index_type`.

* `index_col` is the column name to use as the index, default is None.
* `default_index_type` is the default index type, default is "distributed-sequence". More info about default index type could be found on Spark official [documentation](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/options.html#default-index-type)

Here is an example code snippet for Spark Data:

    import pandas as pd  
    from flaml.automl.spark.utils import to_pandas_on_spark  
    # Creating a dictionary  
    data = {"Square_Feet": [800, 1200, 1800, 1500, 850],  
          "Age_Years": [20, 15, 10, 7, 25],  
          "Price": [100000, 200000, 300000, 240000, 120000]}  
      
    # Creating a pandas DataFrame  
    dataframe = pd.DataFrame(data)  
    label = "Price"  
      
    # Convert to pandas-on-spark dataframe  
    psdf = to_pandas_on_spark(dataframe)  

To use Spark ML models you need to format your data appropriately.
Specifically, use
[`VectorAssembler`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html)
to merge all feature columns into a single vector column.

Here is an example of how to use it:

    from pyspark.ml.feature import VectorAssembler  
    columns = psdf.columns  
    feature_cols = [col for col in columns if col != label]  
    featurizer = VectorAssembler(inputCols=feature_cols, outputCol="features")  
    psdf = featurizer.transform(psdf.to_spark(index_col="index"))["index", "features"]  

Later in conducting the experiment, use your pandas-on-spark data like non-
spark data and pass them using `X_train, y_train` or `dataframe, label`.

### Estimators​ {#estimators}

#### Model List​ {#model-list}

* `lgbm_spark`: The class for fine-tuning Spark version LightGBM models, using [SynapseML](https://microsoft.github.io/SynapseML/docs/features/lightgbm/about/) API.

#### Usage​ {#usage}

First, prepare your data in the required format as described in the previous
section.

By including the models you intend to try in the `estimators_list` argument to
`flaml.automl`, FLAML will start trying configurations for these models. If
your input is Spark data, FLAML will also use estimators with the `_spark`
postfix by default, even if you haven't specified them.

Here is an example code snippet using SparkML models in AutoML:

    import flaml  
    # prepare your data in pandas-on-spark format as we previously mentioned  
      
    automl = flaml.AutoML()  
    settings = {  
        "time_budget": 30,  
        "metric": "r2",  
        "estimator_list": ["lgbm_spark"],  # this setting is optional  
        "task": "regression",  
    }  
      
    automl.fit(  
        dataframe=psdf,  
        label=label,  
        **settings,  
    )  

[Link to
notebook](https://github.com/microsoft.blob/main/notebook/automl_bankrupt_synapseml.ipynb)
| [Open in
colab](https://colab.research.google.com/github/microsoft.blob/main/notebook/automl_bankrupt_synapseml.ipynb)

## Parallel Spark Jobs​ {#parallel-spark-jobs}

You can activate Spark as the parallel backend during parallel tuning in both
[AutoML](.docs/Use-Cases/Task-Oriented-AutoML#parallel-tuning) and
[Hyperparameter Tuning](.docs/Use-Cases/Tune-User-Defined-
Function#parallel-tuning), by setting the `use_spark` to `true`. FLAML will
dispatch your job to the distributed Spark backend using [`joblib-
spark`](https://github.com/joblib/joblib-spark).

Please note that you should not set `use_spark` to `true` when applying AutoML
and Tuning for Spark Data. This is because only SparkML models will be used
for Spark Data in AutoML and Tuning. As SparkML models run in parallel, there
is no need to distribute them with `use_spark` again.

All the Spark-related arguments are stated below. These arguments are
available in both Hyperparameter Tuning and AutoML:

* `use_spark`: boolean, default=False | Whether to use spark to run the training in parallel spark jobs. This can be used to accelerate training on large models and large datasets, but will incur more overhead in time and thus slow down training in some cases. GPU training is not supported yet when use_spark is True. For Spark clusters, by default, we will launch one trial per executor. However, sometimes we want to launch more trials than the number of executors (e.g., local mode). In this case, we can set the environment variable `FLAML_MAX_CONCURRENT` to override the detected `num_executors`. The final number of concurrent trials will be the minimum of `n_concurrent_trials` and `num_executors`.
* `n_concurrent_trials`: int, default=1 | The number of concurrent trials. When n_concurrent_trials > 1, FLAML performes parallel tuning.
* `force_cancel`: boolean, default=False | Whether to forcely cancel Spark jobs if the search time exceeded the time budget. Spark jobs include parallel tuning jobs and Spark-based model training jobs.

An example code snippet for using parallel Spark jobs:

    import flaml  
    automl_experiment = flaml.AutoML()  
    automl_settings = {  
        "time_budget": 30,  
        "metric": "r2",  
        "task": "regression",  
        "n_concurrent_trials": 2,  
        "use_spark": True,  
        "force_cancel": True, # Activating the force_cancel option can immediately halt Spark jobs once they exceed the allocated time_budget.  
    }  
      
    automl.fit(  
        dataframe=dataframe,  
        label=label,  
        **automl_settings,  
    )  

[Link to
notebook](https://github.com/microsoft.blob/main/notebook/integrate_spark.ipynb)
| [Open in
colab](https://colab.research.google.com/github/microsoft.blob/main/notebook/integrate_spark.ipynb)

[Edit this
page](<https://github.com/microsoft.edit/main/website/docs/Examples/Integrate>

* Spark.md)

[Previous« Integrate - Scikit-learn Pipeline](.docs/Examples/Integrate -
Scikit-learn Pipeline)

[NextTune - AzureML pipeline »](.docs/Examples/Tune-AzureML-pipeline)

* Spark ML Estimators
  * Data
  * Estimators
* Parallel Spark Jobs

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

Recent posts

* [All About Agent Descriptions](/autogen/blog/2023/12/29/AgentDescriptions)
* [AgentOptimizer - An Agentic Way to Train Your LLM Agent](/autogen/blog/2023/12/23/AgentOptimizer)
* [AutoGen Studio: Interactively Explore Multi-Agent Workflows](/autogen/blog/2023/12/01/AutoGenStudio)
* [Agent AutoBuild - Automatically Building Multi-agent Systems](/autogen/blog/2023/11/26/Agent-AutoBuild)
* [How to Assess Utility of LLM-powered Applications?](/autogen/blog/2023/11/20/AgentEval)
* [AutoGen Meets GPTs](/autogen/blog/2023/11/13/OAI-assistants)
* [EcoAssistant - Using LLM Assistants More Accurately and Affordably](/autogen/blog/2023/11/09/EcoAssistant)
* [Multimodal with GPT-4V and LLaVA](/autogen/blog/2023/11/06/LMM-Agent)
* [AutoGen's Teachable Agents](/autogen/blog/2023/10/26/TeachableAgent)
* [Retrieval-Augmented Generation (RAG) Applications with AutoGen](/autogen/blog/2023/10/18/RetrieveChat)
* [Use AutoGen for Local LLMs](/autogen/blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](/autogen/blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](/autogen/blog/2023/05/18/GPT-adaptive-humaneval)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](/autogen/blog/2023/04/21/LLM-tuning-math)

# AgentOptimizer - An Agentic Way to Train Your LLM Agent {#agentoptimizer-an-agentic-way-to-train-your-llm-agent}

December 23, 2023 · 7 min read

[![Shaokun
Zhang](https://github.com/skzhang1.png)](https://github.com/skzhang1)

[Shaokun Zhang](https://github.com/skzhang1)

PhD student at the Pennsylvania State University

[![Jieyu Zhang](https://github.com/jieyuz2.png)](https://jieyuz2.github.io/)

[Jieyu Zhang](https://jieyuz2.github.io/)

PhD student at University of Washington

![Overall structure of
AgentOptimizer](/autogen/assets/images/agentoptimizer-33b265dcca6cff56bbf0c9d63ff389f9.png)

 **TL;DR:** Introducing **AgentOptimizer** , a new class for training LLM
agents in the era of LLMs as a service. **AgentOptimizer** is able to prompt
autogen agents to iteratively optimize its function/skills according to the
historical conversation and performance. Checkout one implementation for
**AgentOptimizer** on [MATH](https://github.com/hendrycks/math) dataset
[here](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_agentoptimizer.ipynb).

Paper is coming soon!

## Introduction​ {#introduction}

In traditional ML pipeline, we train a model by updating its weights according
to the loss on the training set, while in the era of LLM agents, how should we
train an agent? Here, we take an initial step towards the agent training.
Inspired by the [function
calling](https://platform.openai.com/docs/guides/function-calling)
capabilities provided by OpenAI, we draw an analogy between model weights and
agent functions/skills, and update an agent’s functions/skills based on its
historical performance on a training set. Specifically, we propose to use the
function calling capabilities to formulate the actions that optimize the
agents’ functions as a set of function calls, to support iteratively **adding,
revising, and removing** existing functions. As an agentic way of training an
agent, our approach helps enhance the agents’ abilities without requiring
access to the LLMs weights.

## AgentOptimizer​ {#agentoptimizer}

 **AgentOptimizer** is a class designed to optimize the agents by improving
their function calls. It contains two core methods:

  1. `step()`: `step()` takes three inputs, including the previous conversation history (history), the statistical information of solving previous problems (statistic), and the current functions (current_functions).

    actions, updated_functions = AgentOptimizer.step(history, statistic, current_functions)  

It has two outputs `actions` and `updated_functions`. `actions` is a series of
actions to manipulate the current functions. And `updated_functions` is the
updated functions after the actions are applied (including code
implementation).

  2. `update_function_call()`: This method takes the agents and actions as input. It updates the functions registered in these agents according to the actions from `step()`. For AssistantAgent, it first uses [update_function_signature](https://microsoft.github.io/autogen/docs/reference/agentchat/conversable_agent/#update_function_signature) to update the function signatures. Then, it updates the functions in the MathUserproxyAgent with the corresponding code implementation gained from `step()`.

Sometimes, the function signatures (JSON schema) returned by the `step()` may
not be valid, and the generated code may also face syntax errors.
**AgentOptimizer** includes mechanisms to check the (1) validity of the
function signatures and (2) code implementation before updating the functions.
Moreover, it also includes mechanisms to check whether each action is
feasible, such as avoiding the removal of a function that is not in the
current functions due to hallucination.

## Pseudocode for the optimization process​ {#pseudocode-for-the-optimization-process}

The optimization process is as follows:

    for - in range(EPOCH):  
        history, statistic, current_functions = solve_problems(train_problems)  
        actions, updated_functions = AgentOptimizer.step(history, statistic, current_functions)  
        AgentOptimizer.update_function_call(actions)  

Given a prepared training dataset, the agents iteratively solve problems from
the training set to obtain conversation history and statistical information.
The functions are then improved using AgentOptimizer. Each iteration can be
regarded as one training step analogous to traditional machine learning, with
the optimization elements being the functions that agents have. After EPOCH
iterations, the agents are expected to obtain better functions that may be
used in future tasks

## The implementation technology behind the AgentOptimizer​ {#the-implementation-technology-behind-the-agentoptimizer}

To obtain stable and structured function signatures and code implementations
from AgentOptimizer, we leverage the function calling capabilities provided by
OpenAI to formulate the actions that manipulate the functions as a set of
function calls. Specifically, we introduce three function calls to manipulate
the current functions at each step: `add_function`, `remove_function`, and
`revise_function`. These calls add, remove, and revise functions in the
existing function list, respectively. This practice could fully leverages the
function calling capabilities of GPT-4 and outputs structured functions with
more stable signatures and code implementation. Below is the JSON schema of
these function calls:

  1. `add_function`: Add one new function that may be used in the future tasks.

    ADD_FUNC = {  
        "type": "function",  
        "function": {  
            "name": "add_function",  
            "description": "Add a function in the context of the conversation. Necessary Python packages must be declared. The name of the function MUST be the same with the function name in the code you generated.",  
            "parameters": {  
                "type": "object",  
                "properties": {  
                    "name": {  
                        "type": "string",  
                        "description": "The name of the function in the code implementation."  
                    },  
                    "description": {  
                        "type": "string",  
                        "description": "A short description of the function."  
                    },  
                    "arguments": {  
                        "type": "string",  
                        "description": "JSON schema of arguments encoded as a string. Please note that the JSON schema only supports specific types including string, integer, object, array, boolean. (do not have float type) For example: { \"url\": { \"type\": \"string\", \"description\": \"The URL\", }}. Please avoid the error 'array schema missing items' when using array type."  
                    },  
                    "packages": {  
                        "type": "string",  
                        "description": "A list of package names imported by the function, and that need to be installed with pip prior to invoking the function. This solves ModuleNotFoundError. It should be string, not list."  
                    },  
                    "code": {  
                        "type": "string",  
                        "description": "The implementation in Python. Do not include the function declaration."  
                    }  
                },  
                "required": ["name", "description", "arguments", "packages", "code"]  
            }  
        }  
    }  

  2. `revise_function`: Revise one existing function (code implementation, function signature) in the current function list according to the conversation history and performance.

    REVISE_FUNC = {  
        "type": "function",  
        "function": {  
            "name": "revise_function",  
            "description": "Revise a function in the context of the conversation. Necessary Python packages must be declared. The name of the function MUST be the same with the function name in the code you generated.",  
            "parameters": {  
                "type": "object",  
                "properties": {  
                    "name": {  
                        "type": "string",  
                        "description": "The name of the function in the code implementation."  
                    },  
                    "description": {  
                        "type": "string",  
                        "description": "A short description of the function."  
                    },  
                    "arguments": {  
                        "type": "string",  
                        "description": "JSON schema of arguments encoded as a string. Please note that the JSON schema only supports specific types including string, integer, object, array, boolean. (do not have float type) For example: { \"url\": { \"type\": \"string\", \"description\": \"The URL\", }}. Please avoid the error 'array schema missing items' when using array type."  
                    },  
                    "packages": {  
                        "type": "string",  
                        "description": "A list of package names imported by the function, and that need to be installed with pip prior to invoking the function. This solves ModuleNotFoundError. It should be string, not list."  
                    },  
                    "code": {  
                        "type": "string",  
                        "description": "The implementation in Python. Do not include the function declaration."  
                    }  
                },  
                "required": ["name", "description", "arguments", "packages", "code"]  
            }  
        }  
    }  

  3. `remove_function`: Remove one existing function in the current function list. It is used to remove the functions that are not useful (redundant) in the future tasks.

    REMOVE_FUNC = {  
        "type": "function",  
        "function": {  
            "name": "remove_function",  
            "description": "Remove one function in the context of the conversation. Once remove one function, the assistant will not use this function in future conversation.",  
            "parameters": {  
                "type": "object",  
                "properties": {  
                    "name": {  
                        "type": "string",  
                        "description": "The name of the function in the code implementation."  
                    }  
                },  
                "required": ["name"]  
            }  
        }  
    }  

## Limitation & Future work​ {#limitation-future-work}

  1. Unlike gradient descent in traditional machine learning training processes, each optimization step does not necessarily lead to better performance on the training set. When the training epoch is small, the agent’s performance may even decrease. One urgent task is to design a better mechanism to guide the optimization process.
  2. The [Current implementation](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_agentoptimizer.ipynb) of AgentOptimizer is mainly for illustration purpose and is just a proof of concept. It is not formally integrated into the autogen with a general interface like optimizing any kinds of agents in any tasks. Currently, it only supports optimizing the multi-agent system in solving problems from [MATH](https://github.com/hendrycks/math) dataset. We will integrate it into autogen with more general interface in the future.

 **Tags:**

* [LLM](/autogen/blog/tags/llm)
* [research](/autogen/blog/tags/research)

[Newer Post« All About Agent
Descriptions](/autogen/blog/2023/12/29/AgentDescriptions)

[Older PostAutoGen Studio: Interactively Explore Multi-Agent Workflows
»](/autogen/blog/2023/12/01/AutoGenStudio)

* Introduction
* AgentOptimizer
* Pseudocode for the optimization process
* The implementation technology behind the AgentOptimizer
* Limitation & Future work

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

Recent posts

* [All About Agent Descriptions](/autogen/blog/2023/12/29/AgentDescriptions)
* [AgentOptimizer - An Agentic Way to Train Your LLM Agent](/autogen/blog/2023/12/23/AgentOptimizer)
* [AutoGen Studio: Interactively Explore Multi-Agent Workflows](/autogen/blog/2023/12/01/AutoGenStudio)
* [Agent AutoBuild - Automatically Building Multi-agent Systems](/autogen/blog/2023/11/26/Agent-AutoBuild)
* [How to Assess Utility of LLM-powered Applications?](/autogen/blog/2023/11/20/AgentEval)
* [AutoGen Meets GPTs](/autogen/blog/2023/11/13/OAI-assistants)
* [EcoAssistant - Using LLM Assistants More Accurately and Affordably](/autogen/blog/2023/11/09/EcoAssistant)
* [Multimodal with GPT-4V and LLaVA](/autogen/blog/2023/11/06/LMM-Agent)
* [AutoGen's Teachable Agents](/autogen/blog/2023/10/26/TeachableAgent)
* [Retrieval-Augmented Generation (RAG) Applications with AutoGen](/autogen/blog/2023/10/18/RetrieveChat)
* [Use AutoGen for Local LLMs](/autogen/blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](/autogen/blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](/autogen/blog/2023/05/18/GPT-adaptive-humaneval)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](/autogen/blog/2023/04/21/LLM-tuning-math)

# How to Assess Utility of LLM-powered Applications? {#how-to-assess-utility-of-llmpowered-applications}

November 20, 2023 · 10 min read

[![Julia
Kiseleva](https://avatars.githubusercontent.com/u/5908392?v=4)](https://github.com/julianakiseleva/)

[Julia Kiseleva](https://github.com/julianakiseleva/)

Senior Researcher at Microsoft Research

[![Negar Arabzadeh](https://github.com/Narabzad.png)](https://www.negara.me/)

[Negar Arabzadeh](https://www.negara.me/)

PhD student at the University of Waterloo

![Fig.1: A verification framework](/autogen/assets/images/agenteval-
CQ-59f82d98878045817d02a3915839a92a.png)

 _Fig.1 illustrates the general flow of AgentEval_

 **TL;DR:**

* As a developer of an LLM-powered application, how can you assess the utility it brings to end users while helping them with their tasks?
* To shed light on the question above, we introduce `AgentEval` — the first version of the framework to assess the utility of any LLM-powered application crafted to assist users in specific tasks. AgentEval aims to simplify the evaluation process by automatically proposing a set of criteria tailored to the unique purpose of your application. This allows for a comprehensive assessment, quantifying the utility of your application against the suggested criteria.
* We demonstrate how `AgentEval` work using [math problems dataset](https://microsoft.github.io/autogen/blog/2023/06/28/MathChat) as an example in the [following notebook](https://github.com/microsoft/autogen/blob/main/notebook/agenteval_cq_math.ipynb). Any feedback would be useful for future development. Please contact us on our [Discord](http://aka.ms/autogen-dc).

## Introduction​ {#introduction}

AutoGen aims to simplify the development of LLM-powered multi-agent systems
for various applications, ultimately making end users' lives easier by
assisting with their tasks. Next, we all yearn to understand how our developed
systems perform, their utility for users, and, perhaps most crucially, how we
can enhance them. Directly evaluating multi-agent systems poses challenges as
current approaches predominantly rely on success metrics – essentially,
whether the agent accomplishes tasks. However, comprehending user interaction
with a system involves far more than success alone. Take math problems, for
instance; it's not merely about the agent solving the problem. Equally
significant is its ability to convey solutions based on various criteria,
including completeness, conciseness, and the clarity of the provided
explanation. Furthermore, success isn't always clearly defined for every task.

Rapid advances in LLMs and multi-agent systems have brought forth many
emerging capabilities that we're keen on translating into tangible utilities
for end users. We introduce the first version of `AgentEval` framework - a
tool crafted to empower developers in swiftly gauging the utility of LLM-
powered applications designed to help end users accomplish the desired task.

![Fig.2: An overview of the tasks taxonomy](/autogen/assets/images/tasks-
taxonomy-c4b7c6fe465e12dddde9235c3ee8c281.png)

 _Fig. 2 provides an overview of the tasks taxonomy_

Let's first look into an overview of the suggested task taxonomy that a multi-
agent system can be designed for. In general, the tasks can be split into two
types, where:

* _Success is not clearly defined_ \- refer to instances when users utilize a system in an assistive manner, seeking suggestions rather than expecting the system to solve the task. For example, a user might request the system to generate an email. In many cases, this generated content serves as a template that the user will later edit. However, defining success precisely for such tasks is relatively complex.
* _Success is clearly defined_ \- refer to instances where we can clearly define whether a system solved the task or not. Consider agents that assist in accomplishing household tasks, where the definition of success is clear and measurable. This category can be further divided into two separate subcategories:
    *_The optimal solution exits_ \- these are tasks where only one solution is possible. For example, if you ask your assistant to turn on the light, the success of this task is clearly defined, and there is only one way to accomplish it.
  * _Multiple solutions exist_ \- increasingly, we observe situations where multiple trajectories of agent behavior can lead to either success or failure. In such cases, it is crucial to differentiate between the various successful and unsuccessful trajectories. For example, when you ask the agent to suggest you a food recipe or tell you a joke.

In our `AgentEval` framework, we are currently focusing on tasks where
_Success is clearly defined_. Next, we will introduce the suggested framework.

## `AgentEval` Framework​ {#agenteval-framework}

Our previous research on [assistive agents in
Minecraft](https://github.com/microsoft/iglu-datasets) suggested that the most
optimal way to obtain human judgments is to present humans with two agents
side by side and ask for preferences. In this setup of pairwise comparison,
humans can develop criteria to explain why they prefer the behavior of one
agent over another. For instance, _' the first agent was faster in
execution,'_ or _' the second agent moves more naturally.'_ So, the
comparative nature led humans to come up with a list of criteria that helps to
infer the utility of the task. With this idea in mind, we designed `AgentEval`
(shown in Fig. 1), where we employ LLMs to help us understand, verify, and
assess task _utility_ for the multi-agent system. Namely:

* The goal of `CriticAgent` is to suggest the list of criteria (Fig. 1), that can be used to assess task utility. This is an example of how `CriticAgent` is defined using `Autogen`:

    critic = autogen.AssistantAgent(  
        name="critic",  
        llm_config={"config_list": config_list},  
        system_message="""You are a helpful assistant. You suggest criteria for evaluating different tasks. They should be distinguishable, quantifiable, and not redundant.  
        Convert the evaluation criteria into a dictionary where the keys are the criteria.  
        The value of each key is a dictionary as follows {"description": criteria description, "accepted_values": possible accepted inputs for this key}  
        Make sure the keys are criteria for assessing the given task. "accepted_values" include the acceptable inputs for each key that are fine-grained and preferably multi-graded levels. "description" includes the criterion description.  
        Return only the dictionary."""  
    )  

Next, the critic is given successful and failed examples of the task
execution; then, it is able to return a list of criteria (Fig. 1). For
reference, use the [following
notebook](https://github.com/microsoft/autogen/blob/main/notebook/agenteval_cq_math.ipynb).

* The goal of `QuantifierAgent` is to quantify each of the suggested criteria (Fig. 1), providing us with an idea of the utility of this system for the given task. Here is an example of how it can be defined:

    quantifier = autogen.AssistantAgent(  
        name="quantifier",  
        llm_config={"config_list": config_list},  
        system_message = """You are a helpful assistant. You quantify the output of different tasks based on the given criteria.  
        The criterion is given in a dictionary format where each key is a distinct criteria.  
        The value of each key is a dictionary as follows {"description": criteria description , "accepted_values": possible accepted inputs for this key}  
        You are going to quantify each of the criteria for a given task based on the task description.  
        Return a dictionary where the keys are the criteria and the values are the assessed performance based on accepted values for each criteria.  
        Return only the dictionary."""  

    )  

## `AgentEval` Results based on Math Problems Dataset​ {#agenteval-results-based-on-math-problems-dataset}

As an example, after running CriticAgent, we obtained the following criteria
to verify the results for math problem dataset:

Criteria| Description| Accepted Values  
---|---|---  
Problem Interpretation| Ability to correctly interpret the problem|
["completely off", "slightly relevant", "relevant", "mostly accurate",
"completely accurate"]  
Mathematical Methodology| Adequacy of the chosen mathematical or algorithmic
methodology for the question| ["inappropriate", "barely adequate", "adequate",
"mostly effective", "completely effective"]  
Calculation Correctness| Accuracy of calculations made and solutions given|
["completely incorrect", "mostly incorrect", "neither", "mostly correct",
"completely correct"]  
Explanation Clarity| Clarity and comprehensibility of explanations, including
language use and structure| ["not at all clear", "slightly clear", "moderately
clear", "very clear", "completely clear"]  
Code Efficiency| Quality of code in terms of efficiency and elegance| ["not at
all efficient", "slightly efficient", "moderately efficient", "very
efficient", "extremely efficient"]  
Code Correctness| Correctness of the provided code| ["completely incorrect",
"mostly incorrect", "partly correct", "mostly correct", "completely correct"]  
  
Then, after running QuantifierAgent, we obtained the results presented in Fig.
3, where you can see three models:

* AgentChat
* ReAct
* GPT-4 Vanilla Solver

Lighter colors represent estimates for failed cases, and brighter colors show
how discovered criteria were quantified.

![Fig.3: Results based on overall math problems dataset `_s` stands for
successful cases, `_f` - stands for failed cases](/autogen/assets/images/math-
problems-plot-03ec81b957c85db6ad9b1da353784b96.png)

 _Fig.3 presents results based on overall math problems dataset `_s` stands
for successful cases, `_f` - stands for failed cases_

We note that while applying agentEval to math problems, the agent was not
exposed to any ground truth information about the problem. As such, this
figure illustrates an estimated performance of the three different agents,
namely, Autogen (blue), Gpt-4 (red), and ReAct (green). We observe that by
comparing the performance of any of the three agents in successful cases (dark
bars of any color) versus unsuccessful cases (lighter version of the same
bar), we note that AgentEval was able to assign higher quantification to
successful cases than that of failed ones. This observation verifies
AgentEval's ability for task utility prediction. Additionally, AgentEval
allows us to go beyond just a binary definition of success, enabling a more
in-depth comparison between successful and failed cases.

It's important not only to identify what is not working but also to recognize
what and why actually went well.

## Limitations and Future Work​ {#limitations-and-future-work}

The current implementation of `AgentEval` has a number of limitations which
are planning to overcome in the future:

* The list of criteria varies per run (unless you store a seed). We would recommend to run `CriticAgent` at least two times, and pick criteria you think is important for your domain.
* The results of the `QuantifierAgent` can vary with each run, so we recommend conducting multiple runs to observe the extent of result variations.

To mitigate the limitations mentioned above, we are working on VerifierAgent,
whose goal is to stabilize the results and provide additional explanations.

## Summary​ {#summary}

`CriticAgent` and `QuantifierAgent` can be applied to the logs of any type of
application, providing you with an in-depth understanding of the utility your
solution brings to the user for a given task.

We would love to hear about how AgentEval works for your application. Any
feedback would be useful for future development. Please contact us on our
[Discord](http://aka.ms/autogen-dc).

## Previous Research​ {#previous-research}

    @InProceedings{pmlr-v176-kiseleva22a,  
      title = "Interactive Grounded Language Understanding in a Collaborative Environment: IGLU 2021",  
      author = "Kiseleva, Julia and Li, Ziming and Aliannejadi, Mohammad and Mohanty, Shrestha and ter Hoeve, Maartje and Burtsev, Mikhail and Skrynnik, Alexey and Zholus, Artem and Panov, Aleksandr and Srinet, Kavya and Szlam, Arthur and Sun, Yuxuan and Hofmann, Katja and C{\^o}t{\'e}, Marc-Alexandre and Awadallah, Ahmed and Abdrazakov, Linar and Churin, Igor and Manggala, Putra and Naszadi, Kata and van der Meer, Michiel and Kim, Taewoon",  
      booktitle = "Proceedings of the NeurIPS 2021 Competitions and Demonstrations Track",  
      pages = "146--161",  
      year = 2022,  
      editor = "Kiela, Douwe and Ciccone, Marco and Caputo, Barbara",  
      volume = 176,  
      series = "Proceedings of Machine Learning Research",  
      month = "06--14 Dec",  
      publisher = "PMLR",  
      pdf =      {https://proceedings.mlr.press/v176/kiseleva22a/kiseleva22a.pdf},  
      url =      {https://proceedings.mlr.press/v176/kiseleva22a.html}.  
    }  

    @InProceedings{pmlr-v220-kiseleva22a,  
      title = "Interactive Grounded Language Understanding in a Collaborative Environment: Retrospective on Iglu 2022 Competition",  
      author = "Kiseleva, Julia and Skrynnik, Alexey and Zholus, Artem and Mohanty, Shrestha and Arabzadeh, Negar and C\^{o}t\'e, Marc-Alexandre and Aliannejadi, Mohammad and Teruel, Milagro and Li, Ziming and Burtsev, Mikhail and ter Hoeve, Maartje and Volovikova, Zoya and Panov, Aleksandr and Sun, Yuxuan and Srinet, Kavya and Szlam, Arthur and Awadallah, Ahmed and Rho, Seungeun and Kwon, Taehwan and Wontae Nam, Daniel and Bivort Haiek, Felipe and Zhang, Edwin and Abdrazakov, Linar and Qingyam, Guo and Zhang, Jason and Guo, Zhibin",  
      booktitle = "Proceedings of the NeurIPS 2022 Competitions Track",  
      pages = "204--216",  
      year = 2022,  
      editor = "Ciccone, Marco and Stolovitzky, Gustavo and Albrecht, Jacob",  
      volume = 220,  
      series = "Proceedings of Machine Learning Research",  
      month = "28 Nov--09 Dec",  
      publisher = "PMLR",  
      pdf = "https://proceedings.mlr.press/v220/kiseleva22a/kiseleva22a.pdf",  
      url = "https://proceedings.mlr.press/v220/kiseleva22a.html".  
    }  

 **Tags:**

* [LLM](/autogen/blog/tags/llm)
* [GPT](/autogen/blog/tags/gpt)
* [evaluation](/autogen/blog/tags/evaluation)
* [task utility](/autogen/blog/tags/task-utility)

[Newer Post« Agent AutoBuild - Automatically Building Multi-agent
Systems](/autogen/blog/2023/11/26/Agent-AutoBuild)

[Older PostAutoGen Meets GPTs »](/autogen/blog/2023/11/13/OAI-assistants)

* Introduction
* `AgentEval` Framework
* `AgentEval` Results based on Math Problems Dataset
* Limitations and Future Work
* Summary
* Previous Research

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
  * automl
  * default
  * onlineml
  * tune
    * tune.scheduler
    * tune.searcher
      * [blendsearch](.docs/reference/tune/searcher/blendsearch)
      * [cfo_cat](.docs/reference/tune/searcher/cfo_cat)
      * [flow2](.docs/reference/tune/searcher/flow2)
      * [online_searcher](.docs/reference/tune/searcher/online_searcher)
      * [search_thread](.docs/reference/tune/searcher/search_thread)
      * [suggestion](.docs/reference/tune/searcher/suggestion)
      * [variant_generator](.docs/reference/tune/searcher/variant_generator)
    * tune.spark
    * [analysis](.docs/reference/tune/analysis)
    * [sample](.docs/reference/tune/sample)
    * [space](.docs/reference/tune/space)
    * [trial](.docs/reference/tune/trial)
    * [trial_runner](.docs/reference/tune/trial_runner)
    * [tune](.docs/reference/tune/tune)
    * [utils](.docs/reference/tune/utils)

On this page

# tune.searcher.search_thread {#tunesearchersearchthread}

## SearchThread Objects​ {#searchthread-objects}

    class SearchThread()  

Class of global or local search thread.

#### __init__​ {#init}
```python
    def __init__(mode: str = "min", search_alg: Optional[Searcher] = None, cost_attr: Optional[str] = TIME_TOTAL_S, eps: Optional[float] = 1.0)  

When search_alg is omitted, use local search FLOW2.

#### suggest​ {#suggest}
```python
    def suggest(trial_id: str) -> Optional[Dict]  

Use the suggest() of the underlying search algorithm.

#### on_trial_complete​ {#ontrialcomplete}
```python
    def on_trial_complete(trial_id: str, result: Optional[Dict] = None, error: bool = False)  

Update the statistics of the thread.

#### reach​ {#reach}
```python
    def reach(thread) -> bool  

Whether the incumbent can reach the incumbent of thread.

#### can_suggest​ {#cansuggest}

    @property  
    def can_suggest() -> bool  

Whether the thread can suggest new configs.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/tune/searcher/search_thread.md)

[Previous«
online_searcher](.docs/reference/tune/searcher/online_searcher)

[Nextsuggestion »](.docs/reference/tune/searcher/suggestion)

* SearchThread Objects

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* [Getting Started](.docs/Getting-Started)
* [Installation](.docs/Installation)
* Use Cases
* Examples
  * [AutoGen - Automated Multi Agent Chat](.docs/Examples/AutoGen-AgentChat)
  * [AutoGen - Tune GPT Models](.docs/Examples/AutoGen-OpenAI)
  * [AutoML - Classification](.docs/Examples/AutoML-Classification)
  * [AutoML - NLP](.docs/Examples/AutoML-NLP)
  * [AutoML - Rank](.docs/Examples/AutoML-Rank)
  * [AutoML - Regression](.docs/Examples/AutoML-Regression)
  * [AutoML - Time Series Forecast](.docs/Examples/AutoML-Time series forecast)
  * [AutoML for LightGBM](.docs/Examples/AutoML-for-LightGBM)
  * [AutoML for XGBoost](.docs/Examples/AutoML-for-XGBoost)
  * [Default - Flamlized Estimator](.docs/Examples/Default-Flamlized)
  * [Integrate - AzureML](.docs/Examples/Integrate - AzureML)
  * [Integrate - Scikit-learn Pipeline](.docs/Examples/Integrate - Scikit-learn Pipeline)
  * [Integrate - Spark](.docs/Examples/Integrate - Spark)
  * [Tune - AzureML pipeline](.docs/Examples/Tune-AzureML-pipeline)
  * [Tune - HuggingFace](.docs/Examples/Tune-HuggingFace)
  * [Tune - Lexicographic Objectives](.docs/Examples/Tune-Lexicographic-objectives)
  * [Tune - PyTorch](.docs/Examples/Tune-PyTorch)
* [Contributing](.docs/Contribute)
* [Research](.docs/Research)

On this page

# Tune - HuggingFace {#tune-huggingface}

This example uses flaml to finetune a transformer model from Huggingface
transformers library.

 _Note_ : `flaml.AutoML` has built-in support for certain finetuning tasks
with a [higher-level API](.docs/Examples/AutoML-NLP). It may be easier
to use that API unless you have special requirements not handled by that API.

### Requirements​ {#requirements}

This example requires GPU. Install dependencies:

    pip install torch transformers datasets "flaml[blendsearch,ray]"  

### Prepare for tuning​ {#prepare-for-tuning}

#### Tokenizer​ {#tokenizer}

    from transformers import AutoTokenizer  
      
    MODEL_NAME = "distilbert-base-uncased"  
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)  
    COLUMN_NAME = "sentence"  
      
    def tokenize(examples):  
        return tokenizer(examples[COLUMN_NAME], truncation=True)  

#### Define training method​ {#define-training-method}

    import flaml  
    import datasets  
    from transformers import AutoModelForSequenceClassification  
      
    TASK = "cola"  
    NUM_LABELS = 2  
      
    def train_distilbert(config: dict):  
        # Load CoLA dataset and apply tokenizer  
        cola_raw = datasets.load_dataset("glue", TASK)  
        cola_encoded = cola_raw.map(tokenize, batched=True)  
        train_dataset, eval_dataset = cola_encoded["train"], cola_encoded["validation"]  
      
        model = AutoModelForSequenceClassification.from_pretrained(  
            MODEL_NAME, num_labels=NUM_LABELS  
        )  
        metric = datasets.load_metric("glue", TASK)  
      
        def compute_metrics(eval_pred):  
            predictions, labels = eval_pred  
            predictions = np.argmax(predictions, axis=1)  
            return metric.compute(predictions=predictions, references=labels)  
      
        training_args = TrainingArguments(  
            output_dir='.',  
            do_eval=False,  
            disable_tqdm=True,  
            logging_steps=20000,  
            save_total_limit=0,  
            **config,  
        )  
      
        trainer = Trainer(  
            model,  
            training_args,  
            train_dataset=train_dataset,  
            eval_dataset=eval_dataset,  
            tokenizer=tokenizer,  
            compute_metrics=compute_metrics,  
        )  
      
        # train model  
        trainer.train()  
      
        # evaluate model  
        eval_output = trainer.evaluate()  
      
        # report the metric to optimize & the metric to log  
        flaml.tune.report(  
            loss=eval_output["eval_loss"],  
            matthews_correlation=eval_output["eval_matthews_correlation"],  
        )  

### Define the search​ {#define-the-search}

We are now ready to define our search. This includes:

* The `search_space` for our hyperparameters
* The `metric` and the `mode` ('max' or 'min') for optimization
* The constraints (`n_cpus`, `n_gpus`, `num_samples`, and `time_budget_s`)

    max_num_epoch = 64  
    search_space = {  
            # You can mix constants with search space objects.  
            "num_train_epochs": flaml.tune.loguniform(1, max_num_epoch),  
            "learning_rate": flaml.tune.loguniform(1e-6, 1e-4),  
            "adam_epsilon": flaml.tune.loguniform(1e-9, 1e-7),  
            "adam_beta1": flaml.tune.uniform(0.8, 0.99),  
            "adam_beta2": flaml.tune.loguniform(98e-2, 9999e-4),  
    }  

# optimization objective  

    HP_METRIC, MODE = "matthews_correlation", "max"  

# resources  

    num_cpus = 4  
    num_gpus = 4  # change according to your GPU resources  

# constraints  

    num_samples = -1  # number of trials, -1 means unlimited  
    time_budget_s = 3600  # time budget in seconds  

### Launch the tuning​ {#launch-the-tuning}

We are now ready to launch the tuning using `flaml.tune.run`:

    import ray  
      
    ray.init(num_cpus=num_cpus, num_gpus=num_gpus)  
    print("Tuning started...")  
    analysis = flaml.tune.run(  
        train_distilbert,  
        search_alg=flaml.CFO(  
            space=search_space,  
            metric=HP_METRIC,  
            mode=MODE,  
            low_cost_partial_config={"num_train_epochs": 1}),  
        resources_per_trial={"gpu": num_gpus, "cpu": num_cpus},  
        local_dir='logs/',  
        num_samples=num_samples,  
        time_budget_s=time_budget_s,  
        use_ray=True,  
    )  

This will run tuning for one hour. At the end we will see a summary.

    == Status ==  
    Memory usage on this node: 32.0/251.6 GiB  
    Using FIFO scheduling algorithm.  
    Resources requested: 0/4 CPUs, 0/4 GPUs, 0.0/150.39 GiB heap, 0.0/47.22 GiB objects (0/1.0 accelerator_type:V100)  
    Result logdir: /home/chiw.notebook/logs/train_distilbert_2021-05-07_02-35-58  
    Number of trials: 22/infinite (22 TERMINATED)  
    Trial name  status  loc adam_beta1  adam_beta2  adam_epsilon    learning_rate   num_train_epochs    iter    total time (s)  loss    matthews_correlation  
    train_distilbert_a0c303d0   TERMINATED      0.939079    0.991865    7.96945e-08 5.61152e-06 1   1   55.6909 0.587986    0  
    train_distilbert_a0c303d1   TERMINATED      0.811036    0.997214    2.05111e-09 2.05134e-06 1.44427 1   71.7663 0.603018    0  
    train_distilbert_c39b2ef0   TERMINATED      0.909395    0.993715    1e-07   5.26543e-06 1   1   53.7619 0.586518    0  
    train_distilbert_f00776e2   TERMINATED      0.968763    0.990019    4.38943e-08 5.98035e-06 1.02723 1   56.8382 0.581313    0  
    train_distilbert_11ab3900   TERMINATED      0.962198    0.991838    7.09296e-08 5.06608e-06 1   1   54.0231 0.585576    0  
    train_distilbert_353025b6   TERMINATED      0.91596 0.991892    8.95426e-08 6.21568e-06 2.15443 1   98.3233 0.531632    0.388893  
    train_distilbert_5728a1de   TERMINATED      0.926933    0.993146    1e-07   1.00902e-05 1   1   55.3726 0.538505    0.280558  
    train_distilbert_9394c2e2   TERMINATED      0.928106    0.990614    4.49975e-08 3.45674e-06 2.72935 1   121.388 0.539177    0.327295  
    train_distilbert_b6543fec   TERMINATED      0.876896    0.992098    1e-07   7.01176e-06 1.59538 1   76.0244 0.527516    0.379177  
    train_distilbert_0071f998   TERMINATED      0.955024    0.991687    7.39776e-08 5.50998e-06 2.90939 1   126.871 0.516225    0.417157  
    train_distilbert_2f830be6   TERMINATED      0.886931    0.989628    7.6127e-08  4.37646e-06 1.53338 1   73.8934 0.551629    0.0655887  
    train_distilbert_7ce03f12   TERMINATED      0.984053    0.993956    8.70144e-08 7.82557e-06 4.08775 1   174.027 0.523732    0.453549  
    train_distilbert_aaab0508   TERMINATED      0.940707    0.993946    1e-07   8.91979e-06 3.40243 1   146.249 0.511288    0.45085  
    train_distilbert_14262454   TERMINATED      0.99    0.991696    4.60093e-08 4.83405e-06 3.4954  1   152.008 0.53506 0.400851  
    train_distilbert_6d211fe6   TERMINATED      0.959277    0.994556    5.40791e-08 1.17333e-05 6.64995 1   271.444 0.609851    0.526802  
    train_distilbert_c980bae4   TERMINATED      0.99    0.993355    1e-07   5.21929e-06 2.51275 1   111.799 0.542276    0.324968  
    train_distilbert_6d0d29d6   TERMINATED      0.965773    0.995182    9.9752e-08  1.15549e-05 13.694  1   527.944 0.923802    0.549474  
    train_distilbert_b16ea82a   TERMINATED      0.952781    0.993931    2.93182e-08 1.19145e-05 3.2293  1   139.844 0.533466    0.451307  
    train_distilbert_eddf7cc0   TERMINATED      0.99    0.997109    8.13498e-08 1.28515e-05 15.5807 1   614.789 0.983285    0.56993  
    train_distilbert_43008974   TERMINATED      0.929089    0.993258    1e-07   1.03892e-05 12.0357 1   474.387 0.857461    0.520022  
    train_distilbert_b3408a4e   TERMINATED      0.99    0.993809    4.67441e-08 1.10418e-05 11.9165 1   474.126 0.828205    0.526164  
    train_distilbert_cfbfb220   TERMINATED      0.979454    0.9999  1e-07   1.49578e-05 20.3715  

### Retrieve the results​ {#retrieve-the-results}

    best_trial = analysis.get_best_trial(HP_METRIC, MODE, "all")  
    metric = best_trial.metric_analysis[HP_METRIC][MODE]  
    print(f"n_trials={len(analysis.trials)}")  
    print(f"time={time.time()-start_time}")  
    print(f"Best model eval {HP_METRIC}: {metric:.4f}")  
    print(f"Best model parameters: {best_trial.config}")  
    # n_trials=22  
    # time=3999.769361972809  
    # Best model eval matthews_correlation: 0.5699  
    # Best model parameters: {'num_train_epochs': 15.580684188655825, 'learning_rate': 1.2851507818900338e-05, 'adam_epsilon': 8.134982521948352e-08, 'adam_beta1': 0.99, 'adam_beta2': 0.9971094424784387}  

[Link to
notebook](https://github.com/microsoft.blob/main/notebook/tune_huggingface.ipynb)
| [Open in
colab](https://colab.research.google.com/github/microsoft.blob/main/notebook/tune_huggingface.ipynb)

[Edit this
page](<https://github.com/microsoft.edit/main/website/docs/Examples/Tune->
HuggingFace.md)

[Previous« Tune - AzureML pipeline](.docs/Examples/Tune-AzureML-
pipeline)

[NextTune - Lexicographic Objectives »](.docs/Examples/Tune-
Lexicographic-objectives)

* Requirements
* Prepare for tuning
* Define the search
* Launch the tuning
* Retrieve the results

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
  * automl
    * automl.contrib
    * automl.nlp
      * automl.nlp.huggingface
        * [trainer](.docs/reference/automl/nlp/huggingface/trainer)
        * [training_args](.docs/reference/automl/nlp/huggingface/training_args)
        * [utils](.docs/reference/automl/nlp/huggingface/utils)
      * [utils](.docs/reference/automl/nlp/utils)
    * automl.spark
    * automl.task
    * automl.time_series
    * [automl](.docs/reference/automl/automl)
    * [data](.docs/reference/automl/data)
    * [ml](.docs/reference/automl/ml)
    * [model](.docs/reference/automl/model)
    * [state](.docs/reference/automl/state)
  * default
  * onlineml
  * tune

On this page

# automl.nlp.huggingface.training_args {#automlnlphuggingfacetrainingargs}

## TrainingArgumentsForAuto Objects​ {#trainingargumentsforauto-objects}

    @dataclass  
    class TrainingArgumentsForAuto(TrainingArguments)  

FLAML custom TrainingArguments.

 **Arguments** :

* `task` _str_ \- the task name for NLP tasks, e.g., seq-classification, token-classification
* `output_dir` _str_ \- data root directory for outputing the log, etc.
* `model_path` _str, optional, defaults to "facebook/muppet-roberta-base"_ \- A string, the path of the language model file, either a path from huggingface model card huggingface.co/models, or a local path for the model.
* `fp16` _bool, optional, defaults to "False"_ \- A bool, whether to use FP16.
* `max_seq_length` _int, optional, defaults to 128_ \- An integer, the max length of the sequence. For token classification task, this argument will be ineffective. pad_to_max_length (bool, optional, defaults to "False"): whether to pad all samples to model maximum sentence length. If False, will pad the samples dynamically when batching to the maximum length in the batch.
* `per_device_eval_batch_size` _int, optional, defaults to 1_ \- An integer, the per gpu evaluation batch size.
* `label_list` _List [str], optional, defaults to None_ \- A list of string, the string list of the label names. When the task is sequence labeling/token classification, there are two formats of the labels: (1) The token labels, i.e., [B-PER, I-PER, B-LOC]; (2) Id labels. For (2), need to pass the label_list (e.g., [B-PER, I-PER, B-LOC]) to convert the Id to token labels when computing the metric with metric_loss_score. See the example in [a simple token classification example](.docs/Examples/AutoML-NLP#a-simple-token-classification-example).

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/automl/nlp/huggingface/training_args.md)

[Previous« trainer](.docs/reference/automl/nlp/huggingface/trainer)

[Nextutils »](.docs/reference/automl/nlp/huggingface/utils)

* TrainingArgumentsForAuto Objects

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

Recent posts

* [Use flaml.autogen for Local LLMs](.blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](.blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](.blog/2023/05/18/GPT-adaptive-humaneval)
* [Surpassing 1 Million Downloads - A Retrospective and a Look into the Future](.blog/2023/05/07/1M-milestone)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](.blog/2023/04/21/LLM-tuning-math)

## [Use flaml.autogen for Local LLMs](.blog/2023/07/14/Local-LLMs) {#use-flamlautogen-for-local-llmsflamlblog20230714localllms}

July 14, 2023 · 3 min read

[![Jiale
Liu](https://github.com/LeoLjl/leoljl.github.io/blob/main/profile.jpg?raw=true)](https://leoljl.github.io)

[Jiale Liu](https://leoljl.github.io)

Undergraduate student at Xidian University

 **TL;DR:** We demonstrate how to use flaml.autogen for local LLM application.
As an example, we will initiate an endpoint using
[FastChat](https://github.com/lm-sys/FastChat) and perform inference on
[ChatGLMv2-6b](https://github.com/THUDM/ChatGLM2-6B).

## Preparations​ {#preparations}

### Clone FastChat​ {#clone-fastchat}

FastChat provides OpenAI-compatible APIs for its supported models, so you can
use FastChat as a local drop-in replacement for OpenAI APIs. However, its code
needs minor modification in order to function properly.

    git clone https://github.com/lm-sys/FastChat.git  
    cd FastChat  

### Download checkpoint​ {#download-checkpoint}

ChatGLM-6B is an open bilingual language model based on General Language Model
(GLM) framework, with 6.2 billion parameters. ChatGLM2-6B is its second-
generation version.

Before downloading from HuggingFace Hub, you need to have Git LFS
[installed](<https://docs.github.com/en/repositories/working-with->
files/managing-large-files/installing-git-large-file-storage).

    git clone https://huggingface.co/THUDM/chatglm2-6b  

## Initiate server​ {#initiate-server}

First, launch the controller

    python -m fastchat.serve.controller  

Then, launch the model worker(s)

    python -m fastchat.serve.model_worker --model-path chatglm2-6b  

Finally, launch the RESTful API server

    python -m fastchat.serve.openai_api_server --host localhost --port 8000  

Normally this will work. However, if you encounter error like
[this](https://github.com/lm-sys/FastChat/issues/1641), commenting out all the
lines containing `finish_reason` in `fastchat/protocol/api_protocal.py` and
`fastchat/protocol/openai_api_protocol.py` will fix the problem. The modified
code looks like:

    class CompletionResponseChoice(BaseModel):  
        index: int  
        text: str  
        logprobs: Optional[int] = None  
        # finish_reason: Optional[Literal["stop", "length"]]  
      
    class CompletionResponseStreamChoice(BaseModel):  
        index: int  
        text: str  
        logprobs: Optional[float] = None  
        # finish_reason: Optional[Literal["stop", "length"]] = None  

## Interact with model using `oai.Completion`​ {#interact-with-model-using-oaicompletion}

Now the models can be directly accessed through openai-python library as well
as `flaml.oai.Completion` and `flaml.oai.ChatCompletion`.

    from flaml import oai  
      
    # create a text completion request  
    response = oai.Completion.create(  
        config_list=[  
            {  
                "model": "chatglm2-6b",  
                "api_base": "http://localhost:8000/v1",  
                "api_type": "open_ai",  
                "api_key": "NULL", # just a placeholder  
            }  
        ],  
        prompt="Hi",  
    )  
    print(response)  
      
    # create a chat completion request  
    response = oai.ChatCompletion.create(  
        config_list=[  
            {  
                "model": "chatglm2-6b",  
                "api_base": "http://localhost:8000/v1",  
                "api_type": "open_ai",  
                "api_key": "NULL",  
            }  
        ],  
        messages=[{"role": "user", "content": "Hi"}]  
    )  
    print(response)  

If you would like to switch to different models, download their checkpoints
and specify model path when launching model worker(s).

## interacting with multiple local LLMs​ {#interacting-with-multiple-local-llms}

If you would like to interact with multiple LLMs on your local machine,
replace the `model_worker` step above with a multi model variant:

    python -m fastchat.serve.multi_model_worker \  
        --model-path lmsys/vicuna-7b-v1.3 \  
        --model-names vicuna-7b-v1.3 \  
        --model-path chatglm2-6b \  
        --model-names chatglm2-6b  

The inference code would be:

    from flaml import oai  
      
    # create a chat completion request  
    response = oai.ChatCompletion.create(  
        config_list=[  
            {  
                "model": "chatglm2-6b",  
                "api_base": "http://localhost:8000/v1",  
                "api_type": "open_ai",  
                "api_key": "NULL",  
            },  
            {  
                "model": "vicuna-7b-v1.3",  
                "api_base": "http://localhost:8000/v1",  
                "api_type": "open_ai",  
                "api_key": "NULL",  
            }  
        ],  
        messages=[{"role": "user", "content": "Hi"}]  
    )  
    print(response)  

## For Further Reading​ {#for-further-reading} {#for-further-reading} {#for-further-reading} {#for-further-reading} {#for-further-reading}

* [Documentation](https://microsoft.github.io/autogen/) about `flaml.autogen`
* [Documentation](https://github.com/lm-sys/FastChat) about FastChat.

 **Tags:**

* [LLM](.blog/tags/llm)
* [FLAMLv2](.blog/tags/flam-lv-2)

## [MathChat - An Conversational Framework to Solve Math {#mathchat-an-conversational-framework-to-solve-math}

Problems](.blog/2023/06/28/MathChat)

June 28, 2023 · 8 min read

[![Yiran
Wu](https://github.com/kevin666aa.png)](https://github.com/kevin666aa)

[Yiran Wu](https://github.com/kevin666aa)

PhD student at Pennsylvania State University

![MathChat
WorkFlow](.assets/images/mathchatflow-926a8ed1975a114ab76c69996942c23a.png)
**TL;DR:**

* **We introduce MathChat, a conversational framework leveraging Large Language Models (LLMs), specifically GPT-4, to solve advanced mathematical problems.**
* **MathChat improves LLM 's performance on challenging math problem-solving, outperforming basic prompting and other strategies by about 6%. The improvement was especially notable in the Algebra category, with a 15% increase in accuracy.**
* **Despite the advancement, GPT-4 still struggles to solve very challenging math problems, even with effective prompting strategies. Further improvements are needed, such as the development of more specific assistant models or the integration of new tools and prompts.**

Recent Large Language Models (LLMs) like GTP-3.5 and GPT-4 have demonstrated
astonishing abilities over previous models on various tasks, such as text
generation, question answering, and code generation. Moreover, these models
can communicate with humans through conversations and remember previous
contexts, making it easier for humans to interact with them. These models play
an increasingly important role in our daily lives assisting people with
different tasks, such as writing emails, summarizing documents, and writing
code.

In this blog post, we probe into the problem-solving capabilities of LLMs.
Specifically, we are interested in their capabilities to solve advanced math
problems, which could be representative of a broader class of problems that
require precise reasoning and also have deterministic solutions.

We introduce MathChat, a conversational framework designed for solving
challenging math problems with LLMs. This framework takes advantage of the
chat-optimized feature of state-of-the-art LLMs, where a user proxy agent and
an LLM assistant work together to tackle math problems. We also test previous
prompting techniques for comparison.

## The MathChat Framework​ {#the-mathchat-framework}

MathChat simulates a conversation between the LLM assistant and a user proxy
agent. As the name indicates, the user proxy agent acts as a proxy for the
user, which is responsible for communicating with the LLM assistant and
continuing the conversation in a desired manner.

The proxy agent first presents a math problem to the LLM assistant, framed by
an initial prompt that guides the assistant in how it should work
collaboratively to solve the problem. We incorporate existing prompting
methods such as CoT (chain-of-thought) and tool-using are incorporated in the
initial prompt. Key components of the prompt include:

* **Tool-using Prompt:** This guides the LLM assistant to use Python code in the correct format to address the problem, allowing the user proxy agent to parse the code and provide the appropriate results.

* **Problem-Solving Strategy Selection Prompt:** The assistant is instructed to choose one of three potential problem-solving strategies, including:

    1. Writing a Python program to solve the problem directly.
    2. Solving the problem directly without using Python, which exercises LLMs' inherent reasoning capabilities.
    3. Solving the problem step by step with Python. This should be used when the first two strategies aren't suitable.

* **Final Answer Encapsulation Prompt:** This part instructs the assistant to put the final answer in _\boxed_ {}.

The prompt design in MathChat is flexible and can easily be refined to include
other tools, such as Wolfram Alpha.

Let's take a look at an example between the **User Proxy Agent** and the **LLM
Assistant** (GPT-4). The conversation focuses on how to solve inequality using
Python. (The conversation is modified for readability.)

Participant| Dialogue  
---|---  
User Proxy Agent| (Initial Prompt: Let's solve a math problem with Python...)
Problem: Find all x that satisfy the inequality (2x+10)(x+3) < (3x+9)(x+8).
Express your answer in interval notation.  
LLM assistant| The key idea to solve this problem is to find the critical
points, which are the points where the inequality is equal. Then, test the
intervals between the critical points to find the interval for which the
inequality holds.  
LLM assistant (continued)| Here's Python code to find the critical points:
`from sympy import symbols, Eq, solve; x = symbols("x"); lhs = (2*x + 10)*(x +
3); rhs = (3*x + 9)*(x + 8); equation = Eq(lhs, rhs); critical_points =
solve(equation, x); critical_points`  
User Proxy Agent| The critical points are: [-14, -3]  
LLM assistant| We found the critical points to be x = -14 and x = -3. Now
we'll test the intervals between these critical points as well as points
before and after the critical points. The intervals are: x < -14, -14 < x <
-3, x > -3.  
LLM assistant (continued)| Here's Python code to test the inequality for each
interval: `from sympy import symbols; x = symbols("x"); inequality = (2*x +
10)*(x + 3) < (3*x + 9)*(x + 8); test_points = [-15, -5, 0]; intervals = [];
for point in test_points: if inequality.subs(x, point):
intervals.append(point); intervals`  
User Proxy Agent| The intervals where the inequality holds true are for test
points: [-15, 0]  
LLM assistant| We have tested the intervals and found that the inequality
holds for: 1. x < -14 (test point: -15) 2. x > -3 (test point: 0). Therefore,
the solution to the inequality (2x+10)(x+3) < (3x+9)(x+8) is x ∈ (-∞, -14) ∪
(-3, ∞)  
  
## Experiment Setup​ {#experiment-setup} {#experiment-setup}

We evaluate the improvement brought by MathChat.

For the experiment, we focus on the level-5 problems from the MATH dataset,
which are composed of high school competition problems. These problems include
the application of theorems and complex equation derivation and are
challenging even for undergraduate students. We evaluate 6 of 7 categories
from the dataset (excluding Geometry): Prealgebra, Algebra, Number Theory,
Counting and Probability, Intermediate Algebra, and Precalculus.

We evaluate GPT-4 and use the default configuration of the OpenAI API. To
access the final performance, we manually compare the final answer with the
correct answer. For the vanilla prompt, Program Synthesis, and MathChat, we
have GPT-4 enclose the final answer in _\boxed_ {}, and we take the return of
the function in PoT as the final answer.

We also evaluate the following methods for comparison:

  1. **Vanilla prompting:** Evaluates GPT-4's direct problem-solving capability. The prompt used is: _" Solve the problem carefully. Put the final answer in \boxed{}"_.

  2. **Program of Thoughts (PoT):** Uses a zero-shot PoT prompt that requests the model to create a _Solver_ function to solve the problem and return the final answer.

  3. **Program Synthesis (PS) prompting:** Like PoT, it prompts the model to write a program to solve the problem. The prompt used is: _" Write a program that answers the following question: {Problem}"_.

## Experiment Results​ {#experiment-results} {#experiment-results} {#experiment-results}

The accuracy on all the problems with difficulty level-5 from different
categories of the MATH dataset with different methods is shown below:

![Result](.assets/images/result-514251f4c387d5b6e11656e1fa33de08.png)

We found that compared to basic prompting, which demonstrates the innate
capabilities of GPT-4, utilizing Python within the context of PoT or PS
strategy improved the overall accuracy by about 10%. This increase was mostly
seen in categories involving more numerical manipulations, such as Counting &
Probability and Number Theory, and in more complex categories like
Intermediate Algebra and Precalculus.

For categories like Algebra and Prealgebra, PoT and PS showed little
improvement, and in some instances, even led to a decrease in accuracy.
However, MathChat was able to enhance total accuracy by around 6% compared to
PoT and PS, showing competitive performance across all categories. Remarkably,
MathChat improved accuracy in the Algebra category by about 15% over other
methods. Note that categories like Intermediate Algebra and Precalculus
remained challenging for all methods, with only about 20% of problems solved
accurately.

The code for experiments can be found at this
[repository](https://github.com/kevin666aa.tree/gpt_math_solver/flaml/autogen/math).
We now provide an implementation of MathChat using the interactive agents in
FLAML. See this
[notebook](https://github.com/microsoft.blob/main/notebook/autogen_agentchat_MathChat.ipynb)
for example usage.

## Future Directions​ {#future-directions}

Despite MathChat's improvements over previous methods, the results show that
complex math problem is still challenging for recent powerful LLMs, like
GPT-4, even with help from external tools.

Further work can be done to enhance this framework or math problem-solving in
general:

* Although enabling the model to use tools like Python can reduce calculation errors, LLMs are still prone to logic errors. Methods like self-consistency (Sample several solutions and take a major vote on the final answer), or self-verification (use another LLM instance to check whether an answer is correct) might improve the performance.
* Sometimes, whether the LLM can solve the problem depends on the plan it uses. Some plans require less computation and logical reasoning, leaving less room for mistakes.
* MathChat has the potential to be adapted into a copilot system, which could assist users with math problems. This system could allow users to be more involved in the problem-solving process, potentially enhancing learning.

## For Further Reading​ {#for-further-reading} {#for-further-reading} {#for-further-reading} {#for-further-reading} {#for-further-reading}

* [Research paper of MathChat](https://arxiv.org/abs/2306.01337)
* [Documentation about `flaml.autogen`](https://microsoft.github.io/autogen/)

 _Are you working on applications that involve math problem-solving? Would you
appreciate additional research or support on the application of LLM-based
agents for math problem-solving? Please join
our[Discord](https://discord.gg/Cppx2vSPVP) server for discussion._

 **Tags:**

* [LLM](.blog/tags/llm)
* [GPT](.blog/tags/gpt)
* [research](.blog/tags/research)

## [Achieve More, Pay Less - Use GPT-4 Smartly](.blog/2023/05/18/GPT- {#achieve-more-pay-less-use-gpt4-smartlyflamlblog20230518gpt}

adaptive-humaneval)

May 18, 2023 · 8 min read

[![Chi Wang](https://github.com/sonichi.png)](<https://www.linkedin.com/in/chi->
wang-49b15b16/)

[Chi Wang](https://www.linkedin.com/in/chi-wang-49b15b16/)

Principal Researcher at Microsoft Research

![An adaptive way of using GPT-3.5 and GPT-4 outperforms GPT-4 in both coding
success rate and inference
cost](.assets/images/humaneval-b2718b0b15d76b8fc59a4094e1bd684c.png)

 **TL;DR:**

* **A case study using the HumanEval benchmark shows that an adaptive way of using multiple GPT models can achieve both much higher accuracy (from 68% to 90%) and lower inference cost (by 18%) than using GPT-4 for coding.**

GPT-4 is a big upgrade of foundation model capability, e.g., in code and math,
accompanied by a much higher (more than 10x) price per token to use over
GPT-3.5-Turbo. On a code completion benchmark,
[HumanEval](https://huggingface.co/datasets/openai_humaneval), developed by
OpenAI, GPT-4 can successfully solve 68% tasks while GPT-3.5-Turbo does 46%.
It is possible to increase the success rate of GPT-4 further by generating
multiple responses or making multiple calls. However, that will further
increase the cost, which is already nearly 20 times of using GPT-3.5-Turbo and
with more restricted API call rate limit. Can we achieve more with less?

In this blog post, we will explore a creative, adaptive way of using GPT
models which leads to a big leap forward.

## Observations​ {#observations}

* GPT-3.5-Turbo can alrady solve 40%-50% tasks. For these tasks if we never use GPT-4, we can save nearly 40-50% cost.
* If we use the saved cost to generate more responses with GPT-4 for the remaining unsolved tasks, it is possible to solve some more of them while keeping the amortized cost down.

The obstacle of leveraging these observations is that we do not know _a
priori_ which tasks can be solved by the cheaper model, which tasks can be
solved by the expensive model, and which tasks can be solved by paying even
more to the expensive model.

To overcome that obstacle, one may want to predict which task requires what
model to solve and how many responses are required for each task. Let's look
at one example code completion task:
```python
    def vowels_count(s):  
        """Write a function vowels_count which takes a string representing  
        a word as input and returns the number of vowels in the string.  
        Vowels in this case are 'a', 'e', 'i', 'o', 'u'. Here, 'y' is also a  
        vowel, but only when it is at the end of the given word.  
      
        Example:  
        >>> vowels_count("abcde")  
        2  
        >>> vowels_count("ACEDY")  
        3  
        """  

Can we predict whether GPT-3.5-Turbo can solve this task or do we need to use
GPT-4? My first guess is that GPT-3.5-Turbo can get it right because the
instruction is fairly straightforward. Yet, it turns out that GPT-3.5-Turbo
does not consistently get it right, if we only give it one chance. It's not
obvious (but an interesting research question!) how to predict the performance
without actually trying.

What else can we do? We notice that: **It 's "easier" to verify a given
solution than finding a correct solution from scratch.**

Some simple example test cases are provided in the docstr. If we already have
a response generated by a model, we can use those test cases to filter wrong
implementations, and either use a more powerful model or generate more
responses, until the result passes the example test cases. Moreover, this step
can be automated by asking GPT-3.5-Turbo to generate assertion statements from
the examples given in the docstr (a simpler task where we can place our bet)
and executing the code.

## Solution​ {#solution}

Combining these observations, we can design a solution with two intuitive
ideas:

* Make use of auto-generated feedback, i.e., code execution results, to filter responses.
* Try inference configurations one by one, until one response can pass the filter.

![Design](.assets/images/design-88630f6d8d95f1e31ef5ce79e81aab54.png)

This solution works adaptively without knowing or predicting which task fits
which configuration. It simply tries multiple configurations one by one,
starting from the cheapest configuration. Note that one configuration can
generate multiple responses (by setting the inference parameter n larger than
1). And different configurations can use the same model and different
inference parameters such as n and temperature. Only one response is returned
and evaluated per task.

An implementation of this solution is provided in
[flaml.autogen](.docs/reference/autogen/code_utils#implement). It uses
the following sequence of configurations:

  1. GPT-3.5-Turbo, n=1, temperature=0
  2. GPT-3.5-Turbo, n=7, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]
  3. GPT-4, n=1, temperature=0
  4. GPT-4, n=2, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]
  5. GPT-4, n=1, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]

## Experiment Results​ {#experiment-results} {#experiment-results} {#experiment-results}

The first figure in this blog post shows the success rate and average
inference cost of the adaptive solution compared with default GPT-4. The
inference cost includes the cost for generating the assertions in our
solution. The generated assertions are not always correct, and programs that
pass/fail the generated assertions are not always right/wrong. Despite of
that, the adaptive solution can increase the success rate (referred to as
pass@1 in the literature) from 68% to 90%, while reducing the cost by 18%.

Here are a few examples of function definitions which are solved by different
configurations in the portfolio.

  1. Solved by GPT-3.5-Turbo, n=1, temperature=0
```python
    def compare(game,guess):  
        """I think we all remember that feeling when the result of some long-awaited  
        event is finally known. The feelings and thoughts you have at that moment are  
        definitely worth noting down and comparing.  
        Your task is to determine if a person correctly guessed the results of a number of matches.  
        You are given two arrays of scores and guesses of equal length, where each index shows a match.  
        Return an array of the same length denoting how far off each guess was. If they have guessed correctly,  
        the value is 0, and if not, the value is the absolute difference between the guess and the score.  
      
      
        example:  
      
        compare([1,2,3,4,5,1],[1,2,3,4,2,-2]) -> [0,0,0,0,3,3]  
        compare([0,5,0,0,0,4],[4,1,1,0,0,-2]) -> [4,4,1,0,0,6]  
        """  

  2. Solved by GPT-3.5-Turbo, n=7, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]: the `vowels_count` function presented earlier.
  3. Solved by GPT-4, n=1, temperature=0:
```python
    def string_xor(a: str, b: str) -> str:  
        """ Input are two strings a and b consisting only of 1s and 0s.  
        Perform binary XOR on these inputs and return result also as a string.  
        >>> string_xor('010', '110')  
        '100'  
        """  

  4. Solved by GPT-4, n=2, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]:
```python
    def is_palindrome(string: str) -> bool:  
        """ Test if given string is a palindrome """  
        return string == string[::-1]  
      
      ```python
    def make_palindrome(string: str) -> str:  
        """ Find the shortest palindrome that begins with a supplied string.  
        Algorithm idea is simple:  
        - Find the longest postfix of supplied string that is a palindrome.  
        - Append to the end of the string reverse of a string prefix that comes before the palindromic suffix.  
        >>> make_palindrome('')  
        ''  
        >>> make_palindrome('cat')  
        'catac'  
        >>> make_palindrome('cata')  
        'catac'  
        """  

  5. Solved by GPT-4, n=1, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]:
```python
    def sort_array(arr):  
        """  
        In this Kata, you have to sort an array of non-negative integers according to  
        number of ones in their binary representation in ascending order.  
        For similar number of ones, sort based on decimal value.  
      
        It must be implemented like this:  
        >>> sort_array([1, 5, 2, 3, 4]) == [1, 2, 3, 4, 5]  
        >>> sort_array([-2, -3, -4, -5, -6]) == [-6, -5, -4, -3, -2]  
        >>> sort_array([1, 0, 2, 3, 4]) [0, 1, 2, 3, 4]  
        """  

The last problem is an example with wrong example test cases in the original
definition. It misleads the adaptive solution because a correct implementation
is regarded as wrong and more trials are made. The last configuration in the
sequence returns the right implementation, even though it does not pass the
auto-generated assertions. This example demonstrates that:

* Our adaptive solution has a certain degree of fault tolerance.
* The success rate and inference cost for the adaptive solution can be further improved if correct example test cases are used.

It is worth noting that the reduced inference cost is the amortized cost over
all the tasks. For each individual task, the cost can be either larger or
smaller than directly using GPT-4. This is the nature of the adaptive
solution: The cost is in general larger for difficult tasks than that for easy
tasks.

An example notebook to run this experiment can be found at:
<https://github.com/microsoft.blob/v1.2.1/notebook/research/autogen_code.ipynb>

## Discussion​ {#discussion}

Our solution is quite simple to implement using a generic interface offered in
[`flaml.autogen`](<https://microsoft.github.io/autogen/docs/Use->
Cases/enhanced_inference#logic-error), yet the result is quite encouraging.

While the specific way of generating assertions is application-specific, the
main ideas are general in LLM operations:

* Generate multiple responses to select - especially useful when selecting a good response is relatively easier than generating a good response at one shot.
* Consider multiple configurations to generate responses - especially useful when:
  * Model and other inference parameter choice affect the utility-cost tradeoff; or
  * Different configurations have complementary effect.

A [previous blog post](.blog/2023/04/21/LLM-tuning-math) provides
evidence that these ideas are relevant in solving math problems too.
`flaml.autogen` uses a technique
[EcoOptiGen](https://arxiv.org/abs/2303.04673) to support inference parameter
tuning and model selection.

There are many directions of extensions in research and development:

* Generalize the way to provide feedback.
* Automate the process of optimizing the configurations.
* Build adaptive agents for different applications.

 _Do you find this approach applicable to your use case? Do you have any other
challenge to share about LLM applications? Do you like to see more support or
research of LLM optimization or automation? Please join
our[Discord](https://discord.gg/Cppx2vSPVP) server for discussion._

## For Further Reading​ {#for-further-reading} {#for-further-reading} {#for-further-reading} {#for-further-reading} {#for-further-reading}

* [Documentation](https://microsoft.github.io/autogen/) about `flaml.autogen` and [Research paper](https://arxiv.org/abs/2303.04673).
* [Blog post](.blog/2023/04/21/LLM-tuning-math) about a related study for math.

 **Tags:**

* [LLM](.blog/tags/llm)
* [GPT](.blog/tags/gpt)
* [research](.blog/tags/research)

## [Surpassing 1 Million Downloads - A Retrospective and a Look into the {#surpassing-1-million-downloads-a-retrospective-and-a-look-into-the}

Future](.blog/2023/05/07/1M-milestone)

May 7, 2023 · 4 min read

[![Qingyun Wu](https://github.com/qingyun-wu.png)](<https://qingyun->
wu.github.io/)

[Qingyun Wu](https://qingyun-wu.github.io/)

Assistant Professor at the Pennsylvania State University

 **TL;DR:**

* **Celebrating FLAML 's milestone: 1 million downloads**
* **Introducing Large Language Model (LLM) support in the upcoming FLAML v2**

This week, FLAML has reached a significant milestone: 1 million downloads.
Originating as an intern research project within Microsoft Research, FLAML has
grown into an open-source library used widely across the industry and
supported by an active community. As we celebrate this milestone, we want to
recognize the passionate contributors and users who have played an essential
role in molding FLAML into the flourishing project it is today. Our heartfelt
gratitude goes out to each of you for your unwavering support, constructive
feedback, and innovative contributions that have driven FLAML to new heights.
A big shoutout to our industrial collaborators from Azure Core, Azure Machine
Learning, Azure Synapse Analytics, Microsoft 365, ML.NET, Vowpal Wabbit,
Anyscale, Databricks, and Wise; and academic collaborators from MIT, Penn
State University, Stevens Institute of Technology, Tel Aviv University, Texas
A & M University, University of Manchester, University of Washington, and The
Chinese University of Hong Kong etc.

We'd also like to take the opportunity to reflect on FLAML's past achievements
and its future roadmap, with a particular focus on large language models (LLM)
and LLMOps.

## FLAML's Journey: Past Achievements and Milestones​ {#flamls-journey-past-achievements-and-milestones}

### Bring AutoML to One's Fingertips​ {#bring-automl-to-ones-fingertips}

FLAML offers an off-the-shelf AutoML solution that enables users to quickly
discover high-quality models or configurations for common ML/AI tasks. By
automatically selecting models and hyperparameters for training or inference,
FLAML saves users time and effort. FLAML has significantly reduced development
time for developers and data scientists alike, while also providing a
convenient way to integrate new algorithms into the pipeline, enabling easy
extensions and large-scale parallel tuning. These features make FLAML a
valuable tool in R&D efforts for many enterprise users. FLAML is capable of
handling a variety of common ML tasks, such as
[classification](<https://microsoft.github.io.docs/Examples/AutoML->
Classification),
[regression](<https://microsoft.github.io.docs/Examples/AutoML->
Regression), [time series
forecasting](<https://microsoft.github.io.docs/Examples/AutoML->
Time%20series%20forecast), [NLP
tasks](https://microsoft.github.io.docs/Examples/AutoML-Rank), and
[generative tasks](https://microsoft.github.io/autogen/), providing a
comprehensive solution for various applications.

### Speed and Efficiency: The FLAML Advantage​ {#speed-and-efficiency-the-flaml-advantage}

What sets FLAML apart from other AutoML libraries is its exceptional
efficiency, thanks to the economical and efficient hyperparameter optimization
and model selection methods developed in our
[research](https://microsoft.github.io.docs/Research). FLAML is also
capable of handling large search spaces with heterogeneous evaluation costs,
complex constraints, guidance, and early stopping. The [zero-shot
AutoML](https://microsoft.github.io.docs/Use-Cases/Zero-Shot-AutoML)
option further reduces the cost of AutoML, making FLAML an even more
attractive solution for a wide range of applications with low resources.

### Easy Customization and Extensibility​ {#easy-customization-and-extensibility}

FLAML is designed for easy extensibility and customization, allowing users to
add custom learners, metrics, search space, etc. For example, the support of
hierarchical search spaces allows one to first choose an ML learner and then
sampling from the hyperparameter space specific to that learner. The level of
customization ranges from minimal (providing only training data and task type
as input) to full (tuning a user-defined function). This flexibility and
support for easy customization have led to FLAML's adoption in various
domains, including security, finance, marketing, engineering, supply chain,
insurance, and healthcare, delivering highly accurate results.

## Embracing Large Language Models in FLAML v2​ {#embracing-large-language-models-in-flaml-v2}

As large language models continue to reshape the AI ecosystem, FLAML is poised
to adapt and grow alongside these advancements. Recognizing the importance of
large language models, we have recently incorporated an autogen package into
FLAML, and are committed to focusing our collective efforts on addressing the
unique challenges that arise in LLMOps (Large Language Model Operations).

In its current iteration, FLAML offers support for model selection and
inference parameter tuning for large language models. We are actively working
on the development of new features, such as low-level inference API with
caching, templating, filtering, and higher-level components like LLM-based
coding and interactive agents, to enable more effective and economical usage
of LLM.

We are eagerly preparing for the launch of FLAML v2, where we will place
special emphasis on incorporating and enhancing features specifically tailored
for large language models (LLMs), further expanding FLAML's capabilities. We
invite contributions from anyone interested in this topic and look forward to
collaborating with the community as we shape the future of FLAML and LLMOps
together.

## For Further Reading​ {#for-further-reading} {#for-further-reading} {#for-further-reading} {#for-further-reading} {#for-further-reading}

* [Documentation about `flaml.autogen`](https://microsoft.github.io/autogen/)
* [Code Example: Tune chatGPT for Math Problem Solving with FLAML](https://github.com/microsoft.blob/main/notebook/autogen_chatgpt_gpt4.ipynb)

 _Do you have any experience to share about LLM applications? Do you like to
see more support or research of LLMOps? Please join
our[Discord](https://discord.gg/Cppx2vSPVP) server for discussion._

 **Tags:**

* [LLM](.blog/tags/llm)
* [LLMOps](.blog/tags/llm-ops)
* [FLAMLv2](.blog/tags/flam-lv-2)

## [Does Model and Inference Parameter Matter in LLM Applications? - A Case {#does-model-and-inference-parameter-matter-in-llm-applications-a-case}

Study for MATH](.blog/2023/04/21/LLM-tuning-math)

April 21, 2023 · 5 min read

[![Chi Wang](https://github.com/sonichi.png)](<https://www.linkedin.com/in/chi->
wang-49b15b16/)

[Chi Wang](https://www.linkedin.com/in/chi-wang-49b15b16/)

Principal Researcher at Microsoft Research

![level 2
algebra](.assets/images/level2algebra-659ba95286432d9945fc89e84d606797.png)

 **TL;DR:**

* **Just by tuning the inference parameters like model, number of responses, temperature etc. without changing any model weights or prompt, the baseline accuracy of untuned gpt-4 can be improved by 20% in high school math competition problems.**
* **For easy problems, the tuned gpt-3.5-turbo model vastly outperformed untuned gpt-4 in accuracy (e.g., 90% vs. 70%) and cost efficiency. For hard problems, the tuned gpt-4 is much more accurate (e.g., 35% vs. 20%) and less expensive than untuned gpt-4.**
* **FLAML can help with model selection, parameter tuning, and cost-saving in LLM applications.**

Large language models (LLMs) are powerful tools that can generate natural
language texts for various applications, such as chatbots, summarization,
translation, and more. GPT-4 is currently the state of the art LLM in the
world. Is model selection irrelevant? What about inference parameters?

In this blog post, we will explore how model and inference parameter matter in
LLM applications, using a case study for [MATH](<https://datasets-benchmarks->
proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-
round2.html), a benchmark for evaluating LLMs on advanced mathematical problem
solving. MATH consists of 12K math competition problems from AMC-10, AMC-12
and AIME. Each problem is accompanied by a step-by-step solution.

We will use the new subpackage [`flaml.autogen`](.docs/Use-
Cases/Autogen) to automatically find the best model and inference parameter
for LLMs on a given task and dataset given an inference budget, using a novel
low-cost search & pruning strategy. FLAML currently supports all the LLMs from
OpenAI, such as GPT-3.5 and GPT-4.

We will use FLAML to perform model selection and inference parameter tuning.
Then we compare the performance and inference cost on solving algebra problems
with the untuned gpt-4. We will also analyze how different difficulty levels
affect the results.

## Experiment Setup​ {#experiment-setup} {#experiment-setup}

We use FLAML to select between the following models with a target inference
budget $0.02 per instance:

* gpt-3.5-turbo, a relatively cheap model that powers the popular ChatGPT app
* gpt-4, the state of the art LLM that costs more than 10 times of gpt-3.5-turbo

We adapt the models using 20 examples in the train set, using the problem
statement as the input and generating the solution as the output. We use the
following inference parameters:

* temperature: The parameter that controls the randomness of the output text. A higher temperature means more diversity but less coherence. We search for the optimal temperature in the range of [0, 1].
* top_p: The parameter that controls the probability mass of the output tokens. Only tokens with a cumulative probability less than or equal to top-p are considered. A lower top-p means more diversity but less coherence. We search for the optimal top-p in the range of [0, 1].
* max_tokens: The maximum number of tokens that can be generated for each output. We search for the optimal max length in the range of [50, 1000].
* n: The number of responses to generate. We search for the optimal n in the range of [1, 100].
* prompt: We use the template: "{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \boxed{{}}." where {problem} will be replaced by the math problem instance.

In this experiment, when n > 1, we find the answer with highest votes among
all the responses and then select it as the final answer to compare with the
ground truth. For example, if n = 5 and 3 of the responses contain a final
answer 301 while 2 of the responses contain a final answer 159, we choose 301
as the final answer. This can help with resolving potential errors due to
randomness. We use the average accuracy and average inference cost as the
metric to evaluate the performance over a dataset. The inference cost of a
particular instance is measured by the price per 1K tokens and the number of
tokens consumed.

## Experiment Results​ {#experiment-results} {#experiment-results} {#experiment-results}

The first figure in this blog post shows the average accuracy and average
inference cost of each configuration on the level 2 Algebra test set.

Surprisingly, the tuned gpt-3.5-turbo model is selected as a better model and
it vastly outperforms untuned gpt-4 in accuracy (92% vs. 70%) with equal or
2.5 times higher inference budget. The same observation can be obtained on the
level 3 Algebra test set.

![level 3
algebra](.assets/images/level3algebra-94e87a683ac8832ac7ae6f41f30131a4.png)

However, the selected model changes on level 4 Algebra.

![level 4
algebra](.assets/images/level4algebra-492beb22490df30d6cc258f061912dcd.png)

This time gpt-4 is selected as the best model. The tuned gpt-4 achieves much
higher accuracy (56% vs. 44%) and lower cost than the untuned gpt-4. On level
5 the result is similar.

![level 5
algebra](.assets/images/level5algebra-8fba701551334296d08580b4b489fe56.png)

We can see that FLAML has found different optimal model and inference
parameters for each subset of a particular level, which shows that these
parameters matter in cost-sensitive LLM applications and need to be carefully
tuned or adapted.

An example notebook to run these experiments can be found at:
<https://github.com/microsoft.blob/v1.2.1/notebook/autogen_chatgpt.ipynb>

## Analysis and Discussion​ {#analysis-and-discussion}

While gpt-3.5-turbo demonstrates competitive accuracy with voted answers in
relatively easy algebra problems under the same inference budget, gpt-4 is a
better choice for the most difficult problems. In general, through parameter
tuning and model selection, we can identify the opportunity to save the
expensive model for more challenging tasks, and improve the overall
effectiveness of a budget-constrained system.

There are many other alternative ways of solving math problems, which we have
not covered in this blog post. When there are choices beyond the inference
parameters, they can be generally tuned via [`flaml.tune`](.docs/Use-
Cases/Tune-User-Defined-Function).

The need for model selection, parameter tuning and cost saving is not specific
to the math problems. The [Auto-GPT](<https://github.com/Significant->
Gravitas/Auto-GPT) project is an example where high cost can easily prevent a
generic complex task to be accomplished as it needs many LLM inference calls.

## For Further Reading​ {#for-further-reading} {#for-further-reading} {#for-further-reading} {#for-further-reading} {#for-further-reading}

* [Research paper about the tuning technique](https://arxiv.org/abs/2303.04673)
* [Documentation about `flaml.autogen`](https://microsoft.github.io/autogen/)

 _Do you have any experience to share about LLM applications? Do you like to
see more support or research of LLM optimization or automation? Please join
our[Discord](https://discord.gg/Cppx2vSPVP) server for discussion._

 **Tags:**

* [LLM](.blog/tags/llm)
* [GPT](.blog/tags/gpt)
* [research](.blog/tags/research)

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

Recent posts

* [All About Agent Descriptions](/autogen/blog/2023/12/29/AgentDescriptions)
* [AgentOptimizer - An Agentic Way to Train Your LLM Agent](/autogen/blog/2023/12/23/AgentOptimizer)
* [AutoGen Studio: Interactively Explore Multi-Agent Workflows](/autogen/blog/2023/12/01/AutoGenStudio)
* [Agent AutoBuild - Automatically Building Multi-agent Systems](/autogen/blog/2023/11/26/Agent-AutoBuild)
* [How to Assess Utility of LLM-powered Applications?](/autogen/blog/2023/11/20/AgentEval)
* [AutoGen Meets GPTs](/autogen/blog/2023/11/13/OAI-assistants)
* [EcoAssistant - Using LLM Assistants More Accurately and Affordably](/autogen/blog/2023/11/09/EcoAssistant)
* [Multimodal with GPT-4V and LLaVA](/autogen/blog/2023/11/06/LMM-Agent)
* [AutoGen's Teachable Agents](/autogen/blog/2023/10/26/TeachableAgent)
* [Retrieval-Augmented Generation (RAG) Applications with AutoGen](/autogen/blog/2023/10/18/RetrieveChat)
* [Use AutoGen for Local LLMs](/autogen/blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](/autogen/blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](/autogen/blog/2023/05/18/GPT-adaptive-humaneval)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](/autogen/blog/2023/04/21/LLM-tuning-math)

# One post tagged with "openai-assistant" {#one-post-tagged-with-openaiassistant}

[View All Tags](/autogen/blog/tags)

## [AutoGen Meets GPTs](/autogen/blog/2023/11/13/OAI-assistants) {#autogen-meets-gptsautogenblog20231113oaiassistants}

November 13, 2023 · 3 min read

[![Gagan
Bansal](https://github.com/gagb.png)](<https://www.linkedin.com/in/gagan->
bansal/)

[Gagan Bansal](https://www.linkedin.com/in/gagan-bansal/)

Senior Researcher at Microsoft Research

![OpenAI
Assistant](/autogen/assets/images/teaser-0b85f5efe5965c70b376b39a4ac34b4e.jpg)

 _AutoGen enables collaboration among multiple ChatGPTs for complex tasks._

## TLDR​ {#tldr}

OpenAI assistants are now integrated into AutoGen via
[`GPTAssistantAgent`](https://github.com/microsoft/autogen/blob/main/autogen/agentchat/contrib/gpt_assistant_agent.py).
This enables multiple OpenAI assistants, which form the backend of the now
popular GPTs, to collaborate and tackle complex tasks. Checkout example
notebooks for reference:

* [Basic example](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_oai_assistant_twoagents_basic.ipynb)
* [Code interpreter](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_oai_code_interpreter.ipynb)
* [Function calls](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_oai_assistant_function_call.ipynb)

## Introduction​ {#introduction}

Earlier last week, OpenAI introduced
[GPTs](https://openai.com/blog/introducing-gpts), giving users ability to
create custom ChatGPTs tailored for them. _But what if these individual GPTs
could collaborate to do even more?_ Fortunately, because of AutoGen, this is
now a reality! AutoGen has been pioneering agents and supporting [multi-agent
workflows](https://aka.ms/autogen-pdf) since earlier this year, and now
(starting with version 0.2.0b5) we are introducing compatibility with the
[Assistant API](https://openai.com/blog/introducing-gpts), which is currently
in beta preview.

To accomplish this, we've added a new (experimental) agent called the
`GPTAssistantAgent` that lets you seamlessly add these new OpenAI assistants
into AutoGen-based multi-agent workflows. This integration shows great
potential and synergy, and we plan to continue enhancing it.

## Installation​ {#installation}

    pip install pyautogen==0.2.0b5  

## Basic Example​ {#basic-example}

Here's a basic example that uses a `UserProxyAgent` to allow an interface with
the `GPTAssistantAgent`.

First, import the new agent and setup `config_list`:

    from autogen import config_list_from_json  
    from autogen.agentchat.contrib.gpt_assistant_agent import GPTAssistantAgent  
    from autogen import UserProxyAgent  
      
    config_list = config_list_from_json("OAI_CONFIG_LIST")  

Then simply define the OpenAI assistant agent and give it the task!

    # creates new assistant using Assistant API  
    gpt_assistant = GPTAssistantAgent(  
        name="assistant",  
        llm_config={  
            "config_list": config_list,  
            "assistant_id": None  
        })  
      
    user_proxy = UserProxyAgent(name="user_proxy",  
        code_execution_config={  
            "work_dir": "coding"  
        },  
        human_input_mode="NEVER")  
      
    user_proxy.initiate_chat(gpt_assistant, message="Print hello world")  

`GPTAssistantAgent` supports both creating new OpenAI assistants or reusing
existing assistants (e.g, by providing an `assistant_id`).

## Code Interpreter Example​ {#code-interpreter-example}

`GPTAssistantAgent` allows you to specify an OpenAI tools (e.g., function
calls, code interpreter, etc). The example below enables an assistant that can
use OpenAI code interpreter to solve tasks.

    # creates new assistant using Assistant API  
    gpt_assistant = GPTAssistantAgent(  
        name="assistant",  
        llm_config={  
            "config_list": config_list,  
            "assistant_id": None,  
            "tools": [  
                {  
                    "type": "code_interpreter"  
                }  
            ],  
        })  
      
    user_proxy = UserProxyAgent(name="user_proxy",  
        code_execution_config={  
            "work_dir": "coding"  
        },  
        human_input_mode="NEVER")  
      
    user_proxy.initiate_chat(gpt_assistant, message="Print hello world")  

Checkout more examples
[here](https://github.com/microsoft/autogen/tree/main/notebook).

## Limitations and Future Work​ {#limitations-and-future-work}

* Group chat managers using GPT assistant are pending.
* GPT assistants with multimodal capabilities haven't been released yet but we are committed to support them.

## Acknowledgements​ {#acknowledgements}

`GPTAssistantAgent` was made possible through collaboration with
[@IANTHEREAL](https://github.com/IANTHEREAL), [Jiale
Liu](https://leoljl.github.io), [Yiran Wu](https://github.com/kevin666aa),
[Qingyun Wu](https://qingyun-wu.github.io/), [Chi
Wang](https://www.microsoft.com/en-us/research/people/chiw/), and many other
AutoGen maintainers.

 **Tags:**

* [openai-assistant](/autogen/blog/tags/openai-assistant)

[**Read More**](/autogen/blog/2023/11/13/OAI-assistants)

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

Recent posts

* [Use flaml.autogen for Local LLMs](.blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](.blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](.blog/2023/05/18/GPT-adaptive-humaneval)
* [Surpassing 1 Million Downloads - A Retrospective and a Look into the Future](.blog/2023/05/07/1M-milestone)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](.blog/2023/04/21/LLM-tuning-math)

# MathChat - An Conversational Framework to Solve Math Problems {#mathchat-an-conversational-framework-to-solve-math-problems}

June 28, 2023 · 8 min read

[![Yiran
Wu](https://github.com/kevin666aa.png)](https://github.com/kevin666aa)

[Yiran Wu](https://github.com/kevin666aa)

PhD student at Pennsylvania State University

![MathChat
WorkFlow](.assets/images/mathchatflow-926a8ed1975a114ab76c69996942c23a.png)
**TL;DR:**

* **We introduce MathChat, a conversational framework leveraging Large Language Models (LLMs), specifically GPT-4, to solve advanced mathematical problems.**
* **MathChat improves LLM 's performance on challenging math problem-solving, outperforming basic prompting and other strategies by about 6%. The improvement was especially notable in the Algebra category, with a 15% increase in accuracy.**
* **Despite the advancement, GPT-4 still struggles to solve very challenging math problems, even with effective prompting strategies. Further improvements are needed, such as the development of more specific assistant models or the integration of new tools and prompts.**

Recent Large Language Models (LLMs) like GTP-3.5 and GPT-4 have demonstrated
astonishing abilities over previous models on various tasks, such as text
generation, question answering, and code generation. Moreover, these models
can communicate with humans through conversations and remember previous
contexts, making it easier for humans to interact with them. These models play
an increasingly important role in our daily lives assisting people with
different tasks, such as writing emails, summarizing documents, and writing
code.

In this blog post, we probe into the problem-solving capabilities of LLMs.
Specifically, we are interested in their capabilities to solve advanced math
problems, which could be representative of a broader class of problems that
require precise reasoning and also have deterministic solutions.

We introduce MathChat, a conversational framework designed for solving
challenging math problems with LLMs. This framework takes advantage of the
chat-optimized feature of state-of-the-art LLMs, where a user proxy agent and
an LLM assistant work together to tackle math problems. We also test previous
prompting techniques for comparison.

## The MathChat Framework​ {#the-mathchat-framework}

MathChat simulates a conversation between the LLM assistant and a user proxy
agent. As the name indicates, the user proxy agent acts as a proxy for the
user, which is responsible for communicating with the LLM assistant and
continuing the conversation in a desired manner.

The proxy agent first presents a math problem to the LLM assistant, framed by
an initial prompt that guides the assistant in how it should work
collaboratively to solve the problem. We incorporate existing prompting
methods such as CoT (chain-of-thought) and tool-using are incorporated in the
initial prompt. Key components of the prompt include:

* **Tool-using Prompt:** This guides the LLM assistant to use Python code in the correct format to address the problem, allowing the user proxy agent to parse the code and provide the appropriate results.

* **Problem-Solving Strategy Selection Prompt:** The assistant is instructed to choose one of three potential problem-solving strategies, including:

    1. Writing a Python program to solve the problem directly.
    2. Solving the problem directly without using Python, which exercises LLMs' inherent reasoning capabilities.
    3. Solving the problem step by step with Python. This should be used when the first two strategies aren't suitable.

* **Final Answer Encapsulation Prompt:** This part instructs the assistant to put the final answer in _\boxed_ {}.

The prompt design in MathChat is flexible and can easily be refined to include
other tools, such as Wolfram Alpha.

Let's take a look at an example between the **User Proxy Agent** and the **LLM
Assistant** (GPT-4). The conversation focuses on how to solve inequality using
Python. (The conversation is modified for readability.)

Participant| Dialogue  
---|---  
User Proxy Agent| (Initial Prompt: Let's solve a math problem with Python...)
Problem: Find all x that satisfy the inequality (2x+10)(x+3) < (3x+9)(x+8).
Express your answer in interval notation.  
LLM assistant| The key idea to solve this problem is to find the critical
points, which are the points where the inequality is equal. Then, test the
intervals between the critical points to find the interval for which the
inequality holds.  
LLM assistant (continued)| Here's Python code to find the critical points:
`from sympy import symbols, Eq, solve; x = symbols("x"); lhs = (2*x + 10)*(x +
3); rhs = (3*x + 9)*(x + 8); equation = Eq(lhs, rhs); critical_points =
solve(equation, x); critical_points`  
User Proxy Agent| The critical points are: [-14, -3]  
LLM assistant| We found the critical points to be x = -14 and x = -3. Now
we'll test the intervals between these critical points as well as points
before and after the critical points. The intervals are: x < -14, -14 < x <
-3, x > -3.  
LLM assistant (continued)| Here's Python code to test the inequality for each
interval: `from sympy import symbols; x = symbols("x"); inequality = (2*x +
10)*(x + 3) < (3*x + 9)*(x + 8); test_points = [-15, -5, 0]; intervals = [];
for point in test_points: if inequality.subs(x, point):
intervals.append(point); intervals`  
User Proxy Agent| The intervals where the inequality holds true are for test
points: [-15, 0]  
LLM assistant| We have tested the intervals and found that the inequality
holds for: 1. x < -14 (test point: -15) 2. x > -3 (test point: 0). Therefore,
the solution to the inequality (2x+10)(x+3) < (3x+9)(x+8) is x ∈ (-∞, -14) ∪
(-3, ∞)  
  
## Experiment Setup​ {#experiment-setup}

We evaluate the improvement brought by MathChat.

For the experiment, we focus on the level-5 problems from the MATH dataset,
which are composed of high school competition problems. These problems include
the application of theorems and complex equation derivation and are
challenging even for undergraduate students. We evaluate 6 of 7 categories
from the dataset (excluding Geometry): Prealgebra, Algebra, Number Theory,
Counting and Probability, Intermediate Algebra, and Precalculus.

We evaluate GPT-4 and use the default configuration of the OpenAI API. To
access the final performance, we manually compare the final answer with the
correct answer. For the vanilla prompt, Program Synthesis, and MathChat, we
have GPT-4 enclose the final answer in _\boxed_ {}, and we take the return of
the function in PoT as the final answer.

We also evaluate the following methods for comparison:

  1. **Vanilla prompting:** Evaluates GPT-4's direct problem-solving capability. The prompt used is: _" Solve the problem carefully. Put the final answer in \boxed{}"_.

  2. **Program of Thoughts (PoT):** Uses a zero-shot PoT prompt that requests the model to create a _Solver_ function to solve the problem and return the final answer.

  3. **Program Synthesis (PS) prompting:** Like PoT, it prompts the model to write a program to solve the problem. The prompt used is: _" Write a program that answers the following question: {Problem}"_.

## Experiment Results​ {#experiment-results}

The accuracy on all the problems with difficulty level-5 from different
categories of the MATH dataset with different methods is shown below:

![Result](.assets/images/result-514251f4c387d5b6e11656e1fa33de08.png)

We found that compared to basic prompting, which demonstrates the innate
capabilities of GPT-4, utilizing Python within the context of PoT or PS
strategy improved the overall accuracy by about 10%. This increase was mostly
seen in categories involving more numerical manipulations, such as Counting &
Probability and Number Theory, and in more complex categories like
Intermediate Algebra and Precalculus.

For categories like Algebra and Prealgebra, PoT and PS showed little
improvement, and in some instances, even led to a decrease in accuracy.
However, MathChat was able to enhance total accuracy by around 6% compared to
PoT and PS, showing competitive performance across all categories. Remarkably,
MathChat improved accuracy in the Algebra category by about 15% over other
methods. Note that categories like Intermediate Algebra and Precalculus
remained challenging for all methods, with only about 20% of problems solved
accurately.

The code for experiments can be found at this
[repository](https://github.com/kevin666aa.tree/gpt_math_solver/flaml/autogen/math).
We now provide an implementation of MathChat using the interactive agents in
FLAML. See this
[notebook](https://github.com/microsoft.blob/main/notebook/autogen_agentchat_MathChat.ipynb)
for example usage.

## Future Directions​ {#future-directions}

Despite MathChat's improvements over previous methods, the results show that
complex math problem is still challenging for recent powerful LLMs, like
GPT-4, even with help from external tools.

Further work can be done to enhance this framework or math problem-solving in
general:

* Although enabling the model to use tools like Python can reduce calculation errors, LLMs are still prone to logic errors. Methods like self-consistency (Sample several solutions and take a major vote on the final answer), or self-verification (use another LLM instance to check whether an answer is correct) might improve the performance.
* Sometimes, whether the LLM can solve the problem depends on the plan it uses. Some plans require less computation and logical reasoning, leaving less room for mistakes.
* MathChat has the potential to be adapted into a copilot system, which could assist users with math problems. This system could allow users to be more involved in the problem-solving process, potentially enhancing learning.

## For Further Reading​ {#for-further-reading}

* [Research paper of MathChat](https://arxiv.org/abs/2306.01337)
* [Documentation about `flaml.autogen`](https://microsoft.github.io/autogen/)

 _Are you working on applications that involve math problem-solving? Would you
appreciate additional research or support on the application of LLM-based
agents for math problem-solving? Please join
our[Discord](https://discord.gg/Cppx2vSPVP) server for discussion._

 **Tags:**

* [LLM](.blog/tags/llm)
* [GPT](.blog/tags/gpt)
* [research](.blog/tags/research)

[Newer Post« Use flaml.autogen for Local LLMs](.blog/2023/07/14/Local-
LLMs)

[Older PostAchieve More, Pay Less - Use GPT-4 Smartly
»](.blog/2023/05/18/GPT-adaptive-humaneval)

* The MathChat Framework
* Experiment Setup
* Experiment Results
* Future Directions
* For Further Reading

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

Recent posts

* [Use flaml.autogen for Local LLMs](.blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](.blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](.blog/2023/05/18/GPT-adaptive-humaneval)
* [Surpassing 1 Million Downloads - A Retrospective and a Look into the Future](.blog/2023/05/07/1M-milestone)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](.blog/2023/04/21/LLM-tuning-math)

# Does Model and Inference Parameter Matter in LLM Applications? - A Case {#does-model-and-inference-parameter-matter-in-llm-applications-a-case}

Study for MATH

April 21, 2023 · 5 min read

[![Chi Wang](https://github.com/sonichi.png)](<https://www.linkedin.com/in/chi->
wang-49b15b16/)

[Chi Wang](https://www.linkedin.com/in/chi-wang-49b15b16/)

Principal Researcher at Microsoft Research

![level 2
algebra](.assets/images/level2algebra-659ba95286432d9945fc89e84d606797.png)

 **TL;DR:**

* **Just by tuning the inference parameters like model, number of responses, temperature etc. without changing any model weights or prompt, the baseline accuracy of untuned gpt-4 can be improved by 20% in high school math competition problems.**
* **For easy problems, the tuned gpt-3.5-turbo model vastly outperformed untuned gpt-4 in accuracy (e.g., 90% vs. 70%) and cost efficiency. For hard problems, the tuned gpt-4 is much more accurate (e.g., 35% vs. 20%) and less expensive than untuned gpt-4.**
* **FLAML can help with model selection, parameter tuning, and cost-saving in LLM applications.**

Large language models (LLMs) are powerful tools that can generate natural
language texts for various applications, such as chatbots, summarization,
translation, and more. GPT-4 is currently the state of the art LLM in the
world. Is model selection irrelevant? What about inference parameters?

In this blog post, we will explore how model and inference parameter matter in
LLM applications, using a case study for [MATH](<https://datasets-benchmarks->
proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-
round2.html), a benchmark for evaluating LLMs on advanced mathematical problem
solving. MATH consists of 12K math competition problems from AMC-10, AMC-12
and AIME. Each problem is accompanied by a step-by-step solution.

We will use the new subpackage
[`flaml.autogen`](.blog/2023/04/21/docs/Use-Cases/Autogen) to
automatically find the best model and inference parameter for LLMs on a given
task and dataset given an inference budget, using a novel low-cost search &
pruning strategy. FLAML currently supports all the LLMs from OpenAI, such as
GPT-3.5 and GPT-4.

We will use FLAML to perform model selection and inference parameter tuning.
Then we compare the performance and inference cost on solving algebra problems
with the untuned gpt-4. We will also analyze how different difficulty levels
affect the results.

## Experiment Setup​ {#experiment-setup}

We use FLAML to select between the following models with a target inference
budget $0.02 per instance:

* gpt-3.5-turbo, a relatively cheap model that powers the popular ChatGPT app
* gpt-4, the state of the art LLM that costs more than 10 times of gpt-3.5-turbo

We adapt the models using 20 examples in the train set, using the problem
statement as the input and generating the solution as the output. We use the
following inference parameters:

* temperature: The parameter that controls the randomness of the output text. A higher temperature means more diversity but less coherence. We search for the optimal temperature in the range of [0, 1].
* top_p: The parameter that controls the probability mass of the output tokens. Only tokens with a cumulative probability less than or equal to top-p are considered. A lower top-p means more diversity but less coherence. We search for the optimal top-p in the range of [0, 1].
* max_tokens: The maximum number of tokens that can be generated for each output. We search for the optimal max length in the range of [50, 1000].
* n: The number of responses to generate. We search for the optimal n in the range of [1, 100].
* prompt: We use the template: "{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \boxed{{}}." where {problem} will be replaced by the math problem instance.

In this experiment, when n > 1, we find the answer with highest votes among
all the responses and then select it as the final answer to compare with the
ground truth. For example, if n = 5 and 3 of the responses contain a final
answer 301 while 2 of the responses contain a final answer 159, we choose 301
as the final answer. This can help with resolving potential errors due to
randomness. We use the average accuracy and average inference cost as the
metric to evaluate the performance over a dataset. The inference cost of a
particular instance is measured by the price per 1K tokens and the number of
tokens consumed.

## Experiment Results​ {#experiment-results}

The first figure in this blog post shows the average accuracy and average
inference cost of each configuration on the level 2 Algebra test set.

Surprisingly, the tuned gpt-3.5-turbo model is selected as a better model and
it vastly outperforms untuned gpt-4 in accuracy (92% vs. 70%) with equal or
2.5 times higher inference budget. The same observation can be obtained on the
level 3 Algebra test set.

![level 3
algebra](.assets/images/level3algebra-94e87a683ac8832ac7ae6f41f30131a4.png)

However, the selected model changes on level 4 Algebra.

![level 4
algebra](.assets/images/level4algebra-492beb22490df30d6cc258f061912dcd.png)

This time gpt-4 is selected as the best model. The tuned gpt-4 achieves much
higher accuracy (56% vs. 44%) and lower cost than the untuned gpt-4. On level
5 the result is similar.

![level 5
algebra](.assets/images/level5algebra-8fba701551334296d08580b4b489fe56.png)

We can see that FLAML has found different optimal model and inference
parameters for each subset of a particular level, which shows that these
parameters matter in cost-sensitive LLM applications and need to be carefully
tuned or adapted.

An example notebook to run these experiments can be found at:
<https://github.com/microsoft.blob/v1.2.1/notebook/autogen_chatgpt.ipynb>

## Analysis and Discussion​ {#analysis-and-discussion}

While gpt-3.5-turbo demonstrates competitive accuracy with voted answers in
relatively easy algebra problems under the same inference budget, gpt-4 is a
better choice for the most difficult problems. In general, through parameter
tuning and model selection, we can identify the opportunity to save the
expensive model for more challenging tasks, and improve the overall
effectiveness of a budget-constrained system.

There are many other alternative ways of solving math problems, which we have
not covered in this blog post. When there are choices beyond the inference
parameters, they can be generally tuned via
[`flaml.tune`](.blog/2023/04/21/docs/Use-Cases/Tune-User-Defined-
Function).

The need for model selection, parameter tuning and cost saving is not specific
to the math problems. The [Auto-GPT](<https://github.com/Significant->
Gravitas/Auto-GPT) project is an example where high cost can easily prevent a
generic complex task to be accomplished as it needs many LLM inference calls.

## For Further Reading​ {#for-further-reading}

* [Research paper about the tuning technique](https://arxiv.org/abs/2303.04673)
* [Documentation about `flaml.autogen`](https://microsoft.github.io/autogen/)

 _Do you have any experience to share about LLM applications? Do you like to
see more support or research of LLM optimization or automation? Please join
our[Discord](https://discord.gg/Cppx2vSPVP) server for discussion._

 **Tags:**

* [LLM](.blog/tags/llm)
* [GPT](.blog/tags/gpt)
* [research](.blog/tags/research)

[Newer Post« Surpassing 1 Million Downloads - A Retrospective and a Look into
the Future](.blog/2023/05/07/1M-milestone)

* Experiment Setup
* Experiment Results
* Analysis and Discussion
* For Further Reading

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* [Getting Started](.docs/Getting-Started)
* [Installation](.docs/Installation)
* Use Cases
* Examples
  * [AutoGen - Automated Multi Agent Chat](.docs/Examples/AutoGen-AgentChat)
  * [AutoGen - Tune GPT Models](.docs/Examples/AutoGen-OpenAI)
  * [AutoML - Classification](.docs/Examples/AutoML-Classification)
  * [AutoML - NLP](.docs/Examples/AutoML-NLP)
  * [AutoML - Rank](.docs/Examples/AutoML-Rank)
  * [AutoML - Regression](.docs/Examples/AutoML-Regression)
  * [AutoML - Time Series Forecast](.docs/Examples/AutoML-Time series forecast)
  * [AutoML for LightGBM](.docs/Examples/AutoML-for-LightGBM)
  * [AutoML for XGBoost](.docs/Examples/AutoML-for-XGBoost)
  * [Default - Flamlized Estimator](.docs/Examples/Default-Flamlized)
  * [Integrate - AzureML](.docs/Examples/Integrate - AzureML)
  * [Integrate - Scikit-learn Pipeline](.docs/Examples/Integrate - Scikit-learn Pipeline)
  * [Integrate - Spark](.docs/Examples/Integrate - Spark)
  * [Tune - AzureML pipeline](.docs/Examples/Tune-AzureML-pipeline)
  * [Tune - HuggingFace](.docs/Examples/Tune-HuggingFace)
  * [Tune - Lexicographic Objectives](.docs/Examples/Tune-Lexicographic-objectives)
  * [Tune - PyTorch](.docs/Examples/Tune-PyTorch)
* [Contributing](.docs/Contribute)
* [Research](.docs/Research)

On this page

# AutoML for LightGBM {#automl-for-lightgbm}

### Prerequisites for this example​ {#prerequisites-for-this-example}

Install the [automl] option.

    pip install "flaml[automl] matplotlib openml"  

### Use built-in LGBMEstimator​ {#use-builtin-lgbmestimator}

    from flaml import AutoML  
    from flaml.automl.data import load_openml_dataset  
      
    # Download [houses dataset](https://www.openml.org/d/537) from OpenML. The task is to predict median price of the house in the region based on demographic composition and a state of housing market in the region.  
    X_train, X_test, y_train, y_test = load_openml_dataset(dataset_id=537, data_dir='./')  
      
    automl = AutoML()  
    settings = {  
        "time_budget": 60,  # total running time in seconds  
        "metric": 'r2',  # primary metrics for regression can be chosen from: ['mae','mse','r2']  
        "estimator_list": ['lgbm'],  # list of ML learners; we tune lightgbm in this example  
        "task": 'regression',  # task type  
        "log_file_name": 'houses_experiment.log',  # flaml log file  
        "seed": 7654321,  # random seed  
    }  
    automl.fit(X_train=X_train, y_train=y_train, **settings)  

#### Sample output​ {#sample-output}

    [flaml.automl: 11-15 19:46:44] {1485} INFO - Data split method: uniform  
    [flaml.automl: 11-15 19:46:44] {1489} INFO - Evaluation method: cv  
    [flaml.automl: 11-15 19:46:44] {1540} INFO - Minimizing error metric: 1-r2  
    [flaml.automl: 11-15 19:46:44] {1577} INFO - List of ML learners in AutoML Run: ['lgbm']  
    [flaml.automl: 11-15 19:46:44] {1826} INFO - iteration 0, current learner lgbm  
    [flaml.automl: 11-15 19:46:44] {1944} INFO - Estimated sufficient time budget=3232s. Estimated necessary time budget=3s.  
    [flaml.automl: 11-15 19:46:44] {2029} INFO -  at 0.5s,  estimator lgbm's best error=0.7383, best estimator lgbm's best error=0.7383  
    [flaml.automl: 11-15 19:46:44] {1826} INFO - iteration 1, current learner lgbm  
    [flaml.automl: 11-15 19:46:44] {2029} INFO -  at 0.6s,  estimator lgbm's best error=0.4774, best estimator lgbm's best error=0.4774  
    [flaml.automl: 11-15 19:46:44] {1826} INFO - iteration 2, current learner lgbm  
    [flaml.automl: 11-15 19:46:44] {2029} INFO -  at 0.7s,  estimator lgbm's best error=0.4774, best estimator lgbm's best error=0.4774  
    [flaml.automl: 11-15 19:46:44] {1826} INFO - iteration 3, current learner lgbm  
    [flaml.automl: 11-15 19:46:44] {2029} INFO -  at 0.9s,  estimator lgbm's best error=0.2985, best estimator lgbm's best error=0.2985  
    [flaml.automl: 11-15 19:46:44] {1826} INFO - iteration 4, current learner lgbm  
    [flaml.automl: 11-15 19:46:45] {2029} INFO -  at 1.3s,  estimator lgbm's best error=0.2337, best estimator lgbm's best error=0.2337  
    [flaml.automl: 11-15 19:46:45] {1826} INFO - iteration 5, current learner lgbm  
    [flaml.automl: 11-15 19:46:45] {2029} INFO -  at 1.4s,  estimator lgbm's best error=0.2337, best estimator lgbm's best error=0.2337  
    [flaml.automl: 11-15 19:46:45] {1826} INFO - iteration 6, current learner lgbm  
    [flaml.automl: 11-15 19:46:46] {2029} INFO -  at 2.5s,  estimator lgbm's best error=0.2219, best estimator lgbm's best error=0.2219  
    [flaml.automl: 11-15 19:46:46] {1826} INFO - iteration 7, current learner lgbm  
    [flaml.automl: 11-15 19:46:46] {2029} INFO -  at 2.9s,  estimator lgbm's best error=0.2219, best estimator lgbm's best error=0.2219  
    [flaml.automl: 11-15 19:46:46] {1826} INFO - iteration 8, current learner lgbm  
    [flaml.automl: 11-15 19:46:48] {2029} INFO -  at 4.5s,  estimator lgbm's best error=0.1764, best estimator lgbm's best error=0.1764  
    [flaml.automl: 11-15 19:46:48] {1826} INFO - iteration 9, current learner lgbm  
    [flaml.automl: 11-15 19:46:54] {2029} INFO -  at 10.5s, estimator lgbm's best error=0.1630, best estimator lgbm's best error=0.1630  
    [flaml.automl: 11-15 19:46:54] {1826} INFO - iteration 10, current learner lgbm  
    [flaml.automl: 11-15 19:46:56] {2029} INFO -  at 12.4s, estimator lgbm's best error=0.1630, best estimator lgbm's best error=0.1630  
    [flaml.automl: 11-15 19:46:56] {1826} INFO - iteration 11, current learner lgbm  
    [flaml.automl: 11-15 19:47:13] {2029} INFO -  at 29.0s, estimator lgbm's best error=0.1630, best estimator lgbm's best error=0.1630  
    [flaml.automl: 11-15 19:47:13] {1826} INFO - iteration 12, current learner lgbm  
    [flaml.automl: 11-15 19:47:15] {2029} INFO -  at 31.1s, estimator lgbm's best error=0.1630, best estimator lgbm's best error=0.1630  
    [flaml.automl: 11-15 19:47:15] {1826} INFO - iteration 13, current learner lgbm  
    [flaml.automl: 11-15 19:47:29] {2029} INFO -  at 45.8s, estimator lgbm's best error=0.1564, best estimator lgbm's best error=0.1564  
    [flaml.automl: 11-15 19:47:33] {2242} INFO - retrain lgbm for 3.2s  
    [flaml.automl: 11-15 19:47:33] {2247} INFO - retrained model: LGBMRegressor(colsample_bytree=0.8025848209352517,  
                  learning_rate=0.09100963138990374, max_bin=255,  
                  min_child_samples=42, n_estimators=363, num_leaves=216,  
                  reg_alpha=0.001113000336715291, reg_lambda=76.50614276906414,  
                  verbose=-1)  
    [flaml.automl: 11-15 19:47:33] {1608} INFO - fit succeeded  
    [flaml.automl: 11-15 19:47:33] {1610} INFO - Time taken to find the best model: 45.75616669654846  
    [flaml.automl: 11-15 19:47:33] {1624} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.  

#### Retrieve best config​ {#retrieve-best-config}

    print('Best hyperparmeter config:', automl.best_config)  
    print('Best r2 on validation data: {0:.4g}'.format(1-automl.best_loss))  
    print('Training duration of best run: {0:.4g} s'.format(automl.best_config_train_time))  
    print(automl.model.estimator)  
    # Best hyperparmeter config: {'n_estimators': 363, 'num_leaves': 216, 'min_child_samples': 42, 'learning_rate': 0.09100963138990374, 'log_max_bin': 8, 'colsample_bytree': 0.8025848209352517, 'reg_alpha': 0.001113000336715291, 'reg_lambda': 76.50614276906414}  
    # Best r2 on validation data: 0.8436  
    # Training duration of best run: 3.229 s  
    # LGBMRegressor(colsample_bytree=0.8025848209352517,  
    #               learning_rate=0.09100963138990374, max_bin=255,  
    #               min_child_samples=42, n_estimators=363, num_leaves=216,  
    #               reg_alpha=0.001113000336715291, reg_lambda=76.50614276906414,  
    #               verbose=-1)  

#### Plot feature importance​ {#plot-feature-importance}

    import matplotlib.pyplot as plt  
    plt.barh(automl.feature_names_in_, automl.feature_importances_)  

![png](data:image/png;base64,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)

#### Compute predictions of testing dataset​ {#compute-predictions-of-testing-dataset}

    y_pred = automl.predict(X_test)  
    print('Predicted labels', y_pred)  
    # Predicted labels [143391.65036562 245535.13731811 153171.44071629 ... 184354.52735963  
    #  235510.49470445 282617.22858956]  

#### Compute different metric values on testing dataset​ {#compute-different-metric-values-on-testing-dataset}

    from flaml.automl.ml import sklearn_metric_loss_score  
      
    print('r2', '=', 1 - sklearn_metric_loss_score('r2', y_pred, y_test))  
    print('mse', '=', sklearn_metric_loss_score('mse', y_pred, y_test))  
    print('mae', '=', sklearn_metric_loss_score('mae', y_pred, y_test))  
    # r2 = 0.8505434326526395  
    # mse = 1975592613.138005  
    # mae = 29471.536046068788  

#### Compare with untuned LightGBM​ {#compare-with-untuned-lightgbm}

    from lightgbm import LGBMRegressor  
      
    lgbm = LGBMRegressor()  
    lgbm.fit(X_train, y_train)  
    y_pred = lgbm.predict(X_test)  
    from flaml.automl.ml import sklearn_metric_loss_score  
      
    print('default lgbm r2', '=', 1 - sklearn_metric_loss_score('r2', y_pred, y_test))  
    # default lgbm r2 = 0.8296179648694404  

#### Plot learning curve​ {#plot-learning-curve}

How does the model accuracy improve as we search for different hyperparameter
configurations?

    from flaml.automl.data import get_output_from_log  
    import numpy as np  
      
    time_history, best_valid_loss_history, valid_loss_history, config_history, metric_history =  
        get_output_from_log(filename=settings['log_file_name'], time_budget=60)  
    plt.title('Learning Curve')  
    plt.xlabel('Wall Clock Time (s)')  
    plt.ylabel('Validation r2')  
    plt.step(time_history, 1 - np.array(best_valid_loss_history), where='post')  
    plt.show()  

![png](data:image/png;base64,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)

### Use a customized LightGBM learner​ {#use-a-customized-lightgbm-learner}

The native API of LightGBM allows one to specify a custom objective function
in the model constructor. You can easily enable it by adding a customized
LightGBM learner in FLAML. In the following example, we show how to add such a
customized LightGBM learner with a custom objective function.

#### Create a customized LightGBM learner with a custom objective function​ {#create-a-customized-lightgbm-learner-with-a-custom-objective-function}

    import numpy as np  
      
      
    # define your customized objective function  
    def my_loss_obj(y_true, y_pred):  
        c = 0.5  
        residual = y_pred - y_true  
        grad = c * residual / (np.abs(residual) + c)  
        hess = c ** 2 / (np.abs(residual) + c) ** 2  
        # rmse grad and hess  
        grad_rmse = residual  
        hess_rmse = 1.0  
      
        # mae grad and hess  
        grad_mae = np.array(residual)  
        grad_mae[grad_mae > 0] = 1.  
        grad_mae[grad_mae <= 0] = -1.  
        hess_mae = 1.0  
      
        coef = [0.4, 0.3, 0.3]  
        return coef[0] * grad + coef[1] * grad_rmse + coef[2] * grad_mae,  
               coef[0] * hess + coef[1] * hess_rmse + coef[2] * hess_mae  
      
      
    from flaml.automl.model import LGBMEstimator  
      
      
    class MyLGBM(LGBMEstimator):  
        """LGBMEstimator with my_loss_obj as the objective function"""  
      
        def __init__(self, **config):  
            super().__init__(objective=my_loss_obj, **config)  

#### Add the customized learner and tune it​ {#add-the-customized-learner-and-tune-it}

    automl = AutoML()  
    automl.add_learner(learner_name='my_lgbm', learner_class=MyLGBM)  
    settings["estimator_list"] = ['my_lgbm']  # change the estimator list  
    automl.fit(X_train=X_train, y_train=y_train, **settings)  

[Link to
notebook](https://github.com/microsoft.blob/main/notebook/automl_lightgbm.ipynb)
| [Open in
colab](https://colab.research.google.com/github/microsoft.blob/main/notebook/automl_lightgbm.ipynb)

[Edit this
page](<https://github.com/microsoft.edit/main/website/docs/Examples/AutoML->
for-LightGBM.md)

[Previous« AutoML - Time Series Forecast](.docs/Examples/AutoML-Time
series forecast)

[NextAutoML for XGBoost »](.docs/Examples/AutoML-for-XGBoost)

* Prerequisites for this example
* Use built-in LGBMEstimator
* Use a customized LightGBM learner

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

Recent posts

* [Use flaml.autogen for Local LLMs](.blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](.blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](.blog/2023/05/18/GPT-adaptive-humaneval)
* [Surpassing 1 Million Downloads - A Retrospective and a Look into the Future](.blog/2023/05/07/1M-milestone)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](.blog/2023/04/21/LLM-tuning-math)

# One post tagged with "LLMOps" {#one-post-tagged-with-llmops}

[View All Tags](.blog/tags)

## [Surpassing 1 Million Downloads - A Retrospective and a Look into the {#surpassing-1-million-downloads-a-retrospective-and-a-look-into-the}

Future](.blog/2023/05/07/1M-milestone)

May 7, 2023 · 4 min read

[![Qingyun Wu](https://github.com/qingyun-wu.png)](<https://qingyun->
wu.github.io/)

[Qingyun Wu](https://qingyun-wu.github.io/)

Assistant Professor at the Pennsylvania State University

 **TL;DR:**

* **Celebrating FLAML 's milestone: 1 million downloads**
* **Introducing Large Language Model (LLM) support in the upcoming FLAML v2**

This week, FLAML has reached a significant milestone: 1 million downloads.
Originating as an intern research project within Microsoft Research, FLAML has
grown into an open-source library used widely across the industry and
supported by an active community. As we celebrate this milestone, we want to
recognize the passionate contributors and users who have played an essential
role in molding FLAML into the flourishing project it is today. Our heartfelt
gratitude goes out to each of you for your unwavering support, constructive
feedback, and innovative contributions that have driven FLAML to new heights.
A big shoutout to our industrial collaborators from Azure Core, Azure Machine
Learning, Azure Synapse Analytics, Microsoft 365, ML.NET, Vowpal Wabbit,
Anyscale, Databricks, and Wise; and academic collaborators from MIT, Penn
State University, Stevens Institute of Technology, Tel Aviv University, Texas
A & M University, University of Manchester, University of Washington, and The
Chinese University of Hong Kong etc.

We'd also like to take the opportunity to reflect on FLAML's past achievements
and its future roadmap, with a particular focus on large language models (LLM)
and LLMOps.

## FLAML's Journey: Past Achievements and Milestones​ {#flamls-journey-past-achievements-and-milestones}

### Bring AutoML to One's Fingertips​ {#bring-automl-to-ones-fingertips}

FLAML offers an off-the-shelf AutoML solution that enables users to quickly
discover high-quality models or configurations for common ML/AI tasks. By
automatically selecting models and hyperparameters for training or inference,
FLAML saves users time and effort. FLAML has significantly reduced development
time for developers and data scientists alike, while also providing a
convenient way to integrate new algorithms into the pipeline, enabling easy
extensions and large-scale parallel tuning. These features make FLAML a
valuable tool in R&D efforts for many enterprise users. FLAML is capable of
handling a variety of common ML tasks, such as
[classification](<https://microsoft.github.io.docs/Examples/AutoML->
Classification),
[regression](<https://microsoft.github.io.docs/Examples/AutoML->
Regression), [time series
forecasting](<https://microsoft.github.io.docs/Examples/AutoML->
Time%20series%20forecast), [NLP
tasks](https://microsoft.github.io.docs/Examples/AutoML-Rank), and
[generative tasks](https://microsoft.github.io/autogen/), providing a
comprehensive solution for various applications.

### Speed and Efficiency: The FLAML Advantage​ {#speed-and-efficiency-the-flaml-advantage}

What sets FLAML apart from other AutoML libraries is its exceptional
efficiency, thanks to the economical and efficient hyperparameter optimization
and model selection methods developed in our
[research](https://microsoft.github.io.docs/Research). FLAML is also
capable of handling large search spaces with heterogeneous evaluation costs,
complex constraints, guidance, and early stopping. The [zero-shot
AutoML](https://microsoft.github.io.docs/Use-Cases/Zero-Shot-AutoML)
option further reduces the cost of AutoML, making FLAML an even more
attractive solution for a wide range of applications with low resources.

### Easy Customization and Extensibility​ {#easy-customization-and-extensibility}

FLAML is designed for easy extensibility and customization, allowing users to
add custom learners, metrics, search space, etc. For example, the support of
hierarchical search spaces allows one to first choose an ML learner and then
sampling from the hyperparameter space specific to that learner. The level of
customization ranges from minimal (providing only training data and task type
as input) to full (tuning a user-defined function). This flexibility and
support for easy customization have led to FLAML's adoption in various
domains, including security, finance, marketing, engineering, supply chain,
insurance, and healthcare, delivering highly accurate results.

## Embracing Large Language Models in FLAML v2​ {#embracing-large-language-models-in-flaml-v2}

As large language models continue to reshape the AI ecosystem, FLAML is poised
to adapt and grow alongside these advancements. Recognizing the importance of
large language models, we have recently incorporated an autogen package into
FLAML, and are committed to focusing our collective efforts on addressing the
unique challenges that arise in LLMOps (Large Language Model Operations).

In its current iteration, FLAML offers support for model selection and
inference parameter tuning for large language models. We are actively working
on the development of new features, such as low-level inference API with
caching, templating, filtering, and higher-level components like LLM-based
coding and interactive agents, to enable more effective and economical usage
of LLM.

We are eagerly preparing for the launch of FLAML v2, where we will place
special emphasis on incorporating and enhancing features specifically tailored
for large language models (LLMs), further expanding FLAML's capabilities. We
invite contributions from anyone interested in this topic and look forward to
collaborating with the community as we shape the future of FLAML and LLMOps
together.

## For Further Reading​ {#for-further-reading}

* [Documentation about `flaml.autogen`](https://microsoft.github.io/autogen/)
* [Code Example: Tune chatGPT for Math Problem Solving with FLAML](https://github.com/microsoft.blob/main/notebook/autogen_chatgpt_gpt4.ipynb)

 _Do you have any experience to share about LLM applications? Do you like to
see more support or research of LLMOps? Please join
our[Discord](https://discord.gg/Cppx2vSPVP) server for discussion._

 **Tags:**

* [LLM](.blog/tags/llm)
* [LLMOps](.blog/tags/llm-ops)
* [FLAMLv2](.blog/tags/flam-lv-2)

[**Read More**](.blog/2023/05/07/1M-milestone)

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

* Reference
  * agentchat
  * oai
  * [code_utils](/autogen/docs/reference/code_utils)
  * [function_utils](/autogen/docs/reference/function_utils)
  * [math_utils](/autogen/docs/reference/math_utils)
  * [retrieve_utils](/autogen/docs/reference/retrieve_utils)
  * [token_count_utils](/autogen/docs/reference/token_count_utils)

On this page

# token_count_utils {#tokencountutils}

#### token_left​ {#tokenleft}
```python
    def token_left(input: Union[str, List, Dict],  
                   model="gpt-3.5-turbo-0613") -> int  

Count number of tokens left for an OpenAI model.

 **Arguments** :

* `input` \- (str, list, dict): Input to the model.
* `model` \- (str): Model name.

 **Returns** :

* `int` \- Number of tokens left that the model can use for completion.

#### count_token​ {#counttoken}
```python
    def count_token(input: Union[str, List, Dict],  
                    model: str = "gpt-3.5-turbo-0613") -> int  

Count number of tokens used by an OpenAI model.

 **Arguments** :

* `input` \- (str, list, dict): Input to the model.
* `model` \- (str): Model name.

 **Returns** :

* `int` \- Number of tokens from the input.

#### num_tokens_from_functions​ {#numtokensfromfunctions}
```python
    def num_tokens_from_functions(functions, model="gpt-3.5-turbo-0613") -> int  

Return the number of tokens used by a list of functions.

 **Arguments** :

* `functions` \- (list): List of function descriptions that will be passed in model.
* `model` \- (str): Model name.

 **Returns** :

* `int` \- Number of tokens from the function descriptions.

[Edit this
page](https://github.com/microsoft/autogen/edit/main/website/docs/reference/token_count_utils.md)

[Previous« retrieve_utils](/autogen/docs/reference/retrieve_utils)

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* [Getting Started](.docs/Getting-Started)
* [Installation](.docs/Installation)
* Use Cases
  * [AutoGen for Large Language Models](.docs/Use-Cases/Autogen)
  * [Task Oriented AutoML](.docs/Use-Cases/Task-Oriented-AutoML)
  * [Tune User Defined Function](.docs/Use-Cases/Tune-User-Defined-Function)
  * [Zero Shot AutoML](.docs/Use-Cases/Zero-Shot-AutoML)
* Examples
* [Contributing](.docs/Contribute)
* [Research](.docs/Research)

On this page

# Task Oriented AutoML {#task-oriented-automl}

## Overview​ {#overview}

[`flaml.AutoML`](.docs/reference/automl/automl#automl-objects) is a
class for task-oriented AutoML. It can be used as a scikit-learn style
estimator with the standard `fit` and `predict` functions. The minimal inputs
from users are the training data and the task type.

* Training data:
  * numpy array. When the input data are stored in numpy array, they are passed to `fit()` as `X_train` and `y_train`.
  * pandas dataframe. When the input data are stored in pandas dataframe, they are passed to `fit()` either as `X_train` and `y_train`, or as `dataframe` and `label`.
* Tasks (specified via `task`):
  * 'classification': classification with tabular data.
  * 'regression': regression with tabular data.
  * 'ts_forecast': time series forecasting.
  * 'ts_forecast_classification': time series forecasting for classification.
  * 'ts_forecast_panel': time series forecasting for panel datasets (multiple time series).
  * 'rank': learning to rank.
  * 'seq-classification': sequence classification.
  * 'seq-regression': sequence regression.
  * 'summarization': text summarization.
  * 'token-classification': token classification.
  * 'multichoice-classification': multichoice classification.

Two optional inputs are `time_budget` and `max_iter` for searching models and
hyperparameters. When both are unspecified, only one model per estimator will
be trained (using our [zero-shot](.docs/Use-Cases/Zero-Shot-AutoML)
technique). When `time_budget` is provided, there can be randomness in the
result due to runtime variance.

A typical way to use `flaml.AutoML`:

    # Prepare training data  
    # ...  
    from flaml import AutoML  
    automl = AutoML()  
    automl.fit(X_train, y_train, task="regression", time_budget=60, **other_settings)  
    # Save the model  
    with open("automl.pkl", "wb") as f:  
        pickle.dump(automl, f, pickle.HIGHEST_PROTOCOL)  
      
    # At prediction time  
    with open("automl.pkl", "rb") as f:  
        automl = pickle.load(f)  
    pred = automl.predict(X_test)  

If users provide the minimal inputs only, `AutoML` uses the default settings
for optimization metric, estimator list etc.

## Customize AutoML.fit()​ {#customize-automlfit}

### Optimization metric​ {#optimization-metric}

The optimization metric is specified via the `metric` argument. It can be
either a string which refers to a built-in metric, or a user-defined function.

* Built-in metric.
  * 'accuracy': 1 - accuracy as the corresponding metric to minimize.
  * 'log_loss': default metric for multiclass classification.
  * 'r2': 1 - r2_score as the corresponding metric to minimize. Default metric for regression.
  * 'rmse': root mean squared error.
  * 'mse': mean squared error.
  * 'mae': mean absolute error.
  * 'mape': mean absolute percentage error.
  * 'roc_auc': minimize 1 - roc_auc_score. Default metric for binary classification.
  * 'roc_auc_ovr': minimize 1 - roc_auc_score with `multi_class="ovr"`.
  * 'roc_auc_ovo': minimize 1 - roc_auc_score with `multi_class="ovo"`.
  * 'roc_auc_weighted': minimize 1 - roc_auc_score with `average="weighted"`.
  * 'roc_auc_ovr_weighted': minimize 1 - roc_auc_score with `multi_class="ovr"` and `average="weighted"`.
  * 'roc_auc_ovo_weighted': minimize 1 - roc_auc_score with `multi_class="ovo"` and `average="weighted"`.
  * 'f1': minimize 1 - f1_score.
  * 'micro_f1': minimize 1 - f1_score with `average="micro"`.
  * 'macro_f1': minimize 1 - f1_score with `average="macro"`.
  * 'ap': minimize 1 - average_precision_score.
  * 'ndcg': minimize 1 - ndcg_score.
  * 'ndcg@k': minimize 1 - ndcg_score@k. k is an integer.
* User-defined function. A customized metric function that requires the following (input) signature, and returns the input config’s value in terms of the metric you want to minimize, and a dictionary of auxiliary information at your choice:
```python
    def custom_metric(  
        X_val, y_val, estimator, labels,  
        X_train, y_train, weight_val=None, weight_train=None,  
        config=None, groups_val=None, groups_train=None,  
    ):  
        return metric_to_minimize, metrics_to_log  
```

For example,
```python
    def custom_metric(  
        X_val, y_val, estimator, labels,  
        X_train, y_train, weight_val=None, weight_train=None,  
        *args,  
    ):  
        from sklearn.metrics import log_loss  
        import time  
      
        start = time.time()  
        y_pred = estimator.predict_proba(X_val)  
        pred_time = (time.time() - start) / len(X_val)  
        val_loss = log_loss(y_val, y_pred, labels=labels, sample_weight=weight_val)  
        y_pred = estimator.predict_proba(X_train)  
        train_loss = log_loss(y_train, y_pred, labels=labels, sample_weight=weight_train)  
        alpha = 0.5  
        return val_loss * (1 + alpha) - alpha * train_loss, {  
            "val_loss": val_loss,  
            "train_loss": train_loss,  
            "pred_time": pred_time,  
        }  

It returns the validation loss penalized by the gap between validation and
training loss as the metric to minimize, and three metrics to log: val_loss,
train_loss and pred_time. The arguments `config`, `groups_val` and
`groups_train` are not used in the function.

### Estimator and search space​ {#estimator-and-search-space}

The estimator list can contain one or more estimator names, each corresponding
to a built-in estimator or a custom estimator. Each estimator has a search
space for hyperparameter configurations. FLAML supports both classical machine
learning models and deep neural networks.

#### Estimator​ {#estimator}

* Built-in estimator.
  * 'lgbm': LGBMEstimator for task "classification", "regression", "rank", "ts_forecast" and "ts_forecast_classification". Hyperparameters: n_estimators, num_leaves, min_child_samples, learning_rate, log_max_bin (logarithm of (max_bin + 1) with base 2), colsample_bytree, reg_alpha, reg_lambda.
  * 'xgboost': XGBoostSkLearnEstimator for task "classification", "regression", "rank", "ts_forecast" and "ts_forecast_classification". Hyperparameters: n_estimators, max_leaves, min_child_weight, learning_rate, subsample, colsample_bylevel, colsample_bytree, reg_alpha, reg_lambda.
  * 'xgb_limitdepth': XGBoostLimitDepthEstimator for task "classification", "regression", "rank", "ts_forecast" and "ts_forecast_classification". Hyperparameters: n_estimators, max_depth, min_child_weight, learning_rate, subsample, colsample_bylevel, colsample_bytree, reg_alpha, reg_lambda.
  * 'rf': RandomForestEstimator for task "classification", "regression", "ts_forecast" and "ts_forecast_classification". Hyperparameters: n_estimators, max_features, max_leaves, criterion (for classification only). Starting from v1.1.0, it uses a fixed random_state by default.
  * 'extra_tree': ExtraTreesEstimator for task "classification", "regression", "ts_forecast" and "ts_forecast_classification". Hyperparameters: n_estimators, max_features, max_leaves, criterion (for classification only). Starting from v1.1.0, it uses a fixed random_state by default.
  * 'histgb': HistGradientBoostingEstimator for task "classification", "regression", "ts_forecast" and "ts_forecast_classification". Hyperparameters: n_estimators, max_leaves, min_samples_leaf, learning_rate, log_max_bin (logarithm of (max_bin + 1) with base 2), l2_regularization. It uses a fixed random_state by default.
  * 'lrl1': LRL1Classifier (sklearn.LogisticRegression with L1 regularization) for task "classification". Hyperparameters: C.
  * 'lrl2': LRL2Classifier (sklearn.LogisticRegression with L2 regularization) for task "classification". Hyperparameters: C.
  * 'catboost': CatBoostEstimator for task "classification" and "regression". Hyperparameters: early_stopping_rounds, learning_rate, n_estimators.
  * 'kneighbor': KNeighborsEstimator for task "classification" and "regression". Hyperparameters: n_neighbors.
  * 'prophet': Prophet for task "ts_forecast". Hyperparameters: changepoint_prior_scale, seasonality_prior_scale, holidays_prior_scale, seasonality_mode.
  * 'arima': ARIMA for task "ts_forecast". Hyperparameters: p, d, q.
  * 'sarimax': SARIMAX for task "ts_forecast". Hyperparameters: p, d, q, P, D, Q, s.
  * 'holt-winters': Holt-Winters (triple exponential smoothing) model for task "ts_forecast". Hyperparameters: seasonal_perdiods, seasonal, use_boxcox, trend, damped_trend.
  * 'transformer': Huggingface transformer models for task "seq-classification", "seq-regression", "multichoice-classification", "token-classification" and "summarization". Hyperparameters: learning_rate, num_train_epochs, per_device_train_batch_size, warmup_ratio, weight_decay, adam_epsilon, seed.
  * 'temporal_fusion_transformer': TemporalFusionTransformerEstimator for task "ts_forecast_panel". Hyperparameters: gradient_clip_val, hidden_size, hidden_continuous_size, attention_head_size, dropout, learning_rate. There is a [known issue](https://github.com/jdb78/pytorch-forecasting/issues/1145) with pytorch-forecast logging.
* Custom estimator. Use custom estimator for:
  * tuning an estimator that is not built-in;
  * customizing search space for a built-in estimator.

#### Guidelines on tuning a custom estimator​ {#guidelines-on-tuning-a-custom-estimator}

To tune a custom estimator that is not built-in, you need to:

  1. Build a custom estimator by inheritting [`flaml.automl.model.BaseEstimator`](.docs/reference/automl/model#baseestimator-objects) or a derived class. For example, if you have a estimator class with scikit-learn style `fit()` and `predict()` functions, you only need to set `self.estimator_class` to be that class in your constructor.

    from flaml.automl.model import SKLearnEstimator  
    # SKLearnEstimator is derived from BaseEstimator  
    import rgf  
      
      
    class MyRegularizedGreedyForest(SKLearnEstimator):  
        def __init__(self, task="binary", **config):  
            super().__init__(task, **config)  
      
            if task in CLASSIFICATION:  
            from rgf.sklearn import RGFClassifier  
      
            self.estimator_class = RGFClassifier  
            else:  
            from rgf.sklearn import RGFRegressor  
      
            self.estimator_class = RGFRegressor  
      
        @classmethod  
        def search_space(cls, data_size, task):  
            space = {  
            "max_leaf": {  
                "domain": tune.lograndint(lower=4, upper=data_size),  
                "low_cost_init_value": 4,  
            },  
            "n_iter": {  
                "domain": tune.lograndint(lower=1, upper=data_size),  
                "low_cost_init_value": 1,  
            },  
            "learning_rate": {"domain": tune.loguniform(lower=0.01, upper=20.0)},  
            "min_samples_leaf": {  
                "domain": tune.lograndint(lower=1, upper=20),  
                "init_value": 20,  
            },  
            }  
            return space  

In the constructor, we set `self.estimator_class` as `RGFClassifier` or
`RGFRegressor` according to the task type. If the estimator you want to tune
does not have a scikit-learn style `fit()` and `predict()` API, you can
override the `fit()` and `predict()` function of
`flaml.automl.model.BaseEstimator`, like
[XGBoostEstimator](.docs/reference/automl/model#xgboostestimator-
objects). Importantly, we also add the `task="binary"` parameter in the
signature of `__init__` so that it doesn't get grouped together with the
`**config` kwargs that determines the parameters with which the underlying
estimator (`self.estimator_class`) is constructed. If your estimator doesn't
use one of the parameters that it is passed, for example some regressors in
`scikit-learn` don't use the `n_jobs` parameter, it is enough to add
`n_jobs=None` to the signature so that it is ignored by the `**config` dict.

  2. Give the custom estimator a name and add it in AutoML. E.g.,

    from flaml import AutoML  
    automl = AutoML()  
    automl.add_learner("rgf", MyRegularizedGreedyForest)  

This registers the `MyRegularizedGreedyForest` class in AutoML, with the name
"rgf".

  3. Tune the newly added custom estimator in either of the following two ways depending on your needs:

* tune rgf alone: `automl.fit(..., estimator_list=["rgf"])`; or
* mix it with other built-in learners: `automl.fit(..., estimator_list=["rgf", "lgbm", "xgboost", "rf"])`.

#### Search space​ {#search-space}

Each estimator class, built-in or not, must have a `search_space` function. In
the `search_space` function, we return a dictionary about the hyperparameters,
the keys of which are the names of the hyperparameters to tune, and each value
is a set of detailed search configurations about the corresponding
hyperparameters represented in a dictionary. A search configuration dictionary
includes the following fields:

* `domain`, which specifies the possible values of the hyperparameter and their distribution. Please refer to [more details about the search space domain](.docs/Use-Cases/Tune-User-Defined-Function#more-details-about-the-search-space-domain).
* `init_value` (optional), which specifies the initial value of the hyperparameter.
* `low_cost_init_value`(optional), which specifies the value of the hyperparameter that is associated with low computation cost. See [cost related hyperparameters](.docs/Use-Cases/Tune-User-Defined-Function#cost-related-hyperparameters) or [FAQ](.docs/FAQ#about-low_cost_partial_config-in-tune) for more details.

In the example above, we tune four hyperparameters, three integers and one
float. They all follow a log-uniform distribution. "max_leaf" and "n_iter"
have "low_cost_init_value" specified as their values heavily influence the
training cost.

To customize the search space for a built-in estimator, use a similar approach
to define a class that inherits the existing estimator. For example,

    from flaml.automl.model import XGBoostEstimator  
      
      
    def logregobj(preds, dtrain):  
        labels = dtrain.get_label()  
        preds = 1.0 / (1.0 + np.exp(-preds))  # transform raw leaf weight  
        grad = preds - labels  
        hess = preds * (1.0 - preds)  
        return grad, hess  
      
      
    class MyXGB1(XGBoostEstimator):  
        """XGBoostEstimator with logregobj as the objective function"""  
      
        def __init__(self, **config):  
            super().__init__(objective=logregobj, **config)  

We override the constructor and set the training objective as a custom
function `logregobj`. The hyperparameters and their search range do not
change. For another example,

    class XGBoost2D(XGBoostSklearnEstimator):  
        @classmethod  
        def search_space(cls, data_size, task):  
            upper = min(32768, int(data_size))  
            return {  
                "n_estimators": {  
                    "domain": tune.lograndint(lower=4, upper=upper),  
                    "low_cost_init_value": 4,  
                },  
                "max_leaves": {  
                    "domain": tune.lograndint(lower=4, upper=upper),  
                    "low_cost_init_value": 4,  
                },  
            }  

We override the `search_space` function to tune two hyperparameters only,
"n_estimators" and "max_leaves". They are both random integers in the log
space, ranging from 4 to data-dependent upper bound. The lower bound for each
corresponds to low training cost, hence the "low_cost_init_value" for each is
set to 4.

##### A shortcut to override the search space​ {#a-shortcut-to-override-the-search-space}

One can use the `custom_hp` argument in
[`AutoML.fit()`](.docs/reference/automl/automl#fit) to override the
search space for an existing estimator quickly. For example, if you would like
to temporarily change the search range of "n_estimators" of xgboost, disable
searching "max_leaves" in random forest, and add "subsample" in the search
space of lightgbm, you can set:

    custom_hp = {  
        "xgboost": {  
            "n_estimators": {  
                "domain": tune.lograndint(lower=new_lower, upper=new_upper),  
                "low_cost_init_value": new_lower,  
            },  
        },  
        "rf": {  
            "max_leaves": {  
                "domain": None,  # disable search  
            },  
        },  
        "lgbm": {  
            "subsample": {  
                "domain": tune.uniform(lower=0.1, upper=1.0),  
                "init_value": 1.0,  
            },  
            "subsample_freq": {  
                "domain": 1,  # subsample_freq must > 0 to enable subsample  
            },  
        },  
    }  

### Constraint​ {#constraint}

There are several types of constraints you can impose.

  1. Constraints on the AutoML process.

* `time_budget`: constrains the wall-clock time (seconds) used by the AutoML process. We provide some tips on how to set time budget.

* `max_iter`: constrains the maximal number of models to try in the AutoML process.

  2. Constraints on the constructor arguments of the estimators.

Some constraints on the estimator can be implemented via the custom learner.
For example,

    class MonotonicXGBoostEstimator(XGBoostSklearnEstimator):  
        @classmethod  
        def search_space(**args):  
            space = super().search_space(**args)  
            space.update({"monotone_constraints": {"domain": "(1, -1)"}})  
            return space  

It adds a monotonicity constraint to XGBoost. This approach can be used to set
any constraint that is an argument in the underlying estimator's constructor.
A shortcut to do this is to use the `custom_hp` argument:

    custom_hp = {  
        "xgboost": {  
            "monotone_constraints": {  
                "domain": "(1, -1)"  # fix the domain as a constant  
            }  
        }  
    }  

  3. Constraints on the models tried in AutoML.

Users can set constraints such as the maximal number of models to try, limit
on training time and prediction time per model.

* `train_time_limit`: training time in seconds.
* `pred_time_limit`: prediction time per instance in seconds.

For example,

    automl.fit(X_train, y_train, max_iter=100, train_time_limit=1, pred_time_limit=1e-3)  

  4. Constraints on the metrics of the ML model tried in AutoML.

When users provide a custom metric function, which returns a primary
optimization metric and a dictionary of additional metrics (typically also
about the model) to log, users can also specify constraints on one or more of
the metrics in the dictionary of additional metrics.

Users need to provide a list of such constraints in the following format: Each
element in this list is a 3-tuple, which shall be expressed in the following
format: the first element of the 3-tuple is the name of the metric, the second
element is the inequality sign chosen from ">=" and "<=", and the third
element is the constraint value. E.g., `('val_loss', '<=', 0.1)`.

For example,

    metric_constraints = [("train_loss", "<=", 0.1), ("val_loss", "<=", 0.1)]  
    automl.fit(X_train, y_train, max_iter=100, train_time_limit=1, metric_constraints=metric_constraints)  

### Ensemble​ {#ensemble}

To use stacked ensemble after the model search, set `ensemble=True` or a dict.
When `ensemble=True`, the final estimator and `passthrough` in the stacker
will be automatically chosen. You can specify customized final estimator or
passthrough option:

* "final_estimator": an instance of the final estimator in the stacker.
* "passthrough": True (default) or False, whether to pass the original features to the stacker.

For example,

    automl.fit(  
        X_train, y_train, task="classification",  
        "ensemble": {  
            "final_estimator": LogisticRegression(),  
            "passthrough": False,  
        },  
    )  

### Resampling strategy​ {#resampling-strategy}

By default, flaml decides the resampling automatically according to the data
size and the time budget. If you would like to enforce a certain resampling
strategy, you can set `eval_method` to be "holdout" or "cv" for holdout or
cross-validation.

For holdout, you can also set:

* `split_ratio`: the fraction for validation data, 0.1 by default.
* `X_val`, `y_val`: a separate validation dataset. When they are passed, the validation metrics will be computed against this given validation dataset. If they are not passed, then a validation dataset will be split from the training data and held out from training during the model search. After the model search, flaml will retrain the model with best configuration on the full training data. You can set`retrain_full` to be `False` to skip the final retraining or "budget" to ask flaml to do its best to retrain within the time budget.

For cross validation, you can also set `n_splits` of the number of folds. By
default it is 5.

#### Data split method​ {#data-split-method}

flaml relies on the provided task type to infer the default splitting
strategy:

* stratified split for classification;
* uniform split for regression;
* time-based split for time series forecasting;
* group-based split for learning to rank.

The data split method for classification can be changed into uniform split by
setting `split_type="uniform"`. The data are shuffled when `split_type in
("uniform", "stratified")`.

For both classification and regression tasks more advanced split
configurations are possible:

* time-based split can be enforced if the data are sorted by timestamps, by setting `split_type="time"`,
* group-based splits can be set by using `split_type="group"` while providing the group identifier for each sample through the `groups` argument. This is also shown in an [example notebook](https://github.com/microsoft.blob/main/notebook/basics/understanding_cross_validation.ipynb).

More in general, `split_type` can also be set as a custom splitter object,
when `eval_method="cv"`. It needs to be an instance of a derived class of
scikit-learn [KFold](<https://scikit->
learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold)
and have `split` and `get_n_splits` methods with the same signatures. To
disable shuffling, the splitter instance must contain the attribute
`shuffle=False`.

### Parallel tuning​ {#parallel-tuning}

When you have parallel resources, you can either spend them in training and
keep the model search sequential, or perform parallel search. Following
scikit-learn, the parameter `n_jobs` specifies how many CPU cores to use for
each training job. The number of parallel trials is specified via the
parameter `n_concurrent_trials`. By default, `n_jobs=-1,
n_concurrent_trials=1`. That is, all the CPU cores (in a single compute node)
are used for training a single model and the search is sequential. When you
have more resources than what each single training job needs, you can consider
increasing `n_concurrent_trials`.

FLAML now support two backends for parallel tuning, i.e., `Ray` and `Spark`.
You can use either of them, but not both for one tuning job.

#### Parallel tuning with Ray​ {#parallel-tuning-with-ray}

To do parallel tuning with Ray, install the `ray` and `blendsearch` options:

    pip install flaml[ray,blendsearch]  

`ray` is used to manage the resources. For example,

    ray.init(num_cpus=16)  

allocates 16 CPU cores. Then, when you run:

    automl.fit(X_train, y_train, n_jobs=4, n_concurrent_trials=4)  

flaml will perform 4 trials in parallel, each consuming 4 CPU cores. The
parallel tuning uses the [BlendSearch](.docs/Use-Cases/Tune-User-
Defined-Function##blendsearch-economical-hyperparameter-optimization-with-
blended-search-strategy) algorithm.

#### Parallel tuning with Spark​ {#parallel-tuning-with-spark}

To do parallel tuning with Spark, install the `spark` and `blendsearch`
options:

> _Spark support is added in v1.1.0_
>  
>  
>      pip install flaml[spark,blendsearch]>=1.1.0  
>     >
>
> Copy

For more details about installing Spark, please refer to
[Installation](.docs/Installation#distributed-tuning).

An example of using Spark for parallel tuning is:

    automl.fit(X_train, y_train, n_concurrent_trials=4, use_spark=True)  

Details about parallel tuning with Spark could be found
[here](.docs/Examples/Integrate - Spark#parallel-spark-jobs). For Spark
clusters, by default, we will launch one trial per executor. However,
sometimes we want to launch more trials than the number of executors (e.g.,
local mode). In this case, we can set the environment variable
`FLAML_MAX_CONCURRENT` to override the detected `num_executors`. The final
number of concurrent trials will be the minimum of `n_concurrent_trials` and
`num_executors`. Also, GPU training is not supported yet when use_spark is
True.

#### **Guidelines on parallel vs sequential tuning**​

 **(1) Considerations on wall-clock time.**

One common motivation for parallel tuning is to save wall-clock time. When
sequential tuning and parallel tuning achieve a similar wall-clock time,
sequential tuning should be preferred. This is a rule of thumb when the HPO
algorithm is sequential by nature (e.g., Bayesian Optimization and FLAML's HPO
algorithms CFO and BS). Sequential tuning allows the HPO algorithms to take
advantage of the historical trial results. Then the question is **How to
estimate the wall-clock-time needed by parallel tuning and sequential
tuning**?

You can use the following way to roughly estimate the wall-clock time in
parallel tuning and sequential tuning: To finish NNN trials of hyperparameter
tuning, i.e., run NNN hyperparameter configurations, the total wall-clock time
needed is N/k∗(SingleTrialTime+Overhead)N/k*(SingleTrialTime +
Overhead)N/k∗(SingleTrialTime+Overhead), in which
SingleTrialTimeSingleTrialTimeSingleTrialTime is the trial time to evaluate a
particular hyperparameter configuration, kkk is the scale of parallelism,
e.g., the number of parallel CPU/GPU cores, and OverheadOverheadOverhead is
the computation overhead.

In sequential tuning, k=1k=1k=1, and in parallel tuning k>1k>1k>1. This may
suggest that parallel tuning has a shorter wall-clock time. But it is not
always the case considering the other two factors
SingleTrialTimeSingleTrialTimeSingleTrialTime, and OverheadOverheadOverhead:

* The OverheadOverheadOverhead in sequential tuning is typically negligible; while in parallel tuning, it is relatively large.

* You can also try to reduce the SingleTrialTimeSingleTrialTimeSingleTrialTime to reduce the wall-clock time in sequential tuning: For example, by increasing the resource consumed by a single trial (distributed or multi-thread training), you can reduce SingleTrialTimeSingleTrialTimeSingleTrialTime. One concrete example is to use the `n_jobs` parameter that sets the number of threads the fitting process can use in many scikit-learn style algorithms.

 **(2) Considerations on randomness.**

Potential reasons that cause randomness:

  1. Parallel tuning: In the case of parallel tuning, the order of trials' finishing time is no longer deterministic. This non-deterministic order, combined with sequential HPO algorithms, leads to a non-deterministic hyperparameter tuning trajectory.

  2. Distributed or multi-thread training: Distributed/multi-thread training may introduce randomness in model training, i.e., the trained model with the same hyperparameter may be different because of such randomness. This model-level randomness may be undesirable in some cases.

### Warm start​ {#warm-start}

We can warm start the AutoML by providing starting points of hyperparameter
configurstions for each estimator. For example, if you have run AutoML for one
hour, after checking the results, you would like to run it for another two
hours, then you can use the best configurations found for each estimator as
the starting points for the new run.

    automl1 = AutoML()  
    automl1.fit(X_train, y_train, time_budget=3600)  
    automl2 = AutoML()  
    automl2.fit(X_train, y_train, time_budget=7200, starting_points=automl1.best_config_per_estimator)  

`starting_points` is a dictionary or a str to specify the starting
hyperparameter config. (1) When it is a dictionary, the keys are the estimator
names. If you do not need to specify starting points for an estimator, exclude
its name from the dictionary. The value for each key can be either a
dictionary of a list of dictionaries, corresponding to one hyperparameter
configuration, or multiple hyperparameter configurations, respectively. (2)
When it is a str: if "data", use data-dependent defaults; if "data:path", use
data-dependent defaults which are stored at path; if "static", use data-
independent defaults. Please find more details about data-dependent defaults
in [zero shot AutoML](.docs/Use-Cases/Zero-Shot-AutoML#combine-zero-
shot-automl-and-hyperparameter-tuning).

### Log the trials​ {#log-the-trials}

The trials are logged in a file if a `log_file_name` is passed. Each trial is
logged as a json record in one line. The best trial's id is logged in the last
line. For example,

    {"record_id": 0, "iter_per_learner": 1, "logged_metric": null, "trial_time": 0.12717914581298828, "wall_clock_time": 0.1728971004486084, "validation_loss": 0.07333333333333332, "config": {"n_estimators": 4, "num_leaves": 4, "min_child_samples": 20, "learning_rate": 0.09999999999999995, "log_max_bin": 8, "colsample_bytree": 1.0, "reg_alpha": 0.0009765625, "reg_lambda": 1.0}, "learner": "lgbm", "sample_size": 150}  
    {"record_id": 1, "iter_per_learner": 3, "logged_metric": null, "trial_time": 0.07027268409729004, "wall_clock_time": 0.3756711483001709, "validation_loss": 0.05333333333333332, "config": {"n_estimators": 4, "num_leaves": 4, "min_child_samples": 12, "learning_rate": 0.2677050123105203, "log_max_bin": 7, "colsample_bytree": 1.0, "reg_alpha": 0.001348364934537134, "reg_lambda": 1.4442580148221913}, "learner": "lgbm", "sample_size": 150}  
    {"curr_best_record_id": 1}  

  1. `iter_per_learner` means how many models have been tried for each learner. The reason you see records like `iter_per_learner=3` for `record_id=1` is that flaml only logs better configs than the previous iters by default, i.e., `log_type='better'`. If you use `log_type='all'` instead, all the trials will be logged.
  2. `trial_time` means the time taken to train and evaluate one config in that trial. `total_search_time` is the total time spent from the beginning of `fit()`.
  3. flaml will adjust the `n_estimators` for lightgbm etc. according to the remaining budget and check the time budget constraint and stop in several places. Most of the time that makes `fit()` stops before the given budget. Occasionally it may run over the time budget slightly. But the log file always contains the best config info and you can recover the best model until any time point using `retrain_from_log()`.

We can also use mlflow for logging:

    mlflow.set_experiment("flaml")  
    with mlflow.start_run():  
        automl.fit(X_train=X_train, y_train=y_train, **settings)  

To disable mlflow logging pre-configured in FLAML, set `mlflow_logging=False`:

    automl = AutoML(mlflow_logging=False)  

or

    automl.fit(X_train=X_train, y_train=y_train, mlflow_logging=False, **settings)  

Setting `mlflow_logging=False` in the constructor will disable mlflow logging
for all the `fit()` calls. Setting `mlflow_logging=False` in `fit()` will
disable mlflow logging for that `fit()` call only.

### Extra fit arguments​ {#extra-fit-arguments}

Extra fit arguments that are needed by the estimators can be passed to
`AutoML.fit()`. For example, if there is a weight associated with each
training example, they can be passed via `sample_weight`. For another example,
`period` can be passed for time series forecaster. For any extra keywork
argument passed to `AutoML.fit()` which has not been explicitly listed in the
function signature, it will be passed to the underlying estimators' `fit()` as
is. For another example, you can set the number of gpus used by each trial
with the `gpu_per_trial` argument, which is only used by TransformersEstimator
and XGBoostSklearnEstimator.

In addition, you can specify the different arguments needed by different
estimators using the `fit_kwargs_by_estimator` argument. For example, you can
set the custom arguments for a Transformers model:

    from flaml.automl.data import load_openml_dataset  
    from flaml import AutoML  
      
    X_train, X_test, y_train, y_test = load_openml_dataset(dataset_id=1169, data_dir="./")  
      
    automl = AutoML()  
    automl_settings = {  
        "task": "classification",  
        "time_budget": 10,  
        "estimator_list": ["catboost", "rf"],  
        "fit_kwargs_by_estimator": {  
            "catboost": {  
                "verbose": True,  # setting the verbosity of catboost to True  
            }  
        },  
    }  
    automl.fit(X_train=X_train, y_train=y_train, **automl_settings)  

## Retrieve the Outcomes​ {#retrieve-the-outcomes}

### Get best model​ {#get-best-model}

The best model can be obtained by the `model` property of an `AutoML`
instance. For example,

    automl.fit(X_train, y_train, task="regression")  
    print(automl.model)  
    # <flaml.automl.model.LGBMEstimator object at 0x7f9b502c4550>  

[`flaml.automl.model.LGBMEstimator`](.docs/reference/automl/model#lgbmestimator-
objects) is a wrapper class for LightGBM models. To access the underlying
model, use the `estimator` property of the `flaml.automl.model.LGBMEstimator`
instance.

    print(automl.model.estimator)  
    '''  
    LGBMRegressor(colsample_bytree=0.7610534336273627,  
                  learning_rate=0.41929025492645006, max_bin=255,  
                  min_child_samples=4, n_estimators=45, num_leaves=4,  
                  reg_alpha=0.0009765625, reg_lambda=0.009280655005879943,  
                  verbose=-1)  
    '''  

Just like a normal LightGBM model, we can inspect it. For example, we can plot
the feature importance:

    import matplotlib.pyplot as plt  
    plt.barh(automl.model.estimator.feature_name_, automl.model.estimator.feature_importances_)  

![png](data:image/png;base64,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)

### Get best configuration​ {#get-best-configuration}

We can find the best estimator's name and best configuration by:

    print(automl.best_estimator)  
    # lgbm  
    print(automl.best_config)  
    # {'n_estimators': 148, 'num_leaves': 18, 'min_child_samples': 3, 'learning_rate': 0.17402065726724145, 'log_max_bin': 8, 'colsample_bytree': 0.6649148062238498, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.0067613624509965}  

We can also find the best configuration per estimator.

    print(automl.best_config_per_estimator)  
    # {'lgbm': {'n_estimators': 148, 'num_leaves': 18, 'min_child_samples': 3, 'learning_rate': 0.17402065726724145, 'log_max_bin': 8, 'colsample_bytree': 0.6649148062238498, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.0067613624509965}, 'rf': None, 'catboost': None, 'xgboost': {'n_estimators': 4, 'max_leaves': 4, 'min_child_weight': 1.8630223791106992, 'learning_rate': 1.0, 'subsample': 0.8513627344387318, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.946138073111236, 'reg_alpha': 0.0018311776973217073, 'reg_lambda': 0.27901659190538414}, 'extra_tree': {'n_estimators': 4, 'max_features': 1.0, 'max_leaves': 4}}  

The `None` value corresponds to the estimators which have not been tried.

Other useful information:

    print(automl.best_config_train_time)  
    # 0.24841618537902832  
    print(automl.best_iteration)  
    # 10  
    print(automl.best_loss)  
    # 0.15448622217577546  
    print(automl.time_to_find_best_model)  
    # 0.4167296886444092  
    print(automl.config_history)  
    # {0: ('lgbm', {'n_estimators': 4, 'num_leaves': 4, 'min_child_samples': 20, 'learning_rate': 0.09999999999999995, 'log_max_bin': 8, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.0}, 1.2300517559051514)}  
    # Meaning: at iteration 0, the config tried is {'n_estimators': 4, 'num_leaves': 4, 'min_child_samples': 20, 'learning_rate': 0.09999999999999995, 'log_max_bin': 8, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.0} for lgbm, and the wallclock time is 1.23s when this trial is finished.  

### Plot learning curve​ {#plot-learning-curve}

To plot how the loss is improved over time during the model search, first load
the search history from the log file:

    from flaml.automl.data import get_output_from_log  
      
    time_history, best_valid_loss_history, valid_loss_history, config_history, metric_history =  
        get_output_from_log(filename=settings["log_file_name"], time_budget=120)  

Then, assuming the optimization metric is "accuracy", we can plot the accuracy
versus wallclock time:

    import matplotlib.pyplot as plt  
    import numpy as np  
      
    plt.title("Learning Curve")  
    plt.xlabel("Wall Clock Time (s)")  
    plt.ylabel("Validation Accuracy")  
    plt.step(time_history, 1 - np.array(best_valid_loss_history), where="post")  
    plt.show()  

![png](data:image/png;base64,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)

The curve suggests that increasing the time budget may further improve the
accuracy.

### How to set time budget​ {#how-to-set-time-budget}

* If you have an exact constraint for the total search time, set it as the time budget.
* If you have flexible time constraints, for example, your desirable time budget is t1=60s, and the longest time budget you can tolerate is t2=3600s, you can try the following two ways:

  1. set t1 as the time budget, and check the message in the console log in the end. If the budget is too small, you will see a warning like

> WARNING - Time taken to find the best model is 91% of the provided time
> budget and not all estimators' hyperparameter search converged. Consider
> increasing the time budget.

  2. set t2 as the time budget, and also set `early_stop=True`. If the early stopping is triggered, you will see a warning like

> WARNING - All estimator hyperparameters local search has converged at least
> once, and the total search time exceeds 10 times the time taken to find the
> best model.

> WARNING - Stopping search as early_stop is set to True.

### How much time is needed to find the best model​ {#how-much-time-is-needed-to-find-the-best-model}

If you want to get a sense of how much time is needed to find the best model,
you can use `max_iter=2` to perform two trials first. The message will be
like:

> INFO - iteration 0, current learner lgbm

> INFO - Estimated sufficient time budget=145194s. Estimated necessary time
> budget=2118s.

> INFO - at 2.6s, estimator lgbm's best error=0.4459, best estimator lgbm's
> best error=0.4459

You will see that the time to finish the first and cheapest trial is 2.6
seconds. The estimated necessary time budget is 2118 seconds, and the
estimated sufficient time budget is 145194 seconds. Note that this is only an
estimated range to help you decide your budget.

When the time budget is set too low, it can happen that no estimator is
trained at all within the budget. In this case, it is recommanded to use
`max_iter` instead of `time_budget`. This ensures that you have enough time to
train a model without worring about variance of the execution time for the
code before starting a trainning.

[Edit this
page](<https://github.com/microsoft.edit/main/website/docs/Use->
Cases/Task-Oriented-AutoML.md)

[Previous« AutoGen for Large Language Models](.docs/Use-Cases/Autogen)

[NextTune User Defined Function »](.docs/Use-Cases/Tune-User-Defined-
Function)

* Overview
* Customize AutoML.fit()
  * Optimization metric
  * Estimator and search space
  * Constraint
  * Ensemble
  * Resampling strategy
  * Parallel tuning
  * Warm start
  * Log the trials
  * Extra fit arguments
* Retrieve the Outcomes
  * Get best model
  * Get best configuration
  * Plot learning curve
  * How to set time budget
  * How much time is needed to find the best model

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* [Getting Started](.docs/Getting-Started)
* [Installation](.docs/Installation)
* Use Cases
* Examples
  * [AutoGen - Automated Multi Agent Chat](.docs/Examples/AutoGen-AgentChat)
  * [AutoGen - Tune GPT Models](.docs/Examples/AutoGen-OpenAI)
  * [AutoML - Classification](.docs/Examples/AutoML-Classification)
  * [AutoML - NLP](.docs/Examples/AutoML-NLP)
  * [AutoML - Rank](.docs/Examples/AutoML-Rank)
  * [AutoML - Regression](.docs/Examples/AutoML-Regression)
  * [AutoML - Time Series Forecast](.docs/Examples/AutoML-Time series forecast)
  * [AutoML for LightGBM](.docs/Examples/AutoML-for-LightGBM)
  * [AutoML for XGBoost](.docs/Examples/AutoML-for-XGBoost)
  * [Default - Flamlized Estimator](.docs/Examples/Default-Flamlized)
  * [Integrate - AzureML](.docs/Examples/Integrate - AzureML)
  * [Integrate - Scikit-learn Pipeline](.docs/Examples/Integrate - Scikit-learn Pipeline)
  * [Integrate - Spark](.docs/Examples/Integrate - Spark)
  * [Tune - AzureML pipeline](.docs/Examples/Tune-AzureML-pipeline)
  * [Tune - HuggingFace](.docs/Examples/Tune-HuggingFace)
  * [Tune - Lexicographic Objectives](.docs/Examples/Tune-Lexicographic-objectives)
  * [Tune - PyTorch](.docs/Examples/Tune-PyTorch)
* [Contributing](.docs/Contribute)
* [Research](.docs/Research)

On this page

# AutoML - Regression {#automl-regression}

### Prerequisites​ {#prerequisites}

Install the [automl] option.

    pip install "flaml[automl]"  

### A basic regression example​ {#a-basic-regression-example}

    from flaml import AutoML  
    from sklearn.datasets import fetch_california_housing  
      
    # Initialize an AutoML instance  
    automl = AutoML()  
    # Specify automl goal and constraint  
    automl_settings = {  
        "time_budget": 1,  # in seconds  
        "metric": 'r2',  
        "task": 'regression',  
        "log_file_name": "california.log",  
    }  
    X_train, y_train = fetch_california_housing(return_X_y=True)  
    # Train with labeled input data  
    automl.fit(X_train=X_train, y_train=y_train,  
               **automl_settings)  
    # Predict  
    print(automl.predict(X_train))  
    # Print the best model  
    print(automl.model.estimator)  

#### Sample output​ {#sample-output}

    [flaml.automl: 11-15 07:08:19] {1485} INFO - Data split method: uniform  
    [flaml.automl: 11-15 07:08:19] {1489} INFO - Evaluation method: holdout  
    [flaml.automl: 11-15 07:08:19] {1540} INFO - Minimizing error metric: 1-r2  
    [flaml.automl: 11-15 07:08:19] {1577} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'catboost', 'xgboost', 'extra_tree']  
    [flaml.automl: 11-15 07:08:19] {1826} INFO - iteration 0, current learner lgbm  
    [flaml.automl: 11-15 07:08:19] {1944} INFO - Estimated sufficient time budget=846s. Estimated necessary time budget=2s.  
    [flaml.automl: 11-15 07:08:19] {2029} INFO -  at 0.2s,  estimator lgbm's best error=0.7393,     best estimator lgbm's best error=0.7393  
    [flaml.automl: 11-15 07:08:19] {1826} INFO - iteration 1, current learner lgbm  
    [flaml.automl: 11-15 07:08:19] {2029} INFO -  at 0.3s,  estimator lgbm's best error=0.7393,     best estimator lgbm's best error=0.7393  
    [flaml.automl: 11-15 07:08:19] {1826} INFO - iteration 2, current learner lgbm  
    [flaml.automl: 11-15 07:08:19] {2029} INFO -  at 0.3s,  estimator lgbm's best error=0.5446,     best estimator lgbm's best error=0.5446  
    [flaml.automl: 11-15 07:08:19] {1826} INFO - iteration 3, current learner lgbm  
    [flaml.automl: 11-15 07:08:19] {2029} INFO -  at 0.4s,  estimator lgbm's best error=0.2807,     best estimator lgbm's best error=0.2807  
    [flaml.automl: 11-15 07:08:19] {1826} INFO - iteration 4, current learner lgbm  
    [flaml.automl: 11-15 07:08:19] {2029} INFO -  at 0.5s,  estimator lgbm's best error=0.2712,     best estimator lgbm's best error=0.2712  
    [flaml.automl: 11-15 07:08:19] {1826} INFO - iteration 5, current learner lgbm  
    [flaml.automl: 11-15 07:08:19] {2029} INFO -  at 0.5s,  estimator lgbm's best error=0.2712,     best estimator lgbm's best error=0.2712  
    [flaml.automl: 11-15 07:08:19] {1826} INFO - iteration 6, current learner lgbm  
    [flaml.automl: 11-15 07:08:20] {2029} INFO -  at 0.6s,  estimator lgbm's best error=0.2712,     best estimator lgbm's best error=0.2712  
    [flaml.automl: 11-15 07:08:20] {1826} INFO - iteration 7, current learner lgbm  
    [flaml.automl: 11-15 07:08:20] {2029} INFO -  at 0.7s,  estimator lgbm's best error=0.2197,     best estimator lgbm's best error=0.2197  
    [flaml.automl: 11-15 07:08:20] {1826} INFO - iteration 8, current learner xgboost  
    [flaml.automl: 11-15 07:08:20] {2029} INFO -  at 0.8s,  estimator xgboost's best error=1.4958,  best estimator lgbm's best error=0.2197  
    [flaml.automl: 11-15 07:08:20] {1826} INFO - iteration 9, current learner xgboost  
    [flaml.automl: 11-15 07:08:20] {2029} INFO -  at 0.8s,  estimator xgboost's best error=1.4958,  best estimator lgbm's best error=0.2197  
    [flaml.automl: 11-15 07:08:20] {1826} INFO - iteration 10, current learner xgboost  
    [flaml.automl: 11-15 07:08:20] {2029} INFO -  at 0.9s,  estimator xgboost's best error=0.7052,  best estimator lgbm's best error=0.2197  
    [flaml.automl: 11-15 07:08:20] {1826} INFO - iteration 11, current learner xgboost  
    [flaml.automl: 11-15 07:08:20] {2029} INFO -  at 0.9s,  estimator xgboost's best error=0.3619,  best estimator lgbm's best error=0.2197  
    [flaml.automl: 11-15 07:08:20] {1826} INFO - iteration 12, current learner xgboost  
    [flaml.automl: 11-15 07:08:20] {2029} INFO -  at 0.9s,  estimator xgboost's best error=0.3619,  best estimator lgbm's best error=0.2197  
    [flaml.automl: 11-15 07:08:20] {1826} INFO - iteration 13, current learner xgboost  
    [flaml.automl: 11-15 07:08:20] {2029} INFO -  at 1.0s,  estimator xgboost's best error=0.3619,  best estimator lgbm's best error=0.2197  
    [flaml.automl: 11-15 07:08:20] {1826} INFO - iteration 14, current learner extra_tree  
    [flaml.automl: 11-15 07:08:20] {2029} INFO -  at 1.1s,  estimator extra_tree's best error=0.7197,       best estimator lgbm's best error=0.2197  
    [flaml.automl: 11-15 07:08:20] {2242} INFO - retrain lgbm for 0.0s  
    [flaml.automl: 11-15 07:08:20] {2247} INFO - retrained model: LGBMRegressor(colsample_bytree=0.7610534336273627,  
                  learning_rate=0.41929025492645006, max_bin=255,  
                  min_child_samples=4, n_estimators=45, num_leaves=4,  
                  reg_alpha=0.0009765625, reg_lambda=0.009280655005879943,  
                  verbose=-1)  
    [flaml.automl: 11-15 07:08:20] {1608} INFO - fit succeeded  
    [flaml.automl: 11-15 07:08:20] {1610} INFO - Time taken to find the best model: 0.7289648056030273  
    [flaml.automl: 11-15 07:08:20] {1624} WARNING - Time taken to find the best model is 73% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.  

### Multi-output regression​ {#multioutput-regression}

We can combine `sklearn.MultiOutputRegressor` and `flaml.AutoML` to do AutoML
for multi-output regression.

    from flaml import AutoML  
    from sklearn.datasets import make_regression  
    from sklearn.model_selection import train_test_split  
    from sklearn.multioutput import MultiOutputRegressor  
      
    # create regression data  
    X, y = make_regression(n_targets=3)  
      
    # split into train and test data  
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)  
      
    # train the model  
    model = MultiOutputRegressor(AutoML(task="regression", time_budget=60))  
    model.fit(X_train, y_train)  
      
    # predict  
    print(model.predict(X_test))  

It will perform AutoML for each target, each taking 60 seconds.

[Edit this
page](<https://github.com/microsoft.edit/main/website/docs/Examples/AutoML->
Regression.md)

[Previous« AutoML - Rank](.docs/Examples/AutoML-Rank)

[NextAutoML - Time Series Forecast »](.docs/Examples/AutoML-Time series
forecast)

* Prerequisites
* A basic regression example
* Multi-output regression

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
  * automl
  * default
  * onlineml
  * tune
    * tune.scheduler
    * tune.searcher
    * tune.spark
    * [analysis](.docs/reference/tune/analysis)
    * [sample](.docs/reference/tune/sample)
    * [space](.docs/reference/tune/space)
    * [trial](.docs/reference/tune/trial)
    * [trial_runner](.docs/reference/tune/trial_runner)
    * [tune](.docs/reference/tune/tune)
    * [utils](.docs/reference/tune/utils)

On this page

# tune.tune {#tunetune}

## ExperimentAnalysis Objects​ {#experimentanalysis-objects}

    class ExperimentAnalysis(EA)  

Class for storing the experiment results.

#### report​ {#report}
```python
    def report(_metric=None, **kwargs)  

A function called by the HPO application to report final or intermediate
results.

 **Example** :

  ```python
    import time  
    from flaml import tune  
      
    def compute_with_config(config):  
        current_time = time.time()  
        metric2minimize = (round(config['x'])-95000)**2  
        time2eval = time.time() - current_time  
        tune.report(metric2minimize=metric2minimize, time2eval=time2eval)  
      
    analysis = tune.run(  
        compute_with_config,  
        config={  
            'x': tune.lograndint(lower=1, upper=1000000),  
            'y': tune.randint(lower=1, upper=1000000)  
        },  
        metric='metric2minimize', mode='min',  
        num_samples=1000000, time_budget_s=60, use_ray=False)  
      
    print(analysis.trials[-1].last_result)  
```

 **Arguments** :

* `_metric` \- Optional default anonymous metric for `tune.report(value)`. (For compatibility with ray.tune.report)
* `**kwargs` \- Any key value pair to be reported.

 **Raises** :

StopIteration (when not using ray, i.e., _use_ray=False): A StopIteration
exception is raised if the trial has been signaled to stop. SystemExit (when
using ray): A SystemExit exception is raised if the trial has been signaled to
stop by ray.

#### run​ {#run}
```python
    def run(evaluation_function, config: Optional[dict] = None, low_cost_partial_config: Optional[dict] = None, cat_hp_cost: Optional[dict] = None, metric: Optional[str] = None, mode: Optional[str] = None, time_budget_s: Union[int, float] = None, points_to_evaluate: Optional[List[dict]] = None, evaluated_rewards: Optional[List] = None, resource_attr: Optional[str] = None, min_resource: Optional[float] = None, max_resource: Optional[float] = None, reduction_factor: Optional[float] = None, scheduler=None, search_alg=None, verbose: Optional[int] = 2, local_dir: Optional[str] = None, num_samples: Optional[int] = 1, resources_per_trial: Optional[dict] = None, config_constraints: Optional[List[Tuple[Callable[[dict], float], str, float]]] = None, metric_constraints: Optional[List[Tuple[str, str, float]]] = None, max_failure: Optional[int] = 100, use_ray: Optional[bool] = False, use_spark: Optional[bool] = False, use_incumbent_result_in_evaluation: Optional[bool] = None, log_file_name: Optional[str] = None, lexico_objectives: Optional[dict] = None, force_cancel: Optional[bool] = False, n_concurrent_trials: Optional[int] = 0, **ray_args, ,)  

The function-based way of performing HPO.

 **Example** :

    import time  
    from flaml import tune  
      
    def compute_with_config(config):  
        current_time = time.time()  
        metric2minimize = (round(config['x'])-95000)**2  
        time2eval = time.time() - current_time  
        tune.report(metric2minimize=metric2minimize, time2eval=time2eval)  
        # if the evaluation fails unexpectedly and the exception is caught,  
        # and it doesn't inform the goodness of the config,  
        # return {}  
        # if the failure indicates a config is bad,  
        # report a bad metric value like np.inf or -np.inf  
        # depending on metric mode being min or max  
      
    analysis = tune.run(  
        compute_with_config,  
        config={  
            'x': tune.lograndint(lower=1, upper=1000000),  
            'y': tune.randint(lower=1, upper=1000000)  
        },  
        metric='metric2minimize', mode='min',  
        num_samples=-1, time_budget_s=60, use_ray=False)  
      
    print(analysis.trials[-1].last_result)  

 **Arguments** :

* `evaluation_function` \- A user-defined evaluation function. It takes a configuration as input, outputs a evaluation result (can be a numerical value or a dictionary of string and numerical value pairs) for the input configuration. For machine learning tasks, it usually involves training and scoring a machine learning model, e.g., through validation loss.
* `config` \- A dictionary to specify the search space.
* `low_cost_partial_config` \- A dictionary from a subset of controlled dimensions to the initial low-cost values. e.g., `{'n_estimators': 4, 'max_leaves': 4}`
* `cat_hp_cost` \- A dictionary from a subset of categorical dimensions to the relative cost of each choice. e.g., `{'tree_method': [1, 1, 2]}` i.e., the relative cost of the three choices of 'tree_method' is 1, 1 and 2 respectively
* `metric` \- A string of the metric name to optimize for.
* `mode` \- A string in ['min', 'max'] to specify the objective as minimization or maximization.
* `time_budget_s` \- int or float | The time budget in seconds.
* `points_to_evaluate` \- A list of initial hyperparameter configurations to run first.
* `evaluated_rewards` _list_ \- If you have previously evaluated the parameters passed in as points_to_evaluate you can avoid re-running those trials by passing in the reward attributes as a list so the optimiser can be told the results without needing to re-compute the trial. Must be the same or shorter length than points_to_evaluate. e.g.,

    points_to_evaluate = [  
        {"b": .99, "cost_related": {"a": 3}},  
        {"b": .99, "cost_related": {"a": 2}},  
    ]  
    evaluated_rewards = [3.0]  

means that you know the reward for the first config in points_to_evaluate is
3.0 and want to inform run().

* `resource_attr` \- A string to specify the resource dimension used by the scheduler via "scheduler".
* `min_resource` \- A float of the minimal resource to use for the resource_attr.
* `max_resource` \- A float of the maximal resource to use for the resource_attr.
* `reduction_factor` \- A float of the reduction factor used for incremental pruning.
* `scheduler` \- A scheduler for executing the experiment. Can be None, 'flaml', 'asha' (or 'async_hyperband', 'asynchyperband') or a custom instance of the TrialScheduler class. Default is None: in this case when resource_attr is provided, the 'flaml' scheduler will be used, otherwise no scheduler will be used. When set 'flaml', an authentic scheduler implemented in FLAML will be used. It does not require users to report intermediate results in evaluation_function. Find more details about this scheduler in this paper <https://arxiv.org/pdf/1911.04706.pdf>). When set 'asha', the input for arguments "resource_attr", "min_resource", "max_resource" and "reduction_factor" will be passed to ASHA's "time_attr", "max_t", "grace_period" and "reduction_factor" respectively. You can also provide a self-defined scheduler instance of the TrialScheduler class. When 'asha' or self-defined scheduler is used, you usually need to report intermediate results in the evaluation function via 'tune.report()'. If you would like to do some cleanup opearation when the trial is stopped by the scheduler, you can catch the `StopIteration` (when not using ray) or `SystemExit` (when using ray) exception explicitly, as shown in the following example. Please find more examples using different types of schedulers and how to set up the corresponding evaluation functions in test/tune/test_scheduler.py, and test/tune/example_scheduler.py.
```python
    def easy_objective(config):  
        width, height = config["width"], config["height"]  
        for step in range(config["steps"]):  
            intermediate_score = evaluation_fn(step, width, height)  
            try:  
                tune.report(iterations=step, mean_loss=intermediate_score)  
            except (StopIteration, SystemExit):  
                # do cleanup operation here  
                return  

* `search_alg` \- An instance/string of the search algorithm to be used. The same instance can be used for iterative tuning. e.g.,

    from flaml import BlendSearch  
    algo = BlendSearch(metric='val_loss', mode='min',  
            space=search_space,  
            low_cost_partial_config=low_cost_partial_config)  
    for i in range(10):  
        analysis = tune.run(compute_with_config,  
            search_alg=algo, use_ray=False)  
        print(analysis.trials[-1].last_result)  

* `verbose` \- 0, 1, 2, or 3. If ray or spark backend is used, their verbosity will be affected by this argument. 0 = silent, 1 = only status updates, 2 = status and brief trial results, 3 = status and detailed trial results. Defaults to 2.

* `local_dir` \- A string of the local dir to save ray logs if ray backend is used; or a local dir to save the tuning log.

* `num_samples` \- An integer of the number of configs to try. Defaults to 1.

* `resources_per_trial` \- A dictionary of the hardware resources to allocate per trial, e.g., `{'cpu': 1}`. It is only valid when using ray backend (by setting 'use_ray = True'). It shall be used when you need to do [parallel tuning](.docs/Use-Cases/Tune-User-Defined-Function#parallel-tuning).

* `config_constraints` \- A list of config constraints to be satisfied. e.g., `config_constraints = [(mem_size, '<=', 1024**3)]`

mem_size is a function which produces a float number for the bytes needed for
a config. It is used to skip configs which do not fit in memory.

* `metric_constraints` \- A list of metric constraints to be satisfied. e.g., `['precision', '>=', 0.9]`. The sign can be ">=" or "<=".

* `max_failure` \- int | the maximal consecutive number of failures to sample a trial before the tuning is terminated.

* `use_ray` \- A boolean of whether to use ray as the backend.

* `use_spark` \- A boolean of whether to use spark as the backend.

* `log_file_name` \- A string of the log file name. Default to None. When set to None: if local_dir is not given, no log file is created; if local_dir is given, the log file name will be autogenerated under local_dir. Only valid when verbose > 0 or use_ray is True.

* `lexico_objectives` \- dict, default=None | It specifics information needed to perform multi-objective optimization with lexicographic preferences. When lexico_objectives is not None, the arguments metric, mode, will be invalid, and flaml's tune uses CFO as the `search_alg`, which makes the input (if provided) `search_alg' invalid. This dictionary shall contain the following fields of key-value pairs:

  * "metrics": a list of optimization objectives with the orders reflecting the priorities/preferences of the objectives.
  * "modes" (optional): a list of optimization modes (each mode either "min" or "max") corresponding to the objectives in the metric list. If not provided, we use "min" as the default mode for all the objectives.
  * "targets" (optional): a dictionary to specify the optimization targets on the objectives. The keys are the metric names (provided in "metric"), and the values are the numerical target values.
  * "tolerances" (optional): a dictionary to specify the optimality tolerances on objectives. The keys are the metric names (provided in "metrics"), and the values are the absolute/percentage tolerance in the form of numeric/string. E.g.,

    lexico_objectives = {  
        "metrics": ["error_rate", "pred_time"],  
        "modes": ["min", "min"],  
        "tolerances": {"error_rate": 0.01, "pred_time": 0.0},  
        "targets": {"error_rate": 0.0},  
    }  

We also support percentage tolerance. E.g.,

    lexico_objectives = {  
        "metrics": ["error_rate", "pred_time"],  
        "modes": ["min", "min"],  
        "tolerances": {"error_rate": "5%", "pred_time": "0%"},  
        "targets": {"error_rate": 0.0},  
    }  

* `force_cancel` \- boolean, default=False | Whether to forcely cancel the PySpark job if overtime.
* `n_concurrent_trials` \- int, default=0 | The number of concurrent trials when perform hyperparameter tuning with Spark. Only valid when use_spark=True and spark is required: `pip install flaml[spark]`. Please check [here](https://spark.apache.org/docs/latest/api/python/getting_started/install.html) for more details about installing Spark. When tune.run() is called from AutoML, it will be overwritten by the value of `n_concurrent_trials` in AutoML. When <= 0, the concurrent trials will be set to the number of executors.
* `**ray_args` \- keyword arguments to pass to ray.tune.run(). Only valid when use_ray=True.

## Tuner Objects​ {#tuner-objects}

    class Tuner()  

Tuner is the class-based way of launching hyperparameter tuning jobs
compatible with Ray Tune 2.

 **Arguments** :

* `trainable` \- A user-defined evaluation function. It takes a configuration as input, outputs a evaluation result (can be a numerical value or a dictionary of string and numerical value pairs) for the input configuration. For machine learning tasks, it usually involves training and scoring a machine learning model, e.g., through validation loss.

* `param_space` \- Search space of the tuning job. One thing to note is that both preprocessor and dataset can be tuned here.

* `tune_config` \- Tuning algorithm specific configs. Refer to ray.tune.tune_config.TuneConfig for more info.

* `run_config` \- Runtime configuration that is specific to individual trials. If passed, this will overwrite the run config passed to the Trainer, if applicable. Refer to ray.air.config.RunConfig for more info.

Usage pattern:

.. code-block:: python

from sklearn.datasets import load_breast_cancer

from ray import tune from ray.data import from_pandas from ray.air.config
import RunConfig, ScalingConfig from ray.train.xgboost import XGBoostTrainer
from ray.tune.tuner import Tuner

def get_dataset(): data_raw = load_breast_cancer(as_frame=True) dataset_df =
data_raw["data"] dataset_df["target"] = data_raw["target"] dataset =
from_pandas(dataset_df) return dataset

trainer = XGBoostTrainer( label_column="target", params={},

* `datasets={"train"` \- get_dataset()}, )

param_space = {

* `"scaling_config"` \- ScalingConfig( num_workers=tune.grid_search([2, 4]), resources_per_worker={

* `"CPU"` \- tune.grid_search([1, 2]), }, ),

# You can even grid search various datasets in Tune. {#you-can-even-grid-search-various-datasets-in-tune}

# "datasets": { {#datasets-}

# "train": tune.grid_search( {#train-tunegridsearch}

# [ds1, ds2] {#ds1-ds2}

# ), {#}

# }, {#}

* `"params"` \- {

* `"objective"` \- "binary:logistic",

* `"tree_method"` \- "approx",

* `"eval_metric"` \- ["logloss", "error"],

* `"eta"` \- tune.loguniform(1e-4, 1e-1),

* `"subsample"` \- tune.uniform(0.5, 1.0),

* `"max_depth"` \- tune.randint(1, 9), }, } tuner = Tuner(trainable=trainer, param_space=param_space, run_config=RunConfig(name="my_tune_run")) analysis = tuner.fit()

To retry a failed tune run, you can then do

.. code-block:: python

tuner = Tuner.restore(experiment_checkpoint_dir) tuner.fit()

`experiment_checkpoint_dir` can be easily located near the end of the console
output of your first failed run.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/tune/tune.md)

[Previous« trial_runner](.docs/reference/tune/trial_runner)

[Nextutils »](.docs/reference/tune/utils)

* ExperimentAnalysis Objects
* Tuner Objects

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

* [Getting Started](/autogen/docs/Getting-Started)
* [Installation](/autogen/docs/Installation)
* Use Cases
  * [Multi-agent Conversation Framework](/autogen/docs/Use-Cases/agent_chat)
  * [Enhanced Inference](/autogen/docs/Use-Cases/enhanced_inference)
* [Contributing](/autogen/docs/Contribute)
* [Research](/autogen/docs/Research)

On this page

# Enhanced Inference {#enhanced-inference}

`autogen.OpenAIWrapper` provides enhanced LLM inference for `openai>=1`.
`autogen.Completion` is a drop-in replacement of `openai.Completion` and
`openai.ChatCompletion` for enhanced LLM inference using `openai<1`. There are
a number of benefits of using `autogen` to perform inference: performance
tuning, API unification, caching, error handling, multi-config inference,
result filtering, templating and so on.

## Tune Inference Parameters (for openai<1)​ {#tune-inference-parameters-for-openai1}

Find a list of examples in this page: [Tune Inference Parameters
Examples](/autogen/docs/Examples#tune-inference-hyperparameters)

### Choices to optimize​ {#choices-to-optimize}

The cost of using foundation models for text generation is typically measured
in terms of the number of tokens in the input and output combined. From the
perspective of an application builder using foundation models, the use case is
to maximize the utility of the generated text under an inference budget
constraint (e.g., measured by the average dollar cost needed to solve a coding
problem). This can be achieved by optimizing the hyperparameters of the
inference, which can significantly affect both the utility and the cost of the
generated text.

The tunable hyperparameters include:

  1. model - this is a required input, specifying the model ID to use.
  2. prompt/messages - the input prompt/messages to the model, which provides the context for the text generation task.
  3. max_tokens - the maximum number of tokens (words or word pieces) to generate in the output.
  4. temperature - a value between 0 and 1 that controls the randomness of the generated text. A higher temperature will result in more random and diverse text, while a lower temperature will result in more predictable text.
  5. top_p - a value between 0 and 1 that controls the sampling probability mass for each token generation. A lower top_p value will make it more likely to generate text based on the most likely tokens, while a higher value will allow the model to explore a wider range of possible tokens.
  6. n - the number of responses to generate for a given prompt. Generating multiple responses can provide more diverse and potentially more useful output, but it also increases the cost of the request.
  7. stop - a list of strings that, when encountered in the generated text, will cause the generation to stop. This can be used to control the length or the validity of the output.
  8. presence_penalty, frequency_penalty - values that control the relative importance of the presence and frequency of certain words or phrases in the generated text.
  9. best_of - the number of responses to generate server-side when selecting the "best" (the one with the highest log probability per token) response for a given prompt.

The cost and utility of text generation are intertwined with the joint effect
of these hyperparameters. There are also complex interactions among subsets of
the hyperparameters. For example, the temperature and top_p are not
recommended to be altered from their default values together because they both
control the randomness of the generated text, and changing both at the same
time can result in conflicting effects; n and best_of are rarely tuned
together because if the application can process multiple outputs, filtering on
the server side causes unnecessary information loss; both n and max_tokens
will affect the total number of tokens generated, which in turn will affect
the cost of the request. These interactions and trade-offs make it difficult
to manually determine the optimal hyperparameter settings for a given text
generation task.

 _Do the choices matter? Check this[blogpost](/autogen/blog/2023/04/21/LLM-
tuning-math) to find example tuning results about gpt-3.5-turbo and gpt-4._

With AutoGen, the tuning can be performed with the following information:

  1. Validation data.
  2. Evaluation function.
  3. Metric to optimize.
  4. Search space.
  5. Budgets: inference and optimization respectively.

### Validation data​ {#validation-data}

Collect a diverse set of instances. They can be stored in an iterable of
dicts. For example, each instance dict can contain "problem" as a key and the
description str of a math problem as the value; and "solution" as a key and
the solution str as the value.

### Evaluation function​ {#evaluation-function}

The evaluation function should take a list of responses, and other keyword
arguments corresponding to the keys in each validation data instance as input,
and output a dict of metrics. For example,
```python
    def eval_math_responses(responses: List[str], solution: str, **args) -> Dict:  
        # select a response from the list of responses  
        answer = voted_answer(responses)  
        # check whether the answer is correct  
        return {"success": is_equivalent(answer, solution)}  

`autogen.code_utils` and `autogen.math_utils` offer some example evaluation
functions for code generation and math problem solving.

### Metric to optimize​ {#metric-to-optimize}

The metric to optimize is usually an aggregated metric over all the tuning
data instances. For example, users can specify "success" as the metric and
"max" as the optimization mode. By default, the aggregation function is taking
the average. Users can provide a customized aggregation function if needed.

### Search space​ {#search-space}

Users can specify the (optional) search range for each hyperparameter.

  1. model. Either a constant str, or multiple choices specified by `flaml.tune.choice`.
  2. prompt/messages. Prompt is either a str or a list of strs, of the prompt templates. messages is a list of dicts or a list of lists, of the message templates. Each prompt/message template will be formatted with each data instance. For example, the prompt template can be: "{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \boxed{{}}." And `{problem}` will be replaced by the "problem" field of each data instance.
  3. max_tokens, n, best_of. They can be constants, or specified by `flaml.tune.randint`, `flaml.tune.qrandint`, `flaml.tune.lograndint` or `flaml.qlograndint`. By default, max_tokens is searched in [50, 1000); n is searched in [1, 100); and best_of is fixed to 1.
  4. stop. It can be a str or a list of strs, or a list of lists of strs or None. Default is None.
  5. temperature or top_p. One of them can be specified as a constant or by `flaml.tune.uniform` or `flaml.tune.loguniform` etc. Please don't provide both. By default, each configuration will choose either a temperature or a top_p in [0, 1] uniformly.
  6. presence_penalty, frequency_penalty. They can be constants or specified by `flaml.tune.uniform` etc. Not tuned by default.

### Budgets​ {#budgets}

One can specify an inference budget and an optimization budget. The inference
budget refers to the average inference cost per data instance. The
optimization budget refers to the total budget allowed in the tuning process.
Both are measured by dollars and follow the price per 1000 tokens.

### Perform tuning​ {#perform-tuning}

Now, you can use `autogen.Completion.tune` for tuning. For example,

    import autogen  
      
    config, analysis = autogen.Completion.tune(  
        data=tune_data,  
        metric="success",  
        mode="max",  
        eval_func=eval_func,  
        inference_budget=0.05,  
        optimization_budget=3,  
        num_samples=-1,  
    )  

`num_samples` is the number of configurations to sample. -1 means unlimited
(until optimization budget is exhausted). The returned `config` contains the
optimized configuration and `analysis` contains an ExperimentAnalysis object
for all the tried configurations and results.

The tuned config can be used to perform inference.

## API unification​ {#api-unification}

`autogen.OpenAIWrapper.create()` can be used to create completions for both
chat and non-chat models, and both OpenAI API and Azure OpenAI API.

    from autogen import OpenAIWrapper  
    # OpenAI endpoint  
    client = OpenAIWrapper()  
    # ChatCompletion  
    response = client.create(messages=[{"role": "user", "content": "2+2="}], model="gpt-3.5-turbo")  
    # extract the response text  
    print(client.extract_text_or_completion_object(response))  
    # get cost of this completion  
    print(response.cost)  
    # Azure OpenAI endpoint  
    client = OpenAIWrapper(api_key=..., base_url=..., api_version=..., api_type="azure")  
    # Completion  
    response = client.create(prompt="2+2=", model="gpt-3.5-turbo-instruct")  
    # extract the response text  
    print(client.extract_text_or_completion_object(response))  

For local LLMs, one can spin up an endpoint using a package like
[FastChat](https://github.com/lm-sys/FastChat), and then use the same API to
send a request. See [here](/autogen/blog/2023/07/14/Local-LLMs) for examples
on how to make inference with local LLMs.

## Usage Summary​ {#usage-summary}

The `OpenAIWrapper` from `autogen` tracks token counts and costs of your API
calls. Use the `create()` method to initiate requests and
`print_usage_summary()` to retrieve a detailed usage report, including total
cost and token usage for both cached and actual requests.

* `mode=["actual", "total"]` (default): print usage summary for all completions and non-caching completions.
* `mode='actual'`: only print non-cached usage.
* `mode='total'`: only print all usage (including cache).

Reset your session's usage data with `clear_usage_summary()` when needed.
[View
Notebook](https://github.com/microsoft/autogen/blob/main/notebook/oai_client_cost.ipynb)

Example usage:

    from autogen import OpenAIWrapper  
      
    client = OpenAIWrapper()  
    client.create(messages=[{"role": "user", "content": "Python learning tips."}], model="gpt-3.5-turbo")  
    client.print_usage_summary()  # Display usage  
    client.clear_usage_summary()  # Reset usage data  

Sample output:

    Usage summary excluding cached usage:  
    Total cost: 0.00015  
    * Model 'gpt-3.5-turbo': cost: 0.00015, prompt_tokens: 25, completion_tokens: 58, total_tokens: 83  
      
    Usage summary including cached usage:  
    Total cost: 0.00027  
    * Model 'gpt-3.5-turbo': cost: 0.00027, prompt_tokens: 50, completion_tokens: 100, total_tokens: 150  

## Caching​ {#caching}

API call results are cached locally and reused when the same request is
issued. This is useful when repeating or continuing experiments for
reproducibility and cost saving. It still allows controlled randomness by
setting the "cache_seed" specified in `OpenAIWrapper.create()` or the
constructor of `OpenAIWrapper`.

    client = OpenAIWrapper(cache_seed=...)  
    client.create(...)  

    client = OpenAIWrapper()  
    client.create(cache_seed=..., ...)  

Caching is enabled by default with cache_seed 41. To disable it please set
`cache_seed` to None.

 _NOTE_. openai v1.1 introduces a new param `seed`. The difference between
autogen's `cache_seed` and openai's `seed` is that:

* autogen uses local disk cache to guarantee the exactly same output is produced for the same input and when cache is hit, no openai api call will be made.
* openai's `seed` is a best-effort deterministic sampling with no guarantee of determinism. When using openai's `seed` with `cache_seed` set to None, even for the same input, an openai api call will be made and there is no guarantee for getting exactly the same output.

## Error handling​ {#error-handling}

### Runtime error​ {#runtime-error}

One can pass a list of configurations of different models/endpoints to
mitigate the rate limits and other runtime error. For example,

    client = OpenAIWrapper(  
        config_list=[  
            {  
                "model": "gpt-4",  
                "api_key": os.environ.get("AZURE_OPENAI_API_KEY"),  
                "api_type": "azure",  
                "base_url": os.environ.get("AZURE_OPENAI_API_BASE"),  
                "api_version": "2023-08-01-preview",  
            },  
            {  
                "model": "gpt-3.5-turbo",  
                "api_key": os.environ.get("OPENAI_API_KEY"),  
                "base_url": "https://api.openai.com/v1",  
            },  
            {  
                "model": "llama2-chat-7B",  
                "base_url": "http://127.0.0.1:8080",  
            }  
        ],  
    )  

`client.create()` will try querying Azure OpenAI gpt-4, OpenAI gpt-3.5-turbo,
and a locally hosted llama2-chat-7B one by one, until a valid result is
returned. This can speed up the development process where the rate limit is a
bottleneck. An error will be raised if the last choice fails. So make sure the
last choice in the list has the best availability.

For convenience, we provide a number of utility functions to load config
lists.

* `get_config_list`: Generates configurations for API calls, primarily from provided API keys.
* `config_list_openai_aoai`: Constructs a list of configurations using both Azure OpenAI and OpenAI endpoints, sourcing API keys from environment variables or local files.
* `config_list_from_json`: Loads configurations from a JSON structure, either from an environment variable or a local JSON file, with the flexibility of filtering configurations based on given criteria.
* `config_list_from_models`: Creates configurations based on a provided list of models, useful when targeting specific models without manually specifying each configuration.
* `config_list_from_dotenv`: Constructs a configuration list from a `.env` file, offering a consolidated way to manage multiple API configurations and keys from a single file.

We suggest that you take a look at this
[notebook](https://github.com/microsoft/autogen/blob/main/notebook/oai_openai_utils.ipynb)
for full code examples of the different methods to configure your model
endpoints.

### Logic error​ {#logic-error}

Another type of error is that the returned response does not satisfy a
requirement. For example, if the response is required to be a valid json
string, one would like to filter the responses that are not. This can be
achieved by providing a list of configurations and a filter function. For
example,
```python
    def valid_json_filter(response, **_):  
        for text in OpenAIWrapper.extract_text_or_completion_object(response):  
            try:  
                json.loads(text)  
                return True  
            except ValueError:  
                pass  
        return False  
      
    client = OpenAIWrapper(  
        config_list=[{"model": "text-ada-001"}, {"model": "gpt-3.5-turbo-instruct"}, {"model": "text-davinci-003"}],  
    )  
    response = client.create(  
        prompt="How to construct a json request to Bing API to search for 'latest AI news'? Return the JSON request.",  
        filter_func=valid_json_filter,  
    )  

The example above will try to use text-ada-001, gpt-3.5-turbo-instruct, and
text-davinci-003 iteratively, until a valid json string is returned or the
last config is used. One can also repeat the same model in the list for
multiple times (with different seeds) to try one model multiple times for
increasing the robustness of the final response.

 _Advanced use case: Check this[blogpost](/autogen/blog/2023/05/18/GPT-
adaptive-humaneval) to find how to improve GPT-4's coding performance from 68%
to 90% while reducing the inference cost._

## Templating​ {#templating}

If the provided prompt or message is a template, it will be automatically
materialized with a given context. For example,

    response = client.create(  
        context={"problem": "How many positive integers, not exceeding 100, are multiples of 2 or 3 but not 4?"},  
        prompt="{problem} Solve the problem carefully.",  
        allow_format_str_template=True,  
        **config  
    )  

A template is either a format str, like the example above, or a function which
produces a str from several input fields, like the example below.
```python
    def content(turn, context):  
        return "\n".join(  
            [  
                context[f"user_message_{turn}"],  
                context[f"external_info_{turn}"]  
            ]  
        )  
      
    messages = [  
        {  
            "role": "system",  
            "content": "You are a teaching assistant of math.",  
        },  
        {  
            "role": "user",  
            "content": partial(content, turn=0),  
        },  
    ]  
    context = {  
        "user_message_0": "Could you explain the solution to Problem 1?",  
        "external_info_0": "Problem 1: ...",  
    }  
      
    response = client.create(context=context, messages=messages, **config)  
    messages.append(  
        {  
            "role": "assistant",  
            "content": client.extract_text(response)[0]  
        }  
    )  
    messages.append(  
        {  
            "role": "user",  
            "content": partial(content, turn=1),  
        },  
    )  
    context.append(  
        {  
            "user_message_1": "Why can't we apply Theorem 1 to Equation (2)?",  
            "external_info_1": "Theorem 1: ...",  
        }  
    )  
    response = client.create(context=context, messages=messages, **config)  

## Logging (for openai<1)​ {#logging-for-openai1}

When debugging or diagnosing an LLM-based system, it is often convenient to
log the API calls and analyze them. `autogen.Completion` and
`autogen.ChatCompletion` offer an easy way to collect the API call histories.
For example, to log the chat histories, simply run:

    autogen.ChatCompletion.start_logging()  

The API calls made after this will be automatically logged. They can be
retrieved at any time by:

    autogen.ChatCompletion.logged_history  

There is a function that can be used to print usage summary (total cost, and
token count usage from each model):

    autogen.ChatCompletion.print_usage_summary()  

To stop logging, use

    autogen.ChatCompletion.stop_logging()  

If one would like to append the history to an existing dict, pass the dict
like:

    autogen.ChatCompletion.start_logging(history_dict=existing_history_dict)  

By default, the counter of API calls will be reset at `start_logging()`. If no
reset is desired, set `reset_counter=False`.

There are two types of logging formats: compact logging and individual API
call logging. The default format is compact. Set `compact=False` in
`start_logging()` to switch.

* Example of a history dict with compact logging.

    {  
        """  
        [  
            {  
                'role': 'system',  
                'content': system_message,  
            },  
            {  
                'role': 'user',  
                'content': user_message_1,  
            },  
            {  
                'role': 'assistant',  
                'content': assistant_message_1,  
            },  
            {  
                'role': 'user',  
                'content': user_message_2,  
            },  
            {  
                'role': 'assistant',  
                'content': assistant_message_2,  
            },  
        ]""": {  
            "created_at": [0, 1],  
            "cost": [0.1, 0.2],  
        }  
    }  

* Example of a history dict with individual API call logging.

    {  
        0: {  
            "request": {  
                "messages": [  
                    {  
                        "role": "system",  
                        "content": system_message,  
                    },  
                    {  
                        "role": "user",  
                        "content": user_message_1,  
                    }  
                ],  
                ... # other parameters in the request  
            },  
            "response": {  
                "choices": [  
                    "messages": {  
                        "role": "assistant",  
                        "content": assistant_message_1,  
                    },  
                ],  
                ... # other fields in the response  
            }  
        },  
        1: {  
            "request": {  
                "messages": [  
                    {  
                        "role": "system",  
                        "content": system_message,  
                    },  
                    {  
                        "role": "user",  
                        "content": user_message_1,  
                    },  
                    {  
                        "role": "assistant",  
                        "content": assistant_message_1,  
                    },  
                    {  
                        "role": "user",  
                        "content": user_message_2,  
                    },  
                ],  
                ... # other parameters in the request  
            },  
            "response": {  
                "choices": [  
                    "messages": {  
                        "role": "assistant",  
                        "content": assistant_message_2,  
                    },  
                ],  
                ... # other fields in the response  
            }  
        },  
    }  

* Example of printing for usage summary

    Total cost: <cost>  
    Token count summary for model <model>: prompt_tokens: <count 1>, completion_tokens: <count 2>, total_tokens: <count 3>  

It can be seen that the individual API call history contains redundant
information of the conversation. For a long conversation the degree of
redundancy is high. The compact history is more efficient and the individual
API call history contains more details.

[Edit this
page](<https://github.com/microsoft/autogen/edit/main/website/docs/Use->
Cases/enhanced_inference.md)

[Previous« Multi-agent Conversation Framework](/autogen/docs/Use-
Cases/agent_chat)

[NextContributing »](/autogen/docs/Contribute)

* Tune Inference Parameters (for openai<1)
  * Choices to optimize
  * Validation data
  * Evaluation function
  * Metric to optimize
  * Search space
  * Budgets
  * Perform tuning
* API unification
* Usage Summary
* Caching
* Error handling
  * Runtime error
  * Logic error
* Templating
* Logging (for openai<1)

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

* Reference
  * agentchat
    * agentchat.contrib
      * agentchat.contrib.capabilities
      * [agent_builder](/autogen/docs/reference/agentchat/contrib/agent_builder)
      * [compressible_agent](/autogen/docs/reference/agentchat/contrib/compressible_agent)
      * [gpt_assistant_agent](/autogen/docs/reference/agentchat/contrib/gpt_assistant_agent)
      * [img_utils](/autogen/docs/reference/agentchat/contrib/img_utils)
      * [llava_agent](/autogen/docs/reference/agentchat/contrib/llava_agent)
      * [math_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/math_user_proxy_agent)
      * [multimodal_conversable_agent](/autogen/docs/reference/agentchat/contrib/multimodal_conversable_agent)
      * [qdrant_retrieve_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/qdrant_retrieve_user_proxy_agent)
      * [retrieve_assistant_agent](/autogen/docs/reference/agentchat/contrib/retrieve_assistant_agent)
      * [retrieve_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/retrieve_user_proxy_agent)
      * [text_analyzer_agent](/autogen/docs/reference/agentchat/contrib/text_analyzer_agent)
    * [agent](/autogen/docs/reference/agentchat/agent)
    * [assistant_agent](/autogen/docs/reference/agentchat/assistant_agent)
    * [conversable_agent](/autogen/docs/reference/agentchat/conversable_agent)
    * [groupchat](/autogen/docs/reference/agentchat/groupchat)
    * [user_proxy_agent](/autogen/docs/reference/agentchat/user_proxy_agent)
  * oai
  * [code_utils](/autogen/docs/reference/code_utils)
  * [function_utils](/autogen/docs/reference/function_utils)
  * [math_utils](/autogen/docs/reference/math_utils)
  * [retrieve_utils](/autogen/docs/reference/retrieve_utils)
  * [token_count_utils](/autogen/docs/reference/token_count_utils)

On this page

# agentchat.contrib.gpt_assistant_agent {#agentchatcontribgptassistantagent}

## GPTAssistantAgent Objects​ {#gptassistantagent-objects}

    class GPTAssistantAgent(ConversableAgent)  

An experimental AutoGen agent class that leverages the OpenAI Assistant API
for conversational capabilities. This agent is unique in its reliance on the
OpenAI Assistant for state management, differing from other agents like
ConversableAgent.

#### __init__​ {#init}
```python
    def __init__(name="GPT Assistant",  
                 instructions: Optional[str] = None,  
                 llm_config: Optional[Union[Dict, bool]] = None,  
                 overwrite_instructions: bool = False,  
                 **kwargs)  

 **Arguments** :

* `name` _str_ \- name of the agent. It will be used to find the existing assistant by name. Please remember to delete an old assistant with the same name if you intend to create a new assistant with the same name.
* `instructions` _str_ \- instructions for the OpenAI assistant configuration. When instructions is not None, the system message of the agent will be set to the provided instructions and used in the assistant run, irrespective of the overwrite_instructions flag. But when instructions is None, and the assistant does not exist, the system message will be set to AssistantAgent.DEFAULT_SYSTEM_MESSAGE. If the assistant exists, the system message will be set to the existing assistant instructions.
* `llm_config` _dict or False_ \- llm inference configuration.
  * assistant_id: ID of the assistant to use. If None, a new assistant will be created.
  * model: Model to use for the assistant (gpt-4-1106-preview, gpt-3.5-turbo-1106).
  * check_every_ms: check thread run status interval
  * tools: Give Assistants access to OpenAI-hosted tools like Code Interpreter and Knowledge Retrieval, or build your own tools using Function calling. ref <https://platform.openai.com/docs/assistants/tools>
  * file_ids: files used by retrieval in run
* `overwrite_instructions` _bool_ \- whether to overwrite the instructions of an existing assistant. This parameter is in effect only when assistant_id is specified in llm_config.
* `kwargs` _dict_ \- Additional configuration options for the agent.
  * verbose (bool): If set to True, enables more detailed output from the assistant thread.
  * Other kwargs: Except verbose, others are passed directly to ConversableAgent.

#### can_execute_function​ {#canexecutefunction}
```python
    def can_execute_function(name: str) -> bool  

Whether the agent can execute the function.

#### reset​ {#reset}
```python
    def reset()  

Resets the agent, clearing any existing conversation thread and unread message
indices.

#### clear_history​ {#clearhistory}
```python
    def clear_history(agent: Optional[Agent] = None)  

Clear the chat history of the agent.

 **Arguments** :

* `agent` \- the agent with whom the chat history to clear. If None, clear the chat history with all agents.

#### pretty_print_thread​ {#prettyprintthread}
```python
    def pretty_print_thread(thread)  

Pretty print the thread.

#### oai_threads​ {#oaithreads}

    @property  
    def oai_threads() -> Dict[Agent, Any]  

Return the threads of the agent.

#### assistant_id​ {#assistantid}

    @property  
    def assistant_id()  

Return the assistant id

#### get_assistant_instructions​ {#getassistantinstructions}
```python
    def get_assistant_instructions()  

Return the assistant instructions from OAI assistant API

#### delete_assistant​ {#deleteassistant}
```python
    def delete_assistant()  

Delete the assistant from OAI assistant API

#### find_matching_assistant​ {#findmatchingassistant}
```python
    def find_matching_assistant(candidate_assistants, instructions, tools,  
                                file_ids)  

Find the matching assistant from a list of candidate assistants. Filter out
candidates with the same name but different instructions, file IDs, and
function names. TODO: implement accurate match based on assistant metadata
fields.

[Edit this
page](https://github.com/microsoft/autogen/edit/main/website/docs/reference/agentchat/contrib/gpt_assistant_agent.md)

[Previous«
compressible_agent](/autogen/docs/reference/agentchat/contrib/compressible_agent)

[Nextimg_utils »](/autogen/docs/reference/agentchat/contrib/img_utils)

* GPTAssistantAgent Objects

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
  * automl
  * default
  * onlineml
  * tune
    * tune.scheduler
      * [online_scheduler](.docs/reference/tune/scheduler/online_scheduler)
      * [trial_scheduler](.docs/reference/tune/scheduler/trial_scheduler)
    * tune.searcher
    * tune.spark
    * [analysis](.docs/reference/tune/analysis)
    * [sample](.docs/reference/tune/sample)
    * [space](.docs/reference/tune/space)
    * [trial](.docs/reference/tune/trial)
    * [trial_runner](.docs/reference/tune/trial_runner)
    * [tune](.docs/reference/tune/tune)
    * [utils](.docs/reference/tune/utils)

On this page

# tune.scheduler.trial_scheduler {#tuneschedulertrialscheduler}

## TrialScheduler Objects​ {#trialscheduler-objects}

    class TrialScheduler()  

Interface for implementing a Trial Scheduler class.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/tune/scheduler/trial_scheduler.md)

[Previous«
online_scheduler](.docs/reference/tune/scheduler/online_scheduler)

[Nextblendsearch »](.docs/reference/tune/searcher/blendsearch)

* TrialScheduler Objects

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

* [Getting Started](/autogen/docs/Getting-Started)
* [Installation](/autogen/docs/Installation)
* Use Cases
  * [Multi-agent Conversation Framework](/autogen/docs/Use-Cases/agent_chat)
  * [Enhanced Inference](/autogen/docs/Use-Cases/enhanced_inference)
* [Contributing](/autogen/docs/Contribute)
* [Research](/autogen/docs/Research)

On this page

# Multi-agent Conversation Framework {#multiagent-conversation-framework}

AutoGen offers a unified multi-agent conversation framework as a high-level
abstraction of using foundation models. It features capable, customizable and
conversable agents which integrate LLMs, tools, and humans via automated agent
chat. By automating chat among multiple capable agents, one can easily make
them collectively perform tasks autonomously or with human feedback, including
tasks that require using tools via code.

This framework simplifies the orchestration, automation and optimization of a
complex LLM workflow. It maximizes the performance of LLM models and overcome
their weaknesses. It enables building next-gen LLM applications based on
multi-agent conversations with minimal effort.

### Agents​ {#agents}

AutoGen abstracts and implements conversable agents designed to solve tasks
through inter-agent conversations. Specifically, the agents in AutoGen have
the following notable features:

* Conversable: Agents in AutoGen are conversable, which means that any agent can send and receive messages from other agents to initiate or continue a conversation

* Customizable: Agents in AutoGen can be customized to integrate LLMs, humans, tools, or a combination of them.

The figure below shows the built-in agents in AutoGen. ![Agent Chat
Example](/autogen/assets/images/autogen_agents-b80434bcb15d46da0c6cbeed28115f38.png)

We have designed a generic
[`ConversableAgent`](/autogen/docs/reference/agentchat/conversable_agent#conversableagent-
objects) class for Agents that are capable of conversing with each other
through the exchange of messages to jointly finish a task. An agent can
communicate with other agents and perform actions. Different agents can differ
in what actions they perform after receiving messages. Two representative
subclasses are
[`AssistantAgent`](/autogen/docs/reference/agentchat/assistant_agent#assistantagent-
objects) and
[`UserProxyAgent`](/autogen/docs/reference/agentchat/user_proxy_agent#userproxyagent-
objects)

* The [`AssistantAgent`](/autogen/docs/reference/agentchat/assistant_agent#assistantagent-objects) is designed to act as an AI assistant, using LLMs by default but not requiring human input or code execution. It could write Python code (in a Python coding block) for a user to execute when a message (typically a description of a task that needs to be solved) is received. Under the hood, the Python code is written by LLM (e.g., GPT-4). It can also receive the execution results and suggest corrections or bug fixes. Its behavior can be altered by passing a new system message. The LLM inference configuration can be configured via [`llm_config`].

* The [`UserProxyAgent`](/autogen/docs/reference/agentchat/user_proxy_agent#userproxyagent-objects) is conceptually a proxy agent for humans, soliciting human input as the agent's reply at each interaction turn by default and also having the capability to execute code and call functions or tools. The [`UserProxyAgent`](/autogen/docs/reference/agentchat/user_proxy_agent#userproxyagent-objects) triggers code execution automatically when it detects an executable code block in the received message and no human user input is provided. Code execution can be disabled by setting the `code_execution_config` parameter to False. LLM-based response is disabled by default. It can be enabled by setting `llm_config` to a dict corresponding to the [inference](/autogen/docs/Use-Cases/enhanced_inference) configuration. When `llm_config` is set as a dictionary, [`UserProxyAgent`](/autogen/docs/reference/agentchat/user_proxy_agent#userproxyagent-objects) can generate replies using an LLM when code execution is not performed.

The auto-reply capability of
[`ConversableAgent`](/autogen/docs/reference/agentchat/conversable_agent#conversableagent-
objects) allows for more autonomous multi-agent communication while retaining
the possibility of human intervention. One can also easily extend it by
registering reply functions with the
[`register_reply()`](/autogen/docs/reference/agentchat/conversable_agent#register_reply)
method.

In the following code, we create an
[`AssistantAgent`](/autogen/docs/reference/agentchat/assistant_agent#assistantagent-
objects) named "assistant" to serve as the assistant and a
[`UserProxyAgent`](/autogen/docs/reference/agentchat/user_proxy_agent#userproxyagent-
objects) named "user_proxy" to serve as a proxy for the human user. We will
later employ these two agents to solve a task.

    from autogen import AssistantAgent, UserProxyAgent  
      
    # create an AssistantAgent instance named "assistant"  
    assistant = AssistantAgent(name="assistant")  
      
    # create a UserProxyAgent instance named "user_proxy"  
    user_proxy = UserProxyAgent(name="user_proxy")  

#### Tool calling​ {#tool-calling}

Tool calling enables agents to interact with external tools and APIs more
efficiently. This feature allows the AI model to intelligently choose to
output a JSON object containing arguments to call specific tools based on the
user's input. A tool to be called is specified with a JSON schema describing
its parameters and their types. Writing such JSON schema is complex and error-
prone and that is why AutoGen framework provides two high level function
decorators for automatically generating such schema using type hints on
standard Python datatypes or Pydantic models:

  1. [`ConversableAgent.register_for_llm`](/autogen/docs/reference/agentchat/conversable_agent#register_for_llm) is used to register the function as a Tool in the `llm_config` of a ConversableAgent. The ConversableAgent agent can propose execution of a registered Tool, but the actual execution will be performed by a UserProxy agent.

  2. [`ConversableAgent.register_for_execution`](/autogen/docs/reference/agentchat/conversable_agent#register_for_execution) is used to register the function in the `function_map` of a UserProxy agent.

The following examples illustrates the process of registering a custom
function for currency exchange calculation that uses type hints and standard
Python datatypes:

  1. First, we import necessary libraries and configure models using [`autogen.config_list_from_json`](/autogen/docs/FAQ#set-your-api-endpoints) function:

    from typing import Literal  
      
    from pydantic import BaseModel, Field  
    from typing_extensions import Annotated  
      
    import autogen  
      
    config_list = autogen.config_list_from_json(  
        "OAI_CONFIG_LIST",  
        filter_dict={  
            "model": ["gpt-4", "gpt-3.5-turbo", "gpt-3.5-turbo-16k"],  
        },  
    )  

  2. We create an assistant agent and user proxy. The assistant will be responsible for suggesting which functions to call and the user proxy for the actual execution of a proposed function:

    llm_config = {  
        "config_list": config_list,  
        "timeout": 120,  
    }  
      
    chatbot = autogen.AssistantAgent(  
        name="chatbot",  
        system_message="For currency exchange tasks, only use the functions you have been provided with. Reply TERMINATE when the task is done.",  
        llm_config=llm_config,  
    )  
      
    # create a UserProxyAgent instance named "user_proxy"  
    user_proxy = autogen.UserProxyAgent(  
        name="user_proxy",  
        is_termination_msg=lambda x: x.get("content", "") and x.get("content", "").rstrip().endswith("TERMINATE"),  
        human_input_mode="NEVER",  
        max_consecutive_auto_reply=10,  
    )  

  3. We define the function `currency_calculator` below as follows and decorate it with two decorators:
    *[`@user_proxy.register_for_execution()`](/autogen/docs/reference/agentchat/conversable_agent#register_for_execution) adding the function `currency_calculator` to `user_proxy.function_map`, and
    * [`@chatbot.register_for_llm`](/autogen/docs/reference/agentchat/conversable_agent#register_for_llm) adding a generated JSON schema of the function to `llm_config` of `chatbot`.

    CurrencySymbol = Literal["USD", "EUR"]  
      
      
    def exchange_rate(base_currency: CurrencySymbol, quote_currency: CurrencySymbol) -> float:  
        if base_currency == quote_currency:  
            return 1.0  
        elif base_currency == "USD" and quote_currency == "EUR":  
            return 1 / 1.1  
        elif base_currency == "EUR" and quote_currency == "USD":  
            return 1.1  
        else:  
            raise ValueError(f"Unknown currencies {base_currency}, {quote_currency}")  
      
      
    @user_proxy.register_for_execution()  
    @chatbot.register_for_llm(description="Currency exchange calculator.")  
    def currency_calculator(  
        base_amount: Annotated[float, "Amount of currency in base_currency"],  
        base_currency: Annotated[CurrencySymbol, "Base currency"] = "USD",  
        quote_currency: Annotated[CurrencySymbol, "Quote currency"] = "EUR",  
    ) -> str:  
        quote_amount = exchange_rate(base_currency, quote_currency) * base_amount  
        return f"{quote_amount} {quote_currency}"  

Notice the use of
[Annotated](https://docs.python.org/3/library/typing.html?highlight=annotated#typing.Annotated)
to specify the type and the description of each parameter. The return value of
the function must be either string or serializable to string using the
[`json.dumps()`](https://docs.python.org/3/library/json.html#json.dumps) or
[`Pydantic` model dump to
JSON](https://docs.pydantic.dev/latest/concepts/serialization/#modelmodel_dump_json)
(both version 1.x and 2.x are supported).

You can check the JSON schema generated by the decorator
`chatbot.llm_config["tools"]`:

    [{'type': 'function', 'function':  
     {'description': 'Currency exchange calculator.',  
      'name': 'currency_calculator',  
      'parameters': {'type': 'object',  
       'properties': {'base_amount': {'type': 'number',  
         'description': 'Amount of currency in base_currency'},  
        'base_currency': {'enum': ['USD', 'EUR'],  
         'type': 'string',  
         'default': 'USD',  
         'description': 'Base currency'},  
        'quote_currency': {'enum': ['USD', 'EUR'],  
         'type': 'string',  
         'default': 'EUR',  
         'description': 'Quote currency'}},  
       'required': ['base_amount']}}}]  

  4. Agents can now use the function as follows:

    user_proxy.initiate_chat(  
        chatbot,  
        message="How much is 123.45 USD in EUR?",  
    )  

Output:

    user_proxy (to chatbot):  
      
    How much is 123.45 USD in EUR?  
      
    --------------------------------------------------------------------------------  
    chatbot (to user_proxy):  
      
    ***** Suggested tool Call: currency_calculator *****  
    Arguments:  
    {"base_amount":123.45,"base_currency":"USD","quote_currency":"EUR"}  
    ********************************************************  
      
    --------------------------------------------------------------------------------  
      
    >>>>>>>> EXECUTING FUNCTION currency_calculator...  
    user_proxy (to chatbot):  
      
    ***** Response from calling function "currency_calculator" *****  
    112.22727272727272 EUR  
    ****************************************************************  
      
    --------------------------------------------------------------------------------  
    chatbot (to user_proxy):  
      
    123.45 USD is equivalent to approximately 112.23 EUR.  
    ...  
      
    TERMINATE  

Use of Pydantic models further simplifies writing of such functions. Pydantic
models can be used for both the parameters of a function and for its return
type. Parameters of such functions will be constructed from JSON provided by
an AI model, while the output will be serialized as JSON encoded string
automatically.

The following example shows how we could rewrite our currency exchange
calculator example:

    # defines a Pydantic model  
    class Currency(BaseModel):  
      # parameter of type CurrencySymbol  
      currency: Annotated[CurrencySymbol, Field(..., description="Currency symbol")]  
      # parameter of type float, must be greater or equal to 0 with default value 0  
      amount: Annotated[float, Field(0, description="Amount of currency", ge=0)]  
      
    @user_proxy.register_for_execution()  
    @chatbot.register_for_llm(description="Currency exchange calculator.")  
    def currency_calculator(  
      base: Annotated[Currency, "Base currency: amount and currency symbol"],  
      quote_currency: Annotated[CurrencySymbol, "Quote currency symbol"] = "USD",  
    ) -> Currency:  
      quote_amount = exchange_rate(base.currency, quote_currency) * base.amount  
      return Currency(amount=quote_amount, currency=quote_currency)  

The generated JSON schema has additional properties such as minimum value
encoded:

    [{'type': 'function', 'function':  
     {'description': 'Currency exchange calculator.',  
      'name': 'currency_calculator',  
      'parameters': {'type': 'object',  
       'properties': {'base': {'properties': {'currency': {'description': 'Currency symbol',  
           'enum': ['USD', 'EUR'],  
           'title': 'Currency',  
           'type': 'string'},  
          'amount': {'default': 0,  
           'description': 'Amount of currency',  
           'minimum': 0.0,  
           'title': 'Amount',  
           'type': 'number'}},  
         'required': ['currency'],  
         'title': 'Currency',  
         'type': 'object',  
         'description': 'Base currency: amount and currency symbol'},  
        'quote_currency': {'enum': ['USD', 'EUR'],  
         'type': 'string',  
         'default': 'USD',  
         'description': 'Quote currency symbol'}},  
       'required': ['base']}}}]  

For more in-depth examples, please check the following:

* Currency calculator examples - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_function_call_currency_calculator.ipynb)

* Use Provided Tools as Functions - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_function_call.ipynb)

* Use Tools via Sync and Async Function Calling - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_function_call_async.ipynb)

## Multi-agent Conversations​ {#multiagent-conversations}

### A Basic Two-Agent Conversation Example​ {#a-basic-twoagent-conversation-example}

Once the participating agents are constructed properly, one can start a multi-
agent conversation session by an initialization step as shown in the following
code:

    # the assistant receives a message from the user, which contains the task description  
    user_proxy.initiate_chat(  
        assistant,  
        message="""What date is today? Which big tech stock has the largest year-to-date gain this year? How much is the gain?""",  
    )  

After the initialization step, the conversation could proceed automatically.
Find a visual illustration of how the user_proxy and assistant collaboratively
solve the above task autonomously below: ![Agent Chat
Example](/autogen/assets/images/agent_example-a965f253ce7d8e1548ff819e19edc5e4.png)

  1. The assistant receives a message from the user_proxy, which contains the task description.
  2. The assistant then tries to write Python code to solve the task and sends the response to the user_proxy.
  3. Once the user_proxy receives a response from the assistant, it tries to reply by either soliciting human input or preparing an automatically generated reply. If no human input is provided, the user_proxy executes the code and uses the result as the auto-reply.
  4. The assistant then generates a further response for the user_proxy. The user_proxy can then decide whether to terminate the conversation. If not, steps 3 and 4 are repeated.

### Supporting Diverse Conversation Patterns​ {#supporting-diverse-conversation-patterns}

#### Conversations with different levels of autonomy, and human-involvement {#conversations-with-different-levels-of-autonomy-and-humaninvolvement}

patterns​

On the one hand, one can achieve fully autonomous conversations after an
initialization step. On the other hand, AutoGen can be used to implement
human-in-the-loop problem-solving by configuring human involvement levels and
patterns (e.g., setting the `human_input_mode` to `ALWAYS`), as human
involvement is expected and/or desired in many applications.

#### Static and dynamic conversations​ {#static-and-dynamic-conversations}

By adopting the conversation-driven control with both programming language and
natural language, AutoGen inherently allows dynamic conversation. Dynamic
conversation allows the agent topology to change depending on the actual flow
of conversation under different input problem instances, while the flow of a
static conversation always follows a pre-defined topology. The dynamic
conversation pattern is useful in complex applications where the patterns of
interaction cannot be predetermined in advance. AutoGen provides two general
approaches to achieving dynamic conversation:

* Registered auto-reply. With the pluggable auto-reply function, one can choose to invoke conversations with other agents depending on the content of the current message and context. A working system demonstrating this type of dynamic conversation can be found in this code example, demonstrating a [dynamic group chat](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_groupchat.ipynb). In the system, we register an auto-reply function in the group chat manager, which lets LLM decide who the next speaker will be in a group chat setting.

* LLM-based function call. In this approach, LLM decides whether or not to call a particular function depending on the conversation status in each inference call. By messaging additional agents in the called functions, the LLM can drive dynamic multi-agent conversation. A working system showcasing this type of dynamic conversation can be found in the [multi-user math problem solving scenario](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_two_users.ipynb), where a student assistant would automatically resort to an expert using function calls.

### Diverse Applications Implemented with AutoGen​ {#diverse-applications-implemented-with-autogen}

The figure below shows six examples of applications built using AutoGen.
![Applications](/autogen/assets/images/app-c414cd164ef912e5e8b40f61042143ad.png)

Find a list of examples in this page: [Automated Agent Chat
Examples](/autogen/docs/Examples#automated-multi-agent-chat)

## For Further Reading​ {#for-further-reading}

 _Interested in the research that leads to this package? Please check the
following papers._

* [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework](https://arxiv.org/abs/2308.08155). Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang and Chi Wang. ArXiv 2023.

* [An Empirical Study on Challenging Math Problem Solving with GPT-4](https://arxiv.org/abs/2306.01337). Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, Chi Wang. ArXiv preprint arXiv:2306.01337 (2023).

[Edit this
page](<https://github.com/microsoft/autogen/edit/main/website/docs/Use->
Cases/agent_chat.md)

[Previous« Installation](/autogen/docs/Installation)

[NextEnhanced Inference »](/autogen/docs/Use-Cases/enhanced_inference)

* Agents
* Multi-agent Conversations
  * A Basic Two-Agent Conversation Example
  * Supporting Diverse Conversation Patterns
  * Diverse Applications Implemented with AutoGen
* For Further Reading

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* [Getting Started](.docs/Getting-Started)
* [Installation](.docs/Installation)
* Use Cases
  * [AutoGen for Large Language Models](.docs/Use-Cases/Autogen)
  * [Task Oriented AutoML](.docs/Use-Cases/Task-Oriented-AutoML)
  * [Tune User Defined Function](.docs/Use-Cases/Tune-User-Defined-Function)
  * [Zero Shot AutoML](.docs/Use-Cases/Zero-Shot-AutoML)
* Examples
* [Contributing](.docs/Contribute)
* [Research](.docs/Research)

On this page

# Tune User Defined Function {#tune-user-defined-function}

[`flaml.tune`](.docs/reference/tune/tune) is a module for economical
hyperparameter tuning. It is used internally by `flaml.AutoML`. It can also be
used to directly tune a user-defined function (UDF), which is not limited to
machine learning model training. You can use `flaml.tune` instead of
`flaml.AutoML` if one of the following is true:

  1. Your machine learning task is not one of the built-in tasks from `flaml.AutoML`.
  2. Your input cannot be represented as X_train + y_train or dataframe + label.
  3. The optimization metric is not measurable via validation data only. For example, when you want to directly optimize a downstream application instead of a model accuracy metric.
  4. You need to tune a function that may not even be a machine learning procedure.

## Basic Tuning Procedure​ {#basic-tuning-procedure}

There are three essential steps (assuming the knowledge of the set of
hyperparameters to tune) to use `flaml.tune` to finish a basic tuning task:

  1. Specify the tuning objective with respect to the hyperparameters.
  2. Specify a search space of the hyperparameters.
  3. Specify tuning constraints, including constraints on the resource budget to do the tuning, constraints on the configurations, or/and constraints on a (or multiple) particular metric(s).

With these steps, you can perform a basic tuning task accordingly.

### Tuning objective​ {#tuning-objective}

Related arguments:

* `evaluation_function`: A user-defined evaluation function.
* `metric`: A string of the metric name to optimize for.
* `mode`: A string in ['min', 'max'] to specify the objective as minimization or maximization.

The first step is to specify your tuning objective. To do it, you should first
specify your evaluation procedure (e.g., perform a machine learning model
training and validation) with respect to the hyperparameters in a user-defined
function `evaluation_function`. The function requires a hyperparameter
configuration as input, and can simply return a metric value in a scalar or
return a dictionary of metric name and metric value pairs.

In the following code, we define an evaluation function with respect to two
hyperparameters named `x` and `y` according to obj:=(x−85000)2−x/yobj :=
(x-85000)^2 - x/yobj:=(x−85000)2−x/y. Note that we use this toy example here
for more accessible demonstration purposes. In real use cases, the evaluation
function usually cannot be written in this closed form, but instead involves a
black-box and expensive evaluation procedure. Please check out [Tune
HuggingFace](.docs/Examples/Tune-HuggingFace), [Tune
PyTorch](.docs/Examples/Tune-PyTorch) and [Tune
LightGBM](.docs/Getting-Started#tune-user-defined-function) for real
examples of tuning tasks.

    import time  
      
    def evaluate_config(config: dict):  
        """evaluate a hyperparameter configuration"""  
        score = (config["x"] - 85000) ** 2 - config["x"] / config["y"]  
        # usually the evaluation takes an non-neglible cost  
        # and the cost could be related to certain hyperparameters  
        # here we simulate this cost by calling the time.sleep() function  
        # here we assume the cost is proportional to x  
        faked_evaluation_cost = config["x"] / 100000  
        time.sleep(faked_evaluation_cost)  
        # we can return a single float as a score on the input config:  
        # return score  
        # or, we can return a dictionary that maps metric name to metric value:  
        return {"score": score, "evaluation_cost": faked_evaluation_cost, "constraint_metric": config["x"] * config["y"]}  

When the evaluation function returns a dictionary of metrics, you need to
specify the name of the metric to optimize via the argument `metric` (this can
be skipped when the function is just returning a scalar). In addition, you
need to specify a mode of your optimization/tuning task (maximization or
minimization) via the argument `mode` by choosing from "min" or "max".

For example,

    flaml.tune.run(evaluation_function=evaluate_config, metric="score", mode="min", ...)  

### Search space​ {#search-space}

Related arguments:

* `config`: A dictionary to specify the search space.
* `low_cost_partial_config` (optional): A dictionary from a subset of controlled dimensions to the initial low-cost values.
* `cat_hp_cost` (optional): A dictionary from a subset of categorical dimensions to the relative cost of each choice.

The second step is to specify a search space of the hyperparameters through
the argument `config`. In the search space, you need to specify valid values
for your hyperparameters and can specify how these values are sampled (e.g.,
from a uniform distribution or a log-uniform distribution).

In the following code example, we include a search space for the two
hyperparameters `x` and `y` as introduced above. The valid values for both are
integers in the range of [1, 100000]. The values for `x` are sampled uniformly
in the specified range (using `tune.randint(lower=1, upper=100000)`), and the
values for `y` are sampled uniformly in logarithmic space of the specified
range (using `tune.lograndit(lower=1, upper=100000)`).

    from flaml import tune  
      
    # construct a search space for the hyperparameters x and y.  
    config_search_space = {  
        "x": tune.lograndint(lower=1, upper=100000),  
        "y": tune.randint(lower=1, upper=100000)  
    }  
      
    # provide the search space to tune.run  
    tune.run(..., config=config_search_space, ...)  

#### **Details and guidelines on hyperparameter search space**​

The corresponding value of a particular hyperparameter in the search space
dictionary is called a _domain_ , for example, `tune.randint(lower=1,
upper=100000)` is the domain for the hyperparameter `y`. The domain specifies
a _type_ and _valid range_ to sample parameters from. Supported types include
float, integer, and categorical.

* **Categorical hyperparameter**

If it is a categorical hyperparameter, then you should use
`tune.choice(possible_choices)` in which `possible_choices` is the list of
possible categorical values of the hyperparameter. For example, if you are
tuning the optimizer used in model training, and the candidate optimizers are
"sgd" and "adam", you should specify the search space in the following way:

    {  
        "optimizer": tune.choice(["sgd", "adam"]),  
    }  

* **Numerical hyperparameter**

If it is a numerical hyperparameter, you need to know whether it takes integer
values or float values. In addition, you need to know:

* The range of valid values, i.e., what are the lower limit and upper limit of the hyperparameter value?
* Do you want to sample in linear scale or log scale? It is a common practice to sample in the log scale if the valid value range is large and the evaluation function changes more regularly with respect to the log domain, as shown in the following example for learning rate tuning. In this code example, we set the lower limit and the upper limit of the learning rate to be 1/1024 and 1.0, respectively. We sample in the log space because model performance changes more regularly in the log scale with respect to the learning rate within such a large search range.

    {  
        "learning_rate": tune.loguniform(lower=1 / 1024, upper=1.0),  
    }  

When the search range of learning rate is small, it is more common to sample
in the linear scale as shown in the following example,

    {  
        "learning_rate": tune.uniform(lower=0.1, upper=0.2),  
    }  

* Do you have quantization granularity requirements?

When you have a desired quantization granularity for the hyperparameter
change, you can use `tune.qlograndint` or `tune.qloguniform` to realize the
quantization requirement. The following code example helps you realize the
need for sampling uniformly in the range of 0.1 and 0.2 with increments of
0.02, i.e., the sampled learning rate can only take values in {0.1, 0.12,
0.14, 0.16, ..., 0.2},

    {  
        "learning_rate": tune.quniform(lower=0.1, upper=0.2, q=0.02),  
    }  

You can find the corresponding search space choice in the table below once you
have answers to the aforementioned three questions.

| Integer| Float  
---|---|---  
linear scale| tune.randint(lower: int, upper: int)| tune.uniform(lower: float,
upper: float)  
log scale| tune.lograndint(lower: int, upper: int, base: float = 10|
tune.loguniform(lower: float, upper: float, base: float = 10)  
linear scale with quantization| tune.qrandint(lower: int, upper: int, q: int =
1)| tune.quniform(lower: float, upper: float, q: float = 1)  
log scale with quantization| tune.qlograndint(lower: int, upper, q: int = 1,
base: float = 10)| tune.qloguniform(lower: float, upper, q: float = 1, base:
float = 10)  
  
See the example below for the commonly used types of domains.

    config = {  
        # Sample a float uniformly between -5.0 and -1.0  
        "uniform": tune.uniform(-5, -1),  
      
        # Sample a float uniformly between 3.2 and 5.4,  
        # rounding to increments of 0.2  
        "quniform": tune.quniform(3.2, 5.4, 0.2),  
      
        # Sample a float uniformly between 0.0001 and 0.01, while  
        # sampling in log space  
        "loguniform": tune.loguniform(1e-4, 1e-2),  
      
        # Sample a float uniformly between 0.0001 and 0.1, while  
        # sampling in log space and rounding to increments of 0.00005  
        "qloguniform": tune.qloguniform(1e-4, 1e-1, 5e-5),  
      
        # Sample a random float from a normal distribution with  
        # mean=10 and sd=2  
        "randn": tune.randn(10, 2),  
      
        # Sample a random float from a normal distribution with  
        # mean=10 and sd=2, rounding to increments of 0.2  
        "qrandn": tune.qrandn(10, 2, 0.2),  
      
        # Sample a integer uniformly between -9 (inclusive) and 15 (exclusive)  
        "randint": tune.randint(-9, 15),  
      
        # Sample a random uniformly between -21 (inclusive) and 12 (inclusive (!))  
        # rounding to increments of 3 (includes 12)  
        "qrandint": tune.qrandint(-21, 12, 3),  
      
        # Sample a integer uniformly between 1 (inclusive) and 10 (exclusive),  
        # while sampling in log space  
        "lograndint": tune.lograndint(1, 10),  
      
        # Sample a integer uniformly between 2 (inclusive) and 10 (inclusive (!)),  
        # while sampling in log space and rounding to increments of 2  
        "qlograndint": tune.qlograndint(2, 10, 2),  
      
        # Sample an option uniformly from the specified choices  
        "choice": tune.choice(["a", "b", "c"]),  
    }  

#### Cost-related hyperparameters​ {#costrelated-hyperparameters}

Cost-related hyperparameters are a subset of the hyperparameters which
directly affect the computation cost incurred in the evaluation of any
hyperparameter configuration. For example, the number of estimators
(`n_estimators`) and the maximum number of leaves (`max_leaves`) are known to
affect the training cost of tree-based learners. So they are cost-related
hyperparameters for tree-based learners.

When cost-related hyperparameters exist, the evaluation cost in the search
space is heterogeneous. In this case, designing a search space with proper
ranges of the hyperparameter values is highly non-trivial. Classical tuning
algorithms such as Bayesian optimization and random search are typically
sensitive to such ranges. It may take them a very high cost to find a good
choice if the ranges are too large. And if the ranges are too small, the
optimal choice(s) may not be included and thus not possible to be found. With
our method, you can use a search space with larger ranges in the case of
heterogeneous cost.

Our search algorithms are designed to finish the tuning process at a low total
cost when the evaluation cost in the search space is heterogeneous. So in such
scenarios, if you are aware of low-cost configurations for the cost-related
hyperparameters, you are encouraged to set them as the
`low_cost_partial_config`, which is a dictionary of a subset of the
hyperparameter coordinates whose value corresponds to a configuration with
known low cost. Using the example of the tree-based methods again, since we
know that small `n_estimators` and `max_leaves` generally correspond to
simpler models and thus lower cost, we set `{'n_estimators': 4, 'max_leaves':
4}` as the `low_cost_partial_config` by default (note that 4 is the lower
bound of search space for these two hyperparameters), e.g., in LGBM. Please
find more details on how the algorithm works here.

In addition, if you are aware of the cost relationship between different
categorical hyperparameter choices, you are encouraged to provide this
information through `cat_hp_cost`. It also helps the search algorithm to
reduce the total cost.

### Tuning constraints​ {#tuning-constraints}

Related arguments:

* `time_budget_s`: The time budget in seconds.
* `num_samples`: An integer of the number of configs to try.
* `config_constraints` (optional): A list of config constraints to be satisfied.
* `metric_constraints` (optional): A list of metric constraints to be satisfied. e.g., `['precision', '>=', 0.9]`.

The third step is to specify constraints of the tuning task. One notable
property of `flaml.tune` is that it is able to finish the tuning process
(obtaining good results) within a required resource constraint. A user can
either provide the resource constraint in terms of wall-clock time (in
seconds) through the argument `time_budget_s`, or in terms of the number of
trials through the argument `num_samples`. The following example shows three
use cases:

    # Set a resource constraint of 60 seconds wall-clock time for the tuning.  
    flaml.tune.run(..., time_budget_s=60, ...)  
      
    # Set a resource constraint of 100 trials for the tuning.  
    flaml.tune.run(..., num_samples=100, ...)  
      
    # Use at most 60 seconds and at most 100 trials for the tuning.  
    flaml.tune.run(..., time_budget_s=60, num_samples=100, ...)  

Optionally, you can provide a list of config constraints to be satisfied
through the argument `config_constraints` and provide a list of metric
constraints to be satisfied through the argument `metric_constraints`. We
provide more details about related use cases in the Advanced Tuning Options
section.

### Put together​ {#put-together}

After the aforementioned key steps, one is ready to perform a tuning task by
calling [`flaml.tune.run()`](.docs/reference/tune/tune#run). Below is a
quick sequential tuning example using the pre-defined search space
`config_search_space` and a minimization (`mode='min'`) objective for the
`score` metric evaluated in `evaluate_config`, using the default serach
algorithm in flaml. The time budget is 10 seconds (`time_budget_s=10`).

    # require: pip install flaml[blendsearch]  
    analysis = tune.run(  
        evaluate_config,  # the function to evaluate a config  
        config=config_search_space,  # the search space defined  
        metric="score",  
        mode="min",  # the optimization mode, "min" or "max"  
        num_samples=-1,  # the maximal number of configs to try, -1 means infinite  
        time_budget_s=10,  # the time budget in seconds  
    )  

### Result analysis​ {#result-analysis}

Once the tuning process finishes, it returns an
[ExperimentAnalysis](.docs/reference/tune/analysis) object, which
provides methods to analyze the tuning.

In the following code example, we retrieve the best configuration found during
the tuning, and retrieve the best trial's result from the returned `analysis`.

    analysis = tune.run(  
        evaluate_config,  # the function to evaluate a config  
        config=config_search_space,  # the search space defined  
        metric="score",  
        mode="min",  # the optimization mode, "min" or "max"  
        num_samples=-1,  # the maximal number of configs to try, -1 means infinite  
        time_budget_s=10,  # the time budget in seconds  
    )  
    print(analysis.best_config)  # the best config  
    print(analysis.best_trial.last_result)  # the best trial's result  

## Advanced Tuning Options​ {#advanced-tuning-options}

There are several advanced tuning options worth mentioning.

### More constraints on the tuning​ {#more-constraints-on-the-tuning}

A user can specify constraints on the configurations to be satisfied via the
argument `config_constraints`. The `config_constraints` receives a list of
such constraints to be satisfied. Specifically, each constraint is a tuple
that consists of (1) a function that takes a configuration as input and
returns a numerical value; (2) an operation chosen from "<=", ">=", "<" or
">"; (3) a numerical threshold.

In the following code example, we constrain the output of `area`, which takes
a configuration as input and outputs a numerical value, to be no larger than
1000.
```python
    def my_model_size(config):  
        return config["n_estimators"] * config["max_leaves"]  
      
    analysis = tune.run(...,  
        config_constraints = [(my_model_size, "<=", 40)],  
    )  

You can also specify a list of metric constraints to be satisfied via the
argument `metric_constraints`. Each element in the `metric_constraints` list
is a tuple that consists of (1) a string specifying the name of the metric
(the metric name must be defined and returned in the user-defined
`evaluation_function`); (2) an operation chosen from "<=" or ">="; (3) a
numerical threshold.

In the following code example, we constrain the metric `training_cost` to be
no larger than 1 second.

    analysis = tune.run(...,  
        metric_constraints = [("training_cost", "<=", 1)]),  

#### **`config_constraints` vs `metric_constraints`:**​

The key difference between these two types of constraints is that the
calculation of constraints in `config_constraints` does not rely on the
computation procedure in the evaluation function, i.e., in
`evaluation_function`. For example, when a constraint only depends on the
config itself, as shown in the code example. Due to this independency,
constraints in `config_constraints` will be checked before evaluation. So
configurations that do not satisfy `config_constraints` will not be evaluated.

### Parallel tuning​ {#parallel-tuning}

Related arguments:

* `use_ray`: A boolean of whether to use ray as the backend.
* `use_spark`: A boolean of whether to use spark as the backend.
* `resources_per_trial`: A dictionary of the hardware resources to allocate per trial, e.g., `{'cpu': 1}`. Only valid when using ray backend.

Details about parallel tuning with Spark could be found
[here](.docs/Examples/Integrate - Spark#parallel-spark-jobs).

You can perform parallel tuning by specifying `use_ray=True` (requiring
flaml[ray] option installed) or `use_spark=True` (requiring flaml[spark]
option installed). You can also limit the amount of resources allocated per
trial by specifying `resources_per_trial`, e.g., `resources_per_trial={'cpu':
2}` when `use_ray=True`.

    # require: pip install flaml[ray]  
    analysis = tune.run(  
        evaluate_config,  # the function to evaluate a config  
        config=config_search_space,  # the search space defined  
        metric="score",  
        mode="min",  # the optimization mode, "min" or "max"  
        num_samples=-1,  # the maximal number of configs to try, -1 means infinite  
        time_budget_s=10,  # the time budget in seconds  
        use_ray=True,  
        resources_per_trial={"cpu": 2}  # limit resources allocated per trial  
    )  
    print(analysis.best_trial.last_result)  # the best trial's result  
    print(analysis.best_config)  # the best config  

    # require: pip install flaml[spark]  
    analysis = tune.run(  
        evaluate_config,  # the function to evaluate a config  
        config=config_search_space,  # the search space defined  
        metric="score",  
        mode="min",  # the optimization mode, "min" or "max"  
        num_samples=-1,  # the maximal number of configs to try, -1 means infinite  
        time_budget_s=10,  # the time budget in seconds  
        use_spark=True,  
    )  
    print(analysis.best_trial.last_result)  # the best trial's result  
    print(analysis.best_config)  # the best config  

 **A headsup about computation overhead.** When parallel tuning is used, there
will be a certain amount of computation overhead in each trial. In case each
trial's original cost is much smaller than the overhead, parallel tuning can
underperform sequential tuning. Sequential tuning is recommended when compute
resource is limited, and each trial can consume all the resources.

### Trial scheduling​ {#trial-scheduling}

Related arguments:

* `scheduler`: A scheduler for executing the trials.
* `resource_attr`: A string to specify the resource dimension used by the scheduler.
* `min_resource`: A float of the minimal resource to use for the resource_attr.
* `max_resource`: A float of the maximal resource to use for the resource_attr.
* `reduction_factor`: A float of the reduction factor used for incremental pruning.

A scheduler can help manage the trials' execution. It can be used to perform
multi-fiedlity evalution, or/and early stopping. You can use two different
types of schedulers in `flaml.tune` via `scheduler`.

#### 1\. An authentic scheduler implemented in FLAML (`scheduler='flaml'`).​ {#1-an-authentic-scheduler-implemented-in-flaml-schedulerflaml}

This scheduler is authentic to the new search algorithms provided by FLAML. In
a nutshell, it starts the search with the minimum resource. It switches
between HPO with the current resource and increasing the resource for
evaluation depending on which leads to faster improvement.

If this scheduler is used, you need to

* Specify a resource dimension. Conceptually a 'resource dimension' is a factor that affects the cost of the evaluation (e.g., sample size, the number of epochs). You need to specify the name of the resource dimension via `resource_attr`. For example, if `resource_attr="sample_size"`, then the config dict passed to the `evaluation_function` would contain a key "sample_size" and its value suggested by the search algorithm. That value should be used in the evaluation function to control the compute cost. The larger is the value, the more expensive the evaluation is.

* Provide the lower and upper limit of the resource dimension via `min_resource` and `max_resource`, and optionally provide `reduction_factor`, which determines the magnitude of resource (multiplicative) increase when we decide to increase the resource.

In the following code example, we consider the sample size as the resource
dimension. It determines how much data is used to perform training as
reflected in the `evaluation_function`. We set the `min_resource` and
`max_resource` to 1000 and the size of the full training dataset,
respectively.

    from flaml import tune  
    from functools import partial  
    from flaml.automl.data import load_openml_task  
      
      
    def obj_from_resource_attr(resource_attr, X_train, X_test, y_train, y_test, config):  
        from lightgbm import LGBMClassifier  
        from sklearn.metrics import accuracy_score  
      
        # in this example sample size is our resource dimension  
        resource = int(config[resource_attr])  
        sampled_X_train = X_train.iloc[:resource]  
        sampled_y_train = y_train[:resource]  
      
        # construct a LGBM model from the config  
        # note that you need to first remove the resource_attr field  
        # from the config as it is not part of the original search space  
        model_config = config.copy()  
        del model_config[resource_attr]  
        model = LGBMClassifier(**model_config)  
      
        model.fit(sampled_X_train, sampled_y_train)  
        y_test_predict = model.predict(X_test)  
        test_loss = 1.0 - accuracy_score(y_test, y_test_predict)  
        return {resource_attr: resource, "loss": test_loss}  
      
      
    X_train, X_test, y_train, y_test = load_openml_task(task_id=7592, data_dir="test/")  
    max_resource = len(y_train)  
    resource_attr = "sample_size"  
    min_resource = 1000  
    analysis = tune.run(  
        partial(obj_from_resource_attr, resource_attr, X_train, X_test, y_train, y_test),  
        config={  
            "n_estimators": tune.lograndint(lower=4, upper=32768),  
            "max_leaves": tune.lograndint(lower=4, upper=32768),  
            "learning_rate": tune.loguniform(lower=1 / 1024, upper=1.0),  
        },  
        metric="loss",  
        mode="min",  
        resource_attr=resource_attr,  
        scheduler="flaml",  
        max_resource=max_resource,  
        min_resource=min_resource,  
        reduction_factor=2,  
        time_budget_s=10,  
        num_samples=-1,  
    )  

You can find more details about this scheduler in [this
paper](https://arxiv.org/pdf/1911.04706.pdf).

#### 2\. A scheduler of the {#2-a-scheduler-of-the}

[`TrialScheduler`](<https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#tune->
schedulers) class from `ray.tune`.​

There is a handful of schedulers of this type implemented in `ray.tune`, for
example,
[ASHA](<https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#asha-tune->
schedulers-ashascheduler),
[HyperBand](<https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#tune->
original-hyperband),
[BOHB](<https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#tune->
scheduler-bohb), etc.

To use this type of scheduler you can either (1) set `scheduler='asha'`, which
will automatically create an
[ASHAScheduler](<https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#asha->
tune-schedulers-ashascheduler) instance using the provided inputs
(`resource_attr`, `min_resource`, `max_resource`, and `reduction_factor`); or
(2) create an instance by yourself and provided it via `scheduler`, as shown
in the following code example,

    #  require: pip install flaml[ray]  
    from ray.tune.schedulers import HyperBandScheduler  
    my_scheduler = HyperBandScheduler(time_attr="sample_size", max_t=max_resource, reduction_factor=2)  
    tune.run(.., scheduler=my_scheduler, ...)  

* Similar to the case where the `flaml` scheduler is used, you need to specify the resource dimension, use the resource dimension accordingly in your `evaluation_function`, and provide the necessary information needed for scheduling, such as `min_resource`, `max_resource` and `reduction_factor` (depending on the requirements of the specific scheduler).

* Different from the case when the `flaml` scheduler is used, the amount of resources to use at each iteration is not suggested by the search algorithm through the `resource_attr` in a configuration. You need to specify the evaluation schedule explicitly by yourself in the `evaluation_function` and **report intermediate results (using`tune.report()`) accordingly**. In the following code example, we use the ASHA scheduler by setting `scheduler="asha"`. We specify `resource_attr`, `min_resource`, `min_resource` and `reduction_factor` the same way as in the previous example (when "flaml" is used as the scheduler). We perform the evaluation in a customized schedule.

* Use ray backend or not? You can choose to use ray backend or not by specifying `use_ray=True` or `use_ray=False`. When ray backend is not used, i.e., `use_ray=False`, you also need to stop the evaluation function by explicitly catching the `StopIteration` exception, as shown in the end of the evaluation function `obj_w_intermediate_report()` in the following code example.
```python
    def obj_w_intermediate_report(resource_attr, X_train, X_test, y_train, y_test, min_resource, max_resource, config):  
        from lightgbm import LGBMClassifier  
        from sklearn.metrics import accuracy_score  

        # a customized schedule to perform the evaluation  
        eval_schedule = [res for res in range(min_resource, max_resource, 5000)] + [max_resource]  
        for resource in eval_schedule:  
            sampled_X_train = X_train.iloc[:resource]  
            sampled_y_train = y_train[:resource]  
      
            # construct a LGBM model from the config  
            model = LGBMClassifier(**config)  
      
            model.fit(sampled_X_train, sampled_y_train)  
            y_test_predict = model.predict(X_test)  
            test_loss = 1.0 - accuracy_score(y_test, y_test_predict)  
            # need to report the resource attribute used and the corresponding intermediate results  
            try:  
                tune.report(sample_size=resource, loss=test_loss)  
            except (StopIteration, SystemExit):  
                # do cleanup operation here  
                return  

    resource_attr = "sample_size"  
    min_resource = 1000  
    max_resource = len(y_train)  
    analysis = tune.run(  
        partial(obj_w_intermediate_report, resource_attr, X_train, X_test, y_train, y_test, min_resource, max_resource),  
        config={  
            "n_estimators": tune.lograndint(lower=4, upper=32768),  
            "learning_rate": tune.loguniform(lower=1 / 1024, upper=1.0),  
        },  
        metric="loss",  
        mode="min",  
        resource_attr=resource_attr,  
        scheduler="asha",  
        max_resource=max_resource,  
        min_resource=min_resource,  
        reduction_factor=2,  
        time_budget_s=10,  
        num_samples = -1,  
    )  

* If you would like to do some cleanup opearation when the trial is stopped by the scheduler, you can do it when you catch the `StopIteration` (when not using ray) or `SystemExit` (when using ray) exception explicitly.

### Warm start​ {#warm-start}

Related arguments:

* `points_to_evaluate`: A list of initial hyperparameter configurations to run first.
* `evaluated_rewards`: If you have previously evaluated the parameters passed in as `points_to_evaluate` , you can avoid re-running those trials by passing in the reward attributes as a list so the optimizer can be told the results without needing to re-compute the trial. Must be the same length or shorter length than `points_to_evaluate`.

If you are aware of some good hyperparameter configurations, you are
encouraged to provide them via `points_to_evaluate`. The search algorithm will
try them first and use them to bootstrap the search.

You can use previously evaluated configurations to warm-start your tuning. For
example, the following code means that you know the reward for the two configs
in points_to_evaluate are 3.99 and 1.99, respectively, and want to inform
`tune.run()`.
```python
    def simple_obj(config):  
        return config["a"] + config["b"]  
      
    from flaml import tune  
    config_search_space = {  
        "a": tune.uniform(lower=0, upper=0.99),  
        "b": tune.uniform(lower=0, upper=3)  
    }  
      
    points_to_evaluate = [  
        {"b": .99, "a": 3},  
        {"b": .99, "a": 2},  
        {"b": .80, "a": 3},  
        {"b": .80, "a": 2},  
    ]  
    evaluated_rewards = [3.99, 2.99]  
      
    analysis = tune.run(  
        simple_obj,  
        config=config_search_space,  
        mode="max",  
        points_to_evaluate=points_to_evaluate,  
        evaluated_rewards=evaluated_rewards,  
        time_budget_s=10,  
        num_samples=-1,  
    )  

### Reproducibility​ {#reproducibility}

By default, there is randomness in our tuning process (for versions <= 0.9.1).
If reproducibility is desired, you could manually set a random seed before
calling `tune.run()`. For example, in the following code, we call
`np.random.seed(100)` to set the random seed. With this random seed, running
the following code multiple times will generate exactly the same search
trajectory. The reproducibility can only be guaranteed in sequential tuning.

    import numpy as np  
    np.random.seed(100)  # This line is not needed starting from version v0.9.2.  
    analysis = tune.run(  
        simple_obj,  
        config=config_search_space,  
        mode="max",  
        num_samples=10,  
    )  

### Lexicographic Objectives​ {#lexicographic-objectives}

We support tuning multiple objectives with lexicographic preference by
providing argument `lexico_objectives` for `tune.run()`. `lexico_objectives`
is a dictionary that contains the following fields of key-value pairs:

* `metrics`: a list of optimization objectives with the orders reflecting the priorities/preferences of the objectives.
* `modes`: (optional) a list of optimization modes (each mode either "min" or "max") corresponding to the objectives in the metric list. If not provided, we use "min" as the default mode for all the objectives.
* `tolerances`: (optional) a dictionary to specify the optimality tolerances on objectives. The keys are the metric names (provided in "metrics"), and the values are the absolute/percentage tolerance in the form of numeric/string.
* `targets`: (optional) a dictionary to specify the optimization targets on the objectives. The keys are the metric names (provided in "metric"), and the values are the numerical target values.

In the following example, we want to minimize `val_loss` and `pred_time` of
the model where `val_loss` has high priority. The tolerances for `val_loss`
and `pre_time` are 0.02 and 0 respectively. We do not set targets for these
two objectives and we set them to -inf for both objectives.

    lexico_objectives = {}  
    lexico_objectives["metrics"] = ["val_loss", "pred_time"]  
    lexico_objectives["modes"] = ["min", "min"]  
    lexico_objectives["tolerances"] = {"val_loss": 0.02, "pred_time": 0.0}  
    lexico_objectives["targets"] = {"val_loss": -float('inf'), "pred_time": -float('inf')}  
      
    # provide the lexico_objectives to tune.run  
    tune.run(..., search_alg=None, lexico_objectives=lexico_objectives)  

We also supports providing percentage tolerance as shown below.

    lexico_objectives["tolerances"] = {"val_loss": "10%", "pred_time": "0%"}  

NOTE:

  1. When lexico_objectives is not None, the arguments metric, mode, will be invalid, and flaml's tune uses CFO as the `search_alg`, which makes the input (if provided) `search_alg` invalid.

  2. This is a new feature that will be released in version 1.1.0 and is subject to change in the future version.

## Hyperparameter Optimization Algorithm​ {#hyperparameter-optimization-algorithm}

To tune the hyperparameters toward your objective, you will want to use a
hyperparameter optimization algorithm which can help suggest hyperparameters
with better performance (regarding your objective). `flaml` offers two HPO
methods: CFO and BlendSearch. `flaml.tune` uses BlendSearch by default when
the option [blendsearch] is installed.

### CFO: Frugal Optimization for Cost-related Hyperparameters​ {#cfo-frugal-optimization-for-costrelated-hyperparameters}

CFO uses the randomized direct search method FLOW2 with adaptive stepsize and
random restart. It requires a low-cost initial point as input if such point
exists. The search begins with the low-cost initial point and gradually move
to high cost region if needed. The local search method has a provable
convergence rate and bounded cost.

About FLOW2: FLOW2 is a simple yet effective randomized direct search method.
It is an iterative optimization method that can optimize for black-box
functions. FLOW2 only requires pairwise comparisons between function values to
perform iterative update. Comparing to existing HPO methods, FLOW2 has the
following appealing properties:

  1. It is applicable to general black-box functions with a good convergence rate in terms of loss.
  2. It provides theoretical guarantees on the total evaluation cost incurred.

The GIFs attached below demonstrate an example search trajectory of FLOW2
shown in the loss and evaluation cost (i.e., the training time ) space
respectively. FLOW2 is used in tuning the # of leaves and the # of trees for
XGBoost. The two background heatmaps show the loss and cost distribution of
all configurations. The black dots are the points evaluated in FLOW2. Black
dots connected by lines are points that yield better loss performance when
evaluated.

![gif](.assets/images/heatmap_loss_cfo_12s-1e20ac4623c024546d2f3b5d355077de.gif)|
![gif](.assets/images/heatmap_cost_cfo_12s-f299c2309b5b414ee2ca83f1050e186c.gif)  
---|---  
  
From the demonstration, we can see that (1) FLOW2 can quickly move toward the
low-loss region, showing good convergence property and (2) FLOW2 tends to
avoid exploring the high-cost region until necessary.

Example:

    from flaml import CFO  
    tune.run(...  
        search_alg=CFO(low_cost_partial_config=low_cost_partial_config),  
    )  

 **Recommended scenario** : There exist cost-related hyperparameters and a
low-cost initial point is known before optimization. If the search space is
complex and CFO gets trapped into local optima, consider using BlendSearch.

### BlendSearch: Economical Hyperparameter Optimization With Blended Search {#blendsearch-economical-hyperparameter-optimization-with-blended-search}

Strategy​

BlendSearch combines local search with global search. It leverages the
frugality of CFO and the space exploration ability of global search methods
such as Bayesian optimization. Like CFO, BlendSearch requires a low-cost
initial point as input if such point exists, and starts the search from there.
Different from CFO, BlendSearch will not wait for the local search to fully
converge before trying new start points. The new start points are suggested by
the global search method and filtered based on their distance to the existing
points in the cost-related dimensions. BlendSearch still gradually increases
the trial cost. It prioritizes among the global search thread and multiple
local search threads based on optimism in face of uncertainty.

Example:

    # require: pip install flaml[blendsearch]  
    from flaml import BlendSearch  
    tune.run(...  
        search_alg=BlendSearch(low_cost_partial_config=low_cost_partial_config),  
    )  

 **Recommended scenario** : Cost-related hyperparameters exist, a low-cost
initial point is known, and the search space is complex such that local search
is prone to be stuck at local optima.

 **Suggestion about using larger search space in BlendSearch**. In
hyperparameter optimization, a larger search space is desirable because it is
more likely to include the optimal configuration (or one of the optimal
configurations) in hindsight. However the performance (especially anytime
performance) of most existing HPO methods is undesirable if the cost of the
configurations in the search space has a large variation. Thus hand-crafted
small search spaces (with relatively homogeneous cost) are often used in
practice for these methods, which is subject to idiosyncrasy. BlendSearch
combines the benefits of local search and global search, which enables a smart
(economical) way of deciding where to explore in the search space even though
it is larger than necessary. This allows users to specify a larger search
space in BlendSearch, which is often easier and a better practice than
narrowing down the search space by hand.

For more technical details, please check our papers.

* [Frugal Optimization for Cost-related Hyperparameters](https://arxiv.org/abs/2005.01571). Qingyun Wu, Chi Wang, Silu Huang. AAAI 2021.

    @inproceedings{wu2021cfo,  
        title={Frugal Optimization for Cost-related Hyperparameters},  
        author={Qingyun Wu and Chi Wang and Silu Huang},  
        year={2021},  
        booktitle={AAAI'21},  
    }  

* [Economical Hyperparameter Optimization With Blended Search Strategy](https://www.microsoft.com/en-us/research/publication/economical-hyperparameter-optimization-with-blended-search-strategy/). Chi Wang, Qingyun Wu, Silu Huang, Amin Saied. ICLR 2021.

    @inproceedings{wang2021blendsearch,  
        title={Economical Hyperparameter Optimization With Blended Search Strategy},  
        author={Chi Wang and Qingyun Wu and Silu Huang and Amin Saied},  
        year={2021},  
        booktitle={ICLR'21},  
    }  

* [Targeted Hyperparameter Optimization with Lexicographic Preferences Over Multiple Objectives](https://openreview.net/forum?id=0Ij9_q567Ma). Shaokun Zhang, Feiran Jia, Chi Wang, Qingyun Wu. ICLR 2023 (notable-top-5%).

    @inproceedings{zhang2023targeted,  
        title={Targeted Hyperparameter Optimization with Lexicographic Preferences Over Multiple Objectives},  
        author={Shaokun Zhang and Feiran Jia and Chi Wang and Qingyun Wu},  
        booktitle={International Conference on Learning Representations},  
        year={2023},  
        url={<https://openreview.net/forum?id=0Ij9_q567Ma}>  
    }  

[Edit this
page](<https://github.com/microsoft.edit/main/website/docs/Use->
Cases/Tune-User-Defined-Function.md)

[Previous« Task Oriented AutoML](.docs/Use-Cases/Task-Oriented-AutoML)

[NextZero Shot AutoML »](.docs/Use-Cases/Zero-Shot-AutoML)

* Basic Tuning Procedure
  * Tuning objective
  * Search space
  * Tuning constraints
  * Put together
  * Result analysis
* Advanced Tuning Options
  * More constraints on the tuning
  * Parallel tuning
  * Trial scheduling
  * Warm start
  * Reproducibility
  * Lexicographic Objectives
* Hyperparameter Optimization Algorithm
  * CFO: Frugal Optimization for Cost-related Hyperparameters
  * BlendSearch: Economical Hyperparameter Optimization With Blended Search Strategy

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
    * autogen.agentchat
      * [agent](.docs/reference/autogen/agentchat/agent)
      * [assistant_agent](.docs/reference/autogen/agentchat/assistant_agent)
      * [conversable_agent](.docs/reference/autogen/agentchat/conversable_agent)
      * [groupchat](.docs/reference/autogen/agentchat/groupchat)
      * [user_proxy_agent](.docs/reference/autogen/agentchat/user_proxy_agent)
    * autogen.oai
    * [code_utils](.docs/reference/autogen/code_utils)
    * [math_utils](.docs/reference/autogen/math_utils)
    * [retrieve_utils](.docs/reference/autogen/retrieve_utils)
  * automl
  * default
  * onlineml
  * tune

On this page

# autogen.agentchat.groupchat {#autogenagentchatgroupchat}

## GroupChat Objects​ {#groupchat-objects}

    @dataclass  
    class GroupChat()  

A group chat class that contains a list of agents and the maximum number of
rounds.

#### agent_names​ {#agentnames}

    @property  
    def agent_names() -> List[str]  

Return the names of the agents in the group chat.

#### reset​ {#reset}
```python
    def reset()  

Reset the group chat.

#### agent_by_name​ {#agentbyname}
```python
    def agent_by_name(name: str) -> Agent  

Find the next speaker based on the message.

#### next_agent​ {#nextagent}
```python
    def next_agent(agent: Agent) -> Agent  

Return the next agent in the list.

#### select_speaker_msg​ {#selectspeakermsg}
```python
    def select_speaker_msg()  

Return the message for selecting the next speaker.

#### select_speaker​ {#selectspeaker}
```python
    def select_speaker(last_speaker: Agent, selector: ConversableAgent)  

Select the next speaker.

## GroupChatManager Objects​ {#groupchatmanager-objects}

    class GroupChatManager(ConversableAgent)  

(In preview) A chat manager agent that can manage a group chat of multiple
agents.

#### run_chat​ {#runchat}
```python
    def run_chat(messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, config: Optional[GroupChat] = None) -> Union[str, Dict, None]  

Run a group chat.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/autogen/agentchat/groupchat.md)

[Previous«
conversable_agent](.docs/reference/autogen/agentchat/conversable_agent)

[Nextuser_proxy_agent
»](.docs/reference/autogen/agentchat/user_proxy_agent)

* GroupChat Objects
* GroupChatManager Objects

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
  * automl
    * automl.contrib
    * automl.nlp
    * automl.spark
      * [metrics](.docs/reference/automl/spark/metrics)
      * [utils](.docs/reference/automl/spark/utils)
    * automl.task
    * automl.time_series
    * [automl](.docs/reference/automl/automl)
    * [data](.docs/reference/automl/data)
    * [ml](.docs/reference/automl/ml)
    * [model](.docs/reference/automl/model)
    * [state](.docs/reference/automl/state)
  * default
  * onlineml
  * tune

On this page

# automl.spark.metrics {#automlsparkmetrics}

#### spark_metric_loss_score​ {#sparkmetriclossscore}
```python
    def spark_metric_loss_score(metric_name: str, y_predict: psSeries, y_true: psSeries, sample_weight: psSeries = None, groups: psSeries = None) -> float  

Compute the loss score of a metric for spark models.

 **Arguments** :

* `metric_name` \- str | the name of the metric.
* `y_predict` \- psSeries | the predicted values.
* `y_true` \- psSeries | the true values.
* `sample_weight` \- psSeries | the sample weights. Default: None.
* `groups` \- psSeries | the group of each row. Default: None.

 **Returns** :

float | the loss score. A lower value indicates a better model.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/automl/spark/metrics.md)

[Previous« utils](.docs/reference/automl/nlp/utils)

[Nextutils »](.docs/reference/automl/spark/utils)

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* [Getting Started](.docs/Getting-Started)
* [Installation](.docs/Installation)
* Use Cases
* Examples
  * [AutoGen - Automated Multi Agent Chat](.docs/Examples/AutoGen-AgentChat)
  * [AutoGen - Tune GPT Models](.docs/Examples/AutoGen-OpenAI)
  * [AutoML - Classification](.docs/Examples/AutoML-Classification)
  * [AutoML - NLP](.docs/Examples/AutoML-NLP)
  * [AutoML - Rank](.docs/Examples/AutoML-Rank)
  * [AutoML - Regression](.docs/Examples/AutoML-Regression)
  * [AutoML - Time Series Forecast](.docs/Examples/AutoML-Time series forecast)
  * [AutoML for LightGBM](.docs/Examples/AutoML-for-LightGBM)
  * [AutoML for XGBoost](.docs/Examples/AutoML-for-XGBoost)
  * [Default - Flamlized Estimator](.docs/Examples/Default-Flamlized)
  * [Integrate - AzureML](.docs/Examples/Integrate - AzureML)
  * [Integrate - Scikit-learn Pipeline](.docs/Examples/Integrate - Scikit-learn Pipeline)
  * [Integrate - Spark](.docs/Examples/Integrate - Spark)
  * [Tune - AzureML pipeline](.docs/Examples/Tune-AzureML-pipeline)
  * [Tune - HuggingFace](.docs/Examples/Tune-HuggingFace)
  * [Tune - Lexicographic Objectives](.docs/Examples/Tune-Lexicographic-objectives)
  * [Tune - PyTorch](.docs/Examples/Tune-PyTorch)
* [Contributing](.docs/Contribute)
* [Research](.docs/Research)

On this page

# Tune - PyTorch {#tune-pytorch}

This example uses flaml to tune a pytorch model on CIFAR10.

## Prepare for tuning​ {#prepare-for-tuning}

### Requirements​ {#requirements}

    pip install torchvision "flaml[blendsearch,ray]"  

Before we are ready for tuning, we first need to define the neural network
that we would like to tune.

### Network Specification​ {#network-specification}

    import torch  
    import torch.nn as nn  
    import torch.nn.functional as F  
    import torch.optim as optim  
    from torch.utils.data import random_split  
    import torchvision  
    import torchvision.transforms as transforms  
      
      
    class Net(nn.Module):  
      
        def __init__(self, l1=120, l2=84):  
            super(Net, self).__init__()  
            self.conv1 = nn.Conv2d(3, 6, 5)  
            self.pool = nn.MaxPool2d(2, 2)  
            self.conv2 = nn.Conv2d(6, 16, 5)  
            self.fc1 = nn.Linear(16 * 5 * 5, l1)  
            self.fc2 = nn.Linear(l1, l2)  
            self.fc3 = nn.Linear(l2, 10)  
      
        def forward(self, x):  
            x = self.pool(F.relu(self.conv1(x)))  
            x = self.pool(F.relu(self.conv2(x)))  
            x = x.view(-1, 16 * 5 * 5)  
            x = F.relu(self.fc1(x))  
            x = F.relu(self.fc2(x))  
            x = self.fc3(x)  
            return x  

### Data​ {#data}
```python
    def load_data(data_dir="data"):  
        transform = transforms.Compose([  
            transforms.ToTensor(),  
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  
        ])  
      
        trainset = torchvision.datasets.CIFAR10(  
            root=data_dir, train=True, download=True, transform=transform)  
      
        testset = torchvision.datasets.CIFAR10(  
            root=data_dir, train=False, download=True, transform=transform)  
      
        return trainset, testset  

### Training​ {#training}

    from ray import tune  
      
    def train_cifar(config, checkpoint_dir=None, data_dir=None):  
        if "l1" not in config:  
            logger.warning(config)  
        net = Net(2**config["l1"], 2**config["l2"])  
      
        device = "cpu"  
        if torch.cuda.is_available():  
            device = "cuda:0"  
            if torch.cuda.device_count() > 1:  
                net = nn.DataParallel(net)  
        net.to(device)  
      
        criterion = nn.CrossEntropyLoss()  
        optimizer = optim.SGD(net.parameters(), lr=config["lr"], momentum=0.9)  
      
        # The `checkpoint_dir` parameter gets passed by Ray Tune when a checkpoint  
        # should be restored.  
        if checkpoint_dir:  
            checkpoint = os.path.join(checkpoint_dir, "checkpoint")  
            model_state, optimizer_state = torch.load(checkpoint)  
            net.load_state_dict(model_state)  
            optimizer.load_state_dict(optimizer_state)  
      
        trainset, testset = load_data(data_dir)  
      
        test_abs = int(len(trainset) * 0.8)  
        train_subset, val_subset = random_split(  
            trainset, [test_abs, len(trainset) - test_abs])  
      
        trainloader = torch.utils.data.DataLoader(  
            train_subset,  
            batch_size=int(2**config["batch_size"]),  
            shuffle=True,  
            num_workers=4)  
        valloader = torch.utils.data.DataLoader(  
            val_subset,  
            batch_size=int(2**config["batch_size"]),  
            shuffle=True,  
            num_workers=4)  
      
        for epoch in range(int(round(config["num_epochs"]))):  # loop over the dataset multiple times  
            running_loss = 0.0  
            epoch_steps = 0  
            for i, data in enumerate(trainloader, 0):  
                # get the inputs; data is a list of [inputs, labels]  
                inputs, labels = data  
                inputs, labels = inputs.to(device), labels.to(device)  
      
                # zero the parameter gradients  
                optimizer.zero_grad()  
      
                # forward + backward + optimize  
                outputs = net(inputs)  
                loss = criterion(outputs, labels)  
                loss.backward()  
                optimizer.step()  
      
                # print statistics  
                running_loss += loss.item()  
                epoch_steps += 1  
                if i % 2000 == 1999:  # print every 2000 mini-batches  
                    print("[%d, %5d] loss: %.3f" % (epoch + 1, i + 1,  
                                                    running_loss / epoch_steps))  
                    running_loss = 0.0  
      
            # Validation loss  
            val_loss = 0.0  
            val_steps = 0  
            total = 0  
            correct = 0  
            for i, data in enumerate(valloader, 0):  
                with torch.no_grad():  
                    inputs, labels = data  
                    inputs, labels = inputs.to(device), labels.to(device)  
      
                    outputs = net(inputs)  
                    _, predicted = torch.max(outputs.data, 1)  
                    total += labels.size(0)  
                    correct += (predicted == labels).sum().item()  
      
                    loss = criterion(outputs, labels)  
                    val_loss += loss.cpu().numpy()  
                    val_steps += 1  
      
            # Here we save a checkpoint. It is automatically registered with  
            # Ray Tune and will potentially be passed as the `checkpoint_dir`  
            # parameter in future iterations.  
            with tune.checkpoint_dir(step=epoch) as checkpoint_dir:  
                path = os.path.join(checkpoint_dir, "checkpoint")  
                torch.save(  
                    (net.state_dict(), optimizer.state_dict()), path)  
      
            tune.report(loss=(val_loss / val_steps), accuracy=correct / total)  
        print("Finished Training")  

### Test Accuracy​ {#test-accuracy}
```python
    def _test_accuracy(net, device="cpu"):  
        trainset, testset = load_data()  
      
        testloader = torch.utils.data.DataLoader(  
            testset, batch_size=4, shuffle=False, num_workers=2)  
      
        correct = 0  
        total = 0  
        with torch.no_grad():  
            for data in testloader:  
                images, labels = data  
                images, labels = images.to(device), labels.to(device)  
                outputs = net(images)  
                _, predicted = torch.max(outputs.data, 1)  
                total += labels.size(0)  
                correct += (predicted == labels).sum().item()  
      
        return correct / total  

## Hyperparameter Optimization​ {#hyperparameter-optimization}

    import numpy as np  
    import flaml  
    import os  
      
    data_dir = os.path.abspath("data")  
    load_data(data_dir)  # Download data for all trials before starting the run  

### Search space​ {#search-space}

    max_num_epoch = 100  
    config = {  
        "l1": tune.randint(2, 9),   # log transformed with base 2  
        "l2": tune.randint(2, 9),   # log transformed with base 2  
        "lr": tune.loguniform(1e-4, 1e-1),  
        "num_epochs": tune.loguniform(1, max_num_epoch),  
        "batch_size": tune.randint(1, 5)    # log transformed with base 2  
    }  

### Budget and resource constraints​ {#budget-and-resource-constraints}

    time_budget_s = 600     # time budget in seconds  
    gpus_per_trial = 0.5    # number of gpus for each trial; 0.5 means two training jobs can share one gpu  
    num_samples = 500       # maximal number of trials  
    np.random.seed(7654321)  

### Launch the tuning​ {#launch-the-tuning}

    import time  
    start_time = time.time()  
    result = flaml.tune.run(  
        tune.with_parameters(train_cifar, data_dir=data_dir),  
        config=config,  
        metric="loss",  
        mode="min",  
        low_cost_partial_config={"num_epochs": 1},  
        max_resource=max_num_epoch,  
        min_resource=1,  
        scheduler="asha",  # Use asha scheduler to perform early stopping based on intermediate results reported  
        resources_per_trial={"cpu": 1, "gpu": gpus_per_trial},  
        local_dir='logs/',  
        num_samples=num_samples,  
        time_budget_s=time_budget_s,  
        use_ray=True)  

### Check the result​ {#check-the-result}

    print(f"#trials={len(result.trials)}")  
    print(f"time={time.time()-start_time}")  
    best_trial = result.get_best_trial("loss", "min", "all")  
    print("Best trial config: {}".format(best_trial.config))  
    print("Best trial final validation loss: {}".format(  
        best_trial.metric_analysis["loss"]["min"]))  
    print("Best trial final validation accuracy: {}".format(  
        best_trial.metric_analysis["accuracy"]["max"]))  
      
    best_trained_model = Net(2**best_trial.config["l1"],  
                             2**best_trial.config["l2"])  
    device = "cpu"  
    if torch.cuda.is_available():  
        device = "cuda:0"  
        if gpus_per_trial > 1:  
            best_trained_model = nn.DataParallel(best_trained_model)  
    best_trained_model.to(device)  
      
    checkpoint_value = getattr(best_trial.checkpoint, "dir_or_data", None) or best_trial.checkpoint.value  
    checkpoint_path = os.path.join(checkpoint_value, "checkpoint")  
      
    model_state, optimizer_state = torch.load(checkpoint_path)  
    best_trained_model.load_state_dict(model_state)  
      
    test_acc = _test_accuracy(best_trained_model, device)  
    print("Best trial test set accuracy: {}".format(test_acc))  

### Sample of output​ {#sample-of-output}

    #trials=44  
    time=1193.913584947586  
    Best trial config: {'l1': 8, 'l2': 8, 'lr': 0.0008818671030627281, 'num_epochs': 55.9513429004283, 'batch_size': 3}  
    Best trial final validation loss: 1.0694482081472874  
    Best trial final validation accuracy: 0.6389  
    Files already downloaded and verified  
    Files already downloaded and verified  
    Best trial test set accuracy: 0.6294  

[Link to
notebook](https://github.com/microsoft.blob/main/notebook/tune_pytorch.ipynb)
| [Open in
colab](https://colab.research.google.com/github/microsoft.blob/main/notebook/tune_pytorch.ipynb)

[Edit this
page](<https://github.com/microsoft.edit/main/website/docs/Examples/Tune->
PyTorch.md)

[Previous« Tune - Lexicographic Objectives](.docs/Examples/Tune-
Lexicographic-objectives)

[NextContributing »](.docs/Contribute)

* Prepare for tuning
  * Requirements
  * Network Specification
  * Data
  * Training
  * Test Accuracy
* Hyperparameter Optimization
  * Search space
  * Budget and resource constraints
  * Launch the tuning
  * Check the result
  * Sample of output

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
  * automl
  * default
  * onlineml
  * tune
    * tune.scheduler
    * tune.searcher
    * tune.spark
    * [analysis](.docs/reference/tune/analysis)
    * [sample](.docs/reference/tune/sample)
    * [space](.docs/reference/tune/space)
    * [trial](.docs/reference/tune/trial)
    * [trial_runner](.docs/reference/tune/trial_runner)
    * [tune](.docs/reference/tune/tune)
    * [utils](.docs/reference/tune/utils)

On this page

# tune.analysis {#tuneanalysis}

## ExperimentAnalysis Objects​ {#experimentanalysis-objects}

    class ExperimentAnalysis()  

Analyze results from a Tune experiment.

#### best_trial​ {#besttrial}

    @property  
    def best_trial() -> Trial  

Get the best trial of the experiment The best trial is determined by comparing
the last trial results using the `metric` and `mode` parameters passed to
`tune.run()`. If you didn't pass these parameters, use `get_best_trial(metric,
mode, scope)` instead.

#### best_config​ {#bestconfig}

    @property  
    def best_config() -> Dict  

Get the config of the best trial of the experiment The best trial is
determined by comparing the last trial results using the `metric` and `mode`
parameters passed to `tune.run()`. If you didn't pass these parameters, use
`get_best_config(metric, mode, scope)` instead.

#### results​ {#results}

    @property  
    def results() -> Dict[str, Dict]  

Get the last result of all the trials of the experiment

#### get_best_trial​ {#getbesttrial}
```python
    def get_best_trial(metric: Optional[str] = None, mode: Optional[str] = None, scope: str = "last", filter_nan_and_inf: bool = True) -> Optional[Trial]  

Retrieve the best trial object. Compares all trials' scores on `metric`. If
`metric` is not specified, `self.default_metric` will be used. If `mode` is
not specified, `self.default_mode` will be used. These values are usually
initialized by passing the `metric` and `mode` parameters to `tune.run()`.

 **Arguments** :

* `metric` _str_ \- Key for trial info to order on. Defaults to `self.default_metric`.
* `mode` _str_ \- One of [min, max]. Defaults to `self.default_mode`.
* `scope` _str_ \- One of [all, last, avg, last-5-avg, last-10-avg]. If `scope=last`, only look at each trial's final step for `metric`, and compare across trials based on `mode=[min,max]`. If `scope=avg`, consider the simple average over all steps for `metric` and compare across trials based on `mode=[min,max]`. If `scope=last-5-avg` or `scope=last-10-avg`, consider the simple average over the last 5 or 10 steps for `metric` and compare across trials based on `mode=[min,max]`. If `scope=all`, find each trial's min/max score for `metric` based on `mode`, and compare trials based on `mode=[min,max]`.
* `filter_nan_and_inf` _bool_ \- If True (default), NaN or infinite values are disregarded and these trials are never selected as the best trial.

#### get_best_config​ {#getbestconfig}
```python
    def get_best_config(metric: Optional[str] = None, mode: Optional[str] = None, scope: str = "last") -> Optional[Dict]  

Retrieve the best config corresponding to the trial. Compares all trials'
scores on `metric`. If `metric` is not specified, `self.default_metric` will
be used. If `mode` is not specified, `self.default_mode` will be used. These
values are usually initialized by passing the `metric` and `mode` parameters
to `tune.run()`.

 **Arguments** :

* `metric` _str_ \- Key for trial info to order on. Defaults to `self.default_metric`.
* `mode` _str_ \- One of [min, max]. Defaults to `self.default_mode`.
* `scope` _str_ \- One of [all, last, avg, last-5-avg, last-10-avg]. If `scope=last`, only look at each trial's final step for `metric`, and compare across trials based on `mode=[min,max]`. If `scope=avg`, consider the simple average over all steps for `metric` and compare across trials based on `mode=[min,max]`. If `scope=last-5-avg` or `scope=last-10-avg`, consider the simple average over the last 5 or 10 steps for `metric` and compare across trials based on `mode=[min,max]`. If `scope=all`, find each trial's min/max score for `metric` based on `mode`, and compare trials based on `mode=[min,max]`.

#### best_result​ {#bestresult}

    @property  
    def best_result() -> Dict  

Get the last result of the best trial of the experiment The best trial is
determined by comparing the last trial results using the `metric` and `mode`
parameters passed to `tune.run()`. If you didn't pass these parameters, use
`get_best_trial(metric, mode, scope).last_result` instead.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/tune/analysis.md)

[Previous« utils](.docs/reference/tune/spark/utils)

[Nextsample »](.docs/reference/tune/sample)

* ExperimentAnalysis Objects

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
  * automl
    * automl.contrib
    * automl.nlp
    * automl.spark
    * automl.task
    * automl.time_series
    * [automl](.docs/reference/automl/automl)
    * [data](.docs/reference/automl/data)
    * [ml](.docs/reference/automl/ml)
    * [model](.docs/reference/automl/model)
    * [state](.docs/reference/automl/state)
  * default
  * onlineml
  * tune

On this page

# automl.data {#automldata}

#### load_openml_dataset​ {#loadopenmldataset}
```python
    def load_openml_dataset(dataset_id, data_dir=None, random_state=0, dataset_format="dataframe")  

Load dataset from open ML.

If the file is not cached locally, download it from open ML.

 **Arguments** :

* `dataset_id` \- An integer of the dataset id in openml.
* `data_dir` \- A string of the path to store and load the data.
* `random_state` \- An integer of the random seed for splitting data.
* `dataset_format` \- A string specifying the format of returned dataset. Default is 'dataframe'. Can choose from ['dataframe', 'array']. If 'dataframe', the returned dataset will be a Pandas DataFrame. If 'array', the returned dataset will be a NumPy array or a SciPy sparse matrix.

 **Returns** :

* `X_train` \- Training data.
* `X_test` \- Test data.
* `y_train` \- A series or array of labels for training data.
* `y_test` \- A series or array of labels for test data.

#### load_openml_task​ {#loadopenmltask}
```python
    def load_openml_task(task_id, data_dir)  

Load task from open ML.

Use the first fold of the task. If the file is not cached locally, download it
from open ML.

 **Arguments** :

* `task_id` \- An integer of the task id in openml.
* `data_dir` \- A string of the path to store and load the data.

 **Returns** :

* `X_train` \- A dataframe of training data.
* `X_test` \- A dataframe of test data.
* `y_train` \- A series of labels for training data.
* `y_test` \- A series of labels for test data.

#### get_output_from_log​ {#getoutputfromlog}

    def get_output_from_log(filename, time_budget)  

Get output from log file.

 **Arguments** :

* `filename` \- A string of the log file name.
* `time_budget` \- A float of the time budget in seconds.

 **Returns** :

* `search_time_list` \- A list of the finished time of each logged iter.
* `best_error_list` \- A list of the best validation error after each logged iter.
* `error_list` \- A list of the validation error of each logged iter.
* `config_list` \- A list of the estimator, sample size and config of each logged iter.
* `logged_metric_list` \- A list of the logged metric of each logged iter.

#### concat​ {#concat}
```python
    def concat(X1, X2)  

concatenate two matrices vertically.

## DataTransformer Objects​ {#datatransformer-objects}

    class DataTransformer()  

Transform input training data.

#### fit_transform​ {#fittransform}
```python
    def fit_transform(X: Union[DataFrame, np.ndarray], y, task: Union[str, "Task"])  

Fit transformer and process the input training data according to the task
type.

 **Arguments** :

* `X` \- A numpy array or a pandas dataframe of training data.
* `y` \- A numpy array or a pandas series of labels.
* `task` \- An instance of type Task, or a str such as 'classification', 'regression'.

 **Returns** :

* `X` \- Processed numpy array or pandas dataframe of training data.
* `y` \- Processed numpy array or pandas series of labels.

#### transform​ {#transform}
```python
    def transform(X: Union[DataFrame, np.array])  

Process data using fit transformer.

 **Arguments** :

* `X` \- A numpy array or a pandas dataframe of training data.

 **Returns** :

* `X` \- Processed numpy array or pandas dataframe of training data.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/automl/data.md)

[Previous« automl](.docs/reference/automl/automl)

[Nextml »](.docs/reference/automl/ml)

* DataTransformer Objects

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

Recent posts

* [All About Agent Descriptions](/autogen/blog/2023/12/29/AgentDescriptions)
* [AgentOptimizer - An Agentic Way to Train Your LLM Agent](/autogen/blog/2023/12/23/AgentOptimizer)
* [AutoGen Studio: Interactively Explore Multi-Agent Workflows](/autogen/blog/2023/12/01/AutoGenStudio)
* [Agent AutoBuild - Automatically Building Multi-agent Systems](/autogen/blog/2023/11/26/Agent-AutoBuild)
* [How to Assess Utility of LLM-powered Applications?](/autogen/blog/2023/11/20/AgentEval)
* [AutoGen Meets GPTs](/autogen/blog/2023/11/13/OAI-assistants)
* [EcoAssistant - Using LLM Assistants More Accurately and Affordably](/autogen/blog/2023/11/09/EcoAssistant)
* [Multimodal with GPT-4V and LLaVA](/autogen/blog/2023/11/06/LMM-Agent)
* [AutoGen's Teachable Agents](/autogen/blog/2023/10/26/TeachableAgent)
* [Retrieval-Augmented Generation (RAG) Applications with AutoGen](/autogen/blog/2023/10/18/RetrieveChat)
* [Use AutoGen for Local LLMs](/autogen/blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](/autogen/blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](/autogen/blog/2023/05/18/GPT-adaptive-humaneval)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](/autogen/blog/2023/04/21/LLM-tuning-math)

# AutoGen Studio: Interactively Explore Multi-Agent Workflows {#autogen-studio-interactively-explore-multiagent-workflows}

December 1, 2023 · 8 min read

[![Victor
Dibia](https://github.com/victordibia.png)](https://github.com/victordibia)

[Victor Dibia](https://github.com/victordibia)

Principal RSDE at Microsoft Research

[![Gagan
Bansal](https://github.com/gagb.png)](<https://www.linkedin.com/in/gagan->
bansal/)

[Gagan Bansal](https://www.linkedin.com/in/gagan-bansal/)

Senior Researcher at Microsoft Research

[![Saleema
Amershi](https://github.com/samershi.png)](https://github.com/samershi)

[Saleema Amershi](https://github.com/samershi)

Senior Principal Research Manager at Microsoft Research

![AutoGen Studio Playground View: Solving a task with multiple agents that
generate a pdf document with
images.](/autogen/assets/images/autogenstudio_home-
cce78dc150d1bb0073620754df73d863.png)

 _AutoGen Studio: Solving a task with multiple agents that generate a pdf
document with images._

## TLDR​ {#tldr}

To help you rapidly prototype multi-agent solutions for your tasks, we are
introducing AutoGen Studio, an interface powered by
[AutoGen](https://github.com/microsoft/autogen/tree/main/autogen). It allows
you to:

* Declaratively define and modify agents and multi-agent workflows through a point and click, drag and drop interface (e.g., you can select the parameters of two agents that will communicate to solve your task).
* Use our UI to create chat sessions with the specified agents and view results (e.g., view chat history, generated files, and time taken).
* Explicitly add skills to your agents and accomplish more tasks.
* Publish your sessions to a local gallery.

AutoGen Studio is open source [code
here](<https://github.com/microsoft/autogen/tree/main/samples/apps/autogen->
studio), and can be installed via pip. Give it a try!

    pip install autogenstudio  

## Introduction​ {#introduction}

The accelerating pace of technology has ushered us into an era where digital
assistants (or agents) are becoming integral to our lives.
[AutoGen](https://github.com/microsoft/autogen/tree/main/autogen) has emerged
as a leading framework for orchestrating the power of agents. In the spirit of
expanding this frontier and democratizing this capability, we are thrilled to
introduce a new user-friendly interface: **AutoGen Studio**.

With AutoGen Studio, users can rapidly create, manage, and interact with
agents that can learn, adapt, and collaborate. As we release this interface
into the open-source community, our ambition is not only to enhance
productivity but to inspire a level of personalized interaction between humans
and agents.

> **Note** : AutoGen Studio is meant to help you rapidly prototype multi-
> agent workflows and demonstrate an example of end user interfaces built with
> AutoGen. It is not meant to be a production-ready app.

## Getting Started with AutoGen Studio​ {#getting-started-with-autogen-studio}

The following guide will help you get AutoGen Studio up and running on your
system.

### Configuring an LLM Provider​ {#configuring-an-llm-provider}

To get started, you need access to a language model. You can get this set up
by following the steps in the AutoGen documentation
[here](https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints).
Configure your environment with either `OPENAI_API_KEY` or
`AZURE_OPENAI_API_KEY`.

For example, in your terminal, you would set the API key like this:

    export OPENAI_API_KEY=<your_api_key>  

You can also specify the model directly in the agent's configuration as shown
below.

    llm_config = LLMConfig(  
        config_list=[{  
            "model": "gpt-4",  
            "api_key": "<azure_api_key>",  
            "base_url": "<azure api base url>",  
            "api_type": "azure",  
            "api_version": "2023-06-01-preview"  
        }],  
        temperature=0,  
    )  

### Installation​ {#installation}

  1. **Install from PyPi**

We recommend using a virtual environment (e.g., conda) to avoid conflicts with
existing Python packages. With Python 3.10 or newer active in your virtual
environment, use pip to install AutoGen Studio:

        pip install autogenstudio  

  2. **Install from Source**

> Note: This approach requires some familiarity with building interfaces in
> React.

If you prefer to install from source, ensure you have Python 3.10+ and Node.js
(version above 14.15.0) installed. Here's how you get started:

    * Clone the AutoGen Studio repository and install its Python dependencies:
        
                pip install -e .  

    * Navigate to the `samples/apps/autogen-studio/frontend` directory, install dependencies, and build the UI:
        
                npm install -g gatsby-cli  
        npm install --global yarn  
        cd frontend  
        yarn install  
        yarn build  

For Windows users, to build the frontend, you may need alternative commands
provided in the [autogen studio
readme](<https://github.com/microsoft/autogen/tree/main/samples/apps/autogen->
studio).

### Running the Application​ {#running-the-application}

Once installed, run the web UI by entering the following in your terminal:

    autogenstudio ui --port 8081  

This will start the application on the specified port. Open your web browser
and go to `http://localhost:8081/` to begin using AutoGen Studio.

Now that you have AutoGen Studio installed and running, you are ready to
explore its capabilities, including defining and modifying agent workflows,
interacting with agents and sessions, and expanding agent skills.

## What Can You Do with AutoGen Studio?​ {#what-can-you-do-with-autogen-studio}

The AutoGen Studio UI is organized into 3 high level sections - **Build** ,
**Playground** , and **Gallery**.

### Build​ {#build}

![Specify
Agents.](/autogen/assets/images/autogenstudio_config-5e0107b565503d4952c9b00dd3d5c10e.png)

This section focuses on defining the properties of agents and agent workflows.
It includes the following concepts:

 **Skills** : Skills are functions (e.g., Python functions) that describe how
to solve a task. In general, a good skill has a descriptive name (e.g.
`generate_images`), extensive docstrings and good defaults (e.g., writing out
files to disk for persistence and reuse). You can add new skills to AutoGen
Studio via the provided UI. At inference time, these skills are made available
to the assistant agent as they address your tasks.

![View and add
skills.](/autogen/assets/images/autogenstudio_skills-7376f0a5aec5917fda817a4793f2b89a.png)

 _AutoGen Studio Build View: View, add or edit skills that an agent can
leverage in addressing tasks._

 **Agents** : This provides an interface to declaratively specify properties
for an AutoGen agent (mirrors most of the members of a base [AutoGen
conversable
agent](https://github.com/microsoft/autogen/blob/main/autogen/agentchat/conversable_agent.py)
class).

 **Agent Workflows** : An agent workflow is a specification of a set of agents
that can work together to accomplish a task. The simplest version of this is a
setup with two agents – a user proxy agent (that represents a user i.e. it
compiles code and prints result) and an assistant that can address task
requests (e.g., generating plans, writing code, evaluating responses,
proposing error recovery steps, etc.). A more complex flow could be a group
chat where even more agents work towards a solution.

### Playground​ {#playground}

![AutoGen Studio Playground View: Solving a task with multiple agents that
generate a pdf document with
images.](/autogen/assets/images/autogenstudio_home-
cce78dc150d1bb0073620754df73d863.png)

 _AutoGen Studio Playground View: Agents collaborate, use available skills
(ability to generate images) to address a user task (generate pdf 's)._

The playground section is focused on interacting with agent workflows defined
in the previous build section. It includes the following concepts:

 **Session** : A session refers to a period of continuous interaction or
engagement with an agent workflow, typically characterized by a sequence of
activities or operations aimed at achieving specific objectives. It includes
the agent workflow configuration, the interactions between the user and the
agents. A session can be “published” to a “gallery”.

 **Chat View** : A chat is a sequence of interactions between a user and an
agent. It is a part of a session.

### Gallery​ {#gallery}

This section is focused on sharing and reusing artifacts (e.g., workflow
configurations, sessions, etc.).

AutoGen Studio comes with 3 example skills: `fetch_profile`, `find_papers`,
`generate_images`. Please feel free to review the repo to learn more about how
they work.

## The AutoGen Studio API​ {#the-autogen-studio-api}

While AutoGen Studio is a web interface, it is powered by an underlying python
API that is reusable and modular. Importantly, we have implemented an API
where agent workflows can be declaratively specified (in JSON), loaded and
run. An example of the current API is shown below. Please consult the [AutoGen
Studio repo](https://microsoft.github.io/autogen/docs/autogenstudio) for more
details.

    import json  
    from autogenstudio import AutoGenWorkFlowManager, AgentWorkFlowConfig  
      
    # load an agent specification in JSON  
    agent_spec = json.load(open('agent_spec.json'))  
      
    # Create an AutoGen Workflow Configuration from the agent specification  
    agent_work_flow_config = FlowConfig(**agent_spec)  
      
    # Create a Workflow from the configuration  
    agent_work_flow = AutoGenWorkFlowManager(agent_work_flow_config)  
      
    # Run the workflow on a task  
    task_query = "What is the height of the Eiffel Tower?"  
    agent_work_flow.run(message=task_query)  

## Road Map and Next Steps​ {#road-map-and-next-steps}

As we continue to develop and refine AutoGen Studio, the road map below
outlines an array of enhancements and new features planned for future
releases. Here's what users can look forward to:

* **Complex Agent Workflows** : We're working on integrating support for more sophisticated agent workflows, such as `GroupChat`, allowing for richer interaction between multiple agents or dynamic topologies.
* **Improved User Experience** : This includes features like streaming intermediate model output for real-time feedback, better summarization of agent responses, information on costs of each interaction. We will also invest in improving the workflow for composing and reusing agents. We will also explore support for more interactive human in the loop feedback to agents.
* **Expansion of Agent Skills** : We will work towards improving the workflow for authoring, composing and reusing agent skills.
* **Community Features** : Facilitation of sharing and collaboration within AutoGen Studio user community is a key goal. We're exploring options for sharing sessions and results more easily among users and contributing to a shared repository of skills, agents, and agent workflows.

### FAQ​ {#faq}

 **Q: Where can I adjust the default skills, agent and workflow
configurations?** A: You can modify agent configurations directly from the UI
or by editing the `autogentstudio/utils/dbdefaults.json` file which is used to
initialize the database.

 **Q: If I want to reset the entire conversation with an agent, how do I go
about it?** A: To reset your conversation history, you can delete the
`database.sqlite` file. If you need to clear user-specific data, remove the
relevant `autogenstudio/web/files/user/<user_id_md5hash>` folder.

 **Q: Is it possible to view the output and messages generated by the agents
during interactions?** A: Yes, you can view the generated messages in the
debug console of the web UI, providing insights into the agent interactions.
Alternatively, you can inspect the `database.sqlite` file for a comprehensive
record of messages.

 **Q: Where can I find documentation and support for AutoGen Studio?** A: We
are constantly working to improve AutoGen Studio. For the latest updates,
please refer to the [AutoGen Studio
Readme](<https://github.com/microsoft/autogen/tree/main/samples/apps/autogen->
studio). For additional support, please open an issue on
[GitHub](https://github.com/microsoft/autogen) or ask questions on
[Discord](https://discord.gg/pAbnFJrkgZ).

 **Tags:**

* [AutoGen](/autogen/blog/tags/auto-gen)
* [UI](/autogen/blog/tags/ui)
* [web](/autogen/blog/tags/web)
* [UX](/autogen/blog/tags/ux)

[Newer Post« AgentOptimizer - An Agentic Way to Train Your LLM
Agent](/autogen/blog/2023/12/23/AgentOptimizer)

[Older PostAgent AutoBuild - Automatically Building Multi-agent Systems
»](/autogen/blog/2023/11/26/Agent-AutoBuild)

* TLDR
* Introduction
* Getting Started with AutoGen Studio
  * Configuring an LLM Provider
  * Installation
  * Running the Application
* What Can You Do with AutoGen Studio?
  * Build
  * Playground
  * Gallery
* The AutoGen Studio API
* Road Map and Next Steps
  * FAQ

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
  * automl
    * automl.contrib
      * [histgb](.docs/reference/automl/contrib/histgb)
    * automl.nlp
    * automl.spark
    * automl.task
    * automl.time_series
    * [automl](.docs/reference/automl/automl)
    * [data](.docs/reference/automl/data)
    * [ml](.docs/reference/automl/ml)
    * [model](.docs/reference/automl/model)
    * [state](.docs/reference/automl/state)
  * default
  * onlineml
  * tune

On this page

# automl.contrib.histgb {#automlcontribhistgb}

## HistGradientBoostingEstimator Objects​ {#histgradientboostingestimator-objects}

    class HistGradientBoostingEstimator(SKLearnEstimator)  

The class for tuning Histogram Gradient Boosting.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/automl/contrib/histgb.md)

[Previous« retrieve_utils](.docs/reference/autogen/retrieve_utils)

[Nexttrainer »](.docs/reference/automl/nlp/huggingface/trainer)

* HistGradientBoostingEstimator Objects

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
    * autogen.agentchat
      * [agent](.docs/reference/autogen/agentchat/agent)
      * [assistant_agent](.docs/reference/autogen/agentchat/assistant_agent)
      * [conversable_agent](.docs/reference/autogen/agentchat/conversable_agent)
      * [groupchat](.docs/reference/autogen/agentchat/groupchat)
      * [user_proxy_agent](.docs/reference/autogen/agentchat/user_proxy_agent)
    * autogen.oai
    * [code_utils](.docs/reference/autogen/code_utils)
    * [math_utils](.docs/reference/autogen/math_utils)
    * [retrieve_utils](.docs/reference/autogen/retrieve_utils)
  * automl
  * default
  * onlineml
  * tune

On this page

# autogen.agentchat.user_proxy_agent {#autogenagentchatuserproxyagent}

## UserProxyAgent Objects​ {#userproxyagent-objects}

    class UserProxyAgent(ConversableAgent)  

(In preview) A proxy agent for the user, that can execute code and provide
feedback to the other agents.

UserProxyAgent is a subclass of ConversableAgent configured with
`human_input_mode` to ALWAYS and `llm_config` to False. By default, the agent
will prompt for human input every time a message is received. Code execution
is enabled by default. LLM-based auto reply is disabled by default. To modify
auto reply, register a method with
[`register_reply`](conversable_agent#register_reply). To modify the way to get
human input, override `get_human_input` method. To modify the way to execute
code blocks, single code block, or function call, override
`execute_code_blocks`, `run_code`, and `execute_function` methods
respectively. To customize the initial message when a conversation starts,
override `generate_init_message` method.

#### __init__​ {#init}
```python
    def __init__(name: str, is_termination_msg: Optional[Callable[[Dict], bool]] = None, max_consecutive_auto_reply: Optional[int] = None, human_input_mode: Optional[str] = "ALWAYS", function_map: Optional[Dict[str, Callable]] = None, code_execution_config: Optional[Union[Dict, bool]] = None, default_auto_reply: Optional[Union[str, Dict, None]] = "", llm_config: Optional[Union[Dict, bool]] = False, system_message: Optional[str] = "")  

 **Arguments** :

* `name` _str_ \- name of the agent.
* `is_termination_msg` _function_ \- a function that takes a message in the form of a dictionary and returns a boolean value indicating if this received message is a termination message. The dict can contain the following keys: "content", "role", "name", "function_call".
* `max_consecutive_auto_reply` _int_ \- the maximum number of consecutive auto replies. default to None (no limit provided, class attribute MAX_CONSECUTIVE_AUTO_REPLY will be used as the limit in this case). The limit only plays a role when human_input_mode is not "ALWAYS".
* `human_input_mode` _str_ \- whether to ask for human inputs every time a message is received. Possible values are "ALWAYS", "TERMINATE", "NEVER". (1) When "ALWAYS", the agent prompts for human input every time a message is received. Under this mode, the conversation stops when the human input is "exit", or when is_termination_msg is True and there is no human input. (2) When "TERMINATE", the agent only prompts for human input only when a termination message is received or the number of auto reply reaches the max_consecutive_auto_reply. (3) When "NEVER", the agent will never prompt for human input. Under this mode, the conversation stops when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.
* `function_map` _dict [str, callable]_ \- Mapping function names (passed to openai) to callable functions.
* `code_execution_config` _dict or False_ \- config for the code execution. To disable code execution, set to False. Otherwise, set to a dictionary with the following keys:
  * work_dir (Optional, str): The working directory for the code execution. If None, a default working directory will be used. The default working directory is the "extensions" directory under "path_to_flaml/autogen".
  * use_docker (Optional, list, str or bool): The docker image to use for code execution. If a list or a str of image name(s) is provided, the code will be executed in a docker container with the first image successfully pulled. If None, False or empty, the code will be executed in the current environment. Default is True, which will be converted into a list. If the code is executed in the current environment, the code must be trusted.
  * timeout (Optional, int): The maximum execution time in seconds.
  * last_n_messages (Experimental, Optional, int): The number of messages to look back for code execution. Default to 1.
* `default_auto_reply` _str or dict or None_ \- the default auto reply message when no code execution or llm based reply is generated.
* `llm_config` _dict or False_ \- llm inference configuration. Please refer to [autogen.Completion.create](.docs/reference/autogen/oai/completion#create) for available options. Default to false, which disables llm-based auto reply.
* `system_message` _str_ \- system message for ChatCompletion inference. Only used when llm_config is not False. Use it to reprogram the agent.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/autogen/agentchat/user_proxy_agent.md)

[Previous« groupchat](.docs/reference/autogen/agentchat/groupchat)

[Nextcompletion »](.docs/reference/autogen/oai/completion)

* UserProxyAgent Objects

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* [Getting Started](.docs/Getting-Started)
* [Installation](.docs/Installation)
* Use Cases
  * [AutoGen for Large Language Models](.docs/Use-Cases/Autogen)
  * [Task Oriented AutoML](.docs/Use-Cases/Task-Oriented-AutoML)
  * [Tune User Defined Function](.docs/Use-Cases/Tune-User-Defined-Function)
  * [Zero Shot AutoML](.docs/Use-Cases/Zero-Shot-AutoML)
* Examples
* [Contributing](.docs/Contribute)
* [Research](.docs/Research)

# AutoGen for Large Language Models {#autogen-for-large-language-models}

Please refer to <https://microsoft.github.io/autogen/>.

[Edit this
page](<https://github.com/microsoft.edit/main/website/docs/Use->
Cases/Autogen.md)

[Previous« Installation](.docs/Installation)

[NextTask Oriented AutoML »](.docs/Use-Cases/Task-Oriented-AutoML)

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
  * automl
    * automl.contrib
    * automl.nlp
    * automl.spark
    * automl.task
    * automl.time_series
    * [automl](.docs/reference/automl/automl)
    * [data](.docs/reference/automl/data)
    * [ml](.docs/reference/automl/ml)
    * [model](.docs/reference/automl/model)
    * [state](.docs/reference/automl/state)
  * default
  * onlineml
  * tune

On this page

# automl.model {#automlmodel}

## BaseEstimator Objects​ {#baseestimator-objects}

    class BaseEstimator()  

The abstract class for all learners.

Typical examples:

* XGBoostEstimator: for regression.
* XGBoostSklearnEstimator: for classification.
* LGBMEstimator, RandomForestEstimator, LRL1Classifier, LRL2Classifier: for both regression and classification.

#### __init__​ {#init}
```python
    def __init__(task="binary", **config)  

Constructor.

 **Arguments** :

* `task` \- A string of the task type, one of 'binary', 'multiclass', 'regression', 'rank', 'seq-classification', 'seq-regression', 'token-classification', 'multichoice-classification', 'summarization', 'ts_forecast', 'ts_forecast_classification'.
* `config` \- A dictionary containing the hyperparameter names, 'n_jobs' as keys. n_jobs is the number of parallel threads.

#### model​ {#model}

    @property  
    def model()  

Trained model after fit() is called, or None before fit() is called.

#### estimator​ {#estimator}

    @property  
    def estimator()  

Trained model after fit() is called, or None before fit() is called.

#### feature_names_in_​ {#featurenamesin}

    @property  
    def feature_names_in_()  

if self. _model has attribute feature_names_in_ , return it. otherwise, if
self. _model has attribute feature_name_ , return it. otherwise, if
self._model has attribute feature_names, return it. otherwise, if self._model
has method get_booster, return the feature names. otherwise, return None.

#### feature_importances_​ {#featureimportances}

    @property  
    def feature_importances_()  

if self. _model has attribute feature_importances_ , return it. otherwise, if
self. _model has attribute coef_ , return it. otherwise, return None.

#### fit​ {#fit} {#fit}
```python
    def fit(X_train, y_train, budget=None, free_mem_ratio=0, **kwargs)  

Train the model from given training data.

 **Arguments** :

* `X_train` \- A numpy array or a dataframe of training data in shape n*m.
* `y_train` \- A numpy array or a series of labels in shape n*1.
* `budget` \- A float of the time budget in seconds.
* `free_mem_ratio` \- A float between 0 and 1 for the free memory ratio to keep during training.

 **Returns** :

* `train_time` \- A float of the training time in seconds.

#### predict​ {#predict} {#predict}
```python
    def predict(X, **kwargs)  

Predict label from features.

 **Arguments** :

* `X` \- A numpy array or a dataframe of featurized instances, shape n*m.

 **Returns** :

A numpy array of shape n*1. Each element is the label for a instance.

#### predict_proba​ {#predictproba} {#predictproba}
```python
    def predict_proba(X, **kwargs)  

Predict the probability of each class from features.

Only works for classification problems

 **Arguments** :

* `X` \- A numpy array of featurized instances, shape n*m.

 **Returns** :

A numpy array of shape n*c. c is the # classes. Each element at (i,j) is the
probability for instance i to be in class j.

#### score​ {#score}
```python
    def score(X_val: DataFrame, y_val: Series, **kwargs)  

Report the evaluation score of a trained estimator.

 **Arguments** :

* `X_val` \- A pandas dataframe of the validation input data.
* `y_val` \- A pandas series of the validation label.
* `kwargs` \- keyword argument of the evaluation function, for example:
  * metric: A string of the metric name or a function e.g., 'accuracy', 'roc_auc', 'roc_auc_ovr', 'roc_auc_ovo', 'f1', 'micro_f1', 'macro_f1', 'log_loss', 'mae', 'mse', 'r2', 'mape'. Default is 'auto'. If metric is given, the score will report the user specified metric. If metric is not given, the metric is set to accuracy for classification and r2 for regression. You can also pass a customized metric function, for examples on how to pass a customized metric function, please check [test/nlp/test_autohf_custom_metric.py](https://github.com/microsoft.blob/main/test/nlp/test_autohf_custom_metric.py) and [test/automl/test_multiclass.py](https://github.com/microsoft.blob/main/test/automl/test_multiclass.py).

 **Returns** :

The evaluation score on the validation dataset.

#### search_space​ {#searchspace}

    @classmethod  
    def search_space(cls, data_size, task, **params)  

[required method] search space.

 **Arguments** :

* `data_size` \- A tuple of two integers, number of rows and columns.
* `task` \- A str of the task type, e.g., "binary", "multiclass", "regression".

 **Returns** :

A dictionary of the search space. Each key is the name of a hyperparameter,
and value is a dict with its domain (required) and low_cost_init_value,
init_value, cat_hp_cost (if applicable). e.g., `{'domain':
tune.randint(lower=1, upper=10), 'init_value': 1}`.

#### size​ {#size}

    @classmethod  
    def size(cls, config: dict) -> float  

[optional method] memory size of the estimator in bytes.

 **Arguments** :

* `config` \- A dict of the hyperparameter config.

 **Returns** :

A float of the memory size required by the estimator to train the given
config.

#### cost_relative2lgbm​ {#costrelative2lgbm}

    @classmethod  
    def cost_relative2lgbm(cls) -> float  

[optional method] relative cost compared to lightgbm.

#### init​ {#init}

    @classmethod  
    def init(cls)  

[optional method] initialize the class.

#### config2params​ {#config2params}
```python
    def config2params(config: dict) -> dict  

[optional method] config dict to params dict

 **Arguments** :

* `config` \- A dict of the hyperparameter config.

 **Returns** :

A dict that will be passed to self.estimator_class's constructor.

## SparkEstimator Objects​ {#sparkestimator-objects}

    class SparkEstimator(BaseEstimator)  

The base class for fine-tuning spark models, using pyspark.ml and SynapseML
API.

#### fit​ {#fit} {#fit}
```python
    def fit(X_train: psDataFrame, y_train: psSeries = None, budget=None, free_mem_ratio=0, index_col: str = "tmp_index_col", **kwargs, ,)  

Train the model from given training data.

 **Arguments** :

* `X_train` \- A pyspark.pandas DataFrame of training data in shape n*m.
* `y_train` \- A pyspark.pandas Series in shape n*1. None if X_train is a pyspark.pandas Dataframe contains y_train.
* `budget` \- A float of the time budget in seconds.
* `free_mem_ratio` \- A float between 0 and 1 for the free memory ratio to keep during training.

 **Returns** :

* `train_time` \- A float of the training time in seconds.

#### predict​ {#predict} {#predict}
```python
    def predict(X, index_col="tmp_index_col", return_all=False, **kwargs)  

Predict label from features.

 **Arguments** :

* `X` \- A pyspark or pyspark.pandas dataframe of featurized instances, shape n*m.
* `index_col` \- A str of the index column name. Default to "tmp_index_col".
* `return_all` \- A bool of whether to return all the prediction results. Default to False.

 **Returns** :

A pyspark.pandas series of shape n*1 if return_all is False. Otherwise, a
pyspark.pandas dataframe.

#### predict_proba​ {#predictproba} {#predictproba}
```python
    def predict_proba(X, index_col="tmp_index_col", return_all=False, **kwargs)  

Predict the probability of each class from features. Only works for
classification problems

 **Arguments** :

* `X` \- A pyspark or pyspark.pandas dataframe of featurized instances, shape n*m.
* `index_col` \- A str of the index column name. Default to "tmp_index_col".
* `return_all` \- A bool of whether to return all the prediction results. Default to False.

 **Returns** :

A pyspark.pandas dataframe of shape n*c. c is the # classes. Each element at
(i,j) is the probability for instance i to be in class j.

## SparkLGBMEstimator Objects​ {#sparklgbmestimator-objects}

    class SparkLGBMEstimator(SparkEstimator)  

The class for fine-tuning spark version lightgbm models, using SynapseML API.

## TransformersEstimator Objects​ {#transformersestimator-objects}

    class TransformersEstimator(BaseEstimator)  

The class for fine-tuning language models, using huggingface transformers API.

## SKLearnEstimator Objects​ {#sklearnestimator-objects}

    class SKLearnEstimator(BaseEstimator)  

The base class for tuning scikit-learn estimators.

Subclasses can modify the function signature of `__init__` to ignore the
values in `config` that are not relevant to the constructor of their
underlying estimator. For example, some regressors in `scikit-learn` don't
accept the `n_jobs` parameter contained in `config`. For these, one can add
`n_jobs=None,` before `**config` to make sure `config` doesn't contain an
`n_jobs` key.

## LGBMEstimator Objects​ {#lgbmestimator-objects}

    class LGBMEstimator(BaseEstimator)  

The class for tuning LGBM, using sklearn API.

## XGBoostEstimator Objects​ {#xgboostestimator-objects}

    class XGBoostEstimator(SKLearnEstimator)  

The class for tuning XGBoost regressor, not using sklearn API.

## XGBoostSklearnEstimator Objects​ {#xgboostsklearnestimator-objects}

    class XGBoostSklearnEstimator(SKLearnEstimator,  LGBMEstimator)  

The class for tuning XGBoost with unlimited depth, using sklearn API.

## XGBoostLimitDepthEstimator Objects​ {#xgboostlimitdepthestimator-objects}

    class XGBoostLimitDepthEstimator(XGBoostSklearnEstimator)  

The class for tuning XGBoost with limited depth, using sklearn API.

## RandomForestEstimator Objects​ {#randomforestestimator-objects}

    class RandomForestEstimator(SKLearnEstimator,  LGBMEstimator)  

The class for tuning Random Forest.

## ExtraTreesEstimator Objects​ {#extratreesestimator-objects}

    class ExtraTreesEstimator(RandomForestEstimator)  

The class for tuning Extra Trees.

## LRL1Classifier Objects​ {#lrl1classifier-objects}

    class LRL1Classifier(SKLearnEstimator)  

The class for tuning Logistic Regression with L1 regularization.

## LRL2Classifier Objects​ {#lrl2classifier-objects}

    class LRL2Classifier(SKLearnEstimator)  

The class for tuning Logistic Regression with L2 regularization.

## CatBoostEstimator Objects​ {#catboostestimator-objects}

    class CatBoostEstimator(BaseEstimator)  

The class for tuning CatBoost.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/automl/model.md)

[Previous« ml](.docs/reference/automl/ml)

[Nextstate »](.docs/reference/automl/state)

* BaseEstimator Objects
* SparkEstimator Objects
* SparkLGBMEstimator Objects
* TransformersEstimator Objects
* SKLearnEstimator Objects
* LGBMEstimator Objects
* XGBoostEstimator Objects
* XGBoostSklearnEstimator Objects
* XGBoostLimitDepthEstimator Objects
* RandomForestEstimator Objects
* ExtraTreesEstimator Objects
* LRL1Classifier Objects
* LRL2Classifier Objects
* CatBoostEstimator Objects

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

Recent posts

* [Use flaml.autogen for Local LLMs](.blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](.blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](.blog/2023/05/18/GPT-adaptive-humaneval)
* [Surpassing 1 Million Downloads - A Retrospective and a Look into the Future](.blog/2023/05/07/1M-milestone)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](.blog/2023/04/21/LLM-tuning-math)

# 3 posts tagged with "research" {#3-posts-tagged-with-research}

[View All Tags](.blog/tags)

## [MathChat - An Conversational Framework to Solve Math {#mathchat-an-conversational-framework-to-solve-math}

Problems](.blog/2023/06/28/MathChat)

June 28, 2023 · 8 min read

[![Yiran
Wu](https://github.com/kevin666aa.png)](https://github.com/kevin666aa)

[Yiran Wu](https://github.com/kevin666aa)

PhD student at Pennsylvania State University

![MathChat
WorkFlow](.assets/images/mathchatflow-926a8ed1975a114ab76c69996942c23a.png)
**TL;DR:**

* **We introduce MathChat, a conversational framework leveraging Large Language Models (LLMs), specifically GPT-4, to solve advanced mathematical problems.**
* **MathChat improves LLM 's performance on challenging math problem-solving, outperforming basic prompting and other strategies by about 6%. The improvement was especially notable in the Algebra category, with a 15% increase in accuracy.**
* **Despite the advancement, GPT-4 still struggles to solve very challenging math problems, even with effective prompting strategies. Further improvements are needed, such as the development of more specific assistant models or the integration of new tools and prompts.**

Recent Large Language Models (LLMs) like GTP-3.5 and GPT-4 have demonstrated
astonishing abilities over previous models on various tasks, such as text
generation, question answering, and code generation. Moreover, these models
can communicate with humans through conversations and remember previous
contexts, making it easier for humans to interact with them. These models play
an increasingly important role in our daily lives assisting people with
different tasks, such as writing emails, summarizing documents, and writing
code.

In this blog post, we probe into the problem-solving capabilities of LLMs.
Specifically, we are interested in their capabilities to solve advanced math
problems, which could be representative of a broader class of problems that
require precise reasoning and also have deterministic solutions.

We introduce MathChat, a conversational framework designed for solving
challenging math problems with LLMs. This framework takes advantage of the
chat-optimized feature of state-of-the-art LLMs, where a user proxy agent and
an LLM assistant work together to tackle math problems. We also test previous
prompting techniques for comparison.

## The MathChat Framework​ {#the-mathchat-framework}

MathChat simulates a conversation between the LLM assistant and a user proxy
agent. As the name indicates, the user proxy agent acts as a proxy for the
user, which is responsible for communicating with the LLM assistant and
continuing the conversation in a desired manner.

The proxy agent first presents a math problem to the LLM assistant, framed by
an initial prompt that guides the assistant in how it should work
collaboratively to solve the problem. We incorporate existing prompting
methods such as CoT (chain-of-thought) and tool-using are incorporated in the
initial prompt. Key components of the prompt include:

* **Tool-using Prompt:** This guides the LLM assistant to use Python code in the correct format to address the problem, allowing the user proxy agent to parse the code and provide the appropriate results.

* **Problem-Solving Strategy Selection Prompt:** The assistant is instructed to choose one of three potential problem-solving strategies, including:

    1. Writing a Python program to solve the problem directly.
    2. Solving the problem directly without using Python, which exercises LLMs' inherent reasoning capabilities.
    3. Solving the problem step by step with Python. This should be used when the first two strategies aren't suitable.

* **Final Answer Encapsulation Prompt:** This part instructs the assistant to put the final answer in _\boxed_ {}.

The prompt design in MathChat is flexible and can easily be refined to include
other tools, such as Wolfram Alpha.

Let's take a look at an example between the **User Proxy Agent** and the **LLM
Assistant** (GPT-4). The conversation focuses on how to solve inequality using
Python. (The conversation is modified for readability.)

Participant| Dialogue  
---|---  
User Proxy Agent| (Initial Prompt: Let's solve a math problem with Python...)
Problem: Find all x that satisfy the inequality (2x+10)(x+3) < (3x+9)(x+8).
Express your answer in interval notation.  
LLM assistant| The key idea to solve this problem is to find the critical
points, which are the points where the inequality is equal. Then, test the
intervals between the critical points to find the interval for which the
inequality holds.  
LLM assistant (continued)| Here's Python code to find the critical points:
`from sympy import symbols, Eq, solve; x = symbols("x"); lhs = (2*x + 10)*(x +
3); rhs = (3*x + 9)*(x + 8); equation = Eq(lhs, rhs); critical_points =
solve(equation, x); critical_points`  
User Proxy Agent| The critical points are: [-14, -3]  
LLM assistant| We found the critical points to be x = -14 and x = -3. Now
we'll test the intervals between these critical points as well as points
before and after the critical points. The intervals are: x < -14, -14 < x <
-3, x > -3.  
LLM assistant (continued)| Here's Python code to test the inequality for each
interval: `from sympy import symbols; x = symbols("x"); inequality = (2*x +
10)*(x + 3) < (3*x + 9)*(x + 8); test_points = [-15, -5, 0]; intervals = [];
for point in test_points: if inequality.subs(x, point):
intervals.append(point); intervals`  
User Proxy Agent| The intervals where the inequality holds true are for test
points: [-15, 0]  
LLM assistant| We have tested the intervals and found that the inequality
holds for: 1. x < -14 (test point: -15) 2. x > -3 (test point: 0). Therefore,
the solution to the inequality (2x+10)(x+3) < (3x+9)(x+8) is x ∈ (-∞, -14) ∪
(-3, ∞)  
  
## Experiment Setup​ {#experiment-setup} {#experiment-setup}

We evaluate the improvement brought by MathChat.

For the experiment, we focus on the level-5 problems from the MATH dataset,
which are composed of high school competition problems. These problems include
the application of theorems and complex equation derivation and are
challenging even for undergraduate students. We evaluate 6 of 7 categories
from the dataset (excluding Geometry): Prealgebra, Algebra, Number Theory,
Counting and Probability, Intermediate Algebra, and Precalculus.

We evaluate GPT-4 and use the default configuration of the OpenAI API. To
access the final performance, we manually compare the final answer with the
correct answer. For the vanilla prompt, Program Synthesis, and MathChat, we
have GPT-4 enclose the final answer in _\boxed_ {}, and we take the return of
the function in PoT as the final answer.

We also evaluate the following methods for comparison:

  1. **Vanilla prompting:** Evaluates GPT-4's direct problem-solving capability. The prompt used is: _" Solve the problem carefully. Put the final answer in \boxed{}"_.

  2. **Program of Thoughts (PoT):** Uses a zero-shot PoT prompt that requests the model to create a _Solver_ function to solve the problem and return the final answer.

  3. **Program Synthesis (PS) prompting:** Like PoT, it prompts the model to write a program to solve the problem. The prompt used is: _" Write a program that answers the following question: {Problem}"_.

## Experiment Results​ {#experiment-results} {#experiment-results} {#experiment-results}

The accuracy on all the problems with difficulty level-5 from different
categories of the MATH dataset with different methods is shown below:

![Result](.assets/images/result-514251f4c387d5b6e11656e1fa33de08.png)

We found that compared to basic prompting, which demonstrates the innate
capabilities of GPT-4, utilizing Python within the context of PoT or PS
strategy improved the overall accuracy by about 10%. This increase was mostly
seen in categories involving more numerical manipulations, such as Counting &
Probability and Number Theory, and in more complex categories like
Intermediate Algebra and Precalculus.

For categories like Algebra and Prealgebra, PoT and PS showed little
improvement, and in some instances, even led to a decrease in accuracy.
However, MathChat was able to enhance total accuracy by around 6% compared to
PoT and PS, showing competitive performance across all categories. Remarkably,
MathChat improved accuracy in the Algebra category by about 15% over other
methods. Note that categories like Intermediate Algebra and Precalculus
remained challenging for all methods, with only about 20% of problems solved
accurately.

The code for experiments can be found at this
[repository](https://github.com/kevin666aa.tree/gpt_math_solver/flaml/autogen/math).
We now provide an implementation of MathChat using the interactive agents in
FLAML. See this
[notebook](https://github.com/microsoft.blob/main/notebook/autogen_agentchat_MathChat.ipynb)
for example usage.

## Future Directions​ {#future-directions}

Despite MathChat's improvements over previous methods, the results show that
complex math problem is still challenging for recent powerful LLMs, like
GPT-4, even with help from external tools.

Further work can be done to enhance this framework or math problem-solving in
general:

* Although enabling the model to use tools like Python can reduce calculation errors, LLMs are still prone to logic errors. Methods like self-consistency (Sample several solutions and take a major vote on the final answer), or self-verification (use another LLM instance to check whether an answer is correct) might improve the performance.
* Sometimes, whether the LLM can solve the problem depends on the plan it uses. Some plans require less computation and logical reasoning, leaving less room for mistakes.
* MathChat has the potential to be adapted into a copilot system, which could assist users with math problems. This system could allow users to be more involved in the problem-solving process, potentially enhancing learning.

## For Further Reading​ {#for-further-reading} {#for-further-reading} {#for-further-reading}

* [Research paper of MathChat](https://arxiv.org/abs/2306.01337)
* [Documentation about `flaml.autogen`](https://microsoft.github.io/autogen/)

 _Are you working on applications that involve math problem-solving? Would you
appreciate additional research or support on the application of LLM-based
agents for math problem-solving? Please join
our[Discord](https://discord.gg/Cppx2vSPVP) server for discussion._

 **Tags:**

* [LLM](.blog/tags/llm)
* [GPT](.blog/tags/gpt)
* [research](.blog/tags/research)

[**Read More**](.blog/2023/06/28/MathChat)

## [Achieve More, Pay Less - Use GPT-4 Smartly](.blog/2023/05/18/GPT- {#achieve-more-pay-less-use-gpt4-smartlyflamlblog20230518gpt}

adaptive-humaneval)

May 18, 2023 · 8 min read

[![Chi Wang](https://github.com/sonichi.png)](<https://www.linkedin.com/in/chi->
wang-49b15b16/)

[Chi Wang](https://www.linkedin.com/in/chi-wang-49b15b16/)

Principal Researcher at Microsoft Research

![An adaptive way of using GPT-3.5 and GPT-4 outperforms GPT-4 in both coding
success rate and inference
cost](.assets/images/humaneval-b2718b0b15d76b8fc59a4094e1bd684c.png)

 **TL;DR:**

* **A case study using the HumanEval benchmark shows that an adaptive way of using multiple GPT models can achieve both much higher accuracy (from 68% to 90%) and lower inference cost (by 18%) than using GPT-4 for coding.**

GPT-4 is a big upgrade of foundation model capability, e.g., in code and math,
accompanied by a much higher (more than 10x) price per token to use over
GPT-3.5-Turbo. On a code completion benchmark,
[HumanEval](https://huggingface.co/datasets/openai_humaneval), developed by
OpenAI, GPT-4 can successfully solve 68% tasks while GPT-3.5-Turbo does 46%.
It is possible to increase the success rate of GPT-4 further by generating
multiple responses or making multiple calls. However, that will further
increase the cost, which is already nearly 20 times of using GPT-3.5-Turbo and
with more restricted API call rate limit. Can we achieve more with less?

In this blog post, we will explore a creative, adaptive way of using GPT
models which leads to a big leap forward.

## Observations​ {#observations}

* GPT-3.5-Turbo can alrady solve 40%-50% tasks. For these tasks if we never use GPT-4, we can save nearly 40-50% cost.
* If we use the saved cost to generate more responses with GPT-4 for the remaining unsolved tasks, it is possible to solve some more of them while keeping the amortized cost down.

The obstacle of leveraging these observations is that we do not know _a
priori_ which tasks can be solved by the cheaper model, which tasks can be
solved by the expensive model, and which tasks can be solved by paying even
more to the expensive model.

To overcome that obstacle, one may want to predict which task requires what
model to solve and how many responses are required for each task. Let's look
at one example code completion task:
```python
    def vowels_count(s):  
        """Write a function vowels_count which takes a string representing  
        a word as input and returns the number of vowels in the string.  
        Vowels in this case are 'a', 'e', 'i', 'o', 'u'. Here, 'y' is also a  
        vowel, but only when it is at the end of the given word.  
      
        Example:  
        >>> vowels_count("abcde")  
        2  
        >>> vowels_count("ACEDY")  
        3  
        """  

Can we predict whether GPT-3.5-Turbo can solve this task or do we need to use
GPT-4? My first guess is that GPT-3.5-Turbo can get it right because the
instruction is fairly straightforward. Yet, it turns out that GPT-3.5-Turbo
does not consistently get it right, if we only give it one chance. It's not
obvious (but an interesting research question!) how to predict the performance
without actually trying.

What else can we do? We notice that: **It 's "easier" to verify a given
solution than finding a correct solution from scratch.**

Some simple example test cases are provided in the docstr. If we already have
a response generated by a model, we can use those test cases to filter wrong
implementations, and either use a more powerful model or generate more
responses, until the result passes the example test cases. Moreover, this step
can be automated by asking GPT-3.5-Turbo to generate assertion statements from
the examples given in the docstr (a simpler task where we can place our bet)
and executing the code.

## Solution​ {#solution}

Combining these observations, we can design a solution with two intuitive
ideas:

* Make use of auto-generated feedback, i.e., code execution results, to filter responses.
* Try inference configurations one by one, until one response can pass the filter.

![Design](.assets/images/design-88630f6d8d95f1e31ef5ce79e81aab54.png)

This solution works adaptively without knowing or predicting which task fits
which configuration. It simply tries multiple configurations one by one,
starting from the cheapest configuration. Note that one configuration can
generate multiple responses (by setting the inference parameter n larger than
1). And different configurations can use the same model and different
inference parameters such as n and temperature. Only one response is returned
and evaluated per task.

An implementation of this solution is provided in
[flaml.autogen](.docs/reference/autogen/code_utils#implement). It uses
the following sequence of configurations:

  1. GPT-3.5-Turbo, n=1, temperature=0
  2. GPT-3.5-Turbo, n=7, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]
  3. GPT-4, n=1, temperature=0
  4. GPT-4, n=2, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]
  5. GPT-4, n=1, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]

## Experiment Results​ {#experiment-results} {#experiment-results} {#experiment-results}

The first figure in this blog post shows the success rate and average
inference cost of the adaptive solution compared with default GPT-4. The
inference cost includes the cost for generating the assertions in our
solution. The generated assertions are not always correct, and programs that
pass/fail the generated assertions are not always right/wrong. Despite of
that, the adaptive solution can increase the success rate (referred to as
pass@1 in the literature) from 68% to 90%, while reducing the cost by 18%.

Here are a few examples of function definitions which are solved by different
configurations in the portfolio.

  1. Solved by GPT-3.5-Turbo, n=1, temperature=0
```python
    def compare(game,guess):  
        """I think we all remember that feeling when the result of some long-awaited  
        event is finally known. The feelings and thoughts you have at that moment are  
        definitely worth noting down and comparing.  
        Your task is to determine if a person correctly guessed the results of a number of matches.  
        You are given two arrays of scores and guesses of equal length, where each index shows a match.  
        Return an array of the same length denoting how far off each guess was. If they have guessed correctly,  
        the value is 0, and if not, the value is the absolute difference between the guess and the score.  
      
      
        example:  
      
        compare([1,2,3,4,5,1],[1,2,3,4,2,-2]) -> [0,0,0,0,3,3]  
        compare([0,5,0,0,0,4],[4,1,1,0,0,-2]) -> [4,4,1,0,0,6]  
        """  

  2. Solved by GPT-3.5-Turbo, n=7, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]: the `vowels_count` function presented earlier.
  3. Solved by GPT-4, n=1, temperature=0:
```python
    def string_xor(a: str, b: str) -> str:  
        """ Input are two strings a and b consisting only of 1s and 0s.  
        Perform binary XOR on these inputs and return result also as a string.  
        >>> string_xor('010', '110')  
        '100'  
        """  

  4. Solved by GPT-4, n=2, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]:
```python
    def is_palindrome(string: str) -> bool:  
        """ Test if given string is a palindrome """  
        return string == string[::-1]  
      
      
    def make_palindrome(string: str) -> str:  
        """ Find the shortest palindrome that begins with a supplied string.  
        Algorithm idea is simple:  
        - Find the longest postfix of supplied string that is a palindrome.  
        - Append to the end of the string reverse of a string prefix that comes before the palindromic suffix.  
        >>> make_palindrome('')  
        ''  
        >>> make_palindrome('cat')  
        'catac'  
        >>> make_palindrome('cata')  
        'catac'  
        """  

  5. Solved by GPT-4, n=1, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]:
```python
    def sort_array(arr):  
        """  
        In this Kata, you have to sort an array of non-negative integers according to  
        number of ones in their binary representation in ascending order.  
        For similar number of ones, sort based on decimal value.  
      
        It must be implemented like this:  
        >>> sort_array([1, 5, 2, 3, 4]) == [1, 2, 3, 4, 5]  
        >>> sort_array([-2, -3, -4, -5, -6]) == [-6, -5, -4, -3, -2]  
        >>> sort_array([1, 0, 2, 3, 4]) [0, 1, 2, 3, 4]  
        """  

The last problem is an example with wrong example test cases in the original
definition. It misleads the adaptive solution because a correct implementation
is regarded as wrong and more trials are made. The last configuration in the
sequence returns the right implementation, even though it does not pass the
auto-generated assertions. This example demonstrates that:

* Our adaptive solution has a certain degree of fault tolerance.
* The success rate and inference cost for the adaptive solution can be further improved if correct example test cases are used.

It is worth noting that the reduced inference cost is the amortized cost over
all the tasks. For each individual task, the cost can be either larger or
smaller than directly using GPT-4. This is the nature of the adaptive
solution: The cost is in general larger for difficult tasks than that for easy
tasks.

An example notebook to run this experiment can be found at:
<https://github.com/microsoft.blob/v1.2.1/notebook/research/autogen_code.ipynb>

## Discussion​ {#discussion}

Our solution is quite simple to implement using a generic interface offered in
[`flaml.autogen`](<https://microsoft.github.io/autogen/docs/Use->
Cases/enhanced_inference#logic-error), yet the result is quite encouraging.

While the specific way of generating assertions is application-specific, the
main ideas are general in LLM operations:

* Generate multiple responses to select - especially useful when selecting a good response is relatively easier than generating a good response at one shot.
* Consider multiple configurations to generate responses - especially useful when:
  * Model and other inference parameter choice affect the utility-cost tradeoff; or
  * Different configurations have complementary effect.

A [previous blog post](.blog/2023/04/21/LLM-tuning-math) provides
evidence that these ideas are relevant in solving math problems too.
`flaml.autogen` uses a technique
[EcoOptiGen](https://arxiv.org/abs/2303.04673) to support inference parameter
tuning and model selection.

There are many directions of extensions in research and development:

* Generalize the way to provide feedback.
* Automate the process of optimizing the configurations.
* Build adaptive agents for different applications.

 _Do you find this approach applicable to your use case? Do you have any other
challenge to share about LLM applications? Do you like to see more support or
research of LLM optimization or automation? Please join
our[Discord](https://discord.gg/Cppx2vSPVP) server for discussion._

## For Further Reading​ {#for-further-reading} {#for-further-reading} {#for-further-reading}

* [Documentation](https://microsoft.github.io/autogen/) about `flaml.autogen` and [Research paper](https://arxiv.org/abs/2303.04673).
* [Blog post](.blog/2023/04/21/LLM-tuning-math) about a related study for math.

 **Tags:**

* [LLM](.blog/tags/llm)
* [GPT](.blog/tags/gpt)
* [research](.blog/tags/research)

[**Read More**](.blog/2023/05/18/GPT-adaptive-humaneval)

## [Does Model and Inference Parameter Matter in LLM Applications? - A Case {#does-model-and-inference-parameter-matter-in-llm-applications-a-case}

Study for MATH](.blog/2023/04/21/LLM-tuning-math)

April 21, 2023 · 5 min read

[![Chi Wang](https://github.com/sonichi.png)](<https://www.linkedin.com/in/chi->
wang-49b15b16/)

[Chi Wang](https://www.linkedin.com/in/chi-wang-49b15b16/)

Principal Researcher at Microsoft Research

![level 2
algebra](.assets/images/level2algebra-659ba95286432d9945fc89e84d606797.png)

 **TL;DR:**

* **Just by tuning the inference parameters like model, number of responses, temperature etc. without changing any model weights or prompt, the baseline accuracy of untuned gpt-4 can be improved by 20% in high school math competition problems.**
* **For easy problems, the tuned gpt-3.5-turbo model vastly outperformed untuned gpt-4 in accuracy (e.g., 90% vs. 70%) and cost efficiency. For hard problems, the tuned gpt-4 is much more accurate (e.g., 35% vs. 20%) and less expensive than untuned gpt-4.**
* **FLAML can help with model selection, parameter tuning, and cost-saving in LLM applications.**

Large language models (LLMs) are powerful tools that can generate natural
language texts for various applications, such as chatbots, summarization,
translation, and more. GPT-4 is currently the state of the art LLM in the
world. Is model selection irrelevant? What about inference parameters?

In this blog post, we will explore how model and inference parameter matter in
LLM applications, using a case study for [MATH](<https://datasets-benchmarks->
proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-
round2.html), a benchmark for evaluating LLMs on advanced mathematical problem
solving. MATH consists of 12K math competition problems from AMC-10, AMC-12
and AIME. Each problem is accompanied by a step-by-step solution.

We will use the new subpackage [`flaml.autogen`](.blog/tags/docs/Use-
Cases/Autogen) to automatically find the best model and inference parameter
for LLMs on a given task and dataset given an inference budget, using a novel
low-cost search & pruning strategy. FLAML currently supports all the LLMs from
OpenAI, such as GPT-3.5 and GPT-4.

We will use FLAML to perform model selection and inference parameter tuning.
Then we compare the performance and inference cost on solving algebra problems
with the untuned gpt-4. We will also analyze how different difficulty levels
affect the results.

## Experiment Setup​ {#experiment-setup} {#experiment-setup}

We use FLAML to select between the following models with a target inference
budget $0.02 per instance:

* gpt-3.5-turbo, a relatively cheap model that powers the popular ChatGPT app
* gpt-4, the state of the art LLM that costs more than 10 times of gpt-3.5-turbo

We adapt the models using 20 examples in the train set, using the problem
statement as the input and generating the solution as the output. We use the
following inference parameters:

* temperature: The parameter that controls the randomness of the output text. A higher temperature means more diversity but less coherence. We search for the optimal temperature in the range of [0, 1].
* top_p: The parameter that controls the probability mass of the output tokens. Only tokens with a cumulative probability less than or equal to top-p are considered. A lower top-p means more diversity but less coherence. We search for the optimal top-p in the range of [0, 1].
* max_tokens: The maximum number of tokens that can be generated for each output. We search for the optimal max length in the range of [50, 1000].
* n: The number of responses to generate. We search for the optimal n in the range of [1, 100].
* prompt: We use the template: "{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \boxed{{}}." where {problem} will be replaced by the math problem instance.

In this experiment, when n > 1, we find the answer with highest votes among
all the responses and then select it as the final answer to compare with the
ground truth. For example, if n = 5 and 3 of the responses contain a final
answer 301 while 2 of the responses contain a final answer 159, we choose 301
as the final answer. This can help with resolving potential errors due to
randomness. We use the average accuracy and average inference cost as the
metric to evaluate the performance over a dataset. The inference cost of a
particular instance is measured by the price per 1K tokens and the number of
tokens consumed.

## Experiment Results​ {#experiment-results} {#experiment-results} {#experiment-results}

The first figure in this blog post shows the average accuracy and average
inference cost of each configuration on the level 2 Algebra test set.

Surprisingly, the tuned gpt-3.5-turbo model is selected as a better model and
it vastly outperforms untuned gpt-4 in accuracy (92% vs. 70%) with equal or
2.5 times higher inference budget. The same observation can be obtained on the
level 3 Algebra test set.

![level 3
algebra](.assets/images/level3algebra-94e87a683ac8832ac7ae6f41f30131a4.png)

However, the selected model changes on level 4 Algebra.

![level 4
algebra](.assets/images/level4algebra-492beb22490df30d6cc258f061912dcd.png)

This time gpt-4 is selected as the best model. The tuned gpt-4 achieves much
higher accuracy (56% vs. 44%) and lower cost than the untuned gpt-4. On level
5 the result is similar.

![level 5
algebra](.assets/images/level5algebra-8fba701551334296d08580b4b489fe56.png)

We can see that FLAML has found different optimal model and inference
parameters for each subset of a particular level, which shows that these
parameters matter in cost-sensitive LLM applications and need to be carefully
tuned or adapted.

An example notebook to run these experiments can be found at:
<https://github.com/microsoft.blob/v1.2.1/notebook/autogen_chatgpt.ipynb>

## Analysis and Discussion​ {#analysis-and-discussion}

While gpt-3.5-turbo demonstrates competitive accuracy with voted answers in
relatively easy algebra problems under the same inference budget, gpt-4 is a
better choice for the most difficult problems. In general, through parameter
tuning and model selection, we can identify the opportunity to save the
expensive model for more challenging tasks, and improve the overall
effectiveness of a budget-constrained system.

There are many other alternative ways of solving math problems, which we have
not covered in this blog post. When there are choices beyond the inference
parameters, they can be generally tuned via
[`flaml.tune`](.blog/tags/docs/Use-Cases/Tune-User-Defined-Function).

The need for model selection, parameter tuning and cost saving is not specific
to the math problems. The [Auto-GPT](<https://github.com/Significant->
Gravitas/Auto-GPT) project is an example where high cost can easily prevent a
generic complex task to be accomplished as it needs many LLM inference calls.

## For Further Reading​ {#for-further-reading} {#for-further-reading} {#for-further-reading}

* [Research paper about the tuning technique](https://arxiv.org/abs/2303.04673)
* [Documentation about `flaml.autogen`](https://microsoft.github.io/autogen/)

 _Do you have any experience to share about LLM applications? Do you like to
see more support or research of LLM optimization or automation? Please join
our[Discord](https://discord.gg/Cppx2vSPVP) server for discussion._

 **Tags:**

* [LLM](.blog/tags/llm)
* [GPT](.blog/tags/gpt)
* [research](.blog/tags/research)

[**Read More**](.blog/2023/04/21/LLM-tuning-math)

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

Recent posts

* [All About Agent Descriptions](/autogen/blog/2023/12/29/AgentDescriptions)
* [AgentOptimizer - An Agentic Way to Train Your LLM Agent](/autogen/blog/2023/12/23/AgentOptimizer)
* [AutoGen Studio: Interactively Explore Multi-Agent Workflows](/autogen/blog/2023/12/01/AutoGenStudio)
* [Agent AutoBuild - Automatically Building Multi-agent Systems](/autogen/blog/2023/11/26/Agent-AutoBuild)
* [How to Assess Utility of LLM-powered Applications?](/autogen/blog/2023/11/20/AgentEval)
* [AutoGen Meets GPTs](/autogen/blog/2023/11/13/OAI-assistants)
* [EcoAssistant - Using LLM Assistants More Accurately and Affordably](/autogen/blog/2023/11/09/EcoAssistant)
* [Multimodal with GPT-4V and LLaVA](/autogen/blog/2023/11/06/LMM-Agent)
* [AutoGen's Teachable Agents](/autogen/blog/2023/10/26/TeachableAgent)
* [Retrieval-Augmented Generation (RAG) Applications with AutoGen](/autogen/blog/2023/10/18/RetrieveChat)
* [Use AutoGen for Local LLMs](/autogen/blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](/autogen/blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](/autogen/blog/2023/05/18/GPT-adaptive-humaneval)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](/autogen/blog/2023/04/21/LLM-tuning-math)

# One post tagged with "task utility" {#one-post-tagged-with-task-utility}

[View All Tags](/autogen/blog/tags)

## [How to Assess Utility of LLM-powered {#how-to-assess-utility-of-llmpowered}

Applications?](/autogen/blog/2023/11/20/AgentEval)

November 20, 2023 · 10 min read

[![Julia
Kiseleva](https://avatars.githubusercontent.com/u/5908392?v=4)](https://github.com/julianakiseleva/)

[Julia Kiseleva](https://github.com/julianakiseleva/)

Senior Researcher at Microsoft Research

[![Negar Arabzadeh](https://github.com/Narabzad.png)](https://www.negara.me/)

[Negar Arabzadeh](https://www.negara.me/)

PhD student at the University of Waterloo

![Fig.1: A verification framework](/autogen/assets/images/agenteval-
CQ-59f82d98878045817d02a3915839a92a.png)

 _Fig.1 illustrates the general flow of AgentEval_

 **TL;DR:**

* As a developer of an LLM-powered application, how can you assess the utility it brings to end users while helping them with their tasks?
* To shed light on the question above, we introduce `AgentEval` — the first version of the framework to assess the utility of any LLM-powered application crafted to assist users in specific tasks. AgentEval aims to simplify the evaluation process by automatically proposing a set of criteria tailored to the unique purpose of your application. This allows for a comprehensive assessment, quantifying the utility of your application against the suggested criteria.
* We demonstrate how `AgentEval` work using [math problems dataset](https://microsoft.github.io/autogen/blog/2023/06/28/MathChat) as an example in the [following notebook](https://github.com/microsoft/autogen/blob/main/notebook/agenteval_cq_math.ipynb). Any feedback would be useful for future development. Please contact us on our [Discord](http://aka.ms/autogen-dc).

## Introduction​ {#introduction}

AutoGen aims to simplify the development of LLM-powered multi-agent systems
for various applications, ultimately making end users' lives easier by
assisting with their tasks. Next, we all yearn to understand how our developed
systems perform, their utility for users, and, perhaps most crucially, how we
can enhance them. Directly evaluating multi-agent systems poses challenges as
current approaches predominantly rely on success metrics – essentially,
whether the agent accomplishes tasks. However, comprehending user interaction
with a system involves far more than success alone. Take math problems, for
instance; it's not merely about the agent solving the problem. Equally
significant is its ability to convey solutions based on various criteria,
including completeness, conciseness, and the clarity of the provided
explanation. Furthermore, success isn't always clearly defined for every task.

Rapid advances in LLMs and multi-agent systems have brought forth many
emerging capabilities that we're keen on translating into tangible utilities
for end users. We introduce the first version of `AgentEval` framework - a
tool crafted to empower developers in swiftly gauging the utility of LLM-
powered applications designed to help end users accomplish the desired task.

![Fig.2: An overview of the tasks taxonomy](/autogen/assets/images/tasks-
taxonomy-c4b7c6fe465e12dddde9235c3ee8c281.png)

 _Fig. 2 provides an overview of the tasks taxonomy_

Let's first look into an overview of the suggested task taxonomy that a multi-
agent system can be designed for. In general, the tasks can be split into two
types, where:

* _Success is not clearly defined_ \- refer to instances when users utilize a system in an assistive manner, seeking suggestions rather than expecting the system to solve the task. For example, a user might request the system to generate an email. In many cases, this generated content serves as a template that the user will later edit. However, defining success precisely for such tasks is relatively complex.
* _Success is clearly defined_ \- refer to instances where we can clearly define whether a system solved the task or not. Consider agents that assist in accomplishing household tasks, where the definition of success is clear and measurable. This category can be further divided into two separate subcategories:
    *_The optimal solution exits_ \- these are tasks where only one solution is possible. For example, if you ask your assistant to turn on the light, the success of this task is clearly defined, and there is only one way to accomplish it.
  * _Multiple solutions exist_ \- increasingly, we observe situations where multiple trajectories of agent behavior can lead to either success or failure. In such cases, it is crucial to differentiate between the various successful and unsuccessful trajectories. For example, when you ask the agent to suggest you a food recipe or tell you a joke.

In our `AgentEval` framework, we are currently focusing on tasks where
_Success is clearly defined_. Next, we will introduce the suggested framework.

## `AgentEval` Framework​ {#agenteval-framework}

Our previous research on [assistive agents in
Minecraft](https://github.com/microsoft/iglu-datasets) suggested that the most
optimal way to obtain human judgments is to present humans with two agents
side by side and ask for preferences. In this setup of pairwise comparison,
humans can develop criteria to explain why they prefer the behavior of one
agent over another. For instance, _' the first agent was faster in
execution,'_ or _' the second agent moves more naturally.'_ So, the
comparative nature led humans to come up with a list of criteria that helps to
infer the utility of the task. With this idea in mind, we designed `AgentEval`
(shown in Fig. 1), where we employ LLMs to help us understand, verify, and
assess task _utility_ for the multi-agent system. Namely:

* The goal of `CriticAgent` is to suggest the list of criteria (Fig. 1), that can be used to assess task utility. This is an example of how `CriticAgent` is defined using `Autogen`:

    critic = autogen.AssistantAgent(  
        name="critic",  
        llm_config={"config_list": config_list},  
        system_message="""You are a helpful assistant. You suggest criteria for evaluating different tasks. They should be distinguishable, quantifiable, and not redundant.  
        Convert the evaluation criteria into a dictionary where the keys are the criteria.  
        The value of each key is a dictionary as follows {"description": criteria description, "accepted_values": possible accepted inputs for this key}  
        Make sure the keys are criteria for assessing the given task. "accepted_values" include the acceptable inputs for each key that are fine-grained and preferably multi-graded levels. "description" includes the criterion description.  
        Return only the dictionary."""  
    )  

Next, the critic is given successful and failed examples of the task
execution; then, it is able to return a list of criteria (Fig. 1). For
reference, use the [following
notebook](https://github.com/microsoft/autogen/blob/main/notebook/agenteval_cq_math.ipynb).

* The goal of `QuantifierAgent` is to quantify each of the suggested criteria (Fig. 1), providing us with an idea of the utility of this system for the given task. Here is an example of how it can be defined:

    quantifier = autogen.AssistantAgent(  
        name="quantifier",  
        llm_config={"config_list": config_list},  
        system_message = """You are a helpful assistant. You quantify the output of different tasks based on the given criteria.  
        The criterion is given in a dictionary format where each key is a distinct criteria.  
        The value of each key is a dictionary as follows {"description": criteria description , "accepted_values": possible accepted inputs for this key}  
        You are going to quantify each of the criteria for a given task based on the task description.  
        Return a dictionary where the keys are the criteria and the values are the assessed performance based on accepted values for each criteria.  
        Return only the dictionary."""  

    )  

## `AgentEval` Results based on Math Problems Dataset​ {#agenteval-results-based-on-math-problems-dataset}

As an example, after running CriticAgent, we obtained the following criteria
to verify the results for math problem dataset:

Criteria| Description| Accepted Values  
---|---|---  
Problem Interpretation| Ability to correctly interpret the problem|
["completely off", "slightly relevant", "relevant", "mostly accurate",
"completely accurate"]  
Mathematical Methodology| Adequacy of the chosen mathematical or algorithmic
methodology for the question| ["inappropriate", "barely adequate", "adequate",
"mostly effective", "completely effective"]  
Calculation Correctness| Accuracy of calculations made and solutions given|
["completely incorrect", "mostly incorrect", "neither", "mostly correct",
"completely correct"]  
Explanation Clarity| Clarity and comprehensibility of explanations, including
language use and structure| ["not at all clear", "slightly clear", "moderately
clear", "very clear", "completely clear"]  
Code Efficiency| Quality of code in terms of efficiency and elegance| ["not at
all efficient", "slightly efficient", "moderately efficient", "very
efficient", "extremely efficient"]  
Code Correctness| Correctness of the provided code| ["completely incorrect",
"mostly incorrect", "partly correct", "mostly correct", "completely correct"]  
  
Then, after running QuantifierAgent, we obtained the results presented in Fig.
3, where you can see three models:

* AgentChat
* ReAct
* GPT-4 Vanilla Solver

Lighter colors represent estimates for failed cases, and brighter colors show
how discovered criteria were quantified.

![Fig.3: Results based on overall math problems dataset `_s` stands for
successful cases, `_f` - stands for failed cases](/autogen/assets/images/math-
problems-plot-03ec81b957c85db6ad9b1da353784b96.png)

 _Fig.3 presents results based on overall math problems dataset `_s` stands
for successful cases, `_f` - stands for failed cases_

We note that while applying agentEval to math problems, the agent was not
exposed to any ground truth information about the problem. As such, this
figure illustrates an estimated performance of the three different agents,
namely, Autogen (blue), Gpt-4 (red), and ReAct (green). We observe that by
comparing the performance of any of the three agents in successful cases (dark
bars of any color) versus unsuccessful cases (lighter version of the same
bar), we note that AgentEval was able to assign higher quantification to
successful cases than that of failed ones. This observation verifies
AgentEval's ability for task utility prediction. Additionally, AgentEval
allows us to go beyond just a binary definition of success, enabling a more
in-depth comparison between successful and failed cases.

It's important not only to identify what is not working but also to recognize
what and why actually went well.

## Limitations and Future Work​ {#limitations-and-future-work}

The current implementation of `AgentEval` has a number of limitations which
are planning to overcome in the future:

* The list of criteria varies per run (unless you store a seed). We would recommend to run `CriticAgent` at least two times, and pick criteria you think is important for your domain.
* The results of the `QuantifierAgent` can vary with each run, so we recommend conducting multiple runs to observe the extent of result variations.

To mitigate the limitations mentioned above, we are working on VerifierAgent,
whose goal is to stabilize the results and provide additional explanations.

## Summary​ {#summary}

`CriticAgent` and `QuantifierAgent` can be applied to the logs of any type of
application, providing you with an in-depth understanding of the utility your
solution brings to the user for a given task.

We would love to hear about how AgentEval works for your application. Any
feedback would be useful for future development. Please contact us on our
[Discord](http://aka.ms/autogen-dc).

## Previous Research​ {#previous-research}

    @InProceedings{pmlr-v176-kiseleva22a,  
      title = "Interactive Grounded Language Understanding in a Collaborative Environment: IGLU 2021",  
      author = "Kiseleva, Julia and Li, Ziming and Aliannejadi, Mohammad and Mohanty, Shrestha and ter Hoeve, Maartje and Burtsev, Mikhail and Skrynnik, Alexey and Zholus, Artem and Panov, Aleksandr and Srinet, Kavya and Szlam, Arthur and Sun, Yuxuan and Hofmann, Katja and C{\^o}t{\'e}, Marc-Alexandre and Awadallah, Ahmed and Abdrazakov, Linar and Churin, Igor and Manggala, Putra and Naszadi, Kata and van der Meer, Michiel and Kim, Taewoon",  
      booktitle = "Proceedings of the NeurIPS 2021 Competitions and Demonstrations Track",  
      pages = "146--161",  
      year = 2022,  
      editor = "Kiela, Douwe and Ciccone, Marco and Caputo, Barbara",  
      volume = 176,  
      series = "Proceedings of Machine Learning Research",  
      month = "06--14 Dec",  
      publisher = "PMLR",  
      pdf =      {https://proceedings.mlr.press/v176/kiseleva22a/kiseleva22a.pdf},  
      url =      {https://proceedings.mlr.press/v176/kiseleva22a.html}.  
    }  

    @InProceedings{pmlr-v220-kiseleva22a,  
      title = "Interactive Grounded Language Understanding in a Collaborative Environment: Retrospective on Iglu 2022 Competition",  
      author = "Kiseleva, Julia and Skrynnik, Alexey and Zholus, Artem and Mohanty, Shrestha and Arabzadeh, Negar and C\^{o}t\'e, Marc-Alexandre and Aliannejadi, Mohammad and Teruel, Milagro and Li, Ziming and Burtsev, Mikhail and ter Hoeve, Maartje and Volovikova, Zoya and Panov, Aleksandr and Sun, Yuxuan and Srinet, Kavya and Szlam, Arthur and Awadallah, Ahmed and Rho, Seungeun and Kwon, Taehwan and Wontae Nam, Daniel and Bivort Haiek, Felipe and Zhang, Edwin and Abdrazakov, Linar and Qingyam, Guo and Zhang, Jason and Guo, Zhibin",  
      booktitle = "Proceedings of the NeurIPS 2022 Competitions Track",  
      pages = "204--216",  
      year = 2022,  
      editor = "Ciccone, Marco and Stolovitzky, Gustavo and Albrecht, Jacob",  
      volume = 220,  
      series = "Proceedings of Machine Learning Research",  
      month = "28 Nov--09 Dec",  
      publisher = "PMLR",  
      pdf = "https://proceedings.mlr.press/v220/kiseleva22a/kiseleva22a.pdf",  
      url = "https://proceedings.mlr.press/v220/kiseleva22a.html".  
    }  

 **Tags:**

* [LLM](/autogen/blog/tags/llm)
* [GPT](/autogen/blog/tags/gpt)
* [evaluation](/autogen/blog/tags/evaluation)
* [task utility](/autogen/blog/tags/task-utility)

[**Read More**](/autogen/blog/2023/11/20/AgentEval)

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

On this page

# Examples {#examples}

## Automated Multi Agent Chat​ {#automated-multi-agent-chat}

AutoGen offers conversable agents powered by LLM, tool or human, which can be
used to perform tasks collectively via automated chat. This framework allows
tool use and human participation via multi-agent conversation. Please find
documentation about this feature [here](/autogen/docs/Use-Cases/agent_chat).

Links to notebook examples:

  1. **Code Generation, Execution, and Debugging**

    * Automated Task Solving with Code Generation, Execution & Debugging - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_auto_feedback_from_code_execution.ipynb)
    * Automated Code Generation and Question Answering with Retrieval Augmented Agents - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_RetrieveChat.ipynb)
    * Automated Code Generation and Question Answering with [Qdrant](https://qdrant.tech/) based Retrieval Augmented Agents - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_qdrant_RetrieveChat.ipynb)
  2.  **Multi-Agent Collaboration ( >3 Agents)**

    * Automated Task Solving by Group Chat (with 3 group member agents and 1 manager agent) - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_groupchat.ipynb)
    * Automated Data Visualization by Group Chat (with 3 group member agents and 1 manager agent) - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_groupchat_vis.ipynb)
    * Automated Complex Task Solving by Group Chat (with 6 group member agents and 1 manager agent) - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_groupchat_research.ipynb)
    * Automated Task Solving with Coding & Planning Agents - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_planning.ipynb)
    * Automated Task Solving with agents divided into 2 groups - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_hierarchy_flow_using_select_speaker.ipynb)
    * Automated Task Solving with transition paths specified in a graph - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_graph_modelling_language_using_select_speaker.ipynb)
  3.  **Applications**

    * Automated Chess Game Playing & Chitchatting by GPT-4 Agents - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_chess.ipynb)
    * Automated Continual Learning from New Data - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_stream.ipynb)
    * [OptiGuide](https://github.com/microsoft/optiguide) \- Coding, Tool Using, Safeguarding & Question Answering for Supply Chain Optimization
  4.  **Tool Use**

    *  **Web Search** : Solve Tasks Requiring Web Info - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_web_info.ipynb)
    * Use Provided Tools as Functions - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_function_call.ipynb)
    * Use Tools via Sync and Async Function Calling - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_function_call_async.ipynb)
    * Task Solving with Langchain Provided Tools as Functions - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_langchain.ipynb)
    *  **RAG** : Group Chat with Retrieval Augmented Generation (with 5 group member agents and 1 manager agent) - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_groupchat_RAG.ipynb)
    * Function Inception: Enable AutoGen agents to update/remove functions during conversations. - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_inception_function.ipynb)
    * Agent Chat with Whisper - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_video_transcript_translate_with_whisper.ipynb)
    * Constrained Responses via Guidance - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_guidance.ipynb)
  5.  **Human Involvement**

    * Simple example in ChatGPT style [View example](https://github.com/microsoft/autogen/blob/main/samples/simple_chat.py)
    * Auto Code Generation, Execution, Debugging and **Human Feedback** \- [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_human_feedback.ipynb)
    * Automated Task Solving with GPT-4 + **Multiple Human Users** \- [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_two_users.ipynb)
    * Agent Chat with **Async Human Inputs** \- [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/Async_human_input.ipynb)
  6.  **Agent Teaching and Learning**

    * Teach Agents New Skills & Reuse via Automated Chat - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_teaching.ipynb)
    * Teach Agents New Facts, User Preferences and Skills Beyond Coding - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_teachability.ipynb)
    * Teach OpenAI Assistants Through GPTAssistantAgent - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_teachable_oai_assistants.ipynb)
    * Agent Optimizer: Train Agents in an Agentic Way - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_agentoptimizer.ipynb)
  7.  **Multi-Agent Chat with OpenAI Assistants in the loop**

    * Hello-World Chat with OpenAi Assistant in AutoGen - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_oai_assistant_twoagents_basic.ipynb)
    * Chat with OpenAI Assistant using Function Call - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_oai_assistant_function_call.ipynb)
    * Chat with OpenAI Assistant with Code Interpreter - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_oai_code_interpreter.ipynb)
    * Chat with OpenAI Assistant with Retrieval Augmentation - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_oai_assistant_retrieval.ipynb)
    * OpenAI Assistant in a Group Chat - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_oai_assistant_groupchat.ipynb)
  8.  **Multimodal Agent**

    * Multimodal Agent Chat with DALLE and GPT-4V - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_dalle_and_gpt4v.ipynb)
    * Multimodal Agent Chat with Llava - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_llava.ipynb)
    * Multimodal Agent Chat with GPT-4V - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_gpt-4v.ipynb)
  9.  **Long Context Handling**

    * Conversations with Chat History Compression Enabled - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_compression.ipynb)
  10.  **Evaluation and Assessment**

    * AgentEval: A Multi-Agent System for Assess Utility of LLM-powered Applications - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agenteval_cq_math.ipynb)
  11.  **Automatic Agent Building**

    * Automatically Build Multi-agent System with AgentBuilder - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/autobuild_basic.ipynb)
    * Automatically Build Multi-agent System from Agent Library - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/autobuild_agent_library.ipynb)

## Enhanced Inferences​ {#enhanced-inferences}

### Utilities​ {#utilities}

* API Unification - [View Documentation with Code Example](https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference/#api-unification)
* Utility Functions to Help Managing API configurations effectively - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/oai_openai_utils.ipynb)
* Cost Calculation - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/oai_client_cost.ipynb)

### Inference Hyperparameters Tuning​ {#inference-hyperparameters-tuning}

AutoGen offers a cost-effective hyperparameter optimization technique
[EcoOptiGen](https://arxiv.org/abs/2303.04673) for tuning Large Language
Models. The research study finds that tuning hyperparameters can significantly
improve the utility of them. Please find documentation about this feature
[here](/autogen/docs/Use-Cases/enhanced_inference).

Links to notebook examples:

* [Optimize for Code Generation](https://github.com/microsoft/autogen/blob/main/notebook/oai_completion.ipynb) | [Open in colab](https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/oai_completion.ipynb)
* [Optimize for Math](https://github.com/microsoft/autogen/blob/main/notebook/oai_chatgpt_gpt4.ipynb) | [Open in colab](https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/oai_chatgpt_gpt4.ipynb)

[Edit this
page](https://github.com/microsoft/autogen/edit/main/website/docs/Examples.md)

* Automated Multi Agent Chat
* Enhanced Inferences
  * Utilities
  * Inference Hyperparameters Tuning

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
  * automl
    * automl.contrib
    * automl.nlp
    * automl.spark
    * automl.task
    * automl.time_series
      * [sklearn](.docs/reference/automl/time_series/sklearn)
      * [tft](.docs/reference/automl/time_series/tft)
      * [ts_data](.docs/reference/automl/time_series/ts_data)
      * [ts_model](.docs/reference/automl/time_series/ts_model)
    * [automl](.docs/reference/automl/automl)
    * [data](.docs/reference/automl/data)
    * [ml](.docs/reference/automl/ml)
    * [model](.docs/reference/automl/model)
    * [state](.docs/reference/automl/state)
  * default
  * onlineml
  * tune

On this page

# automl.time_series.ts_model {#automltimeseriestsmodel}

## Prophet Objects​ {#prophet-objects}

    class Prophet(TimeSeriesEstimator)  

The class for tuning Prophet.

## ARIMA Objects​ {#arima-objects}

    class ARIMA(StatsModelsEstimator)  

The class for tuning ARIMA.

## SARIMAX Objects​ {#sarimax-objects}

    class SARIMAX(StatsModelsEstimator)  

The class for tuning SARIMA.

## HoltWinters Objects​ {#holtwinters-objects}

    class HoltWinters(StatsModelsEstimator)  

The class for tuning Holt Winters model, aka 'Triple Exponential Smoothing'.

## TS_SKLearn Objects​ {#tssklearn-objects}

    class TS_SKLearn(TimeSeriesEstimator)  

The class for tuning SKLearn Regressors for time-series forecasting

## LGBM_TS Objects​ {#lgbmts-objects}

    class LGBM_TS(TS_SKLearn)  

The class for tuning LGBM Regressor for time-series forecasting

## XGBoost_TS Objects​ {#xgboostts-objects}

    class XGBoost_TS(TS_SKLearn)  

The class for tuning XGBoost Regressor for time-series forecasting

## RF_TS Objects​ {#rfts-objects}

    class RF_TS(TS_SKLearn)  

The class for tuning Random Forest Regressor for time-series forecasting

## ExtraTrees_TS Objects​ {#extratreests-objects}

    class ExtraTrees_TS(TS_SKLearn)  

The class for tuning Extra Trees Regressor for time-series forecasting

## XGBoostLimitDepth_TS Objects​ {#xgboostlimitdepthts-objects}

    class XGBoostLimitDepth_TS(TS_SKLearn)  

The class for tuning XGBoost Regressor with unlimited depth for time-series
forecasting

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/automl/time_series/ts_model.md)

[Previous« ts_data](.docs/reference/automl/time_series/ts_data)

[Nextautoml »](.docs/reference/automl/automl)

* Prophet Objects
* ARIMA Objects
* SARIMAX Objects
* HoltWinters Objects
* TS_SKLearn Objects
* LGBM_TS Objects
* XGBoost_TS Objects
* RF_TS Objects
* ExtraTrees_TS Objects
* XGBoostLimitDepth_TS Objects

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

Recent posts

* [Use flaml.autogen for Local LLMs](.blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](.blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](.blog/2023/05/18/GPT-adaptive-humaneval)
* [Surpassing 1 Million Downloads - A Retrospective and a Look into the Future](.blog/2023/05/07/1M-milestone)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](.blog/2023/04/21/LLM-tuning-math)

# 2 posts tagged with "FLAMLv2" {#2-posts-tagged-with-flamlv2}

[View All Tags](.blog/tags)

## [Use flaml.autogen for Local LLMs](.blog/2023/07/14/Local-LLMs) {#use-flamlautogen-for-local-llmsflamlblog20230714localllms}

July 14, 2023 · 3 min read

[![Jiale
Liu](https://github.com/LeoLjl/leoljl.github.io/blob/main/profile.jpg?raw=true)](https://leoljl.github.io)

[Jiale Liu](https://leoljl.github.io)

Undergraduate student at Xidian University

 **TL;DR:** We demonstrate how to use flaml.autogen for local LLM application.
As an example, we will initiate an endpoint using
[FastChat](https://github.com/lm-sys/FastChat) and perform inference on
[ChatGLMv2-6b](https://github.com/THUDM/ChatGLM2-6B).

## Preparations​ {#preparations}

### Clone FastChat​ {#clone-fastchat}

FastChat provides OpenAI-compatible APIs for its supported models, so you can
use FastChat as a local drop-in replacement for OpenAI APIs. However, its code
needs minor modification in order to function properly.

    git clone https://github.com/lm-sys/FastChat.git  
    cd FastChat  

### Download checkpoint​ {#download-checkpoint}

ChatGLM-6B is an open bilingual language model based on General Language Model
(GLM) framework, with 6.2 billion parameters. ChatGLM2-6B is its second-
generation version.

Before downloading from HuggingFace Hub, you need to have Git LFS
[installed](<https://docs.github.com/en/repositories/working-with->
files/managing-large-files/installing-git-large-file-storage).

    git clone https://huggingface.co/THUDM/chatglm2-6b  

## Initiate server​ {#initiate-server}

First, launch the controller

    python -m fastchat.serve.controller  

Then, launch the model worker(s)

    python -m fastchat.serve.model_worker --model-path chatglm2-6b  

Finally, launch the RESTful API server

    python -m fastchat.serve.openai_api_server --host localhost --port 8000  

Normally this will work. However, if you encounter error like
[this](https://github.com/lm-sys/FastChat/issues/1641), commenting out all the
lines containing `finish_reason` in `fastchat/protocol/api_protocal.py` and
`fastchat/protocol/openai_api_protocol.py` will fix the problem. The modified
code looks like:

    class CompletionResponseChoice(BaseModel):  
        index: int  
        text: str  
        logprobs: Optional[int] = None  
        # finish_reason: Optional[Literal["stop", "length"]]  
      
    class CompletionResponseStreamChoice(BaseModel):  
        index: int  
        text: str  
        logprobs: Optional[float] = None  
        # finish_reason: Optional[Literal["stop", "length"]] = None  

## Interact with model using `oai.Completion`​ {#interact-with-model-using-oaicompletion}

Now the models can be directly accessed through openai-python library as well
as `flaml.oai.Completion` and `flaml.oai.ChatCompletion`.

    from flaml import oai  
      
    # create a text completion request  
    response = oai.Completion.create(  
        config_list=[  
            {  
                "model": "chatglm2-6b",  
                "api_base": "http://localhost:8000/v1",  
                "api_type": "open_ai",  
                "api_key": "NULL", # just a placeholder  
            }  
        ],  
        prompt="Hi",  
    )  
    print(response)  
      
    # create a chat completion request  
    response = oai.ChatCompletion.create(  
        config_list=[  
            {  
                "model": "chatglm2-6b",  
                "api_base": "http://localhost:8000/v1",  
                "api_type": "open_ai",  
                "api_key": "NULL",  
            }  
        ],  
        messages=[{"role": "user", "content": "Hi"}]  
    )  
    print(response)  

If you would like to switch to different models, download their checkpoints
and specify model path when launching model worker(s).

## interacting with multiple local LLMs​ {#interacting-with-multiple-local-llms}

If you would like to interact with multiple LLMs on your local machine,
replace the `model_worker` step above with a multi model variant:

    python -m fastchat.serve.multi_model_worker \  
        --model-path lmsys/vicuna-7b-v1.3 \  
        --model-names vicuna-7b-v1.3 \  
        --model-path chatglm2-6b \  
        --model-names chatglm2-6b  

The inference code would be:

    from flaml import oai  
      
    # create a chat completion request  
    response = oai.ChatCompletion.create(  
        config_list=[  
            {  
                "model": "chatglm2-6b",  
                "api_base": "http://localhost:8000/v1",  
                "api_type": "open_ai",  
                "api_key": "NULL",  
            },  
            {  
                "model": "vicuna-7b-v1.3",  
                "api_base": "http://localhost:8000/v1",  
                "api_type": "open_ai",  
                "api_key": "NULL",  
            }  
        ],  
        messages=[{"role": "user", "content": "Hi"}]  
    )  
    print(response)  

## For Further Reading​ {#for-further-reading} {#for-further-reading}

* [Documentation](https://microsoft.github.io/autogen/) about `flaml.autogen`
* [Documentation](https://github.com/lm-sys/FastChat) about FastChat.

 **Tags:**

* [LLM](.blog/tags/llm)
* [FLAMLv2](.blog/tags/flam-lv-2)

[**Read More**](.blog/2023/07/14/Local-LLMs)

## [Surpassing 1 Million Downloads - A Retrospective and a Look into the {#surpassing-1-million-downloads-a-retrospective-and-a-look-into-the}

Future](.blog/2023/05/07/1M-milestone)

May 7, 2023 · 4 min read

[![Qingyun Wu](https://github.com/qingyun-wu.png)](<https://qingyun->
wu.github.io/)

[Qingyun Wu](https://qingyun-wu.github.io/)

Assistant Professor at the Pennsylvania State University

 **TL;DR:**

* **Celebrating FLAML 's milestone: 1 million downloads**
* **Introducing Large Language Model (LLM) support in the upcoming FLAML v2**

This week, FLAML has reached a significant milestone: 1 million downloads.
Originating as an intern research project within Microsoft Research, FLAML has
grown into an open-source library used widely across the industry and
supported by an active community. As we celebrate this milestone, we want to
recognize the passionate contributors and users who have played an essential
role in molding FLAML into the flourishing project it is today. Our heartfelt
gratitude goes out to each of you for your unwavering support, constructive
feedback, and innovative contributions that have driven FLAML to new heights.
A big shoutout to our industrial collaborators from Azure Core, Azure Machine
Learning, Azure Synapse Analytics, Microsoft 365, ML.NET, Vowpal Wabbit,
Anyscale, Databricks, and Wise; and academic collaborators from MIT, Penn
State University, Stevens Institute of Technology, Tel Aviv University, Texas
A & M University, University of Manchester, University of Washington, and The
Chinese University of Hong Kong etc.

We'd also like to take the opportunity to reflect on FLAML's past achievements
and its future roadmap, with a particular focus on large language models (LLM)
and LLMOps.

## FLAML's Journey: Past Achievements and Milestones​ {#flamls-journey-past-achievements-and-milestones}

### Bring AutoML to One's Fingertips​ {#bring-automl-to-ones-fingertips}

FLAML offers an off-the-shelf AutoML solution that enables users to quickly
discover high-quality models or configurations for common ML/AI tasks. By
automatically selecting models and hyperparameters for training or inference,
FLAML saves users time and effort. FLAML has significantly reduced development
time for developers and data scientists alike, while also providing a
convenient way to integrate new algorithms into the pipeline, enabling easy
extensions and large-scale parallel tuning. These features make FLAML a
valuable tool in R&D efforts for many enterprise users. FLAML is capable of
handling a variety of common ML tasks, such as
[classification](<https://microsoft.github.io.docs/Examples/AutoML->
Classification),
[regression](<https://microsoft.github.io.docs/Examples/AutoML->
Regression), [time series
forecasting](<https://microsoft.github.io.docs/Examples/AutoML->
Time%20series%20forecast), [NLP
tasks](https://microsoft.github.io.docs/Examples/AutoML-Rank), and
[generative tasks](https://microsoft.github.io/autogen/), providing a
comprehensive solution for various applications.

### Speed and Efficiency: The FLAML Advantage​ {#speed-and-efficiency-the-flaml-advantage}

What sets FLAML apart from other AutoML libraries is its exceptional
efficiency, thanks to the economical and efficient hyperparameter optimization
and model selection methods developed in our
[research](https://microsoft.github.io.docs/Research). FLAML is also
capable of handling large search spaces with heterogeneous evaluation costs,
complex constraints, guidance, and early stopping. The [zero-shot
AutoML](https://microsoft.github.io.docs/Use-Cases/Zero-Shot-AutoML)
option further reduces the cost of AutoML, making FLAML an even more
attractive solution for a wide range of applications with low resources.

### Easy Customization and Extensibility​ {#easy-customization-and-extensibility}

FLAML is designed for easy extensibility and customization, allowing users to
add custom learners, metrics, search space, etc. For example, the support of
hierarchical search spaces allows one to first choose an ML learner and then
sampling from the hyperparameter space specific to that learner. The level of
customization ranges from minimal (providing only training data and task type
as input) to full (tuning a user-defined function). This flexibility and
support for easy customization have led to FLAML's adoption in various
domains, including security, finance, marketing, engineering, supply chain,
insurance, and healthcare, delivering highly accurate results.

## Embracing Large Language Models in FLAML v2​ {#embracing-large-language-models-in-flaml-v2}

As large language models continue to reshape the AI ecosystem, FLAML is poised
to adapt and grow alongside these advancements. Recognizing the importance of
large language models, we have recently incorporated an autogen package into
FLAML, and are committed to focusing our collective efforts on addressing the
unique challenges that arise in LLMOps (Large Language Model Operations).

In its current iteration, FLAML offers support for model selection and
inference parameter tuning for large language models. We are actively working
on the development of new features, such as low-level inference API with
caching, templating, filtering, and higher-level components like LLM-based
coding and interactive agents, to enable more effective and economical usage
of LLM.

We are eagerly preparing for the launch of FLAML v2, where we will place
special emphasis on incorporating and enhancing features specifically tailored
for large language models (LLMs), further expanding FLAML's capabilities. We
invite contributions from anyone interested in this topic and look forward to
collaborating with the community as we shape the future of FLAML and LLMOps
together.

## For Further Reading​ {#for-further-reading} {#for-further-reading}

* [Documentation about `flaml.autogen`](https://microsoft.github.io/autogen/)
* [Code Example: Tune chatGPT for Math Problem Solving with FLAML](https://github.com/microsoft.blob/main/notebook/autogen_chatgpt_gpt4.ipynb)

 _Do you have any experience to share about LLM applications? Do you like to
see more support or research of LLMOps? Please join
our[Discord](https://discord.gg/Cppx2vSPVP) server for discussion._

 **Tags:**

* [LLM](.blog/tags/llm)
* [LLMOps](.blog/tags/llm-ops)
* [FLAMLv2](.blog/tags/flam-lv-2)

[**Read More**](.blog/2023/05/07/1M-milestone)

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

Recent posts

* [All About Agent Descriptions](/autogen/blog/2023/12/29/AgentDescriptions)
* [AgentOptimizer - An Agentic Way to Train Your LLM Agent](/autogen/blog/2023/12/23/AgentOptimizer)
* [AutoGen Studio: Interactively Explore Multi-Agent Workflows](/autogen/blog/2023/12/01/AutoGenStudio)
* [Agent AutoBuild - Automatically Building Multi-agent Systems](/autogen/blog/2023/11/26/Agent-AutoBuild)
* [How to Assess Utility of LLM-powered Applications?](/autogen/blog/2023/11/20/AgentEval)
* [AutoGen Meets GPTs](/autogen/blog/2023/11/13/OAI-assistants)
* [EcoAssistant - Using LLM Assistants More Accurately and Affordably](/autogen/blog/2023/11/09/EcoAssistant)
* [Multimodal with GPT-4V and LLaVA](/autogen/blog/2023/11/06/LMM-Agent)
* [AutoGen's Teachable Agents](/autogen/blog/2023/10/26/TeachableAgent)
* [Retrieval-Augmented Generation (RAG) Applications with AutoGen](/autogen/blog/2023/10/18/RetrieveChat)
* [Use AutoGen for Local LLMs](/autogen/blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](/autogen/blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](/autogen/blog/2023/05/18/GPT-adaptive-humaneval)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](/autogen/blog/2023/04/21/LLM-tuning-math)

# AutoGen Meets GPTs {#autogen-meets-gpts}

November 13, 2023 · 3 min read

[![Gagan
Bansal](https://github.com/gagb.png)](<https://www.linkedin.com/in/gagan->
bansal/)

[Gagan Bansal](https://www.linkedin.com/in/gagan-bansal/)

Senior Researcher at Microsoft Research

![OpenAI
Assistant](/autogen/assets/images/teaser-0b85f5efe5965c70b376b39a4ac34b4e.jpg)

 _AutoGen enables collaboration among multiple ChatGPTs for complex tasks._

## TLDR​ {#tldr}

OpenAI assistants are now integrated into AutoGen via
[`GPTAssistantAgent`](https://github.com/microsoft/autogen/blob/main/autogen/agentchat/contrib/gpt_assistant_agent.py).
This enables multiple OpenAI assistants, which form the backend of the now
popular GPTs, to collaborate and tackle complex tasks. Checkout example
notebooks for reference:

* [Basic example](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_oai_assistant_twoagents_basic.ipynb)
* [Code interpreter](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_oai_code_interpreter.ipynb)
* [Function calls](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_oai_assistant_function_call.ipynb)

## Introduction​ {#introduction}

Earlier last week, OpenAI introduced
[GPTs](https://openai.com/blog/introducing-gpts), giving users ability to
create custom ChatGPTs tailored for them. _But what if these individual GPTs
could collaborate to do even more?_ Fortunately, because of AutoGen, this is
now a reality! AutoGen has been pioneering agents and supporting [multi-agent
workflows](https://aka.ms/autogen-pdf) since earlier this year, and now
(starting with version 0.2.0b5) we are introducing compatibility with the
[Assistant API](https://openai.com/blog/introducing-gpts), which is currently
in beta preview.

To accomplish this, we've added a new (experimental) agent called the
`GPTAssistantAgent` that lets you seamlessly add these new OpenAI assistants
into AutoGen-based multi-agent workflows. This integration shows great
potential and synergy, and we plan to continue enhancing it.

## Installation​ {#installation}

    pip install pyautogen==0.2.0b5  

## Basic Example​ {#basic-example}

Here's a basic example that uses a `UserProxyAgent` to allow an interface with
the `GPTAssistantAgent`.

First, import the new agent and setup `config_list`:

    from autogen import config_list_from_json  
    from autogen.agentchat.contrib.gpt_assistant_agent import GPTAssistantAgent  
    from autogen import UserProxyAgent  
      
    config_list = config_list_from_json("OAI_CONFIG_LIST")  

Then simply define the OpenAI assistant agent and give it the task!

    # creates new assistant using Assistant API  
    gpt_assistant = GPTAssistantAgent(  
        name="assistant",  
        llm_config={  
            "config_list": config_list,  
            "assistant_id": None  
        })  
      
    user_proxy = UserProxyAgent(name="user_proxy",  
        code_execution_config={  
            "work_dir": "coding"  
        },  
        human_input_mode="NEVER")  
      
    user_proxy.initiate_chat(gpt_assistant, message="Print hello world")  

`GPTAssistantAgent` supports both creating new OpenAI assistants or reusing
existing assistants (e.g, by providing an `assistant_id`).

## Code Interpreter Example​ {#code-interpreter-example}

`GPTAssistantAgent` allows you to specify an OpenAI tools (e.g., function
calls, code interpreter, etc). The example below enables an assistant that can
use OpenAI code interpreter to solve tasks.

    # creates new assistant using Assistant API  
    gpt_assistant = GPTAssistantAgent(  
        name="assistant",  
        llm_config={  
            "config_list": config_list,  
            "assistant_id": None,  
            "tools": [  
                {  
                    "type": "code_interpreter"  
                }  
            ],  
        })  
      
    user_proxy = UserProxyAgent(name="user_proxy",  
        code_execution_config={  
            "work_dir": "coding"  
        },  
        human_input_mode="NEVER")  
      
    user_proxy.initiate_chat(gpt_assistant, message="Print hello world")  

Checkout more examples
[here](https://github.com/microsoft/autogen/tree/main/notebook).

## Limitations and Future Work​ {#limitations-and-future-work}

* Group chat managers using GPT assistant are pending.
* GPT assistants with multimodal capabilities haven't been released yet but we are committed to support them.

## Acknowledgements​ {#acknowledgements}

`GPTAssistantAgent` was made possible through collaboration with
[@IANTHEREAL](https://github.com/IANTHEREAL), [Jiale
Liu](https://leoljl.github.io), [Yiran Wu](https://github.com/kevin666aa),
[Qingyun Wu](https://qingyun-wu.github.io/), [Chi
Wang](https://www.microsoft.com/en-us/research/people/chiw/), and many other
AutoGen maintainers.

 **Tags:**

* [openai-assistant](/autogen/blog/tags/openai-assistant)

[Newer Post« How to Assess Utility of LLM-powered
Applications?](/autogen/blog/2023/11/20/AgentEval)

[Older PostEcoAssistant - Using LLM Assistants More Accurately and Affordably
»](/autogen/blog/2023/11/09/EcoAssistant)

* TLDR
* Introduction
* Installation
* Basic Example
* Code Interpreter Example
* Limitations and Future Work
* Acknowledgements

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
    * autogen.agentchat
      * [agent](.docs/reference/autogen/agentchat/agent)
      * [assistant_agent](.docs/reference/autogen/agentchat/assistant_agent)
      * [conversable_agent](.docs/reference/autogen/agentchat/conversable_agent)
      * [groupchat](.docs/reference/autogen/agentchat/groupchat)
      * [user_proxy_agent](.docs/reference/autogen/agentchat/user_proxy_agent)
    * autogen.oai
    * [code_utils](.docs/reference/autogen/code_utils)
    * [math_utils](.docs/reference/autogen/math_utils)
    * [retrieve_utils](.docs/reference/autogen/retrieve_utils)
  * automl
  * default
  * onlineml
  * tune

On this page

# autogen.agentchat.assistant_agent {#autogenagentchatassistantagent}

## AssistantAgent Objects​ {#assistantagent-objects}

    class AssistantAgent(ConversableAgent)  

(In preview) Assistant agent, designed to solve a task with LLM.

AssistantAgent is a subclass of ConversableAgent configured with a default
system message. The default system message is designed to solve a task with
LLM, including suggesting python code blocks and debugging. `human_input_mode`
is default to "NEVER" and `code_execution_config` is default to False. This
agent doesn't execute code by default, and expects the user to execute the
code.

#### __init__​ {#init}
```python
    def __init__(name: str, system_message: Optional[str] = DEFAULT_SYSTEM_MESSAGE, llm_config: Optional[Union[Dict, bool]] = None, is_termination_msg: Optional[Callable[[Dict], bool]] = None, max_consecutive_auto_reply: Optional[int] = None, human_input_mode: Optional[str] = "NEVER", code_execution_config: Optional[Union[Dict, bool]] = False, **kwargs, ,)  

 **Arguments** :

* `name` _str_ \- agent name.
* `system_message` _str_ \- system message for the ChatCompletion inference. Please override this attribute if you want to reprogram the agent.
* `llm_config` _dict_ \- llm inference configuration. Please refer to [autogen.Completion.create](.docs/reference/autogen/oai/completion#create) for available options.
* `is_termination_msg` _function_ \- a function that takes a message in the form of a dictionary and returns a boolean value indicating if this received message is a termination message. The dict can contain the following keys: "content", "role", "name", "function_call".
* `max_consecutive_auto_reply` _int_ \- the maximum number of consecutive auto replies. default to None (no limit provided, class attribute MAX_CONSECUTIVE_AUTO_REPLY will be used as the limit in this case). The limit only plays a role when human_input_mode is not "ALWAYS".
* `**kwargs` _dict_ \- Please refer to other kwargs in [ConversableAgent](.docs/reference/autogen/agentchat/conversable_agent#__init__).

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/autogen/agentchat/assistant_agent.md)

[Previous« agent](.docs/reference/autogen/agentchat/agent)

[Nextconversable_agent
»](.docs/reference/autogen/agentchat/conversable_agent)

* AssistantAgent Objects

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

Recent posts

* [All About Agent Descriptions](/autogen/blog/2023/12/29/AgentDescriptions)
* [AgentOptimizer - An Agentic Way to Train Your LLM Agent](/autogen/blog/2023/12/23/AgentOptimizer)
* [AutoGen Studio: Interactively Explore Multi-Agent Workflows](/autogen/blog/2023/12/01/AutoGenStudio)
* [Agent AutoBuild - Automatically Building Multi-agent Systems](/autogen/blog/2023/11/26/Agent-AutoBuild)
* [How to Assess Utility of LLM-powered Applications?](/autogen/blog/2023/11/20/AgentEval)
* [AutoGen Meets GPTs](/autogen/blog/2023/11/13/OAI-assistants)
* [EcoAssistant - Using LLM Assistants More Accurately and Affordably](/autogen/blog/2023/11/09/EcoAssistant)
* [Multimodal with GPT-4V and LLaVA](/autogen/blog/2023/11/06/LMM-Agent)
* [AutoGen's Teachable Agents](/autogen/blog/2023/10/26/TeachableAgent)
* [Retrieval-Augmented Generation (RAG) Applications with AutoGen](/autogen/blog/2023/10/18/RetrieveChat)
* [Use AutoGen for Local LLMs](/autogen/blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](/autogen/blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](/autogen/blog/2023/05/18/GPT-adaptive-humaneval)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](/autogen/blog/2023/04/21/LLM-tuning-math)

# 2 posts tagged with "AutoGen" {#2-posts-tagged-with-autogen}

[View All Tags](/autogen/blog/tags)

## [All About Agent Descriptions](/autogen/blog/2023/12/29/AgentDescriptions) {#all-about-agent-descriptionsautogenblog20231229agentdescriptions}

December 29, 2023 · 9 min read

[![Adam
Fourney](https://github.com/afourney.png)](https://www.adamfourney.com)

[Adam Fourney](https://www.adamfourney.com)

Principal Researcher Microsoft Research

## TLDR​ {#tldr} {#tldr}

AutoGen 0.2.2 introduces a
[description](https://microsoft.github.io/autogen/docs/reference/agentchat/conversable_agent#__init__)
field to ConversableAgent (and all subclasses), and changes GroupChat so that
it uses agent `description`s rather than `system_message`s when choosing which
agents should speak next.

This is expected to simplify GroupChat’s job, improve orchestration, and make
it easier to implement new GroupChat or GroupChat-like alternatives.

If you are a developer, and things were already working well for you, no
action is needed -- backward compatibility is ensured because the
`description` field defaults to the `system_message` when no description is
provided.

However, if you were struggling with getting GroupChat to work, you can now
try updating the `description` field.

## Introduction​ {#introduction} {#introduction}

As AutoGen matures and developers build increasingly complex combinations of
agents, orchestration is becoming an important capability. At present,
[GroupChat](<https://microsoft.github.io/autogen/docs/reference/agentchat/groupchat#groupchat->
objects) and the
[GroupChatManager](<https://microsoft.github.io/autogen/docs/reference/agentchat/groupchat#groupchatmanager->
objects) are the main built-in tools for orchestrating conversations between 3
or more agents. For orchestrators like GroupChat to work well, they need to
know something about each agent so that they can decide who should speak and
when. Prior to AutoGen 0.2.2, GroupChat relied on each agent's
`system_message` and `name` to learn about each participating agent. This is
likely fine when the system prompt is short and sweet, but can lead to
problems when the instructions are very long (e.g., with the
[AssistantAgent](https://microsoft.github.io/autogen/docs/reference/agentchat/assistant_agent)),
or non-existent (e.g., with the
[UserProxyAgent](https://microsoft.github.io/autogen/docs/reference/agentchat/user_proxy_agent)).

AutoGen 0.2.2 introduces a
[description](https://microsoft.github.io/autogen/docs/reference/agentchat/conversable_agent#__init__)
field to all agents, and replaces the use of the `system_message` for
orchestration in GroupChat and all future orchestrators. The `description`
field defaults to the `system_message` to ensure backwards compatibility, so
you may not need to change anything with your code if things are working well
for you. However, if you were struggling with GroupChat, give setting the
`description` field a try.

The remainder of this post provides an example of how using the `description`
field simplifies GroupChat's job, provides some evidence of its effectiveness,
and provides tips for writing good descriptions.

## Example​ {#example}

The current GroupChat orchestration system prompt has the following template:

    You are in a role play game. The following roles are available:  
      
    {self._participant_roles(agents)}.  
      
    Read the following conversation.  
    Then select the next role from {[agent.name for agent in agents]} to play. Only return the role.  

Suppose that you wanted to include 3 agents: A UserProxyAgent, an
AssistantAgent, and perhaps a GuardrailsAgent.

Prior to 0.2.2, this template would expand to:

    You are in a role play game. The following roles are available:  
      
    assistant: You are a helpful AI assistant.  
    Solve tasks using your coding and language skills.  
    In the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for the user to execute.  
    1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time, check the operating system. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself.  
    2. When you need to perform some task with code, use the code to perform the task and output the result. Finish the task smartly.  
    Solve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill.  
    When using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can't modify your code. So do not suggest incomplete code which requires users to modify. Don't use a code block if it's not intended to be executed by the user.  
    If you want the user to save the code in a file before executing it, put # filename: <filename> inside the code block as the first line. Don't include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use 'print' function for the output when relevant. Check the execution result returned by the user.  
    If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.  
    When you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible.  
    Reply "TERMINATE" in the end when everything is done.  
    user_proxy:  
    guardrails_agent: You are a guardrails agent and are tasked with ensuring that all parties adhere to the following responsible AI policies:  
    - You MUST TERMINATE the conversation if it involves writing or running HARMFUL or DESTRUCTIVE code.  
    - You MUST TERMINATE the conversation if it involves discussions of anything relating to hacking, computer exploits, or computer security.  
    - You MUST TERMINATE the conversation if it involves violent or graphic content such as Harm to Others, Self-Harm, Suicide.  
    - You MUST TERMINATE the conversation if it involves demeaning speech, hate speech, discriminatory remarks, or any form of harassment based on race, gender, sexuality, religion, nationality, disability, or any other protected characteristic.  
    - You MUST TERMINATE the conversation if it involves seeking or giving  advice in highly regulated domains such as medical advice, mental health, legal advice or financial advice  
    - You MUST TERMINATE the conversation if it involves illegal activities including when encouraging or providing guidance on illegal activities.  
    - You MUST TERMINATE the conversation if it involves manipulative or deceptive Content including scams, phishing and spread false information.  
    - You MUST TERMINATE the conversation if it involves involve sexually explicit content or discussions.  
    - You MUST TERMINATE the conversation if it involves sharing or soliciting personal, sensitive, or confidential information from users. This includes financial details, health records, and other private matters.  
    - You MUST TERMINATE the conversation if it involves deep personal problems such as dealing with serious personal issues, mental health concerns, or crisis situations.  
    If you decide that the conversation must be terminated, explain your reasoning then output the uppercase word "TERMINATE". If, on the other hand, you decide the conversation is acceptable by the above standards, indicate as much, then ask the other parties to proceed.  
      
    Read the following conversation.  
    Then select the next role from [assistant, user_proxy, guardrails_agent] to play. Only return the role.  

As you can see, this description is super confusing:

* It is hard to make out where each agent's role-description ends
* `You` appears numerous times, and refers to three separate agents (GroupChatManager, AssistantAgent, and GuardrailsAgent)
* It takes a lot of tokens!

Consequently, it's not hard to see why the GroupChat manager sometimes
struggles with this orchestration task.

With AutoGen 0.2.2 onward, GroupChat instead relies on the description field.
With a description field the orchestration prompt becomes:

    You are in a role play game. The following roles are available:  
      
    assistant: A helpful and general-purpose AI assistant that has strong language skills, Python skills, and Linux command line skills.  
    user_proxy: A user that can run Python code or input command line commands at a Linux terminal and report back the execution results.  
    guradrails_agent: An agent that ensures the conversation conforms to responsible AI guidelines.  
      
    Read the following conversation.  
    Then select the next role from [assistant, user_proxy, guardrails_agent] to play. Only return the role.  

This is much easier to parse and understand, and it doesn't use nearly as many
tokens. Moreover, the following experiment provides early evidence that it
works.

## An Experiment with Distraction​ {#an-experiment-with-distraction}

To illustrate the impact of the `description` field, we set up a three-agent
experiment with a reduced 26-problem subset of the HumanEval benchmark. Here,
three agents were added to a GroupChat to solve programming problems. The
three agents were:

* Coder (default Assistant prompt)
* UserProxy (configured to execute code)
* ExecutiveChef (added as a distraction)

The Coder and UserProxy used the AssistantAgent and UserProxy defaults
(provided above), while the ExecutiveChef was given the system prompt:

    You are an executive chef with 28 years of industry experience. You can answer questions about menu planning, meal preparation, and cooking techniques.  

The ExecutiveChef is clearly the distractor here -- given that no HumanEval
problems are food-related, the GroupChat should rarely consult with the chef.
However, when configured with GPT-3.5-turbo-16k, we can clearly see the
GroupChat struggling with orchestration:

#### With versions prior to 0.2.2, using `system_message`:​ {#with-versions-prior-to-022-using-systemmessage}

* The Agents solve 3 out of 26 problems on their first turn
* The ExecutiveChef is called upon 54 times! (almost as much as the Coder at 68 times)

#### With version 0.2.2, using `description`:​ {#with-version-022-using-description}

* The Agents solve 7 out of 26 problems on the first turn
* The ExecutiveChef is called upon 27 times! (versus 84 times for the Coder)

Using the `description` field doubles performance on this task and halves the
incidence of calling upon the distractor agent.

## Tips for Writing Good Descriptions​ {#tips-for-writing-good-descriptions}

Since `descriptions` serve a different purpose than `system_message`s, it is
worth reviewing what makes a good agent description. While descriptions are
new, the following tips appear to lead to good results:

* Avoid using the 1st or 2nd person perspective. Descriptions should not contain "I" or "You", unless perhaps "You" is in reference to the GroupChat / orchestrator
* Include any details that might help the orchestrator know when to call upon the agent
* Keep descriptions short (e.g., "A helpful AI assistant with strong natural language and Python coding skills.").

The main thing to remember is that **the description is for the benefit of the
GroupChatManager, not for the Agent 's own use or instruction**.

## Conclusion​ {#conclusion}

AutoGen 0.2.2 introduces a `description`, becoming the main way agents
describe themselves to orchestrators like GroupChat. Since the `description`
defaults to the `system_message`, there's nothing you need to change if you
were already satisfied with how your group chats were working. However, we
expect this feature to generally improve orchestration, so please consider
experimenting with the `description` field if you are struggling with
GroupChat or want to boost performance.

 **Tags:**

* [AutoGen](/autogen/blog/tags/auto-gen)

[**Read More**](/autogen/blog/2023/12/29/AgentDescriptions)

## [AutoGen Studio: Interactively Explore Multi-Agent {#autogen-studio-interactively-explore-multiagent}

Workflows](/autogen/blog/2023/12/01/AutoGenStudio)

December 1, 2023 · 8 min read

[![Victor
Dibia](https://github.com/victordibia.png)](https://github.com/victordibia)

[Victor Dibia](https://github.com/victordibia)

Principal RSDE at Microsoft Research

[![Gagan
Bansal](https://github.com/gagb.png)](<https://www.linkedin.com/in/gagan->
bansal/)

[Gagan Bansal](https://www.linkedin.com/in/gagan-bansal/)

Senior Researcher at Microsoft Research

[![Saleema
Amershi](https://github.com/samershi.png)](https://github.com/samershi)

[Saleema Amershi](https://github.com/samershi)

Senior Principal Research Manager at Microsoft Research

![AutoGen Studio Playground View: Solving a task with multiple agents that
generate a pdf document with
images.](/autogen/assets/images/autogenstudio_home-
cce78dc150d1bb0073620754df73d863.png)

 _AutoGen Studio: Solving a task with multiple agents that generate a pdf
document with images._

## TLDR​ {#tldr} {#tldr}

To help you rapidly prototype multi-agent solutions for your tasks, we are
introducing AutoGen Studio, an interface powered by
[AutoGen](https://github.com/microsoft/autogen/tree/main/autogen). It allows
you to:

* Declaratively define and modify agents and multi-agent workflows through a point and click, drag and drop interface (e.g., you can select the parameters of two agents that will communicate to solve your task).
* Use our UI to create chat sessions with the specified agents and view results (e.g., view chat history, generated files, and time taken).
* Explicitly add skills to your agents and accomplish more tasks.
* Publish your sessions to a local gallery.

AutoGen Studio is open source [code
here](<https://github.com/microsoft/autogen/tree/main/samples/apps/autogen->
studio), and can be installed via pip. Give it a try!

    pip install autogenstudio  

## Introduction​ {#introduction} {#introduction}

The accelerating pace of technology has ushered us into an era where digital
assistants (or agents) are becoming integral to our lives.
[AutoGen](https://github.com/microsoft/autogen/tree/main/autogen) has emerged
as a leading framework for orchestrating the power of agents. In the spirit of
expanding this frontier and democratizing this capability, we are thrilled to
introduce a new user-friendly interface: **AutoGen Studio**.

With AutoGen Studio, users can rapidly create, manage, and interact with
agents that can learn, adapt, and collaborate. As we release this interface
into the open-source community, our ambition is not only to enhance
productivity but to inspire a level of personalized interaction between humans
and agents.

> **Note** : AutoGen Studio is meant to help you rapidly prototype multi-
> agent workflows and demonstrate an example of end user interfaces built with
> AutoGen. It is not meant to be a production-ready app.

## Getting Started with AutoGen Studio​ {#getting-started-with-autogen-studio}

The following guide will help you get AutoGen Studio up and running on your
system.

### Configuring an LLM Provider​ {#configuring-an-llm-provider}

To get started, you need access to a language model. You can get this set up
by following the steps in the AutoGen documentation
[here](https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints).
Configure your environment with either `OPENAI_API_KEY` or
`AZURE_OPENAI_API_KEY`.

For example, in your terminal, you would set the API key like this:

    export OPENAI_API_KEY=<your_api_key>  

You can also specify the model directly in the agent's configuration as shown
below.

    llm_config = LLMConfig(  
        config_list=[{  
            "model": "gpt-4",  
            "api_key": "<azure_api_key>",  
            "base_url": "<azure api base url>",  
            "api_type": "azure",  
            "api_version": "2023-06-01-preview"  
        }],  
        temperature=0,  
    )  

### Installation​ {#installation}

  1. **Install from PyPi**

We recommend using a virtual environment (e.g., conda) to avoid conflicts with
existing Python packages. With Python 3.10 or newer active in your virtual
environment, use pip to install AutoGen Studio:

        pip install autogenstudio  

  2. **Install from Source**

> Note: This approach requires some familiarity with building interfaces in
> React.

If you prefer to install from source, ensure you have Python 3.10+ and Node.js
(version above 14.15.0) installed. Here's how you get started:

    * Clone the AutoGen Studio repository and install its Python dependencies:
        
                pip install -e .  

    * Navigate to the `samples/apps/autogen-studio/frontend` directory, install dependencies, and build the UI:
        
                npm install -g gatsby-cli  
        npm install --global yarn  
        cd frontend  
        yarn install  
        yarn build  

For Windows users, to build the frontend, you may need alternative commands
provided in the [autogen studio
readme](<https://github.com/microsoft/autogen/tree/main/samples/apps/autogen->
studio).

### Running the Application​ {#running-the-application}

Once installed, run the web UI by entering the following in your terminal:

    autogenstudio ui --port 8081  

This will start the application on the specified port. Open your web browser
and go to `http://localhost:8081/` to begin using AutoGen Studio.

Now that you have AutoGen Studio installed and running, you are ready to
explore its capabilities, including defining and modifying agent workflows,
interacting with agents and sessions, and expanding agent skills.

## What Can You Do with AutoGen Studio?​ {#what-can-you-do-with-autogen-studio}

The AutoGen Studio UI is organized into 3 high level sections - **Build** ,
**Playground** , and **Gallery**.

### Build​ {#build}

![Specify
Agents.](/autogen/assets/images/autogenstudio_config-5e0107b565503d4952c9b00dd3d5c10e.png)

This section focuses on defining the properties of agents and agent workflows.
It includes the following concepts:

 **Skills** : Skills are functions (e.g., Python functions) that describe how
to solve a task. In general, a good skill has a descriptive name (e.g.
`generate_images`), extensive docstrings and good defaults (e.g., writing out
files to disk for persistence and reuse). You can add new skills to AutoGen
Studio via the provided UI. At inference time, these skills are made available
to the assistant agent as they address your tasks.

![View and add
skills.](/autogen/assets/images/autogenstudio_skills-7376f0a5aec5917fda817a4793f2b89a.png)

 _AutoGen Studio Build View: View, add or edit skills that an agent can
leverage in addressing tasks._

 **Agents** : This provides an interface to declaratively specify properties
for an AutoGen agent (mirrors most of the members of a base [AutoGen
conversable
agent](https://github.com/microsoft/autogen/blob/main/autogen/agentchat/conversable_agent.py)
class).

 **Agent Workflows** : An agent workflow is a specification of a set of agents
that can work together to accomplish a task. The simplest version of this is a
setup with two agents – a user proxy agent (that represents a user i.e. it
compiles code and prints result) and an assistant that can address task
requests (e.g., generating plans, writing code, evaluating responses,
proposing error recovery steps, etc.). A more complex flow could be a group
chat where even more agents work towards a solution.

### Playground​ {#playground}

![AutoGen Studio Playground View: Solving a task with multiple agents that
generate a pdf document with
images.](/autogen/assets/images/autogenstudio_home-
cce78dc150d1bb0073620754df73d863.png)

 _AutoGen Studio Playground View: Agents collaborate, use available skills
(ability to generate images) to address a user task (generate pdf 's)._

The playground section is focused on interacting with agent workflows defined
in the previous build section. It includes the following concepts:

 **Session** : A session refers to a period of continuous interaction or
engagement with an agent workflow, typically characterized by a sequence of
activities or operations aimed at achieving specific objectives. It includes
the agent workflow configuration, the interactions between the user and the
agents. A session can be “published” to a “gallery”.

 **Chat View** : A chat is a sequence of interactions between a user and an
agent. It is a part of a session.

### Gallery​ {#gallery}

This section is focused on sharing and reusing artifacts (e.g., workflow
configurations, sessions, etc.).

AutoGen Studio comes with 3 example skills: `fetch_profile`, `find_papers`,
`generate_images`. Please feel free to review the repo to learn more about how
they work.

## The AutoGen Studio API​ {#the-autogen-studio-api}

While AutoGen Studio is a web interface, it is powered by an underlying python
API that is reusable and modular. Importantly, we have implemented an API
where agent workflows can be declaratively specified (in JSON), loaded and
run. An example of the current API is shown below. Please consult the [AutoGen
Studio repo](https://microsoft.github.io/autogen/docs/autogenstudio) for more
details.

    import json  
    from autogenstudio import AutoGenWorkFlowManager, AgentWorkFlowConfig  
      
    # load an agent specification in JSON  
    agent_spec = json.load(open('agent_spec.json'))  
      
    # Create an AutoGen Workflow Configuration from the agent specification  
    agent_work_flow_config = FlowConfig(**agent_spec)  
      
    # Create a Workflow from the configuration  
    agent_work_flow = AutoGenWorkFlowManager(agent_work_flow_config)  
      
    # Run the workflow on a task  
    task_query = "What is the height of the Eiffel Tower?"  
    agent_work_flow.run(message=task_query)  

## Road Map and Next Steps​ {#road-map-and-next-steps}

As we continue to develop and refine AutoGen Studio, the road map below
outlines an array of enhancements and new features planned for future
releases. Here's what users can look forward to:

* **Complex Agent Workflows** : We're working on integrating support for more sophisticated agent workflows, such as `GroupChat`, allowing for richer interaction between multiple agents or dynamic topologies.
* **Improved User Experience** : This includes features like streaming intermediate model output for real-time feedback, better summarization of agent responses, information on costs of each interaction. We will also invest in improving the workflow for composing and reusing agents. We will also explore support for more interactive human in the loop feedback to agents.
* **Expansion of Agent Skills** : We will work towards improving the workflow for authoring, composing and reusing agent skills.
* **Community Features** : Facilitation of sharing and collaboration within AutoGen Studio user community is a key goal. We're exploring options for sharing sessions and results more easily among users and contributing to a shared repository of skills, agents, and agent workflows.

### FAQ​ {#faq}

 **Q: Where can I adjust the default skills, agent and workflow
configurations?** A: You can modify agent configurations directly from the UI
or by editing the `autogentstudio/utils/dbdefaults.json` file which is used to
initialize the database.

 **Q: If I want to reset the entire conversation with an agent, how do I go
about it?** A: To reset your conversation history, you can delete the
`database.sqlite` file. If you need to clear user-specific data, remove the
relevant `autogenstudio/web/files/user/<user_id_md5hash>` folder.

 **Q: Is it possible to view the output and messages generated by the agents
during interactions?** A: Yes, you can view the generated messages in the
debug console of the web UI, providing insights into the agent interactions.
Alternatively, you can inspect the `database.sqlite` file for a comprehensive
record of messages.

 **Q: Where can I find documentation and support for AutoGen Studio?** A: We
are constantly working to improve AutoGen Studio. For the latest updates,
please refer to the [AutoGen Studio
Readme](<https://github.com/microsoft/autogen/tree/main/samples/apps/autogen->
studio). For additional support, please open an issue on
[GitHub](https://github.com/microsoft/autogen) or ask questions on
[Discord](https://discord.gg/pAbnFJrkgZ).

 **Tags:**

* [AutoGen](/autogen/blog/tags/auto-gen)
* [UI](/autogen/blog/tags/ui)
* [web](/autogen/blog/tags/web)
* [UX](/autogen/blog/tags/ux)

[**Read More**](/autogen/blog/2023/12/01/AutoGenStudio)

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

* Reference
  * agentchat
  * oai
    * [client](/autogen/docs/reference/oai/client)
    * [completion](/autogen/docs/reference/oai/completion)
    * [openai_utils](/autogen/docs/reference/oai/openai_utils)
  * [code_utils](/autogen/docs/reference/code_utils)
  * [function_utils](/autogen/docs/reference/function_utils)
  * [math_utils](/autogen/docs/reference/math_utils)
  * [retrieve_utils](/autogen/docs/reference/retrieve_utils)
  * [token_count_utils](/autogen/docs/reference/token_count_utils)

On this page

# oai.client {#oaiclient}

## OpenAIWrapper Objects​ {#openaiwrapper-objects}

    class OpenAIWrapper()  

A wrapper class for openai client.

#### __init__​ {#init}
```python
    def __init__(*,  
                 config_list: Optional[List[Dict[str, Any]]] = None,  
                 **base_config: Any)  

 **Arguments** :

* `config_list` \- a list of config dicts to override the base_config. They can contain additional kwargs as allowed in the [create](/autogen/docs/reference/oai/client#create) method. E.g.,

    config_list=[  
        {  
            "model": "gpt-4",  
            "api_key": os.environ.get("AZURE_OPENAI_API_KEY"),  
            "api_type": "azure",  
            "base_url": os.environ.get("AZURE_OPENAI_API_BASE"),  
            "api_version": "2023-03-15-preview",  
        },  
        {  
            "model": "gpt-3.5-turbo",  
            "api_key": os.environ.get("OPENAI_API_KEY"),  
            "api_type": "open_ai",  
            "base_url": "https://api.openai.com/v1",  
        },  
        {  
            "model": "llama-7B",  
            "base_url": "http://127.0.0.1:8080",  
            "api_type": "open_ai",  
        }  
    ]  

* `base_config` \- base config. It can contain both keyword arguments for openai client and additional kwargs.

#### create​ {#create}
```python
    def create(**config: Any) -> ChatCompletion  

Make a completion for a given config using openai's clients. Besides the
kwargs allowed in openai's client, we allow the following additional kwargs.
The config in each client will be overridden by the config.

 **Arguments** :

* context (Dict | None): The context to instantiate the prompt or messages. Default to None. It needs to contain keys that are used by the prompt template or the filter function. E.g., `prompt="Complete the following sentence: {prefix}, context={"prefix": "Today I feel"}`. The actual prompt will be: "Complete the following sentence: Today I feel". More examples can be found at [templating](/autogen/docs/Use-Cases/enhanced_inference#templating).
* `cache_seed` (int | None) for the cache. Default to 41. An integer cache_seed is useful when implementing "controlled randomness" for the completion. None for no caching.
* filter_func (Callable | None): A function that takes in the context and the response and returns a boolean to indicate whether the response is valid. E.g.,
```python
    def yes_or_no_filter(context, response):  
        return context.get("yes_or_no_choice", False) is False or any(  
            text in ["Yes.", "No."] for text in client.extract_text_or_completion_object(response)  
        )  

* allow_format_str_template (bool | None): Whether to allow format string template in the config. Default to false.
* api_version (str | None): The api version. Default to None. E.g., "2023-08-01-preview".

#### print_usage_summary​ {#printusagesummary}
```python
    def print_usage_summary(  
            mode: Union[str, List[str]] = ["actual", "total"]) -> None  

Print the usage summary.

#### clear_usage_summary​ {#clearusagesummary}
```python
    def clear_usage_summary() -> None  

Clear the usage summary.

#### cost​ {#cost}
```python
    def cost(response: Union[ChatCompletion, Completion]) -> float  

Calculate the cost of the response.

#### extract_text_or_completion_object​ {#extracttextorcompletionobject}

    @classmethod  
    def extract_text_or_completion_object(  
        cls, response: Union[ChatCompletion, Completion]  
    ) -> Union[List[str], List[ChatCompletionMessage]]  

Extract the text or ChatCompletion objects from a completion or chat response.

 **Arguments** :

* `response` _ChatCompletion | Completion_ \- The response from openai.

 **Returns** :

A list of text, or a list of ChatCompletion objects if
function_call/tool_calls are present.

[Edit this
page](https://github.com/microsoft/autogen/edit/main/website/docs/reference/oai/client.md)

[Previous«
user_proxy_agent](/autogen/docs/reference/agentchat/user_proxy_agent)

[Nextcompletion »](/autogen/docs/reference/oai/completion)

* OpenAIWrapper Objects

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
  * automl
  * default
  * onlineml
  * tune
    * tune.scheduler
    * tune.searcher
    * tune.spark
    * [analysis](.docs/reference/tune/analysis)
    * [sample](.docs/reference/tune/sample)
    * [space](.docs/reference/tune/space)
    * [trial](.docs/reference/tune/trial)
    * [trial_runner](.docs/reference/tune/trial_runner)
    * [tune](.docs/reference/tune/tune)
    * [utils](.docs/reference/tune/utils)

On this page

# tune.space {#tunespace}

#### is_constant​ {#isconstant}
```python
    def is_constant(space: Union[Dict, List]) -> bool  

Whether the search space is all constant.

 **Returns** :

A bool of whether the search space is all constant.

#### define_by_run_func​ {#definebyrunfunc}
```python
    def define_by_run_func(trial, space: Dict, path: str = "") -> Optional[Dict[str, Any]]  

Define-by-run function to create the search space.

 **Returns** :

A dict with constant values.

#### unflatten_hierarchical​ {#unflattenhierarchical}
```python
    def unflatten_hierarchical(config: Dict, space: Dict) -> Tuple[Dict, Dict]  

Unflatten hierarchical config.

#### add_cost_to_space​ {#addcosttospace}
```python
    def add_cost_to_space(space: Dict, low_cost_point: Dict, choice_cost: Dict)  

Update the space in place by adding low_cost_point and choice_cost.

 **Returns** :

A dict with constant values.

#### normalize​ {#normalize}
```python
    def normalize(config: Dict, space: Dict, reference_config: Dict, normalized_reference_config: Dict, recursive: bool = False)  

Normalize config in space according to reference_config.

Normalize each dimension in config to [0,1].

#### indexof​ {#indexof}
```python
    def indexof(domain: Dict, config: Dict) -> int  

Find the index of config in domain.categories.

#### complete_config​ {#completeconfig}
```python
    def complete_config(partial_config: Dict, space: Dict, flow2, disturb: bool = False, lower: Optional[Dict] = None, upper: Optional[Dict] = None) -> Tuple[Dict, Dict]  

Complete partial config in space.

 **Returns** :

config, space.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/tune/space.md)

[Previous« sample](.docs/reference/tune/sample)

[Nexttrial »](.docs/reference/tune/trial)

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

* Reference
  * agentchat
    * agentchat.contrib
      * agentchat.contrib.capabilities
      * [agent_builder](/autogen/docs/reference/agentchat/contrib/agent_builder)
      * [compressible_agent](/autogen/docs/reference/agentchat/contrib/compressible_agent)
      * [gpt_assistant_agent](/autogen/docs/reference/agentchat/contrib/gpt_assistant_agent)
      * [img_utils](/autogen/docs/reference/agentchat/contrib/img_utils)
      * [llava_agent](/autogen/docs/reference/agentchat/contrib/llava_agent)
      * [math_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/math_user_proxy_agent)
      * [multimodal_conversable_agent](/autogen/docs/reference/agentchat/contrib/multimodal_conversable_agent)
      * [qdrant_retrieve_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/qdrant_retrieve_user_proxy_agent)
      * [retrieve_assistant_agent](/autogen/docs/reference/agentchat/contrib/retrieve_assistant_agent)
      * [retrieve_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/retrieve_user_proxy_agent)
      * [text_analyzer_agent](/autogen/docs/reference/agentchat/contrib/text_analyzer_agent)
    * [agent](/autogen/docs/reference/agentchat/agent)
    * [assistant_agent](/autogen/docs/reference/agentchat/assistant_agent)
    * [conversable_agent](/autogen/docs/reference/agentchat/conversable_agent)
    * [groupchat](/autogen/docs/reference/agentchat/groupchat)
    * [user_proxy_agent](/autogen/docs/reference/agentchat/user_proxy_agent)
  * oai
  * [code_utils](/autogen/docs/reference/code_utils)
  * [function_utils](/autogen/docs/reference/function_utils)
  * [math_utils](/autogen/docs/reference/math_utils)
  * [retrieve_utils](/autogen/docs/reference/retrieve_utils)
  * [token_count_utils](/autogen/docs/reference/token_count_utils)

On this page

# agentchat.contrib.retrieve_user_proxy_agent {#agentchatcontribretrieveuserproxyagent}

## RetrieveUserProxyAgent Objects​ {#retrieveuserproxyagent-objects}

    class RetrieveUserProxyAgent(UserProxyAgent)  

#### __init__​ {#init}
```python
    def __init__(name="RetrieveChatAgent",  
                 human_input_mode: Optional[str] = "ALWAYS",  
                 is_termination_msg: Optional[Callable[[Dict], bool]] = None,  
                 retrieve_config: Optional[Dict] = None,  
                 **kwargs)  

 **Arguments** :

* `name` _str_ \- name of the agent.

* `human_input_mode` _str_ \- whether to ask for human inputs every time a message is received. Possible values are "ALWAYS", "TERMINATE", "NEVER". (1) When "ALWAYS", the agent prompts for human input every time a message is received. Under this mode, the conversation stops when the human input is "exit", or when is_termination_msg is True and there is no human input. (2) When "TERMINATE", the agent only prompts for human input only when a termination message is received or the number of auto reply reaches the max_consecutive_auto_reply. (3) When "NEVER", the agent will never prompt for human input. Under this mode, the conversation stops when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.

* `is_termination_msg` _function_ \- a function that takes a message in the form of a dictionary and returns a boolean value indicating if this received message is a termination message. The dict can contain the following keys: "content", "role", "name", "function_call".

* `retrieve_config` _dict or None_ \- config for the retrieve agent. To use default config, set to None. Otherwise, set to a dictionary with the following keys:

  * task (Optional, str): the task of the retrieve chat. Possible values are "code", "qa" and "default". System prompt will be different for different tasks. The default value is `default`, which supports both code and qa.
  * client (Optional, chromadb.Client): the chromadb client. If key not provided, a default client `chromadb.Client()` will be used. If you want to use other vector db, extend this class and override the `retrieve_docs` function.
  * docs_path (Optional, Union[str, List[str]]): the path to the docs directory. It can also be the path to a single file, the url to a single file or a list of directories, files and urls. Default is None, which works only if the collection is already created.
  * extra_docs (Optional, bool): when true, allows adding documents with unique IDs without overwriting existing ones; when false, it replaces existing documents using default IDs, risking collection overwrite., when set to true it enables the system to assign unique IDs starting from "length+i" for new document chunks, preventing the replacement of existing documents and facilitating the addition of more content to the collection.. By default, "extra_docs" is set to false, starting document IDs from zero. This poses a risk as new documents might overwrite existing ones, potentially causing unintended loss or alteration of data in the collection.
  * collection_name (Optional, str): the name of the collection. If key not provided, a default name `autogen-docs` will be used.
  * model (Optional, str): the model to use for the retrieve chat. If key not provided, a default model `gpt-4` will be used.
  * chunk_token_size (Optional, int): the chunk token size for the retrieve chat. If key not provided, a default size `max_tokens * 0.4` will be used.
  * context_max_tokens (Optional, int): the context max token size for the retrieve chat. If key not provided, a default size `max_tokens * 0.8` will be used.
  * chunk_mode (Optional, str): the chunk mode for the retrieve chat. Possible values are "multi_lines" and "one_line". If key not provided, a default mode `multi_lines` will be used.
  * must_break_at_empty_line (Optional, bool): chunk will only break at empty line if True. Default is True. If chunk_mode is "one_line", this parameter will be ignored.
  * embedding_model (Optional, str): the embedding model to use for the retrieve chat. If key not provided, a default model `all-MiniLM-L6-v2` will be used. All available models can be found at `https://www.sbert.net/docs/pretrained_models.html`. The default model is a fast model. If you want to use a high performance model, `all-mpnet-base-v2` is recommended.
  * embedding_function (Optional, Callable): the embedding function for creating the vector db. Default is None, SentenceTransformer with the given `embedding_model` will be used. If you want to use OpenAI, Cohere, HuggingFace or other embedding functions, you can pass it here, follow the examples in `https://docs.trychroma.com/embeddings`.
  * customized_prompt (Optional, str): the customized prompt for the retrieve chat. Default is None.
  * customized_answer_prefix (Optional, str): the customized answer prefix for the retrieve chat. Default is "". If not "" and the customized_answer_prefix is not in the answer, `Update Context` will be triggered.
  * update_context (Optional, bool): if False, will not apply `Update Context` for interactive retrieval. Default is True.
  * get_or_create (Optional, bool): if True, will create/return a collection for the retrieve chat. This is the same as that used in chromadb. Default is False. Will raise ValueError if the collection already exists and get_or_create is False. Will be set to True if docs_path is None.
  * custom_token_count_function (Optional, Callable): a custom function to count the number of tokens in a string. The function should take (text:str, model:str) as input and return the token_count(int). the retrieve_config["model"] will be passed in the function. Default is autogen.token_count_utils.count_token that uses tiktoken, which may not be accurate for non-OpenAI models.
  * custom_text_split_function (Optional, Callable): a custom function to split a string into a list of strings. Default is None, will use the default function in `autogen.retrieve_utils.split_text_to_chunks`.
  * custom_text_types (Optional, List[str]): a list of file types to be processed. Default is `autogen.retrieve_utils.TEXT_FORMATS`. This only applies to files under the directories in `docs_path`. Explicitly included files and urls will be chunked regardless of their types.
  * recursive (Optional, bool): whether to search documents recursively in the docs_path. Default is True.
* `**kwargs` _dict_ \- other kwargs in [UserProxyAgent](/autogen/docs/reference/agentchat/user_proxy_agent#__init__).

Example of overriding retrieve_docs: If you have set up a customized vector
db, and it's not compatible with chromadb, you can easily plug in it with
below code.

    class MyRetrieveUserProxyAgent(RetrieveUserProxyAgent):  
        def query_vector_db(  
            self,  
            query_texts: List[str],  
            n_results: int = 10,  
            search_string: str = "",  
            **kwargs,  
        ) -> Dict[str, Union[List[str], List[List[str]]]]:  
            # define your own query function here  
            pass  
      
        def retrieve_docs(self, problem: str, n_results: int = 20, search_string: str = "", **kwargs):  
            results = self.query_vector_db(  
                query_texts=[problem],  
                n_results=n_results,  
                search_string=search_string,  
                **kwargs,  
            )  
      
            self._results = results  
            print("doc_ids: ", results["ids"])  

#### retrieve_docs​ {#retrievedocs}
```python
    def retrieve_docs(problem: str, n_results: int = 20, search_string: str = "")  

Retrieve docs based on the given problem and assign the results to the class
property `_results`. In case you want to customize the retrieval process, such
as using a different vector db whose APIs are not compatible with chromadb or
filter results with metadata, you can override this function. Just keep the
current parameters and add your own parameters with default values, and keep
the results in below type.

Type of the results: Dict[str, List[List[Any]]], should have keys "ids" and
"documents", "ids" for the ids of the retrieved docs and "documents" for the
contents of the retrieved docs. Any other keys are optional. Refer to
`chromadb.api.types.QueryResult` as an example. ids: List[string] documents:
List[List[string]]

 **Arguments** :

* `problem` _str_ \- the problem to be solved.
* `n_results` _int_ \- the number of results to be retrieved. Default is 20.
* `search_string` _str_ \- only docs that contain an exact match of this string will be retrieved. Default is "".

#### generate_init_message​ {#generateinitmessage}
```python
    def generate_init_message(problem: str,  
                              n_results: int = 20,  
                              search_string: str = "")  

Generate an initial message with the given problem and prompt.

 **Arguments** :

* `problem` _str_ \- the problem to be solved.
* `n_results` _int_ \- the number of results to be retrieved.
* `search_string` _str_ \- only docs containing this string will be retrieved.

 **Returns** :

* `str` \- the generated prompt ready to be sent to the assistant agent.

[Edit this
page](https://github.com/microsoft/autogen/edit/main/website/docs/reference/agentchat/contrib/retrieve_user_proxy_agent.md)

[Previous«
retrieve_assistant_agent](/autogen/docs/reference/agentchat/contrib/retrieve_assistant_agent)

[Nexttext_analyzer_agent
»](/autogen/docs/reference/agentchat/contrib/text_analyzer_agent)

* RetrieveUserProxyAgent Objects

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

Recent posts

* [All About Agent Descriptions](/autogen/blog/2023/12/29/AgentDescriptions)
* [AgentOptimizer - An Agentic Way to Train Your LLM Agent](/autogen/blog/2023/12/23/AgentOptimizer)
* [AutoGen Studio: Interactively Explore Multi-Agent Workflows](/autogen/blog/2023/12/01/AutoGenStudio)
* [Agent AutoBuild - Automatically Building Multi-agent Systems](/autogen/blog/2023/11/26/Agent-AutoBuild)
* [How to Assess Utility of LLM-powered Applications?](/autogen/blog/2023/11/20/AgentEval)
* [AutoGen Meets GPTs](/autogen/blog/2023/11/13/OAI-assistants)
* [EcoAssistant - Using LLM Assistants More Accurately and Affordably](/autogen/blog/2023/11/09/EcoAssistant)
* [Multimodal with GPT-4V and LLaVA](/autogen/blog/2023/11/06/LMM-Agent)
* [AutoGen's Teachable Agents](/autogen/blog/2023/10/26/TeachableAgent)
* [Retrieval-Augmented Generation (RAG) Applications with AutoGen](/autogen/blog/2023/10/18/RetrieveChat)
* [Use AutoGen for Local LLMs](/autogen/blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](/autogen/blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](/autogen/blog/2023/05/18/GPT-adaptive-humaneval)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](/autogen/blog/2023/04/21/LLM-tuning-math)

# One post tagged with "evaluation" {#one-post-tagged-with-evaluation}

[View All Tags](/autogen/blog/tags)

## [How to Assess Utility of LLM-powered {#how-to-assess-utility-of-llmpowered}

Applications?](/autogen/blog/2023/11/20/AgentEval)

November 20, 2023 · 10 min read

[![Julia
Kiseleva](https://avatars.githubusercontent.com/u/5908392?v=4)](https://github.com/julianakiseleva/)

[Julia Kiseleva](https://github.com/julianakiseleva/)

Senior Researcher at Microsoft Research

[![Negar Arabzadeh](https://github.com/Narabzad.png)](https://www.negara.me/)

[Negar Arabzadeh](https://www.negara.me/)

PhD student at the University of Waterloo

![Fig.1: A verification framework](/autogen/assets/images/agenteval-
CQ-59f82d98878045817d02a3915839a92a.png)

 _Fig.1 illustrates the general flow of AgentEval_

 **TL;DR:**

* As a developer of an LLM-powered application, how can you assess the utility it brings to end users while helping them with their tasks?
* To shed light on the question above, we introduce `AgentEval` — the first version of the framework to assess the utility of any LLM-powered application crafted to assist users in specific tasks. AgentEval aims to simplify the evaluation process by automatically proposing a set of criteria tailored to the unique purpose of your application. This allows for a comprehensive assessment, quantifying the utility of your application against the suggested criteria.
* We demonstrate how `AgentEval` work using [math problems dataset](https://microsoft.github.io/autogen/blog/2023/06/28/MathChat) as an example in the [following notebook](https://github.com/microsoft/autogen/blob/main/notebook/agenteval_cq_math.ipynb). Any feedback would be useful for future development. Please contact us on our [Discord](http://aka.ms/autogen-dc).

## Introduction​ {#introduction}

AutoGen aims to simplify the development of LLM-powered multi-agent systems
for various applications, ultimately making end users' lives easier by
assisting with their tasks. Next, we all yearn to understand how our developed
systems perform, their utility for users, and, perhaps most crucially, how we
can enhance them. Directly evaluating multi-agent systems poses challenges as
current approaches predominantly rely on success metrics – essentially,
whether the agent accomplishes tasks. However, comprehending user interaction
with a system involves far more than success alone. Take math problems, for
instance; it's not merely about the agent solving the problem. Equally
significant is its ability to convey solutions based on various criteria,
including completeness, conciseness, and the clarity of the provided
explanation. Furthermore, success isn't always clearly defined for every task.

Rapid advances in LLMs and multi-agent systems have brought forth many
emerging capabilities that we're keen on translating into tangible utilities
for end users. We introduce the first version of `AgentEval` framework - a
tool crafted to empower developers in swiftly gauging the utility of LLM-
powered applications designed to help end users accomplish the desired task.

![Fig.2: An overview of the tasks taxonomy](/autogen/assets/images/tasks-
taxonomy-c4b7c6fe465e12dddde9235c3ee8c281.png)

 _Fig. 2 provides an overview of the tasks taxonomy_

Let's first look into an overview of the suggested task taxonomy that a multi-
agent system can be designed for. In general, the tasks can be split into two
types, where:

* _Success is not clearly defined_ \- refer to instances when users utilize a system in an assistive manner, seeking suggestions rather than expecting the system to solve the task. For example, a user might request the system to generate an email. In many cases, this generated content serves as a template that the user will later edit. However, defining success precisely for such tasks is relatively complex.
* _Success is clearly defined_ \- refer to instances where we can clearly define whether a system solved the task or not. Consider agents that assist in accomplishing household tasks, where the definition of success is clear and measurable. This category can be further divided into two separate subcategories:
    *_The optimal solution exits_ \- these are tasks where only one solution is possible. For example, if you ask your assistant to turn on the light, the success of this task is clearly defined, and there is only one way to accomplish it.
  * _Multiple solutions exist_ \- increasingly, we observe situations where multiple trajectories of agent behavior can lead to either success or failure. In such cases, it is crucial to differentiate between the various successful and unsuccessful trajectories. For example, when you ask the agent to suggest you a food recipe or tell you a joke.

In our `AgentEval` framework, we are currently focusing on tasks where
_Success is clearly defined_. Next, we will introduce the suggested framework.

## `AgentEval` Framework​ {#agenteval-framework}

Our previous research on [assistive agents in
Minecraft](https://github.com/microsoft/iglu-datasets) suggested that the most
optimal way to obtain human judgments is to present humans with two agents
side by side and ask for preferences. In this setup of pairwise comparison,
humans can develop criteria to explain why they prefer the behavior of one
agent over another. For instance, _' the first agent was faster in
execution,'_ or _' the second agent moves more naturally.'_ So, the
comparative nature led humans to come up with a list of criteria that helps to
infer the utility of the task. With this idea in mind, we designed `AgentEval`
(shown in Fig. 1), where we employ LLMs to help us understand, verify, and
assess task _utility_ for the multi-agent system. Namely:

* The goal of `CriticAgent` is to suggest the list of criteria (Fig. 1), that can be used to assess task utility. This is an example of how `CriticAgent` is defined using `Autogen`:

    critic = autogen.AssistantAgent(  
        name="critic",  
        llm_config={"config_list": config_list},  
        system_message="""You are a helpful assistant. You suggest criteria for evaluating different tasks. They should be distinguishable, quantifiable, and not redundant.  
        Convert the evaluation criteria into a dictionary where the keys are the criteria.  
        The value of each key is a dictionary as follows {"description": criteria description, "accepted_values": possible accepted inputs for this key}  
        Make sure the keys are criteria for assessing the given task. "accepted_values" include the acceptable inputs for each key that are fine-grained and preferably multi-graded levels. "description" includes the criterion description.  
        Return only the dictionary."""  
    )  

Next, the critic is given successful and failed examples of the task
execution; then, it is able to return a list of criteria (Fig. 1). For
reference, use the [following
notebook](https://github.com/microsoft/autogen/blob/main/notebook/agenteval_cq_math.ipynb).

* The goal of `QuantifierAgent` is to quantify each of the suggested criteria (Fig. 1), providing us with an idea of the utility of this system for the given task. Here is an example of how it can be defined:

    quantifier = autogen.AssistantAgent(  
        name="quantifier",  
        llm_config={"config_list": config_list},  
        system_message = """You are a helpful assistant. You quantify the output of different tasks based on the given criteria.  
        The criterion is given in a dictionary format where each key is a distinct criteria.  
        The value of each key is a dictionary as follows {"description": criteria description , "accepted_values": possible accepted inputs for this key}  
        You are going to quantify each of the criteria for a given task based on the task description.  
        Return a dictionary where the keys are the criteria and the values are the assessed performance based on accepted values for each criteria.  
        Return only the dictionary."""  

    )  

## `AgentEval` Results based on Math Problems Dataset​ {#agenteval-results-based-on-math-problems-dataset}

As an example, after running CriticAgent, we obtained the following criteria
to verify the results for math problem dataset:

Criteria| Description| Accepted Values  
---|---|---  
Problem Interpretation| Ability to correctly interpret the problem|
["completely off", "slightly relevant", "relevant", "mostly accurate",
"completely accurate"]  
Mathematical Methodology| Adequacy of the chosen mathematical or algorithmic
methodology for the question| ["inappropriate", "barely adequate", "adequate",
"mostly effective", "completely effective"]  
Calculation Correctness| Accuracy of calculations made and solutions given|
["completely incorrect", "mostly incorrect", "neither", "mostly correct",
"completely correct"]  
Explanation Clarity| Clarity and comprehensibility of explanations, including
language use and structure| ["not at all clear", "slightly clear", "moderately
clear", "very clear", "completely clear"]  
Code Efficiency| Quality of code in terms of efficiency and elegance| ["not at
all efficient", "slightly efficient", "moderately efficient", "very
efficient", "extremely efficient"]  
Code Correctness| Correctness of the provided code| ["completely incorrect",
"mostly incorrect", "partly correct", "mostly correct", "completely correct"]  
  
Then, after running QuantifierAgent, we obtained the results presented in Fig.
3, where you can see three models:

* AgentChat
* ReAct
* GPT-4 Vanilla Solver

Lighter colors represent estimates for failed cases, and brighter colors show
how discovered criteria were quantified.

![Fig.3: Results based on overall math problems dataset `_s` stands for
successful cases, `_f` - stands for failed cases](/autogen/assets/images/math-
problems-plot-03ec81b957c85db6ad9b1da353784b96.png)

 _Fig.3 presents results based on overall math problems dataset `_s` stands
for successful cases, `_f` - stands for failed cases_

We note that while applying agentEval to math problems, the agent was not
exposed to any ground truth information about the problem. As such, this
figure illustrates an estimated performance of the three different agents,
namely, Autogen (blue), Gpt-4 (red), and ReAct (green). We observe that by
comparing the performance of any of the three agents in successful cases (dark
bars of any color) versus unsuccessful cases (lighter version of the same
bar), we note that AgentEval was able to assign higher quantification to
successful cases than that of failed ones. This observation verifies
AgentEval's ability for task utility prediction. Additionally, AgentEval
allows us to go beyond just a binary definition of success, enabling a more
in-depth comparison between successful and failed cases.

It's important not only to identify what is not working but also to recognize
what and why actually went well.

## Limitations and Future Work​ {#limitations-and-future-work}

The current implementation of `AgentEval` has a number of limitations which
are planning to overcome in the future:

* The list of criteria varies per run (unless you store a seed). We would recommend to run `CriticAgent` at least two times, and pick criteria you think is important for your domain.
* The results of the `QuantifierAgent` can vary with each run, so we recommend conducting multiple runs to observe the extent of result variations.

To mitigate the limitations mentioned above, we are working on VerifierAgent,
whose goal is to stabilize the results and provide additional explanations.

## Summary​ {#summary}

`CriticAgent` and `QuantifierAgent` can be applied to the logs of any type of
application, providing you with an in-depth understanding of the utility your
solution brings to the user for a given task.

We would love to hear about how AgentEval works for your application. Any
feedback would be useful for future development. Please contact us on our
[Discord](http://aka.ms/autogen-dc).

## Previous Research​ {#previous-research}

    @InProceedings{pmlr-v176-kiseleva22a,  
      title = "Interactive Grounded Language Understanding in a Collaborative Environment: IGLU 2021",  
      author = "Kiseleva, Julia and Li, Ziming and Aliannejadi, Mohammad and Mohanty, Shrestha and ter Hoeve, Maartje and Burtsev, Mikhail and Skrynnik, Alexey and Zholus, Artem and Panov, Aleksandr and Srinet, Kavya and Szlam, Arthur and Sun, Yuxuan and Hofmann, Katja and C{\^o}t{\'e}, Marc-Alexandre and Awadallah, Ahmed and Abdrazakov, Linar and Churin, Igor and Manggala, Putra and Naszadi, Kata and van der Meer, Michiel and Kim, Taewoon",  
      booktitle = "Proceedings of the NeurIPS 2021 Competitions and Demonstrations Track",  
      pages = "146--161",  
      year = 2022,  
      editor = "Kiela, Douwe and Ciccone, Marco and Caputo, Barbara",  
      volume = 176,  
      series = "Proceedings of Machine Learning Research",  
      month = "06--14 Dec",  
      publisher = "PMLR",  
      pdf =      {https://proceedings.mlr.press/v176/kiseleva22a/kiseleva22a.pdf},  
      url =      {https://proceedings.mlr.press/v176/kiseleva22a.html}.  
    }  

    @InProceedings{pmlr-v220-kiseleva22a,  
      title = "Interactive Grounded Language Understanding in a Collaborative Environment: Retrospective on Iglu 2022 Competition",  
      author = "Kiseleva, Julia and Skrynnik, Alexey and Zholus, Artem and Mohanty, Shrestha and Arabzadeh, Negar and C\^{o}t\'e, Marc-Alexandre and Aliannejadi, Mohammad and Teruel, Milagro and Li, Ziming and Burtsev, Mikhail and ter Hoeve, Maartje and Volovikova, Zoya and Panov, Aleksandr and Sun, Yuxuan and Srinet, Kavya and Szlam, Arthur and Awadallah, Ahmed and Rho, Seungeun and Kwon, Taehwan and Wontae Nam, Daniel and Bivort Haiek, Felipe and Zhang, Edwin and Abdrazakov, Linar and Qingyam, Guo and Zhang, Jason and Guo, Zhibin",  
      booktitle = "Proceedings of the NeurIPS 2022 Competitions Track",  
      pages = "204--216",  
      year = 2022,  
      editor = "Ciccone, Marco and Stolovitzky, Gustavo and Albrecht, Jacob",  
      volume = 220,  
      series = "Proceedings of Machine Learning Research",  
      month = "28 Nov--09 Dec",  
      publisher = "PMLR",  
      pdf = "https://proceedings.mlr.press/v220/kiseleva22a/kiseleva22a.pdf",  
      url = "https://proceedings.mlr.press/v220/kiseleva22a.html".  
    }  

 **Tags:**

* [LLM](/autogen/blog/tags/llm)
* [GPT](/autogen/blog/tags/gpt)
* [evaluation](/autogen/blog/tags/evaluation)
* [task utility](/autogen/blog/tags/task-utility)

[**Read More**](/autogen/blog/2023/11/20/AgentEval)

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
  * automl
  * default
  * onlineml
  * tune
    * tune.scheduler
    * tune.searcher
      * [blendsearch](.docs/reference/tune/searcher/blendsearch)
      * [cfo_cat](.docs/reference/tune/searcher/cfo_cat)
      * [flow2](.docs/reference/tune/searcher/flow2)
      * [online_searcher](.docs/reference/tune/searcher/online_searcher)
      * [search_thread](.docs/reference/tune/searcher/search_thread)
      * [suggestion](.docs/reference/tune/searcher/suggestion)
      * [variant_generator](.docs/reference/tune/searcher/variant_generator)
    * tune.spark
    * [analysis](.docs/reference/tune/analysis)
    * [sample](.docs/reference/tune/sample)
    * [space](.docs/reference/tune/space)
    * [trial](.docs/reference/tune/trial)
    * [trial_runner](.docs/reference/tune/trial_runner)
    * [tune](.docs/reference/tune/tune)
    * [utils](.docs/reference/tune/utils)

On this page

# tune.searcher.cfo_cat {#tunesearchercfocat}

## FLOW2Cat Objects​ {#flow2cat-objects}

    class FLOW2Cat(FLOW2)  

Local search algorithm optimized for categorical variables.

## CFOCat Objects​ {#cfocat-objects}

    class CFOCat(CFO)  

CFO optimized for categorical variables.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/tune/searcher/cfo_cat.md)

[Previous« blendsearch](.docs/reference/tune/searcher/blendsearch)

[Nextflow2 »](.docs/reference/tune/searcher/flow2)

* FLOW2Cat Objects
* CFOCat Objects

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
  * automl
    * automl.contrib
    * automl.nlp
    * automl.spark
    * automl.task
    * automl.time_series
      * [sklearn](.docs/reference/automl/time_series/sklearn)
      * [tft](.docs/reference/automl/time_series/tft)
      * [ts_data](.docs/reference/automl/time_series/ts_data)
      * [ts_model](.docs/reference/automl/time_series/ts_model)
    * [automl](.docs/reference/automl/automl)
    * [data](.docs/reference/automl/data)
    * [ml](.docs/reference/automl/ml)
    * [model](.docs/reference/automl/model)
    * [state](.docs/reference/automl/state)
  * default
  * onlineml
  * tune

On this page

# automl.time_series.ts_data {#automltimeseriestsdata}

## TimeSeriesDataset Objects​ {#timeseriesdataset-objects}

    @dataclass  
    class TimeSeriesDataset()  

#### to_univariate​ {#tounivariate}
```python
    def to_univariate() -> Dict[str, "TimeSeriesDataset"]  

Convert a multivariate TrainingData to a dict of univariate ones @param df:
@return:

#### fourier_series​ {#fourierseries}
```python
    def fourier_series(feature: pd.Series, name: str)  

Assume feature goes from 0 to 1 cyclically, transform that into Fourier @param
feature: input feature @return: sin(2pi _feature), cos(2pi_ feature)

## DataTransformerTS Objects​ {#datatransformerts-objects}

    class DataTransformerTS()  

Transform input time series training data.

#### fit​ {#fit}
```python
    def fit(X: Union[DataFrame, np.array], y)  

Fit transformer.

 **Arguments** :

* `X` \- A numpy array or a pandas dataframe of training data.
* `y` \- A numpy array or a pandas series of labels.

 **Returns** :

* `X` \- Processed numpy array or pandas dataframe of training data.
* `y` \- Processed numpy array or pandas series of labels.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/automl/time_series/ts_data.md)

[Previous« tft](.docs/reference/automl/time_series/tft)

[Nextts_model »](.docs/reference/automl/time_series/ts_model)

* TimeSeriesDataset Objects
* DataTransformerTS Objects

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
  * automl
    * automl.contrib
    * automl.nlp
    * automl.spark
    * automl.task
      * [task](.docs/reference/automl/task/task)
      * [time_series_task](.docs/reference/automl/task/time_series_task)
    * automl.time_series
    * [automl](.docs/reference/automl/automl)
    * [data](.docs/reference/automl/data)
    * [ml](.docs/reference/automl/ml)
    * [model](.docs/reference/automl/model)
    * [state](.docs/reference/automl/state)
  * default
  * onlineml
  * tune

On this page

# automl.task.task {#automltasktask}

## Task Objects​ {#task-objects}

    class Task(ABC)  

Abstract base class for a machine learning task.

Class definitions should implement abstract methods and provide a non-empty
dictionary of estimator classes. A Task can be suitable to be used for
multiple machine-learning tasks (e.g. classification or regression) or be
implemented specifically for a single one depending on the generality of data
validation and model evaluation methods implemented. The implementation of a
Task may optionally use the training data and labels to determine data and
task specific details, such as in determining if a problem is single-label or
multi-label.

FLAML evaluates at runtime how to behave exactly, relying on the task instance
to provide implementations of operations which vary between tasks.

#### __init__​ {#init}
```python
    def __init__(task_name: str, X_train: Optional[Union[np.ndarray, DataFrame, psDataFrame]] = None, y_train: Optional[Union[np.ndarray, DataFrame, Series, psSeries]] = None)  

Constructor.

 **Arguments** :

* `task_name` \- String name for this type of task. Used when the Task can be generic and implement a number of types of sub-task.
* `X_train` \- Optional. Some Task types may use the data shape or features to determine details of their usage, such as in binary vs multilabel classification.
* `y_train` \- Optional. Some Task types may use the data shape or features to determine details of their usage, such as in binary vs multilabel classification.

#### __str__​ {#str}
```python
    def __str__() -> str  

Name of this task type.

#### evaluate_model_CV​ {#evaluatemodelcv}

    @abstractmethod  
    def evaluate_model_CV(config: dict, estimator: "flaml.automl.ml.BaseEstimator", X_train_all: Union[np.ndarray, DataFrame, psDataFrame], y_train_all: Union[np.ndarray, DataFrame, Series, psSeries], budget: int, kf, eval_metric: str, best_val_loss: float, log_training_metric: bool = False, fit_kwargs: Optional[dict] = {}) -> Tuple[float, float, float, float]  

Evaluate the model using cross-validation.

 **Arguments** :

* `config` \- configuration used in the evaluation of the metric.
* `estimator` \- Estimator class of the model.
* `X_train_all` \- Complete training feature data.
* `y_train_all` \- Complete training target data.
* `budget` \- Training time budget.
* `kf` \- Cross-validation index generator.
* `eval_metric` \- Metric name to be used for evaluation.
* `best_val_loss` \- Best current validation-set loss.
* `log_training_metric` \- Bool defaults False. Enables logging of the training metric.
* `fit_kwargs` \- Additional kwargs passed to the estimator's fit method.

 **Returns** :

validation loss, metric value, train time, prediction time

#### validate_data​ {#validatedata}

    @abstractmethod  
    def validate_data(automl: "flaml.automl.automl.AutoML", state: "flaml.automl.state.AutoMLState", X_train_all: Union[np.ndarray, DataFrame, psDataFrame, None], y_train_all: Union[np.ndarray, DataFrame, Series, psSeries, None], dataframe: Union[DataFrame, None], label: str, X_val: Optional[Union[np.ndarray, DataFrame, psDataFrame]] = None, y_val: Optional[Union[np.ndarray, DataFrame, Series, psSeries]] = None, groups_val: Optional[List[str]] = None, groups: Optional[List[str]] = None)  

Validate that the data is suitable for this task type.

 **Arguments** :

* `automl` \- The AutoML instance from which this task has been constructed.
* `state` \- The AutoMLState instance for this run.
* `X_train_all` \- The complete data set or None if dataframe is supplied.
* `y_train_all` \- The complete target set or None if dataframe is supplied.
* `dataframe` \- A dataframe constaining the complete data set with targets.
* `label` \- The name of the target column in dataframe.
* `X_val` \- Optional. A data set for validation.
* `y_val` \- Optional. A target vector corresponding to X_val for validation.
* `groups_val` \- Group labels (with matching length to y_val) or group counts (with sum equal to length of y_val) for validation data. Need to be consistent with groups.
* `groups` \- Group labels (with matching length to y_train) or groups counts (with sum equal to length of y_train) for training data.

 **Raises** :

* `AssertionError` \- The data provided is invalid for this task type and configuration.

#### prepare_data​ {#preparedata}

    @abstractmethod  
    def prepare_data(state: "flaml.automl.state.AutoMLState", X_train_all: Union[np.ndarray, DataFrame, psDataFrame], y_train_all: Union[np.ndarray, DataFrame, Series, psSeries, None], auto_augment: bool, eval_method: str, split_type: str, split_ratio: float, n_splits: int, data_is_df: bool, sample_weight_full: Optional[List[float]] = None)

Prepare the data for fitting or inference.

 **Arguments** :

* `automl` \- The AutoML instance from which this task has been constructed.
* `state` \- The AutoMLState instance for this run.
* `X_train_all` \- The complete data set or None if dataframe is supplied. Must contain the target if y_train_all is None
* `y_train_all` \- The complete target set or None if supplied in X_train_all.
* `auto_augment` \- If true, task-specific data augmentations will be applied.
* `eval_method` \- A string of resampling strategy, one of ['auto', 'cv', 'holdout'].
* `split_type` \- str or splitter object, default="auto" | the data split type.
  * A valid splitter object is an instance of a derived class of scikit-learn [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold) and have `split` and `get_n_splits` methods with the same signatures. Set eval_method to "cv" to use the splitter object.
  * Valid str options depend on different tasks. For classification tasks, valid choices are ["auto", 'stratified', 'uniform', 'time', 'group']. "auto" -> stratified. For regression tasks, valid choices are ["auto", 'uniform', 'time']. "auto" -> uniform. For time series forecast tasks, must be "auto" or 'time'. For ranking task, must be "auto" or 'group'.
* `split_ratio` \- A float of the valiation data percentage for holdout.
* `n_splits` \- An integer of the number of folds for cross - validation.
* `data_is_df` \- True if the data was provided as a DataFrame else False.
* `sample_weight_full` \- A 1d arraylike of the sample weight.

 **Raises** :

* `AssertionError` \- The configuration provided is invalid for this task type and data.

#### decide_split_type​ {#decidesplittype}

    @abstractmethod  
    def decide_split_type(split_type: str, y_train_all: Union[np.ndarray, DataFrame, Series, psSeries, None], fit_kwargs: dict, groups: Optional[List[str]] = None) -> str  

Choose an appropriate data split type for this data and task.

If split_type is 'auto' then this is determined based on the task type and
data. If a specific split_type is requested then the choice is validated to be
appropriate.

 **Arguments** :

* `split_type` \- Either 'auto' or a task appropriate split type.
* `y_train_all` \- The complete set of targets.
* `fit_kwargs` \- Additional kwargs passed to the estimator's fit method.
* `groups` \- Optional. Group labels (with matching length to y_train) or groups counts (with sum equal to length of y_train) for training data.

 **Returns** :

The determined appropriate split type.

 **Raises** :

* `AssertionError` \- The requested split_type is invalid for this task, configuration and data.

#### preprocess​ {#preprocess}

    @abstractmethod  
    def preprocess(X: Union[np.ndarray, DataFrame, psDataFrame], transformer: Optional["flaml.automl.data.DataTransformer"] = None) -> Union[np.ndarray, DataFrame]  

Preprocess the data ready for fitting or inference with this task type.

 **Arguments** :

* `X` \- The data set to process.
* `transformer` \- A DataTransformer instance to be used in processing.

 **Returns** :

The preprocessed data set having the same type as the input.

#### default_estimator_list​ {#defaultestimatorlist}

    @abstractmethod  
    def default_estimator_list(estimator_list: Union[List[str], str] = "auto", is_spark_dataframe: bool = False) -> List[str]  

Return the list of default estimators registered for this task type.

If 'auto' is provided then the default list is returned, else the provided
list will be validated given this task type.

 **Arguments** :

* `estimator_list` \- Either 'auto' or a list of estimator names to be validated.
* `is_spark_dataframe` \- True if the data is a spark dataframe.

 **Returns** :

A list of valid estimator names for this task type.

#### default_metric​ {#defaultmetric}

    @abstractmethod  
    def default_metric(metric: str) -> str  

Return the default metric for this task type.

If 'auto' is provided then the default metric for this task will be returned.
Otherwise, the provided metric name is validated for this task type.

 **Arguments** :

* `metric` \- The name of a metric to be used in evaluation of models during fitting or validation.

 **Returns** :

The default metric, or the provided metric if it is valid for this task type.

#### __eq__​ {#eq}
```python
    def __eq__(other: str) -> bool  

For backward compatibility with all the string comparisons to task

#### estimator_class_from_str​ {#estimatorclassfromstr}
```python
    def estimator_class_from_str(estimator_name: str) -> "flaml.automl.ml.BaseEstimator"  

Determine the estimator class corresponding to the provided name.

 **Arguments** :

* `estimator_name` \- Name of the desired estimator.

 **Returns** :

The estimator class corresponding to the provided name.

 **Raises** :

* `ValueError` \- The provided estimator_name has not been registered for this task type.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/automl/task/task.md)

[Previous« utils](.docs/reference/automl/spark/utils)

[Nexttime_series_task »](.docs/reference/automl/task/time_series_task)

* Task Objects

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

* Reference
  * agentchat
    * agentchat.contrib
      * agentchat.contrib.capabilities
        * [agent_capability](/autogen/docs/reference/agentchat/contrib/capabilities/agent_capability)
        * [teachability](/autogen/docs/reference/agentchat/contrib/capabilities/teachability)
      * [agent_builder](/autogen/docs/reference/agentchat/contrib/agent_builder)
      * [compressible_agent](/autogen/docs/reference/agentchat/contrib/compressible_agent)
      * [gpt_assistant_agent](/autogen/docs/reference/agentchat/contrib/gpt_assistant_agent)
      * [img_utils](/autogen/docs/reference/agentchat/contrib/img_utils)
      * [llava_agent](/autogen/docs/reference/agentchat/contrib/llava_agent)
      * [math_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/math_user_proxy_agent)
      * [multimodal_conversable_agent](/autogen/docs/reference/agentchat/contrib/multimodal_conversable_agent)
      * [qdrant_retrieve_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/qdrant_retrieve_user_proxy_agent)
      * [retrieve_assistant_agent](/autogen/docs/reference/agentchat/contrib/retrieve_assistant_agent)
      * [retrieve_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/retrieve_user_proxy_agent)
      * [text_analyzer_agent](/autogen/docs/reference/agentchat/contrib/text_analyzer_agent)
    * [agent](/autogen/docs/reference/agentchat/agent)
    * [assistant_agent](/autogen/docs/reference/agentchat/assistant_agent)
    * [conversable_agent](/autogen/docs/reference/agentchat/conversable_agent)
    * [groupchat](/autogen/docs/reference/agentchat/groupchat)
    * [user_proxy_agent](/autogen/docs/reference/agentchat/user_proxy_agent)
  * oai
  * [code_utils](/autogen/docs/reference/code_utils)
  * [function_utils](/autogen/docs/reference/function_utils)
  * [math_utils](/autogen/docs/reference/math_utils)
  * [retrieve_utils](/autogen/docs/reference/retrieve_utils)
  * [token_count_utils](/autogen/docs/reference/token_count_utils)

On this page

# agentchat.contrib.capabilities.teachability {#agentchatcontribcapabilitiesteachability}

## Teachability Objects​ {#teachability-objects}

    class Teachability(AgentCapability)  

Teachability uses a vector database to give an agent the ability to remember
user teachings, where the user is any caller (human or not) sending messages
to the teachable agent. Teachability is designed to be composable with other
agent capabilities. To make any conversable agent teachable, instantiate both
the agent and the Teachability class, then pass the agent to
teachability.add_to_agent(agent). Note that teachable agents in a group chat
must be given unique path_to_db_dir values.

#### __init__​ {#init} {#init}
```python
    def __init__(verbosity: Optional[int] = 0,  
                 reset_db: Optional[bool] = False,  
                 path_to_db_dir: Optional[str] = "./tmp/teachable_agent_db",  
                 recall_threshold: Optional[float] = 1.5,  
                 max_num_retrievals: Optional[int] = 10,  
                 llm_config: Optional[Union[Dict, bool]] = None)  

 **Arguments** :

* `verbosity` _Optional, int_ \- # 0 (default) for basic info, 1 to add memory operations, 2 for analyzer messages, 3 for memo lists.
* `reset_db` _Optional, bool_ \- True to clear the DB before starting. Default False.
* `path_to_db_dir` _Optional, str_ \- path to the directory where this particular agent's DB is stored. Default "./tmp/teachable_agent_db"
* `recall_threshold` _Optional, float_ \- The maximum distance for retrieved memos, where 0.0 is exact match. Default 1.5. Larger values allow more (but less relevant) memos to be recalled.
* `max_num_retrievals` _Optional, int_ \- The maximum number of memos to retrieve from the DB. Default 10.
* `llm_config` _dict or False_ \- llm inference configuration passed to TextAnalyzerAgent. If None, TextAnalyzerAgent uses llm_config from the teachable agent.

#### add_to_agent​ {#addtoagent}
```python
    def add_to_agent(agent: ConversableAgent)  

Adds teachability to the given agent.

#### prepopulate_db​ {#prepopulatedb}
```python
    def prepopulate_db()  

Adds a few arbitrary memos to the DB.

#### process_last_message​ {#processlastmessage}
```python
    def process_last_message(text)  

Appends any relevant memos to the message text, and stores any apparent
teachings in new memos. Uses TextAnalyzerAgent to make decisions about memo
storage and retrieval.

## MemoStore Objects​ {#memostore-objects}

    class MemoStore()  

Provides memory storage and retrieval for a teachable agent, using a vector
database. Each DB entry (called a memo) is a pair of strings: an input text
and an output text. The input text might be a question, or a task to perform.
The output text might be an answer to the question, or advice on how to
perform the task. Vector embeddings are currently supplied by Chroma's default
Sentence Transformers.

#### __init__​ {#init} {#init}
```python
    def __init__(verbosity, reset, path_to_db_dir)  

 **Arguments** :

* verbosity (Optional, int): 1 to print memory operations, 0 to omit them. 3+ to print memo lists.
* path_to_db_dir (Optional, str): path to the directory where the DB is stored.

#### list_memos​ {#listmemos}
```python
    def list_memos()  

Prints the contents of MemoStore.

#### reset_db​ {#resetdb}
```python
    def reset_db()  

Forces immediate deletion of the DB's contents, in memory and on disk.

#### add_input_output_pair​ {#addinputoutputpair}
```python
    def add_input_output_pair(input_text, output_text)  

Adds an input-output pair to the vector DB.

#### get_nearest_memo​ {#getnearestmemo}
```python
    def get_nearest_memo(query_text)  

Retrieves the nearest memo to the given query text.

#### get_related_memos​ {#getrelatedmemos}
```python
    def get_related_memos(query_text, n_results, threshold)  

Retrieves memos that are related to the given query text within the specified
distance threshold.

#### prepopulate​ {#prepopulate}
```python
    def prepopulate()  

Adds a few arbitrary examples to the vector DB, just to make retrieval less
trivial.

[Edit this
page](https://github.com/microsoft/autogen/edit/main/website/docs/reference/agentchat/contrib/capabilities/teachability.md)

[Previous«
agent_capability](/autogen/docs/reference/agentchat/contrib/capabilities/agent_capability)

[Nextagent_builder »](/autogen/docs/reference/agentchat/contrib/agent_builder)

* Teachability Objects
* MemoStore Objects

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

* Reference
  * agentchat
    * agentchat.contrib
      * agentchat.contrib.capabilities
      * [agent_builder](/autogen/docs/reference/agentchat/contrib/agent_builder)
      * [compressible_agent](/autogen/docs/reference/agentchat/contrib/compressible_agent)
      * [gpt_assistant_agent](/autogen/docs/reference/agentchat/contrib/gpt_assistant_agent)
      * [img_utils](/autogen/docs/reference/agentchat/contrib/img_utils)
      * [llava_agent](/autogen/docs/reference/agentchat/contrib/llava_agent)
      * [math_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/math_user_proxy_agent)
      * [multimodal_conversable_agent](/autogen/docs/reference/agentchat/contrib/multimodal_conversable_agent)
      * [qdrant_retrieve_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/qdrant_retrieve_user_proxy_agent)
      * [retrieve_assistant_agent](/autogen/docs/reference/agentchat/contrib/retrieve_assistant_agent)
      * [retrieve_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/retrieve_user_proxy_agent)
      * [text_analyzer_agent](/autogen/docs/reference/agentchat/contrib/text_analyzer_agent)
    * [agent](/autogen/docs/reference/agentchat/agent)
    * [assistant_agent](/autogen/docs/reference/agentchat/assistant_agent)
    * [conversable_agent](/autogen/docs/reference/agentchat/conversable_agent)
    * [groupchat](/autogen/docs/reference/agentchat/groupchat)
    * [user_proxy_agent](/autogen/docs/reference/agentchat/user_proxy_agent)
  * oai
  * [code_utils](/autogen/docs/reference/code_utils)
  * [function_utils](/autogen/docs/reference/function_utils)
  * [math_utils](/autogen/docs/reference/math_utils)
  * [retrieve_utils](/autogen/docs/reference/retrieve_utils)
  * [token_count_utils](/autogen/docs/reference/token_count_utils)

On this page

# agentchat.contrib.agent_builder {#agentchatcontribagentbuilder}

## AgentBuilder Objects​ {#agentbuilder-objects}

    class AgentBuilder()  

AgentBuilder can help user build an automatic task solving process powered by
multi-agent system. Specifically, our building pipeline includes initialize
and build. In build(), we prompt a LLM to create multiple participant agents,
and specify whether this task need programming to solve. User can save the
built agents' config by calling save(), and load the saved configs by load(),
which can skip the building process.

#### __init__​ {#init}
```python
    def __init__(config_file_or_env: Optional[str] = "OAI_CONFIG_LIST",  
                 config_file_location: Optional[str] = "",  
                 builder_model: Optional[str] = "gpt-4",  
                 agent_model: Optional[str] = "gpt-4",  
                 host: Optional[str] = "localhost",  
                 endpoint_building_timeout: Optional[int] = 600,  
                 max_tokens: Optional[int] = 945,  
                 max_agents: Optional[int] = 5)  

(These APIs are experimental and may change in the future.)

 **Arguments** :

* `config_file_or_env` \- path or environment of the OpenAI api configs.
* `builder_model` \- specify a model as the backbone of build manager.
* `agent_model` \- specify a model as the backbone of participant agents.
* `host` \- endpoint host.
* `endpoint_building_timeout` \- timeout for building up an endpoint server.
* `max_tokens` \- max tokens for each agent.
* `max_agents` \- max agents for each task.

#### clear_agent​ {#clearagent}
```python
    def clear_agent(agent_name: str, recycle_endpoint: Optional[bool] = True)  

Clear a specific agent by name.

 **Arguments** :

* `agent_name` \- the name of agent.
* `recycle_endpoint` \- trigger for recycle the endpoint server. If true, the endpoint will be recycled when there is no agent depending on.

#### clear_all_agents​ {#clearallagents}
```python
    def clear_all_agents(recycle_endpoint: Optional[bool] = True)  

Clear all cached agents.

#### build​ {#build}
```python
    def build(building_task: str,  
              default_llm_config: Dict,  
              coding: Optional[bool] = None,  
              code_execution_config: Optional[Dict] = None,  
              use_oai_assistant: Optional[bool] = False,  
              **kwargs) -> Tuple[List[autogen.ConversableAgent], Dict]  

Auto build agents based on the building task.

 **Arguments** :

* `building_task` \- instruction that helps build manager (gpt-4) to decide what agent should be built.
* `coding` \- use to identify if the user proxy (a code interpreter) should be added.
* `code_execution_config` \- specific configs for user proxy (e.g., last_n_messages, work_dir, ...).
* `default_llm_config` \- specific configs for LLM (e.g., config_list, seed, temperature, ...).
* `use_oai_assistant` \- use OpenAI assistant api instead of self-constructed agent.

 **Returns** :

* `agent_list` \- a list of agents.
* `cached_configs` \- cached configs.

#### build_from_library​ {#buildfromlibrary}
```python
    def build_from_library(  
            building_task: str,  
            library_path_or_json: str,  
            default_llm_config: Dict,  
            coding: Optional[bool] = True,  
            code_execution_config: Optional[Dict] = None,  
            use_oai_assistant: Optional[bool] = False,  
            embedding_model: Optional[str] = None,  
            **kwargs) -> Tuple[List[autogen.ConversableAgent], Dict]  

Build agents from a library. The library is a list of agent configs, which
contains the name and system_message for each agent. We use a build manager to
decide what agent in that library should be involved to the task.

 **Arguments** :

* `building_task` \- instruction that helps build manager (gpt-4) to decide what agent should be built.
* `library_path_or_json` \- path or JSON string config of agent library.
* `default_llm_config` \- specific configs for LLM (e.g., config_list, seed, temperature, ...).
* `coding` \- use to identify if the user proxy (a code interpreter) should be added.
* `code_execution_config` \- specific configs for user proxy (e.g., last_n_messages, work_dir, ...).
* `use_oai_assistant` \- use OpenAI assistant api instead of self-constructed agent.
* `embedding_model` \- a Sentence-Transformers model use for embedding similarity to select agents from library. if None, an openai model will be prompted to select agents. As reference, chromadb use "all-mpnet-base- v2" as default.

 **Returns** :

* `agent_list` \- a list of agents.
* `cached_configs` \- cached configs.

#### save​ {#save}
```python
    def save(filepath: Optional[str] = None) -> str  

Save building configs. If the filepath is not specific, this function will
create a filename by encrypt the building _task string by md5 with
"save_config_" prefix, and save config to the local path.

 **Arguments** :

* `filepath` \- save path.

 **Returns** :

* `filepath` \- path save.

#### load​ {#load}
```python
    def load(filepath: Optional[str] = None,  
             config_json: Optional[str] = None,  
             use_oai_assistant: Optional[bool] = False,  
             **kwargs) -> Tuple[List[autogen.ConversableAgent], Dict]  

Load building configs and call the build function to complete building without
calling online LLMs' api.

 **Arguments** :

* `filepath` \- filepath or JSON string for the save config.
* `config_json` \- JSON string for the save config.
* `use_oai_assistant` \- use OpenAI assistant api instead of self-constructed agent.

 **Returns** :

* `agent_list` \- a list of agents.
* `cached_configs` \- cached configs.

[Edit this
page](https://github.com/microsoft/autogen/edit/main/website/docs/reference/agentchat/contrib/agent_builder.md)

[Previous«
teachability](/autogen/docs/reference/agentchat/contrib/capabilities/teachability)

[Nextcompressible_agent
»](/autogen/docs/reference/agentchat/contrib/compressible_agent)

* AgentBuilder Objects

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

* Reference
  * agentchat
    * agentchat.contrib
      * agentchat.contrib.capabilities
      * [agent_builder](/autogen/docs/reference/agentchat/contrib/agent_builder)
      * [compressible_agent](/autogen/docs/reference/agentchat/contrib/compressible_agent)
      * [gpt_assistant_agent](/autogen/docs/reference/agentchat/contrib/gpt_assistant_agent)
      * [img_utils](/autogen/docs/reference/agentchat/contrib/img_utils)
      * [llava_agent](/autogen/docs/reference/agentchat/contrib/llava_agent)
      * [math_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/math_user_proxy_agent)
      * [multimodal_conversable_agent](/autogen/docs/reference/agentchat/contrib/multimodal_conversable_agent)
      * [qdrant_retrieve_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/qdrant_retrieve_user_proxy_agent)
      * [retrieve_assistant_agent](/autogen/docs/reference/agentchat/contrib/retrieve_assistant_agent)
      * [retrieve_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/retrieve_user_proxy_agent)
      * [text_analyzer_agent](/autogen/docs/reference/agentchat/contrib/text_analyzer_agent)
    * [agent](/autogen/docs/reference/agentchat/agent)
    * [assistant_agent](/autogen/docs/reference/agentchat/assistant_agent)
    * [conversable_agent](/autogen/docs/reference/agentchat/conversable_agent)
    * [groupchat](/autogen/docs/reference/agentchat/groupchat)
    * [user_proxy_agent](/autogen/docs/reference/agentchat/user_proxy_agent)
  * oai
  * [code_utils](/autogen/docs/reference/code_utils)
  * [function_utils](/autogen/docs/reference/function_utils)
  * [math_utils](/autogen/docs/reference/math_utils)
  * [retrieve_utils](/autogen/docs/reference/retrieve_utils)
  * [token_count_utils](/autogen/docs/reference/token_count_utils)

On this page

# agentchat.contrib.compressible_agent {#agentchatcontribcompressibleagent}

## CompressibleAgent Objects​ {#compressibleagent-objects}

    class CompressibleAgent(ConversableAgent)  

(Experimental) CompressibleAgent agent. While this agent retains all the
default functionalities of the `AssistantAgent`, it also provides the added
feature of compression when activated through the `compress_config` setting.

`compress_config` is set to False by default, making this agent equivalent to
the `AssistantAgent`. This agent does not work well in a GroupChat: The
compressed messages will not be sent to all the agents in the group. The
default system message is the same as AssistantAgent. `human_input_mode` is
default to "NEVER" and `code_execution_config` is default to False. This agent
doesn't execute code or function call by default.

#### __init__​ {#init}
```python
    def __init__(name: str,  
                 system_message: Optional[str] = DEFAULT_SYSTEM_MESSAGE,  
                 is_termination_msg: Optional[Callable[[Dict], bool]] = None,  
                 max_consecutive_auto_reply: Optional[int] = None,  
                 human_input_mode: Optional[str] = "NEVER",  
                 function_map: Optional[Dict[str, Callable]] = None,  
                 code_execution_config: Optional[Union[Dict, bool]] = False,  
                 llm_config: Optional[Union[Dict, bool]] = None,  
                 default_auto_reply: Optional[Union[str, Dict, None]] = "",  
                 compress_config: Optional[Dict] = False)  

 **Arguments** :

* `name` _str_ \- agent name.
* `system_message` _str_ \- system message for the ChatCompletion inference. Please override this attribute if you want to reprogram the agent.
* `llm_config` _dict_ \- llm inference configuration. Please refer to [OpenAIWrapper.create](/autogen/docs/reference/oai/client#create) for available options.
* `is_termination_msg` _function_ \- a function that takes a message in the form of a dictionary and returns a boolean value indicating if this received message is a termination message. The dict can contain the following keys: "content", "role", "name", "function_call".
* `max_consecutive_auto_reply` _int_ \- the maximum number of consecutive auto replies. default to None (no limit provided, class attribute MAX_CONSECUTIVE_AUTO_REPLY will be used as the limit in this case). The limit only plays a role when human_input_mode is not "ALWAYS".
* `compress_config` _dict or True/False_ \- config for compression before oai_reply. Default to False. You should contain the following keys:
  * "mode" (Optional, str, default to "TERMINATE"): Choose from ["COMPRESS", "TERMINATE", "CUSTOMIZED"].
* `"TERMINATE"` \- terminate the conversation ONLY when token count exceeds the max limit of current model. `trigger_count` is NOT used in this mode.
* `"COMPRESS"` \- compress the messages when the token count exceeds the limit.
* `"CUSTOMIZED"` \- pass in a customized function to compress the messages.
  * "compress_function" (Optional, callable, default to None): Must be provided when mode is "CUSTOMIZED". The function should takes a list of messages and returns a tuple of (is_compress_success: bool, compressed_messages: List[Dict]).
  * "trigger_count" (Optional, float, int, default to 0.7): the threshold to trigger compression. If a float between (0, 1], it is the percentage of token used. if a int, it is the number of tokens used.
  * "async" (Optional, bool, default to False): whether to compress asynchronously.
  * "broadcast" (Optional, bool, default to True): whether to update the compressed message history to sender.
  * "verbose" (Optional, bool, default to False): Whether to print the content before and after compression. Used when mode="COMPRESS".
  * "leave_last_n" (Optional, int, default to 0): If provided, the last n messages will not be compressed. Used when mode="COMPRESS".
* `**kwargs` _dict_ \- Please refer to other kwargs in [ConversableAgent](/autogen/docs/reference/agentchat/conversable_agent#__init__).

#### generate_reply​ {#generatereply}
```python
    def generate_reply(  
            messages: Optional[List[Dict]] = None,  
            sender: Optional[Agent] = None,  
            exclude: Optional[List[Callable]] = None) -> Union[str, Dict, None]  

Adding to line 202:

        if messages is not None and messages != self._oai_messages[sender]:  
            messages = self._oai_messages[sender]  

#### on_oai_token_limit​ {#onoaitokenlimit}
```python
    def on_oai_token_limit(  
            messages: Optional[List[Dict]] = None,  
            sender: Optional[Agent] = None,  
            config: Optional[Any] = None) -> Tuple[bool, Union[str, Dict, None]]  

(Experimental) Compress previous messages when a threshold of tokens is
reached.

TODO: async compress TODO: maintain a list for old oai messages (messages
before compression)

#### compress_messages​ {#compressmessages}
```python
    def compress_messages(  
            messages: Optional[List[Dict]] = None,  
            config: Optional[Any] = None  
    ) -> Tuple[bool, Union[str, Dict, None, List]]  

Compress a list of messages into one message.

The first message (the initial prompt) will not be compressed. The rest of the
messages will be compressed into one message, the model is asked to
distinguish the role of each message: USER, ASSISTANT, FUNCTION_CALL,
FUNCTION_RETURN. Check out the compress_sys_msg.

TODO: model used in compression agent is different from assistant agent: For
example, if original model used by is gpt-4; we start compressing at 70% of
usage, 70% of 8092 = 5664; and we use gpt 3.5 here max_toke = 4096, it will
raise error. choosinng model automatically?

[Edit this
page](https://github.com/microsoft/autogen/edit/main/website/docs/reference/agentchat/contrib/compressible_agent.md)

[Previous«
agent_builder](/autogen/docs/reference/agentchat/contrib/agent_builder)

[Nextgpt_assistant_agent
»](/autogen/docs/reference/agentchat/contrib/gpt_assistant_agent)

* CompressibleAgent Objects

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
    * autogen.agentchat
    * autogen.oai
    * [code_utils](.docs/reference/autogen/code_utils)
    * [math_utils](.docs/reference/autogen/math_utils)
    * [retrieve_utils](.docs/reference/autogen/retrieve_utils)
  * automl
  * default
  * onlineml
  * tune

On this page

# autogen.math_utils {#autogenmathutils}

#### solve_problem​ {#solveproblem}
```python
    def solve_problem(problem: str, **config) -> str  

(Experimental) Solve the math problem.

 **Arguments** :

* `problem` _str_ \- The problem statement.
* `config` _Optional, dict_ \- The configuration for the API call.

 **Returns** :

* `str` \- The solution to the problem.

#### remove_boxed​ {#removeboxed}
```python
    def remove_boxed(string: str) -> Optional[str]  

Source: <https://github.com/hendrycks/math> Extract the text within a
\boxed{...} environment.

 **Example** :

> > remove_boxed("\boxed{\frac{2}{3}}")

\frac{2}{3}

#### last_boxed_only_string​ {#lastboxedonlystring}
```python
    def last_boxed_only_string(string: str) -> Optional[str]  

Source: <https://github.com/hendrycks/math> Extract the last \boxed{...} or
\fbox{...} element from a string.

#### is_equiv​ {#isequiv}
```python
    def is_equiv(str1: Optional[str], str2: Optional[str]) -> float  

Returns (as a float) whether two strings containing math are equivalent up to
differences of formatting in

* units
* fractions
* square roots
* superfluous LaTeX. Source: <https://github.com/hendrycks/math>

#### is_equiv_chain_of_thought​ {#isequivchainofthought}
```python
    def is_equiv_chain_of_thought(str1: str, str2: str) -> float  

Strips the solution first before calling `is_equiv`.

#### eval_math_responses​ {#evalmathresponses}
```python
    def eval_math_responses(responses, solution=None, **args)  

Select a response for a math problem using voting, and check if the response
is correct if the solution is provided.

 **Arguments** :

* `responses` _list_ \- The list of responses.
* `solution` _str_ \- The canonical solution.

 **Returns** :

* `dict` \- The success metrics.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/autogen/math_utils.md)

[Previous« code_utils](.docs/reference/autogen/code_utils)

[Nextretrieve_utils »](.docs/reference/autogen/retrieve_utils)

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
    * autogen.agentchat
    * autogen.oai
      * [completion](.docs/reference/autogen/oai/completion)
      * [openai_utils](.docs/reference/autogen/oai/openai_utils)
    * [code_utils](.docs/reference/autogen/code_utils)
    * [math_utils](.docs/reference/autogen/math_utils)
    * [retrieve_utils](.docs/reference/autogen/retrieve_utils)
  * automl
  * default
  * onlineml
  * tune

On this page

# autogen.oai.openai_utils {#autogenoaiopenaiutils}

#### get_key​ {#getkey}
```python
    def get_key(config)  

Get a unique identifier of a configuration.

 **Arguments** :

* `config` _dict or list_ \- A configuration.

 **Returns** :

* `tuple` \- A unique identifier which can be used as a key for a dict.

#### get_config_list​ {#getconfiglist}
```python
    def get_config_list(api_keys: List, api_bases: Optional[List] = None, api_type: Optional[str] = None, api_version: Optional[str] = None) -> List[Dict]  
```

Get a list of configs for openai api calls.

 **Arguments** :

* `api_keys` _list_ \- The api keys for openai api calls.
* `api_bases` _list, optional_ \- The api bases for openai api calls.
* `api_type` _str, optional_ \- The api type for openai api calls.
* `api_version` _str, optional_ \- The api version for openai api calls.

#### config_list_openai_aoai​ {#configlistopenaiaoai}
```python
    def config_list_openai_aoai(key_file_path: Optional[str] = ".", openai_api_key_file: Optional[str] = "key_openai.txt", aoai_api_key_file: Optional[str] = "key_aoai.txt", aoai_api_base_file: Optional[str] = "base_aoai.txt", exclude: Optional[str] = None) -> List[Dict]  
```

Get a list of configs for openai + azure openai api calls.

 **Arguments** :

* `key_file_path` _str, optional_ \- The path to the key files.
* `openai_api_key_file` _str, optional_ \- The file name of the openai api key.
* `aoai_api_key_file` _str, optional_ \- The file name of the azure openai api key.
* `aoai_api_base_file` _str, optional_ \- The file name of the azure openai api base.
* `exclude` _str, optional_ \- The api type to exclude, "openai" or "aoai".

 **Returns** :

* `list` \- A list of configs for openai api calls.

#### config_list_from_models​ {#configlistfrommodels}
```python
    def config_list_from_models(key_file_path: Optional[str] = ".", openai_api_key_file: Optional[str] = "key_openai.txt", aoai_api_key_file: Optional[str] = "key_aoai.txt", aoai_api_base_file: Optional[str] = "base_aoai.txt", exclude: Optional[str] = None, model_list: Optional[list] = None) -> List[Dict]  

Get a list of configs for api calls with models in the model list.

 **Arguments** :

* `key_file_path` _str, optional_ \- The path to the key files.
* `openai_api_key_file` _str, optional_ \- The file name of the openai api key.
* `aoai_api_key_file` _str, optional_ \- The file name of the azure openai api key.
* `aoai_api_base_file` _str, optional_ \- The file name of the azure openai api base.
* `exclude` _str, optional_ \- The api type to exclude, "openai" or "aoai".
* `model_list` _list, optional_ \- The model list.

 **Returns** :

* `list` \- A list of configs for openai api calls.

#### config_list_gpt4_gpt35​ {#configlistgpt4gpt35}

    def config_list_gpt4_gpt35(key_file_path: Optional[str] = ".", openai_api_key_file: Optional[str] = "key_openai.txt", aoai_api_key_file: Optional[str] = "key_aoai.txt", aoai_api_base_file: Optional[str] = "base_aoai.txt", exclude: Optional[str] = None) -> List[Dict]  
```

Get a list of configs for gpt-4 followed by gpt-3.5 api calls.

 **Arguments** :

* `key_file_path` _str, optional_ \- The path to the key files.
* `openai_api_key_file` _str, optional_ \- The file name of the openai api key.
* `aoai_api_key_file` _str, optional_ \- The file name of the azure openai api key.
* `aoai_api_base_file` _str, optional_ \- The file name of the azure openai api base.
* `exclude` _str, optional_ \- The api type to exclude, "openai" or "aoai".

 **Returns** :

* `list` \- A list of configs for openai api calls.

#### filter_config​ {#filterconfig}
```python
    def filter_config(config_list, filter_dict)  

Filter the config list by provider and model.

 **Arguments** :

* `config_list` _list_ \- The config list.
* `filter_dict` _dict, optional_ \- The filter dict with keys corresponding to a field in each config, and values corresponding to lists of acceptable values for each key.

 **Returns** :

* `list` \- The filtered config list.

#### config_list_from_json​ {#configlistfromjson}
```python
    def config_list_from_json(env_or_file: str, file_location: Optional[str] = "", filter_dict: Optional[Dict[str, Union[List[Union[str, None]], Set[Union[str, None]]]]] = None) -> List[Dict]  
```

Get a list of configs from a json parsed from an env variable or a file.

 **Arguments** :

* `env_or_file` _str_ \- The env variable name or file name.
* `file_location` _str, optional_ \- The file location.
* `filter_dict` _dict, optional_ \- The filter dict with keys corresponding to a field in each config, and values corresponding to lists of acceptable values for each key. e.g.,

    filter_dict = {  
        "api_type": ["open_ai", None],  # None means a missing key is acceptable  
        "model": ["gpt-3.5-turbo", "gpt-4"],  
    }  

 **Returns** :

* `list` \- A list of configs for openai api calls.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/autogen/oai/openai_utils.md)

[Previous« completion](.docs/reference/autogen/oai/completion)

[Nextcode_utils »](.docs/reference/autogen/code_utils)

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* [Getting Started](.docs/Getting-Started)
* [Installation](.docs/Installation)
* Use Cases
* Examples
  * [AutoGen - Automated Multi Agent Chat](.docs/Examples/AutoGen-AgentChat)
  * [AutoGen - Tune GPT Models](.docs/Examples/AutoGen-OpenAI)
  * [AutoML - Classification](.docs/Examples/AutoML-Classification)
  * [AutoML - NLP](.docs/Examples/AutoML-NLP)
  * [AutoML - Rank](.docs/Examples/AutoML-Rank)
  * [AutoML - Regression](.docs/Examples/AutoML-Regression)
  * [AutoML - Time Series Forecast](.docs/Examples/AutoML-Time series forecast)
  * [AutoML for LightGBM](.docs/Examples/AutoML-for-LightGBM)
  * [AutoML for XGBoost](.docs/Examples/AutoML-for-XGBoost)
  * [Default - Flamlized Estimator](.docs/Examples/Default-Flamlized)
  * [Integrate - AzureML](.docs/Examples/Integrate - AzureML)
  * [Integrate - Scikit-learn Pipeline](.docs/Examples/Integrate - Scikit-learn Pipeline)
  * [Integrate - Spark](.docs/Examples/Integrate - Spark)
  * [Tune - AzureML pipeline](.docs/Examples/Tune-AzureML-pipeline)
  * [Tune - HuggingFace](.docs/Examples/Tune-HuggingFace)
  * [Tune - Lexicographic Objectives](.docs/Examples/Tune-Lexicographic-objectives)
  * [Tune - PyTorch](.docs/Examples/Tune-PyTorch)
* [Contributing](.docs/Contribute)
* [Research](.docs/Research)

On this page

# AutoML - Time Series Forecast {#automl-time-series-forecast}

### Prerequisites​ {#prerequisites}

Install the [automl,ts_forecast] option.

    pip install "flaml[automl,ts_forecast]"  

### Simple NumPy Example​ {#simple-numpy-example}

    import numpy as np  
    from flaml import AutoML  
      
    X_train = np.arange('2014-01', '2022-01', dtype='datetime64[M]')  
    y_train = np.random.random(size=84)  
    automl = AutoML()  
    automl.fit(X_train=X_train[:84],  # a single column of timestamp  
               y_train=y_train,  # value for each timestamp  
               period=12,  # time horizon to forecast, e.g., 12 months  
               task='ts_forecast', time_budget=15,  # time budget in seconds  
               log_file_name="ts_forecast.log",  
               eval_method="holdout",  
              )  
    print(automl.predict(X_train[84:]))  

#### Sample output​ {#sample-output} {#sample-output}

    [flaml.automl: 01-21 08:01:20] {2018} INFO - task = ts_forecast  
    [flaml.automl: 01-21 08:01:20] {2020} INFO - Data split method: time  
    [flaml.automl: 01-21 08:01:20] {2024} INFO - Evaluation method: holdout  
    [flaml.automl: 01-21 08:01:20] {2124} INFO - Minimizing error metric: mape  
    [flaml.automl: 01-21 08:01:21] {2181} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth', 'prophet', 'arima', 'sarimax']  
    [flaml.automl: 01-21 08:01:21] {2434} INFO - iteration 0, current learner lgbm  
    [flaml.automl: 01-21 08:01:21] {2547} INFO - Estimated sufficient time budget=1429s. Estimated necessary time budget=1s.  
    [flaml.automl: 01-21 08:01:21] {2594} INFO -  at 0.9s,  estimator lgbm's best error=0.9811,     best estimator lgbm's best error=0.9811  
    [flaml.automl: 01-21 08:01:21] {2434} INFO - iteration 1, current learner lgbm  
    [flaml.automl: 01-21 08:01:21] {2594} INFO -  at 0.9s,  estimator lgbm's best error=0.9811,     best estimator lgbm's best error=0.9811  
    [flaml.automl: 01-21 08:01:21] {2434} INFO - iteration 2, current learner lgbm  
    [flaml.automl: 01-21 08:01:21] {2594} INFO -  at 0.9s,  estimator lgbm's best error=0.9811,     best estimator lgbm's best error=0.9811  
    [flaml.automl: 01-21 08:01:21] {2434} INFO - iteration 3, current learner lgbm  
    [flaml.automl: 01-21 08:01:21] {2594} INFO -  at 1.0s,  estimator lgbm's best error=0.9811,     best estimator lgbm's best error=0.9811  
    [flaml.automl: 01-21 08:01:21] {2434} INFO - iteration 4, current learner lgbm  
    [flaml.automl: 01-21 08:01:21] {2594} INFO -  at 1.0s,  estimator lgbm's best error=0.9811,     best estimator lgbm's best error=0.9811  
    [flaml.automl: 01-21 08:01:21] {2434} INFO - iteration 5, current learner lgbm  
    [flaml.automl: 01-21 08:01:21] {2594} INFO -  at 1.0s,  estimator lgbm's best error=0.9811,     best estimator lgbm's best error=0.9811  
    [flaml.automl: 01-21 08:01:21] {2434} INFO - iteration 6, current learner lgbm  
    [flaml.automl: 01-21 08:01:21] {2594} INFO -  at 1.0s,  estimator lgbm's best error=0.9652,     best estimator lgbm's best error=0.9652  
    [flaml.automl: 01-21 08:01:21] {2434} INFO - iteration 7, current learner lgbm  
    [flaml.automl: 01-21 08:01:21] {2594} INFO -  at 1.0s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:21] {2434} INFO - iteration 8, current learner lgbm  
    [flaml.automl: 01-21 08:01:21] {2594} INFO -  at 1.0s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:21] {2434} INFO - iteration 9, current learner lgbm  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.1s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 10, current learner lgbm  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.1s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 11, current learner lgbm  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.1s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 12, current learner lgbm  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.1s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 13, current learner lgbm  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.1s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 14, current learner lgbm  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.1s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 15, current learner lgbm  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.2s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 16, current learner lgbm  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.2s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 17, current learner lgbm  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.2s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 18, current learner rf  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.2s,  estimator rf's best error=1.0994,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 19, current learner rf  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.2s,  estimator rf's best error=1.0848,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 20, current learner xgboost  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.3s,  estimator xgboost's best error=1.0271,  best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 21, current learner rf  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.3s,  estimator rf's best error=1.0848,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 22, current learner xgboost  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.3s,  estimator xgboost's best error=1.0015,  best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 23, current learner xgboost  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.3s,  estimator xgboost's best error=1.0015,  best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 24, current learner xgboost  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.3s,  estimator xgboost's best error=1.0015,  best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 25, current learner extra_tree  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.3s,  estimator extra_tree's best error=1.0130,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 26, current learner extra_tree  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.4s,  estimator extra_tree's best error=1.0130,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 27, current learner extra_tree  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.4s,  estimator extra_tree's best error=1.0130,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 28, current learner extra_tree  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.4s,  estimator extra_tree's best error=1.0130,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 29, current learner extra_tree  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.4s,  estimator extra_tree's best error=0.9499,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 30, current learner lgbm  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.5s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 31, current learner lgbm  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.5s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 32, current learner lgbm  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.5s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 33, current learner extra_tree  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.5s,  estimator extra_tree's best error=0.9499,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 34, current learner lgbm  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.5s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 35, current learner xgboost  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.5s,  estimator xgboost's best error=1.0015,  best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 36, current learner extra_tree  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.6s,  estimator extra_tree's best error=0.9499,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 37, current learner extra_tree  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.6s,  estimator extra_tree's best error=0.9499,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 38, current learner extra_tree  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.6s,  estimator extra_tree's best error=0.9499,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 39, current learner xgboost  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.6s,  estimator xgboost's best error=1.0015,  best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 40, current learner extra_tree  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.6s,  estimator extra_tree's best error=0.9499,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 41, current learner extra_tree  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.7s,  estimator extra_tree's best error=0.9499,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 42, current learner lgbm  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.7s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 43, current learner extra_tree  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.7s,  estimator extra_tree's best error=0.9499,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 44, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.7s,  estimator xgb_limitdepth's best error=1.5815,   best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 45, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.8s,  estimator xgb_limitdepth's best error=0.9683,   best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 46, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.8s,  estimator xgb_limitdepth's best error=0.9683,   best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 47, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.8s,  estimator xgb_limitdepth's best error=0.9683,   best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 48, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.9s,  estimator xgb_limitdepth's best error=0.9683,   best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 49, current learner lgbm  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.9s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 50, current learner extra_tree  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.9s,  estimator extra_tree's best error=0.9499,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 51, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.9s,  estimator xgb_limitdepth's best error=0.9683,   best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 52, current learner xgboost  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 2.0s,  estimator xgboost's best error=1.0015,  best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 53, current learner xgboost  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 2.0s,  estimator xgboost's best error=1.0015,  best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 54, current learner lgbm  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 2.0s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 55, current learner lgbm  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 2.0s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 56, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 2.0s,  estimator xgb_limitdepth's best error=0.9683,   best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 57, current learner rf  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 2.0s,  estimator rf's best error=1.0848,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 58, current learner xgboost  
    [flaml.automl: 01-21 08:01:23] {2594} INFO -  at 2.1s,  estimator xgboost's best error=1.0015,  best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:23] {2434} INFO - iteration 59, current learner extra_tree  
    [flaml.automl: 01-21 08:01:23] {2594} INFO -  at 2.1s,  estimator extra_tree's best error=0.9499,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:23] {2434} INFO - iteration 60, current learner lgbm  
    [flaml.automl: 01-21 08:01:23] {2594} INFO -  at 2.1s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:23] {2434} INFO - iteration 61, current learner extra_tree  
    [flaml.automl: 01-21 08:01:23] {2594} INFO -  at 2.1s,  estimator extra_tree's best error=0.9499,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:23] {2434} INFO - iteration 62, current learner lgbm  
    [flaml.automl: 01-21 08:01:23] {2594} INFO -  at 2.1s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:23] {2434} INFO - iteration 63, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:23] {2594} INFO -  at 2.2s,  estimator xgb_limitdepth's best error=0.9683,   best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:23] {2434} INFO - iteration 64, current learner prophet  
    [flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.2s,  estimator prophet's best error=1.5706,  best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 65, current learner arima  
    [flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.2s,  estimator arima's best error=0.5693,    best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 66, current learner arima  
    [flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.4s,  estimator arima's best error=0.5693,    best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 67, current learner sarimax  
    [flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.4s,  estimator sarimax's best error=0.5693,  best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 68, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.5s,  estimator xgb_limitdepth's best error=0.9683,   best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 69, current learner sarimax  
    [flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.6s,  estimator sarimax's best error=0.5693,  best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 70, current learner sarimax  
    [flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.6s,  estimator sarimax's best error=0.5693,  best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 71, current learner arima  
    [flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.6s,  estimator arima's best error=0.5693,    best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 72, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.6s,  estimator xgb_limitdepth's best error=0.9683,   best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 73, current learner arima  
    [flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.7s,  estimator arima's best error=0.5693,    best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 74, current learner sarimax  
    [flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.7s,  estimator sarimax's best error=0.5693,  best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 75, current learner arima  
    [flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.8s,  estimator arima's best error=0.5693,    best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 76, current learner sarimax  
    [flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.9s,  estimator sarimax's best error=0.5693,  best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 77, current learner arima  
    [flaml.automl: 01-21 08:01:25] {2594} INFO -  at 5.0s,  estimator arima's best error=0.5693,    best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 78, current learner sarimax  
    [flaml.automl: 01-21 08:01:26] {2594} INFO -  at 5.1s,  estimator sarimax's best error=0.5693,  best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:26] {2434} INFO - iteration 79, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:26] {2594} INFO -  at 5.1s,  estimator xgb_limitdepth's best error=0.9683,   best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:26] {2434} INFO - iteration 80, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:26] {2594} INFO -  at 5.1s,  estimator xgb_limitdepth's best error=0.9683,   best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:26] {2434} INFO - iteration 81, current learner sarimax  
    [flaml.automl: 01-21 08:01:26] {2594} INFO -  at 5.1s,  estimator sarimax's best error=0.5693,  best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:26] {2434} INFO - iteration 82, current learner prophet  
    [flaml.automl: 01-21 08:01:27] {2594} INFO -  at 6.6s,  estimator prophet's best error=1.4076,  best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:27] {2434} INFO - iteration 83, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:27] {2594} INFO -  at 6.6s,  estimator xgb_limitdepth's best error=0.9683,   best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:27] {2434} INFO - iteration 84, current learner sarimax  
    [flaml.automl: 01-21 08:01:27] {2594} INFO -  at 6.6s,  estimator sarimax's best error=0.5693,  best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:27] {2434} INFO - iteration 85, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:27] {2594} INFO -  at 6.6s,  estimator xgb_limitdepth's best error=0.9683,   best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:27] {2434} INFO - iteration 86, current learner sarimax  
    [flaml.automl: 01-21 08:01:27] {2594} INFO -  at 6.8s,  estimator sarimax's best error=0.5693,  best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:27] {2434} INFO - iteration 87, current learner arima  
    [flaml.automl: 01-21 08:01:27] {2594} INFO -  at 6.8s,  estimator arima's best error=0.5693,    best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:27] {2434} INFO - iteration 88, current learner sarimax  
    [flaml.automl: 01-21 08:01:27] {2594} INFO -  at 6.9s,  estimator sarimax's best error=0.5693,  best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:27] {2434} INFO - iteration 89, current learner arima  
    [flaml.automl: 01-21 08:01:27] {2594} INFO -  at 6.9s,  estimator arima's best error=0.5693,    best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:27] {2434} INFO - iteration 90, current learner arima  
    [flaml.automl: 01-21 08:01:27] {2594} INFO -  at 7.0s,  estimator arima's best error=0.5693,    best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:27] {2434} INFO - iteration 91, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:27] {2594} INFO -  at 7.0s,  estimator xgb_limitdepth's best error=0.9683,   best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:27] {2434} INFO - iteration 92, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:27] {2594} INFO -  at 7.0s,  estimator xgb_limitdepth's best error=0.9683,   best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:27] {2434} INFO - iteration 93, current learner sarimax  
    [flaml.automl: 01-21 08:01:28] {2594} INFO -  at 7.0s,  estimator sarimax's best error=0.5600,  best estimator sarimax's best error=0.5600  
    [flaml.automl: 01-21 08:01:28] {2434} INFO - iteration 94, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:28] {2594} INFO -  at 7.1s,  estimator xgb_limitdepth's best error=0.9683,   best estimator sarimax's best error=0.5600  
    [flaml.automl: 01-21 08:01:28] {2434} INFO - iteration 95, current learner sarimax  
    [flaml.automl: 01-21 08:01:28] {2594} INFO -  at 7.2s,  estimator sarimax's best error=0.5600,  best estimator sarimax's best error=0.5600  
    [flaml.automl: 01-21 08:01:28] {2434} INFO - iteration 96, current learner arima  
    [flaml.automl: 01-21 08:01:28] {2594} INFO -  at 7.2s,  estimator arima's best error=0.5693,    best estimator sarimax's best error=0.5600  
    [flaml.automl: 01-21 08:01:28] {2434} INFO - iteration 97, current learner arima  
    [flaml.automl: 01-21 08:01:28] {2594} INFO -  at 7.2s,  estimator arima's best error=0.5693,    best estimator sarimax's best error=0.5600  
    [flaml.automl: 01-21 08:01:28] {2434} INFO - iteration 98, current learner extra_tree  
    [flaml.automl: 01-21 08:01:28] {2594} INFO -  at 7.3s,  estimator extra_tree's best error=0.9499,       best estimator sarimax's best error=0.5600  
    [flaml.automl: 01-21 08:01:28] {2434} INFO - iteration 99, current learner sarimax  
    [flaml.automl: 01-21 08:01:28] {2594} INFO -  at 7.3s,  estimator sarimax's best error=0.5600,  best estimator sarimax's best error=0.5600  
    [flaml.automl: 01-21 08:01:28] {2434} INFO - iteration 100, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:28] {2594} INFO -  at 7.3s,  estimator xgb_limitdepth's best error=0.9683,   best estimator sarimax's best error=0.5600  

### Univariate time series​ {#univariate-time-series}

    import statsmodels.api as sm  
      
    data = sm.datasets.co2.load_pandas().data  
    # data is given in weeks, but the task is to predict monthly, so use monthly averages instead  
    data = data['co2'].resample('MS').mean()  
    data = data.bfill().ffill()  # makes sure there are no missing values  
    data = data.to_frame().reset_index()  
    num_samples = data.shape[0]  
    time_horizon = 12  
    split_idx = num_samples - time_horizon  
    train_df = data[:split_idx]  # train_df is a dataframe with two columns: timestamp and label  
    X_test = data[split_idx:]['index'].to_frame()  # X_test is a dataframe with dates for prediction  
    y_test = data[split_idx:]['co2']  # y_test is a series of the values corresponding to the dates for prediction  
      
    from flaml import AutoML  
      
    automl = AutoML()  
    settings = {  
        "time_budget": 10,  # total running time in seconds  
        "metric": 'mape',  # primary metric for validation: 'mape' is generally used for forecast tasks  
        "task": 'ts_forecast',  # task type  
        "log_file_name": 'CO2_forecast.log',  # flaml log file  
        "eval_method": "holdout",  # validation method can be chosen from ['auto', 'holdout', 'cv']  
        "seed": 7654321,  # random seed  
    }  
      
    automl.fit(dataframe=train_df,  # training data  
               label='co2',  # label column  
               period=time_horizon,  # key word argument 'period' must be included for forecast task)  
               **settings)  

#### Sample output​ {#sample-output} {#sample-output}

    [flaml.automl: 01-21 07:54:04] {2018} INFO - task = ts_forecast  
    [flaml.automl: 01-21 07:54:04] {2020} INFO - Data split method: time  
    [flaml.automl: 01-21 07:54:04] {2024} INFO - Evaluation method: holdout  
    [flaml.automl: 01-21 07:54:04] {2124} INFO - Minimizing error metric: mape  
    Importing plotly failed. Interactive plots will not work.  
    [flaml.automl: 01-21 07:54:04] {2181} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth', 'prophet', 'arima', 'sarimax']  
    [flaml.automl: 01-21 07:54:04] {2434} INFO - iteration 0, current learner lgbm  
    [flaml.automl: 01-21 07:54:05] {2547} INFO - Estimated sufficient time budget=2145s. Estimated necessary time budget=2s.  
    [flaml.automl: 01-21 07:54:05] {2594} INFO -  at 0.9s,  estimator lgbm's best error=0.0621,     best estimator lgbm's best error=0.0621  
    [flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 1, current learner lgbm  
    [flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.0s,  estimator lgbm's best error=0.0574,     best estimator lgbm's best error=0.0574  
    [flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 2, current learner lgbm  
    [flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.0s,  estimator lgbm's best error=0.0464,     best estimator lgbm's best error=0.0464  
    [flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 3, current learner lgbm  
    [flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.0s,  estimator lgbm's best error=0.0464,     best estimator lgbm's best error=0.0464  
    [flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 4, current learner lgbm  
    [flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.0s,  estimator lgbm's best error=0.0365,     best estimator lgbm's best error=0.0365  
    [flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 5, current learner lgbm  
    [flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.1s,  estimator lgbm's best error=0.0192,     best estimator lgbm's best error=0.0192  
    [flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 6, current learner lgbm  
    [flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.1s,  estimator lgbm's best error=0.0192,     best estimator lgbm's best error=0.0192  
    [flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 7, current learner lgbm  
    [flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.1s,  estimator lgbm's best error=0.0192,     best estimator lgbm's best error=0.0192  
    [flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 8, current learner lgbm  
    [flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.2s,  estimator lgbm's best error=0.0110,     best estimator lgbm's best error=0.0110  
    [flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 9, current learner lgbm  
    [flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.2s,  estimator lgbm's best error=0.0110,     best estimator lgbm's best error=0.0110  
    [flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 10, current learner lgbm  
    [flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.2s,  estimator lgbm's best error=0.0036,     best estimator lgbm's best error=0.0036  
    [flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 11, current learner lgbm  
    [flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.4s,  estimator lgbm's best error=0.0023,     best estimator lgbm's best error=0.0023  
    [flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 12, current learner lgbm  
    [flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.4s,  estimator lgbm's best error=0.0023,     best estimator lgbm's best error=0.0023  
    [flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 13, current learner lgbm  
    [flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.5s,  estimator lgbm's best error=0.0021,     best estimator lgbm's best error=0.0021  
    [flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 14, current learner lgbm  
    [flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.6s,  estimator lgbm's best error=0.0021,     best estimator lgbm's best error=0.0021  
    [flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 15, current learner lgbm  
    [flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.7s,  estimator lgbm's best error=0.0020,     best estimator lgbm's best error=0.0020  
    [flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 16, current learner lgbm  
    [flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.8s,  estimator lgbm's best error=0.0017,     best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 17, current learner lgbm  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 1.9s,  estimator lgbm's best error=0.0017,     best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 18, current learner lgbm  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.0s,  estimator lgbm's best error=0.0017,     best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 19, current learner lgbm  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.1s,  estimator lgbm's best error=0.0017,     best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 20, current learner rf  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.1s,  estimator rf's best error=0.0228,       best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 21, current learner rf  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.1s,  estimator rf's best error=0.0210,       best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 22, current learner xgboost  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.2s,  estimator xgboost's best error=0.6738,  best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 23, current learner xgboost  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.2s,  estimator xgboost's best error=0.6738,  best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 24, current learner xgboost  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.2s,  estimator xgboost's best error=0.1717,  best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 25, current learner xgboost  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.3s,  estimator xgboost's best error=0.0249,  best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 26, current learner xgboost  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.3s,  estimator xgboost's best error=0.0249,  best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 27, current learner xgboost  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.3s,  estimator xgboost's best error=0.0242,  best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 28, current learner extra_tree  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.4s,  estimator extra_tree's best error=0.0245,       best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 29, current learner extra_tree  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.4s,  estimator extra_tree's best error=0.0160,       best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 30, current learner lgbm  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.5s,  estimator lgbm's best error=0.0017,     best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 31, current learner lgbm  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.6s,  estimator lgbm's best error=0.0017,     best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 32, current learner rf  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.6s,  estimator rf's best error=0.0210,       best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 33, current learner extra_tree  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.6s,  estimator extra_tree's best error=0.0160,       best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 34, current learner lgbm  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.8s,  estimator lgbm's best error=0.0017,     best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 35, current learner extra_tree  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.8s,  estimator extra_tree's best error=0.0158,       best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 36, current learner xgb_limitdepth  
    [flaml.automl: 01-21 07:54:07] {2594} INFO -  at 2.8s,  estimator xgb_limitdepth's best error=0.0447,   best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:07] {2434} INFO - iteration 37, current learner xgb_limitdepth  
    [flaml.automl: 01-21 07:54:07] {2594} INFO -  at 2.9s,  estimator xgb_limitdepth's best error=0.0447,   best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:07] {2434} INFO - iteration 38, current learner xgb_limitdepth  
    [flaml.automl: 01-21 07:54:07] {2594} INFO -  at 2.9s,  estimator xgb_limitdepth's best error=0.0029,   best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:07] {2434} INFO - iteration 39, current learner xgb_limitdepth  
    [flaml.automl: 01-21 07:54:07] {2594} INFO -  at 3.0s,  estimator xgb_limitdepth's best error=0.0018,   best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:07] {2434} INFO - iteration 40, current learner xgb_limitdepth  
    [flaml.automl: 01-21 07:54:07] {2594} INFO -  at 3.1s,  estimator xgb_limitdepth's best error=0.0018,   best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:07] {2434} INFO - iteration 41, current learner xgb_limitdepth  
    [flaml.automl: 01-21 07:54:07] {2594} INFO -  at 3.1s,  estimator xgb_limitdepth's best error=0.0018,   best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:07] {2434} INFO - iteration 42, current learner xgb_limitdepth  
    [flaml.automl: 01-21 07:54:07] {2594} INFO -  at 3.3s,  estimator xgb_limitdepth's best error=0.0018,   best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:07] {2434} INFO - iteration 43, current learner prophet  
    [flaml.automl: 01-21 07:54:09] {2594} INFO -  at 5.5s,  estimator prophet's best error=0.0008,  best estimator prophet's best error=0.0008  
    [flaml.automl: 01-21 07:54:09] {2434} INFO - iteration 44, current learner arima  
    [flaml.automl: 01-21 07:54:10] {2594} INFO -  at 6.1s,  estimator arima's best error=0.0047,    best estimator prophet's best error=0.0008  
    [flaml.automl: 01-21 07:54:10] {2434} INFO - iteration 45, current learner sarimax  
    [flaml.automl: 01-21 07:54:10] {2594} INFO -  at 6.4s,  estimator sarimax's best error=0.0047,  best estimator prophet's best error=0.0008  
    [flaml.automl: 01-21 07:54:10] {2434} INFO - iteration 46, current learner lgbm  
    [flaml.automl: 01-21 07:54:10] {2594} INFO -  at 6.5s,  estimator lgbm's best error=0.0017,     best estimator prophet's best error=0.0008  
    [flaml.automl: 01-21 07:54:10] {2434} INFO - iteration 47, current learner sarimax  
    [flaml.automl: 01-21 07:54:10] {2594} INFO -  at 6.6s,  estimator sarimax's best error=0.0047,  best estimator prophet's best error=0.0008  
    [flaml.automl: 01-21 07:54:10] {2434} INFO - iteration 48, current learner sarimax  
    [flaml.automl: 01-21 07:54:11] {2594} INFO -  at 6.9s,  estimator sarimax's best error=0.0047,  best estimator prophet's best error=0.0008  
    [flaml.automl: 01-21 07:54:11] {2434} INFO - iteration 49, current learner arima  
    [flaml.automl: 01-21 07:54:11] {2594} INFO -  at 6.9s,  estimator arima's best error=0.0047,    best estimator prophet's best error=0.0008  
    [flaml.automl: 01-21 07:54:11] {2434} INFO - iteration 50, current learner xgb_limitdepth  
    [flaml.automl: 01-21 07:54:11] {2594} INFO -  at 7.0s,  estimator xgb_limitdepth's best error=0.0018,   best estimator prophet's best error=0.0008  
    [flaml.automl: 01-21 07:54:11] {2434} INFO - iteration 51, current learner sarimax  
    [flaml.automl: 01-21 07:54:11] {2594} INFO -  at 7.5s,  estimator sarimax's best error=0.0047,  best estimator prophet's best error=0.0008  
    [flaml.automl: 01-21 07:54:11] {2434} INFO - iteration 52, current learner xgboost  
    [flaml.automl: 01-21 07:54:11] {2594} INFO -  at 7.6s,  estimator xgboost's best error=0.0242,  best estimator prophet's best error=0.0008  
    [flaml.automl: 01-21 07:54:11] {2434} INFO - iteration 53, current learner prophet  
    [flaml.automl: 01-21 07:54:13] {2594} INFO -  at 9.3s,  estimator prophet's best error=0.0005,  best estimator prophet's best error=0.0005  
    [flaml.automl: 01-21 07:54:13] {2434} INFO - iteration 54, current learner sarimax  
    [flaml.automl: 01-21 07:54:13] {2594} INFO -  at 9.4s,  estimator sarimax's best error=0.0047,  best estimator prophet's best error=0.0005  
    [flaml.automl: 01-21 07:54:13] {2434} INFO - iteration 55, current learner xgb_limitdepth  
    [flaml.automl: 01-21 07:54:13] {2594} INFO -  at 9.8s,  estimator xgb_limitdepth's best error=0.0018,   best estimator prophet's best error=0.0005  
    [flaml.automl: 01-21 07:54:13] {2434} INFO - iteration 56, current learner xgboost  
    [flaml.automl: 01-21 07:54:13] {2594} INFO -  at 9.8s,  estimator xgboost's best error=0.0242,  best estimator prophet's best error=0.0005  
    [flaml.automl: 01-21 07:54:13] {2434} INFO - iteration 57, current learner lgbm  
    [flaml.automl: 01-21 07:54:14] {2594} INFO -  at 9.9s,  estimator lgbm's best error=0.0017,     best estimator prophet's best error=0.0005  
    [flaml.automl: 01-21 07:54:14] {2434} INFO - iteration 58, current learner rf  
    [flaml.automl: 01-21 07:54:14] {2594} INFO -  at 10.0s, estimator rf's best error=0.0146,       best estimator prophet's best error=0.0005  
    [flaml.automl: 01-21 07:54:14] {2824} INFO - retrain prophet for 0.6s  
    [flaml.automl: 01-21 07:54:14] {2831} INFO - retrained model: <prophet.forecaster.Prophet object at 0x7fb68ea65d60>  
    [flaml.automl: 01-21 07:54:14] {2210} INFO - fit succeeded  
    [flaml.automl: 01-21 07:54:14] {2211} INFO - Time taken to find the best model: 9.339771270751953  
    [flaml.automl: 01-21 07:54:14] {2222} WARNING - Time taken to find the best model is 93% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.  

#### Compute and plot predictions​ {#compute-and-plot-predictions}

The example plotting code requires matplotlib.

    flaml_y_pred = automl.predict(X_test)  
    import matplotlib.pyplot as plt  
      
    plt.plot(X_test, y_test, label='Actual level')  
    plt.plot(X_test, flaml_y_pred, label='FLAML forecast')  
    plt.xlabel('Date')  
    plt.ylabel('CO2 Levels')  
    plt.legend()  

![png](.assets/images/CO2-8a52a5b6467f2f3c0b4bc0fc516d5a62.png)

### Multivariate Time Series (Forecasting with Exogenous Variables)​ {#multivariate-time-series-forecasting-with-exogenous-variables}
```python
    import pandas as pd  
      
    # pd.set_option("display.max_rows", None, "display.max_columns", None)  
    multi_df = pd.read_csv(  
        "https://raw.githubusercontent.com/srivatsan88/YouTubeLI/master/dataset/nyc_energy_consumption.csv"  
    )  
      
    # preprocessing data  
    multi_df["timeStamp"] = pd.to_datetime(multi_df["timeStamp"])  
    multi_df = multi_df.set_index("timeStamp")  
    multi_df = multi_df.resample("D").mean()  
    multi_df["temp"] = multi_df["temp"].fillna(method="ffill")  
    multi_df["precip"] = multi_df["precip"].fillna(method="ffill")  
    multi_df = multi_df[:-2]  # last two rows are NaN for 'demand' column so remove them  
    multi_df = multi_df.reset_index()  
      
    # Using temperature values create categorical values  
    # where 1 denotes daily tempurature is above monthly average and 0 is below.  
    def get_monthly_avg(data):  
        data["month"] = data["timeStamp"].dt.month  
        data = data[["month", "temp"]].groupby("month")  
        data = data.agg({"temp": "mean"})  
        return data  
      
    monthly_avg = get_monthly_avg(multi_df).to_dict().get("temp")  
      
    def above_monthly_avg(date, temp):  
        month = date.month  
        if temp > monthly_avg.get(month):  
            return 1  
        else:  
            return 0  
      
    multi_df["temp_above_monthly_avg"] = multi_df.apply(  
        lambda x: above_monthly_avg(x["timeStamp"], x["temp"]), axis=1  
    )  
      
    del multi_df["month"]  # remove temperature column to reduce redundancy  
      
    # split data into train and test  
    num_samples = multi_df.shape[0]  
    multi_time_horizon = 180  
    split_idx = num_samples - multi_time_horizon  
    multi_train_df = multi_df[:split_idx]  
    multi_test_df = multi_df[split_idx:]  
      
    multi_X_test = multi_test_df[  
        ["timeStamp", "precip", "temp", "temp_above_monthly_avg"]  
    ]  # test dataframe must contain values for the regressors / multivariate variables  
    multi_y_test = multi_test_df["demand"]  
      
    # initialize AutoML instance  
    automl = AutoML()  
      
    # configure AutoML settings  
    settings = {  
        "time_budget": 10,  # total running time in seconds  
        "metric": "mape",  # primary metric  
        "task": "ts_forecast",  # task type  
        "log_file_name": "energy_forecast_categorical.log",  # flaml log file  
        "eval_method": "holdout",  
        "log_type": "all",  
        "label": "demand",  
    }  
      
    # train the model  
    automl.fit(dataframe=df, **settings, period=time_horizon)  
      
    # predictions  
    print(automl.predict(multi_X_test))  
```

#### Sample Output​ {#sample-output} {#sample-output} {#sample-output}

    [flaml.automl: 08-13 01:03:11] {2540} INFO - task = ts_forecast  
    [flaml.automl: 08-13 01:03:11] {2542} INFO - Data split method: time  
    [flaml.automl: 08-13 01:03:11] {2545} INFO - Evaluation method: holdout  
    [flaml.automl: 08-13 01:03:11] {2664} INFO - Minimizing error metric: mape  
    [flaml.automl: 08-13 01:03:12] {2806} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth', 'prophet', 'arima', 'sarimax']  
    [flaml.automl: 08-13 01:03:12] {3108} INFO - iteration 0, current learner lgbm  
    [flaml.automl: 08-13 01:03:12] {3241} INFO - Estimated sufficient time budget=7681s. Estimated necessary time budget=8s.  
    [flaml.automl: 08-13 01:03:12] {3288} INFO -  at 0.8s,  estimator lgbm's best error=0.0854, best estimator lgbm's best error=0.0854  
    [flaml.automl: 08-13 01:03:12] {3108} INFO - iteration 1, current learner lgbm  
    [flaml.automl: 08-13 01:03:12] {3288} INFO -  at 0.9s,  estimator lgbm's best error=0.0854, best estimator lgbm's best error=0.0854  
    [flaml.automl: 08-13 01:03:12] {3108} INFO - iteration 2, current learner lgbm  
    [flaml.automl: 08-13 01:03:12] {3288} INFO -  at 0.9s,  estimator lgbm's best error=0.0525, best estimator lgbm's best error=0.0525  
    [flaml.automl: 08-13 01:03:12] {3108} INFO - iteration 3, current learner lgbm  
    [flaml.automl: 08-13 01:03:12] {3288} INFO -  at 0.9s,  estimator lgbm's best error=0.0525, best estimator lgbm's best error=0.0525  
    [flaml.automl: 08-13 01:03:12] {3108} INFO - iteration 4, current learner lgbm  
    [flaml.automl: 08-13 01:03:12] {3288} INFO -  at 1.0s,  estimator lgbm's best error=0.0406, best estimator lgbm's best error=0.0406  
    [flaml.automl: 08-13 01:03:12] {3108} INFO - iteration 5, current learner lgbm  
    [flaml.automl: 08-13 01:03:12] {3288} INFO -  at 1.0s,  estimator lgbm's best error=0.0406, best estimator lgbm's best error=0.0406  
    [flaml.automl: 08-13 01:03:12] {3108} INFO - iteration 6, current learner lgbm  
    [flaml.automl: 08-13 01:03:12] {3288} INFO -  at 1.0s,  estimator lgbm's best error=0.0406, best estimator lgbm's best error=0.0406  
    [flaml.automl: 08-13 01:03:12] {3108} INFO - iteration 7, current learner lgbm  
    [flaml.automl: 08-13 01:03:13] {3288} INFO -  at 1.1s,  estimator lgbm's best error=0.0393, best estimator lgbm's best error=0.0393  
    [flaml.automl: 08-13 01:03:13] {3108} INFO - iteration 8, current learner lgbm  
    [flaml.automl: 08-13 01:03:13] {3288} INFO -  at 1.1s,  estimator lgbm's best error=0.0393, best estimator lgbm's best error=0.0393  
    [flaml.automl: 08-13 01:03:13] {3108} INFO - iteration 9, current learner lgbm  
    ...  
                  silent=True, subsample=1.0, subsample_for_bin=200000,  
                  subsample_freq=0, verbose=-1)  
    [flaml.automl: 08-13 01:03:22] {2837} INFO - fit succeeded  
    [flaml.automl: 08-13 01:03:22] {2838} INFO - Time taken to find the best model: 3.4941744804382324  

### Forecasting Discrete Variables​ {#forecasting-discrete-variables}

```python
    from hcrystalball.utils import get_sales_data  
    import numpy as np  
    from flaml import AutoML  
      
    time_horizon = 30  
    df = get_sales_data(n_dates=180, n_assortments=1, n_states=1, n_stores=1)  
    df = df[["Sales", "Open", "Promo", "Promo2"]]  
      
    # feature engineering - create a discrete value column  
    # 1 denotes above mean and 0 denotes below mean  
    df["above_mean_sales"] = np.where(df["Sales"] > df["Sales"].mean(), 1, 0)  
    df.reset_index(inplace=True)  
      
    # train-test split  
    discrete_train_df = df[:-time_horizon]  
    discrete_test_df = df[-time_horizon:]  
    discrete_X_train, discrete_X_test = (  
        discrete_train_df[["Date", "Open", "Promo", "Promo2"]],  
        discrete_test_df[["Date", "Open", "Promo", "Promo2"]],  
    )  
    discrete_y_train, discrete_y_test = discrete_train_df["above_mean_sales"], discrete_test_df["above_mean_sales"]  
      
    # initialize AutoML instance  
    automl = AutoML()  
      
    # configure the settings  
    settings = {  
        "time_budget": 15,  # total running time in seconds  
        "metric": "accuracy",  # primary metric  
        "task": "ts_forecast_classification",  # task type  
        "log_file_name": "sales_classification_forecast.log",  # flaml log file  
        "eval_method": "holdout",  
    }  
      
    # train the model  
    automl.fit(X_train=discrete_X_train,  
               y_train=discrete_y_train,  
               **settings,  
               period=time_horizon)  
      
    # make predictions  
    discrete_y_pred = automl.predict(discrete_X_test)  
    print("Predicted label", discrete_y_pred)  
    print("True label", discrete_y_test)  
```
#### Sample Output​ {#sample-output} {#sample-output} {#sample-output}

    [flaml.automl: 02-28 21:53:03] {2060} INFO - task = ts_forecast_classification  
    [flaml.automl: 02-28 21:53:03] {2062} INFO - Data split method: time  
    [flaml.automl: 02-28 21:53:03] {2066} INFO - Evaluation method: holdout  
    [flaml.automl: 02-28 21:53:03] {2147} INFO - Minimizing error metric: 1-accuracy  
    [flaml.automl: 02-28 21:53:03] {2205} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth']  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 0, current learner lgbm  
    [flaml.automl: 02-28 21:53:03] {2573} INFO - Estimated sufficient time budget=269s. Estimated necessary time budget=0s.  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.1s,  estimator lgbm's best error=0.2667, best estimator lgbm's best error=0.2667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 1, current learner lgbm  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.1s,  estimator lgbm's best error=0.2667, best estimator lgbm's best error=0.2667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 2, current learner lgbm  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.1s,  estimator lgbm's best error=0.1333, best estimator lgbm's best error=0.1333  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 3, current learner rf  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.2s,  estimator rf's best error=0.1333,   best estimator lgbm's best error=0.1333  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 4, current learner xgboost  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.2s,  estimator xgboost's best error=0.1333,  best estimator lgbm's best error=0.1333  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 5, current learner lgbm  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.2s,  estimator lgbm's best error=0.1333, best estimator lgbm's best error=0.1333  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 6, current learner rf  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.3s,  estimator rf's best error=0.0667,   best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 7, current learner lgbm  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.3s,  estimator lgbm's best error=0.0667, best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 8, current learner lgbm  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.3s,  estimator lgbm's best error=0.0667, best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 9, current learner lgbm  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.4s,  estimator lgbm's best error=0.0667, best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 10, current learner rf  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.4s,  estimator rf's best error=0.0667,   best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 11, current learner rf  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.4s,  estimator rf's best error=0.0667,   best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 12, current learner xgboost  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.5s,  estimator xgboost's best error=0.1333,  best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 13, current learner extra_tree  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.5s,  estimator extra_tree's best error=0.1333,   best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 14, current learner xgb_limitdepth  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.5s,  estimator xgb_limitdepth's best error=0.0667,   best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 15, current learner xgboost  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.6s,  estimator xgboost's best error=0.0667,  best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 16, current learner xgb_limitdepth  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.6s,  estimator xgb_limitdepth's best error=0.0667,   best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 17, current learner rf  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.6s,  estimator rf's best error=0.0667,   best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 18, current learner xgb_limitdepth  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.7s,  estimator xgb_limitdepth's best error=0.0667,   best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 19, current learner lgbm  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.7s,  estimator lgbm's best error=0.0667, best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 20, current learner extra_tree  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.7s,  estimator extra_tree's best error=0.0667,   best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 21, current learner xgboost  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.7s,  estimator xgboost's best error=0.0667,  best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 22, current learner extra_tree  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.8s,  estimator extra_tree's best error=0.0667,   best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 23, current learner rf  
    [flaml.automl: 02-28 21:53:04] {2620} INFO -  at 0.8s,  estimator rf's best error=0.0667,   best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:04] {2458} INFO - iteration 24, current learner xgboost  
    [flaml.automl: 02-28 21:53:04] {2620} INFO -  at 0.9s,  estimator xgboost's best error=0.0333,  best estimator xgboost's best error=0.0333  
    [flaml.automl: 02-28 21:53:04] {2458} INFO - iteration 25, current learner xgb_limitdepth  
    [flaml.automl: 02-28 21:53:04] {2620} INFO -  at 0.9s,  estimator xgb_limitdepth's best error=0.0667,   best estimator xgboost's best error=0.0333  
    [flaml.automl: 02-28 21:53:04] {2458} INFO - iteration 26, current learner xgb_limitdepth  
    [flaml.automl: 02-28 21:53:04] {2620} INFO -  at 0.9s,  estimator xgb_limitdepth's best error=0.0667,   best estimator xgboost's best error=0.0333  
    [flaml.automl: 02-28 21:53:04] {2458} INFO - iteration 27, current learner xgboost  
    [flaml.automl: 02-28 21:53:04] {2620} INFO -  at 0.9s,  estimator xgboost's best error=0.0333,  best estimator xgboost's best error=0.0333  
    [flaml.automl: 02-28 21:53:04] {2458} INFO - iteration 28, current learner extra_tree  
    [flaml.automl: 02-28 21:53:04] {2620} INFO -  at 1.0s,  estimator extra_tree's best error=0.0667,   best estimator xgboost's best error=0.0333  
    [flaml.automl: 02-28 21:53:04] {2458} INFO - iteration 29, current learner xgb_limitdepth  
    [flaml.automl: 02-28 21:53:04] {2620} INFO -  at 1.0s,  estimator xgb_limitdepth's best error=0.0667,   best estimator xgboost's best error=0.0333  
    [flaml.automl: 02-28 21:53:04] {2850} INFO - retrain xgboost for 0.0s  
    [flaml.automl: 02-28 21:53:04] {2857} INFO - retrained model: XGBClassifier(base_score=0.5, booster='gbtree',  
                  colsample_bylevel=0.9826753651836615, colsample_bynode=1,  
                  colsample_bytree=0.9725493834064914, gamma=0, gpu_id=-1,  
                  grow_policy='lossguide', importance_type='gain',  
                  interaction_constraints='', learning_rate=0.1665803484560213,  
                  max_delta_step=0, max_depth=0, max_leaves=4,  
                  min_child_weight=0.5649012460525115, missing=nan,  
                  monotone_constraints='()', n_estimators=4, n_jobs=-1,  
                  num_parallel_tree=1, objective='binary:logistic', random_state=0,  
                  reg_alpha=0.009638363373006869, reg_lambda=0.143703802530408,  
                  scale_pos_weight=1, subsample=0.9643606787051899,  
                  tree_method='hist', use_label_encoder=False,  
                  validate_parameters=1, verbosity=0)  
    [flaml.automl: 02-28 21:53:04] {2234} INFO - fit succeeded  
    [flaml.automl: 02-28 21:53:04] {2235} INFO - Time taken to find the best model: 0.8547139167785645  

### Forecasting with Panel Datasets​ {#forecasting-with-panel-datasets}

Panel time series datasets involves multiple individual time series. For
example, see Stallion demand dataset from PyTorch Forecasting, orginally from
Kaggle.
```python
    def get_stalliion_data():  
        from pytorch_forecasting.data.examples import get_stallion_data  
      
        data = get_stallion_data()  
        # add time index - For datasets with no missing values, FLAML will automate this process  
        data["time_idx"] = data["date"].dt.year * 12 + data["date"].dt.month  
        data["time_idx"] -= data["time_idx"].min()  
        # add additional features  
        data["month"] = data.date.dt.month.astype(str).astype(  
            "category"  
        )  # categories have be strings  
        data["log_volume"] = np.log(data.volume + 1e-8)  
        data["avg_volume_by_sku"] = data.groupby(  
            ["time_idx", "sku"], observed=True  
        ).volume.transform("mean")  
        data["avg_volume_by_agency"] = data.groupby(  
            ["time_idx", "agency"], observed=True  
        ).volume.transform("mean")  
        # we want to encode special days as one variable and thus need to first reverse one-hot encoding  
        special_days = [  
            "easter_day",  
            "good_friday",  
            "new_year",  
            "christmas",  
            "labor_day",  
            "independence_day",  
            "revolution_day_memorial",  
            "regional_games",  
            "beer_capital",  
            "music_fest",  
        ]  
        data[special_days] = (  
            data[special_days]  
            .apply(lambda x: x.map({0: "-", 1: x.name}))  
            .astype("category")  
        )  
        return data, special_days  
      
    data, special_days = get_stalliion_data()  
    time_horizon = 6  # predict six months  
    training_cutoff = data["time_idx"].max() - time_horizon  
    data["time_idx"] = data["time_idx"].astype("int")  
    ts_col = data.pop("date")  
    data.insert(0, "date", ts_col)  
    # FLAML assumes input is not sorted, but we sort here for comparison purposes with y_test  
    data = data.sort_values(["agency", "sku", "date"])  
    X_train = data[lambda x: x.time_idx <= training_cutoff]  
    X_test = data[lambda x: x.time_idx > training_cutoff]  
    y_train = X_train.pop("volume")  
    y_test = X_test.pop("volume")  
    automl = AutoML()  
    # Configure settings for FLAML model  
    settings = {  
        "time_budget": budget,  # total running time in seconds  
        "metric": "mape",  # primary metric  
        "task": "ts_forecast_panel",  # task type  
        "log_file_name": "test/stallion_forecast.log",  # flaml log file  
        "eval_method": "holdout",  
    }  
    # Specify kwargs for TimeSeriesDataSet used by TemporalFusionTransformerEstimator  
    fit_kwargs_by_estimator = {  
        "tft": {  
            "max_encoder_length": 24,  
            "static_categoricals": ["agency", "sku"],  
            "static_reals": ["avg_population_2017", "avg_yearly_household_income_2017"],  
            "time_varying_known_categoricals": ["special_days", "month"],  
            "variable_groups": {  
                "special_days": special_days  
            },  # group of categorical variables can be treated as one variable  
            "time_varying_known_reals": [  
                "time_idx",  
                "price_regular",  
                "discount_in_percent",  
            ],  
            "time_varying_unknown_categoricals": [],  
            "time_varying_unknown_reals": [  
                "y",  # always need a 'y' column for the target column  
                "log_volume",  
                "industry_volume",  
                "soda_volume",  
                "avg_max_temp",  
                "avg_volume_by_agency",  
                "avg_volume_by_sku",  
            ],  
            "batch_size": 256,  
            "max_epochs": 1,  
            "gpu_per_trial": -1,  
        }  
    }  
    # Train the model  
    automl.fit(  
        X_train=X_train,  
        y_train=y_train,  
        **settings,  
        period=time_horizon,  
        group_ids=["agency", "sku"],  
        fit_kwargs_by_estimator=fit_kwargs_by_estimator,  
    )  
    # Compute predictions of testing dataset  
    y_pred = automl.predict(X_test)  
    print(y_test)  
    print(y_pred)  
    # best model  
    print(automl.model.estimator)  
```

#### Sample Output​ {#sample-output} {#sample-output} {#sample-output}

    [flaml.automl: 07-28 21:26:03] {2478} INFO - task = ts_forecast_panel  
    [flaml.automl: 07-28 21:26:03] {2480} INFO - Data split method: time  
    [flaml.automl: 07-28 21:26:03] {2483} INFO - Evaluation method: holdout  
    [flaml.automl: 07-28 21:26:03] {2552} INFO - Minimizing error metric: mape  
    [flaml.automl: 07-28 21:26:03] {2694} INFO - List of ML learners in AutoML Run: ['tft']  
    [flaml.automl: 07-28 21:26:03] {2986} INFO - iteration 0, current learner tft  
    GPU available: False, used: False  
    TPU available: False, using: 0 TPU cores  
    IPU available: False, using: 0 IPUs  
      
       | Name                               | Type                            | Params  
    ----------------------------------------------------------------------------------------  
    0  | loss                               | QuantileLoss                    | 0  
    1  | logging_metrics                    | ModuleList                      | 0  
    2  | input_embeddings                   | MultiEmbedding                  | 1.3 K  
    3  | prescalers                         | ModuleDict                      | 256  
    4  | static_variable_selection          | VariableSelectionNetwork        | 3.4 K  
    5  | encoder_variable_selection         | VariableSelectionNetwork        | 8.0 K  
    6  | decoder_variable_selection         | VariableSelectionNetwork        | 2.7 K  
    7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K  
    8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K  
    9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K  
    10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K  
    11 | lstm_encoder                       | LSTM                            | 4.4 K  
    12 | lstm_decoder                       | LSTM                            | 4.4 K  
    13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544  
    14 | post_lstm_add_norm_encoder         | AddNorm                         | 32  
    15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K  
    16 | multihead_attn                     | InterpretableMultiHeadAttention | 676  
    17 | post_attn_gate_norm                | GateAddNorm                     | 576  
    18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K  
    19 | pre_output_gate_norm               | GateAddNorm                     | 576  
    20 | output_layer                       | Linear                          | 119  
    ----------------------------------------------------------------------------------------  
    33.6 K    Trainable params  
    0         Non-trainable params  
    33.6 K    Total params  
    0.135     Total estimated model params size (MB)  
      
    Epoch 19: 100%|██████████| 129/129 [00:56<00:00,  2.27it/s, loss=45.9, v_num=2, train_loss_step=43.00, val_loss=65.20, train_loss_epoch=46.50]  
      
    [flaml.automl: 07-28 21:46:46] {3114} INFO - Estimated sufficient time budget=12424212s. Estimated necessary time budget=12424s.  
    [flaml.automl: 07-28 21:46:46] {3161} INFO -  at 1242.6s,\testimator tft's best error=1324290483134574.7500,\tbest estimator tft's best error=1324290483134574.7500  
    GPU available: False, used: False  
    TPU available: False, using: 0 TPU cores  
    IPU available: False, using: 0 IPUs  
      
       | Name                               | Type                            | Params  
    ----------------------------------------------------------------------------------------  
    0  | loss                               | QuantileLoss                    | 0  
    1  | logging_metrics                    | ModuleList                      | 0  
    2  | input_embeddings                   | MultiEmbedding                  | 1.3 K  
    3  | prescalers                         | ModuleDict                      | 256  
    4  | static_variable_selection          | VariableSelectionNetwork        | 3.4 K  
    5  | encoder_variable_selection         | VariableSelectionNetwork        | 8.0 K  
    6  | decoder_variable_selection         | VariableSelectionNetwork        | 2.7 K  
    7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K  
    8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K  
    9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K  
    10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K  
    11 | lstm_encoder                       | LSTM                            | 4.4 K  
    12 | lstm_decoder                       | LSTM                            | 4.4 K  
    13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544  
    14 | post_lstm_add_norm_encoder         | AddNorm                         | 32  
    15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K  
    16 | multihead_attn                     | InterpretableMultiHeadAttention | 676  
    17 | post_attn_gate_norm                | GateAddNorm                     | 576  
    18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K  
    19 | pre_output_gate_norm               | GateAddNorm                     | 576  
    20 | output_layer                       | Linear                          | 119  
    ----------------------------------------------------------------------------------------  
    33.6 K    Trainable params  
    0         Non-trainable params  
    33.6 K    Total params  
    0.135     Total estimated model params size (MB)  
    Epoch 19: 100%|██████████| 145/145 [01:03<00:00,  2.28it/s, loss=45.2, v_num=3, train_loss_step=46.30, val_loss=67.60, train_loss_epoch=48.10]  
    [flaml.automl: 07-28 22:08:05] {3425} INFO - retrain tft for 1279.6s  
    [flaml.automl: 07-28 22:08:05] {3432} INFO - retrained model: TemporalFusionTransformer(  
      (loss): QuantileLoss()  
      (logging_metrics): ModuleList(  
        (0): SMAPE()  
        (1): MAE()  
        (2): RMSE()  
        (3): MAPE()  
      )  
      (input_embeddings): MultiEmbedding(  
        (embeddings): ModuleDict(  
          (agency): Embedding(58, 16)  
          (sku): Embedding(25, 10)  
          (special_days): TimeDistributedEmbeddingBag(11, 6, mode=sum)  
          (month): Embedding(12, 6)  
        )  
      )  
      (prescalers): ModuleDict(  
        (avg_population_2017): Linear(in_features=1, out_features=8, bias=True)  
        (avg_yearly_household_income_2017): Linear(in_features=1, out_features=8, bias=True)  
        (encoder_length): Linear(in_features=1, out_features=8, bias=True)  
        (y_center): Linear(in_features=1, out_features=8, bias=True)  
        (y_scale): Linear(in_features=1, out_features=8, bias=True)  
        (time_idx): Linear(in_features=1, out_features=8, bias=True)  
        (price_regular): Linear(in_features=1, out_features=8, bias=True)  
        (discount_in_percent): Linear(in_features=1, out_features=8, bias=True)  
        (relative_time_idx): Linear(in_features=1, out_features=8, bias=True)  
        (y): Linear(in_features=1, out_features=8, bias=True)  
        (log_volume): Linear(in_features=1, out_features=8, bias=True)  
        (industry_volume): Linear(in_features=1, out_features=8, bias=True)  
        (soda_volume): Linear(in_features=1, out_features=8, bias=True)  
        (avg_max_temp): Linear(in_features=1, out_features=8, bias=True)  
        (avg_volume_by_agency): Linear(in_features=1, out_features=8, bias=True)  
        (avg_volume_by_sku): Linear(in_features=1, out_features=8, bias=True)  
      )  
      (static_variable_selection): VariableSelectionNetwork(  
        (flattened_grn): GatedResidualNetwork(  
          (resample_norm): ResampleNorm(  
            (resample): TimeDistributedInterpolation()  
            (gate): Sigmoid()  
            (norm): LayerNorm((7,), eps=1e-05, elementwise_affine=True)  
          )  
          (fc1): Linear(in_features=66, out_features=7, bias=True)  
          (elu): ELU(alpha=1.0)  
          (fc2): Linear(in_features=7, out_features=7, bias=True)  
          (gate_norm): GateAddNorm(  
            (glu): GatedLinearUnit(  
              (dropout): Dropout(p=0.1, inplace=False)  
              (fc): Linear(in_features=7, out_features=14, bias=True)  
            )  
            (add_norm): AddNorm(  
              (norm): LayerNorm((7,), eps=1e-05, elementwise_affine=True)  
            )  
          )  
        )  
        (single_variable_grns): ModuleDict(  
          (agency): ResampleNorm(  
            (gate): Sigmoid()  
            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
          )  
          (sku): ResampleNorm(  
            (resample): TimeDistributedInterpolation()  
            (gate): Sigmoid()  
            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
          )  
          (avg_population_2017): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
          (avg_yearly_household_income_2017): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
          (encoder_length): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
          (y_center): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
          (y_scale): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
        )  
        (prescalers): ModuleDict(  
          (avg_population_2017): Linear(in_features=1, out_features=8, bias=True)  
          (avg_yearly_household_income_2017): Linear(in_features=1, out_features=8, bias=True)  
          (encoder_length): Linear(in_features=1, out_features=8, bias=True)  
          (y_center): Linear(in_features=1, out_features=8, bias=True)  
          (y_scale): Linear(in_features=1, out_features=8, bias=True)  
        )  
        (softmax): Softmax(dim=-1)  
      )  
      (encoder_variable_selection): VariableSelectionNetwork(  
        (flattened_grn): GatedResidualNetwork(  
          (resample_norm): ResampleNorm(  
            (resample): TimeDistributedInterpolation()  
            (gate): Sigmoid()  
            (norm): LayerNorm((13,), eps=1e-05, elementwise_affine=True)  
          )  
          (fc1): Linear(in_features=100, out_features=13, bias=True)  
          (elu): ELU(alpha=1.0)  
          (context): Linear(in_features=16, out_features=13, bias=False)  
          (fc2): Linear(in_features=13, out_features=13, bias=True)  
          (gate_norm): GateAddNorm(  
            (glu): GatedLinearUnit(  
              (dropout): Dropout(p=0.1, inplace=False)  
              (fc): Linear(in_features=13, out_features=26, bias=True)  
            )  
            (add_norm): AddNorm(  
              (norm): LayerNorm((13,), eps=1e-05, elementwise_affine=True)  
            )  
          )  
        )  
        (single_variable_grns): ModuleDict(  
          (special_days): ResampleNorm(  
            (resample): TimeDistributedInterpolation()  
            (gate): Sigmoid()  
            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
          )  
          (month): ResampleNorm(  
            (resample): TimeDistributedInterpolation()  
            (gate): Sigmoid()  
            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
          )  
          (time_idx): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
          (price_regular): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
          (discount_in_percent): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
          (relative_time_idx): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
          (y): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
          (log_volume): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
          (industry_volume): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
          (soda_volume): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
          (avg_max_temp): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
          (avg_volume_by_agency): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
          (avg_volume_by_sku): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
        )  
        (prescalers): ModuleDict(  
          (time_idx): Linear(in_features=1, out_features=8, bias=True)  
          (price_regular): Linear(in_features=1, out_features=8, bias=True)  
          (discount_in_percent): Linear(in_features=1, out_features=8, bias=True)  
          (relative_time_idx): Linear(in_features=1, out_features=8, bias=True)  
          (y): Linear(in_features=1, out_features=8, bias=True)  
          (log_volume): Linear(in_features=1, out_features=8, bias=True)  
          (industry_volume): Linear(in_features=1, out_features=8, bias=True)  
          (soda_volume): Linear(in_features=1, out_features=8, bias=True)  
          (avg_max_temp): Linear(in_features=1, out_features=8, bias=True)  
          (avg_volume_by_agency): Linear(in_features=1, out_features=8, bias=True)  
          (avg_volume_by_sku): Linear(in_features=1, out_features=8, bias=True)  
        )  
        (softmax): Softmax(dim=-1)  
      )  
      (decoder_variable_selection): VariableSelectionNetwork(  
        (flattened_grn): GatedResidualNetwork(  
          (resample_norm): ResampleNorm(  
            (resample): TimeDistributedInterpolation()  
            (gate): Sigmoid()  
            (norm): LayerNorm((6,), eps=1e-05, elementwise_affine=True)  
          )  
          (fc1): Linear(in_features=44, out_features=6, bias=True)  
          (elu): ELU(alpha=1.0)  
          (context): Linear(in_features=16, out_features=6, bias=False)  
          (fc2): Linear(in_features=6, out_features=6, bias=True)  
          (gate_norm): GateAddNorm(  
            (glu): GatedLinearUnit(  
              (dropout): Dropout(p=0.1, inplace=False)  
              (fc): Linear(in_features=6, out_features=12, bias=True)  
            )  
            (add_norm): AddNorm(  
              (norm): LayerNorm((6,), eps=1e-05, elementwise_affine=True)  
            )  
          )  
        )  
        (single_variable_grns): ModuleDict(  
          (special_days): ResampleNorm(  
            (resample): TimeDistributedInterpolation()  
            (gate): Sigmoid()  
            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
          )  
          (month): ResampleNorm(  
            (resample): TimeDistributedInterpolation()  
            (gate): Sigmoid()  
            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
          )  
          (time_idx): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
          (price_regular): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
          (discount_in_percent): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
          (relative_time_idx): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
        )  
        (prescalers): ModuleDict(  
          (time_idx): Linear(in_features=1, out_features=8, bias=True)  
          (price_regular): Linear(in_features=1, out_features=8, bias=True)  
          (discount_in_percent): Linear(in_features=1, out_features=8, bias=True)  
          (relative_time_idx): Linear(in_features=1, out_features=8, bias=True)  
        )  
        (softmax): Softmax(dim=-1)  
      )  
      (static_context_variable_selection): GatedResidualNetwork(  
        (fc1): Linear(in_features=16, out_features=16, bias=True)  
        (elu): ELU(alpha=1.0)  
        (fc2): Linear(in_features=16, out_features=16, bias=True)  
        (gate_norm): GateAddNorm(  
          (glu): GatedLinearUnit(  
            (dropout): Dropout(p=0.1, inplace=False)  
            (fc): Linear(in_features=16, out_features=32, bias=True)  
          )  
          (add_norm): AddNorm(  
            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
          )  
        )  
      )  
      (static_context_initial_hidden_lstm): GatedResidualNetwork(  
        (fc1): Linear(in_features=16, out_features=16, bias=True)  
        (elu): ELU(alpha=1.0)  
        (fc2): Linear(in_features=16, out_features=16, bias=True)  
        (gate_norm): GateAddNorm(  
          (glu): GatedLinearUnit(  
            (dropout): Dropout(p=0.1, inplace=False)  
            (fc): Linear(in_features=16, out_features=32, bias=True)  
          )  
          (add_norm): AddNorm(  
            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
          )  
        )  
      )  
      (static_context_initial_cell_lstm): GatedResidualNetwork(  
        (fc1): Linear(in_features=16, out_features=16, bias=True)  
        (elu): ELU(alpha=1.0)  
        (fc2): Linear(in_features=16, out_features=16, bias=True)  
        (gate_norm): GateAddNorm(  
          (glu): GatedLinearUnit(  
            (dropout): Dropout(p=0.1, inplace=False)  
            (fc): Linear(in_features=16, out_features=32, bias=True)  
          )  
          (add_norm): AddNorm(  
            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
          )  
        )  
      )  
      (static_context_enrichment): GatedResidualNetwork(  
        (fc1): Linear(in_features=16, out_features=16, bias=True)  
        (elu): ELU(alpha=1.0)  
        (fc2): Linear(in_features=16, out_features=16, bias=True)  
        (gate_norm): GateAddNorm(  
          (glu): GatedLinearUnit(  
            (dropout): Dropout(p=0.1, inplace=False)  
            (fc): Linear(in_features=16, out_features=32, bias=True)  
          )  
          (add_norm): AddNorm(  
            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
          )  
        )  
      )  
      (lstm_encoder): LSTM(16, 16, num_layers=2, batch_first=True, dropout=0.1)  
      (lstm_decoder): LSTM(16, 16, num_layers=2, batch_first=True, dropout=0.1)  
      (post_lstm_gate_encoder): GatedLinearUnit(  
        (dropout): Dropout(p=0.1, inplace=False)  
        (fc): Linear(in_features=16, out_features=32, bias=True)  
      )  
      (post_lstm_gate_decoder): GatedLinearUnit(  
        (dropout): Dropout(p=0.1, inplace=False)  
        (fc): Linear(in_features=16, out_features=32, bias=True)  
      )  
      (post_lstm_add_norm_encoder): AddNorm(  
        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
      )  
      (post_lstm_add_norm_decoder): AddNorm(  
        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
      )  
      (static_enrichment): GatedResidualNetwork(  
        (fc1): Linear(in_features=16, out_features=16, bias=True)  
        (elu): ELU(alpha=1.0)  
        (context): Linear(in_features=16, out_features=16, bias=False)  
        (fc2): Linear(in_features=16, out_features=16, bias=True)  
        (gate_norm): GateAddNorm(  
          (glu): GatedLinearUnit(  
            (dropout): Dropout(p=0.1, inplace=False)  
            (fc): Linear(in_features=16, out_features=32, bias=True)  
          )  
          (add_norm): AddNorm(  
            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
          )  
        )  
      )  
      (multihead_attn): InterpretableMultiHeadAttention(  
        (dropout): Dropout(p=0.1, inplace=False)  
        (v_layer): Linear(in_features=16, out_features=4, bias=True)  
        (q_layers): ModuleList(  
          (0): Linear(in_features=16, out_features=4, bias=True)  
          (1): Linear(in_features=16, out_features=4, bias=True)  
          (2): Linear(in_features=16, out_features=4, bias=True)  
          (3): Linear(in_features=16, out_features=4, bias=True)  
        )  
        (k_layers): ModuleList(  
          (0): Linear(in_features=16, out_features=4, bias=True)  
          (1): Linear(in_features=16, out_features=4, bias=True)  
          (2): Linear(in_features=16, out_features=4, bias=True)  
          (3): Linear(in_features=16, out_features=4, bias=True)  
        )  
        (attention): ScaledDotProductAttention(  
          (softmax): Softmax(dim=2)  
        )  
        (w_h): Linear(in_features=4, out_features=16, bias=False)  
      )  
      (post_attn_gate_norm): GateAddNorm(  
        (glu): GatedLinearUnit(  
          (dropout): Dropout(p=0.1, inplace=False)  
          (fc): Linear(in_features=16, out_features=32, bias=True)  
        )  
        (add_norm): AddNorm(  
          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
        )  
      )  
      (pos_wise_ff): GatedResidualNetwork(  
        (fc1): Linear(in_features=16, out_features=16, bias=True)  
        (elu): ELU(alpha=1.0)  
        (fc2): Linear(in_features=16, out_features=16, bias=True)  
        (gate_norm): GateAddNorm(  
          (glu): GatedLinearUnit(  
            (dropout): Dropout(p=0.1, inplace=False)  
            (fc): Linear(in_features=16, out_features=32, bias=True)  
          )  
          (add_norm): AddNorm(  
            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
          )  
        )  
      )  
      (pre_output_gate_norm): GateAddNorm(  
        (glu): GatedLinearUnit(  
          (fc): Linear(in_features=16, out_features=32, bias=True)  
        )  
        (add_norm): AddNorm(  
          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
        )  
      )  
      (output_layer): Linear(in_features=16, out_features=7, bias=True)  
    )  
    [flaml.automl: 07-28 22:08:05] {2725} INFO - fit succeeded  
    [flaml.automl: 07-28 22:08:05] {2726} INFO - Time taken to find the best model: 1242.6435902118683  
    [flaml.automl: 07-28 22:08:05] {2737} WARNING - Time taken to find the best model is 414% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.\n"  
         ]  
        }  
       ],  

[Link to
notebook](https://github.com/microsoft.blob/main/notebook/automl_time_series_forecast.ipynb)
| [Open in
colab](https://colab.research.google.com/github/microsoft.blob/main/notebook/automl_time_series_forecast.ipynb)

[Edit this
page](<https://github.com/microsoft.edit/main/website/docs/Examples/AutoML->
Time series forecast.md)

[Previous« AutoML - Regression](.docs/Examples/AutoML-Regression)

[NextAutoML for LightGBM »](.docs/Examples/AutoML-for-LightGBM)

* Prerequisites
* Simple NumPy Example
* Univariate time series
* Multivariate Time Series (Forecasting with Exogenous Variables)
* Forecasting Discrete Variables
* Forecasting with Panel Datasets

* [Getting Started](.docs/Getting-Started)
* [Installation](.docs/Installation)
* Use Cases
* Examples
  * [AutoGen - Automated Multi Agent Chat](.docs/Examples/AutoGen-AgentChat)
  * [AutoGen - Tune GPT Models](.docs/Examples/AutoGen-OpenAI)
  * [AutoML - Classification](.docs/Examples/AutoML-Classification)
  * [AutoML - NLP](.docs/Examples/AutoML-NLP)
  * [AutoML - Rank](.docs/Examples/AutoML-Rank)
  * [AutoML - Regression](.docs/Examples/AutoML-Regression)
  * [AutoML - Time Series Forecast](.docs/Examples/AutoML-Time series forecast)
  * [AutoML for LightGBM](.docs/Examples/AutoML-for-LightGBM)
  * [AutoML for XGBoost](.docs/Examples/AutoML-for-XGBoost)
  * [Default - Flamlized Estimator](.docs/Examples/Default-Flamlized)
  * [Integrate - AzureML](.docs/Examples/Integrate - AzureML)
  * [Integrate - Scikit-learn Pipeline](.docs/Examples/Integrate - Scikit-learn Pipeline)
  * [Integrate - Spark](.docs/Examples/Integrate - Spark)
  * [Tune - AzureML pipeline](.docs/Examples/Tune-AzureML-pipeline)
  * [Tune - HuggingFace](.docs/Examples/Tune-HuggingFace)
  * [Tune - Lexicographic Objectives](.docs/Examples/Tune-Lexicographic-objectives)
  * [Tune - PyTorch](.docs/Examples/Tune-PyTorch)
* [Contributing](.docs/Contribute)
* [Research](.docs/Research)

On this page

# Tune - Lexicographic Objectives {#tune-lexicographic-objectives}

## Requirements​ {#requirements}

    pip install "flaml>=1.1.0" thop torchvision torch  

Tuning multiple objectives with Lexicographic preference is a new feature
added in version 1.1.0 and is subject to change in future versions.

## Tuning accurate and efficient neural networks with lexicographic {#tuning-accurate-and-efficient-neural-networks-with-lexicographic}

preference​

### Data​ {#data}

    import torch  
    import thop  
    import torch.nn as nn  
    from flaml import tune  
    import torch.nn.functional as F  
    import torchvision  
    import numpy as np  
    import os  
      
    DEVICE = torch.device("cpu")  
    BATCHSIZE = 128  
    N_TRAIN_EXAMPLES = BATCHSIZE * 30  
    N_VALID_EXAMPLES = BATCHSIZE * 10  
    data_dir = os.path.abspath("data")  
      
    train_dataset = torchvision.datasets.FashionMNIST(  
        data_dir,  
        train=True,  
        download=True,  
        transform=torchvision.transforms.ToTensor(),  
    )  
      
    train_loader = torch.utils.data.DataLoader(  
        torch.utils.data.Subset(train_dataset, list(range(N_TRAIN_EXAMPLES))),  
        batch_size=BATCHSIZE,  
        shuffle=True,  
    )  
      
    val_dataset = torchvision.datasets.FashionMNIST(  
        data_dir, train=False, transform=torchvision.transforms.ToTensor()  
    )  
      
    val_loader = torch.utils.data.DataLoader(  
        torch.utils.data.Subset(val_dataset, list(range(N_VALID_EXAMPLES))),  
        batch_size=BATCHSIZE,  
        shuffle=True,  

### Specific the model​ {#specific-the-model}
```python
    def define_model(configuration):  
        n_layers = configuration["n_layers"]  
        layers = []  
        in_features = 28 * 28  
        for i in range(n_layers):  
            out_features = configuration["n_units_l{}".format(i)]  
            layers.append(nn.Linear(in_features, out_features))  
            layers.append(nn.ReLU())  
            p = configuration["dropout_{}".format(i)]  
            layers.append(nn.Dropout(p))  
            in_features = out_features  
        layers.append(nn.Linear(in_features, 10))  
        layers.append(nn.LogSoftmax(dim=1))  
        return nn.Sequential(*layers)  
```
### Train​ {#train}
```python
    def train_model(model, optimizer, train_loader):  
        model.train()  
        for batch_idx, (data, target) in enumerate(train_loader):  
            data, target = data.view(-1, 28 * 28).to(DEVICE), target.to(DEVICE)  
            optimizer.zero_grad()  
            F.nll_loss(model(data), target).backward()  
            optimizer.step()  
```
### Metrics​ {#metrics}
```python
    def eval_model(model, valid_loader):  
        model.eval()  
        correct = 0  
        with torch.no_grad():  
            for batch_idx, (data, target) in enumerate(valid_loader):  
                data, target = data.view(-1, 28 * 28).to(DEVICE), target.to(DEVICE)  
                pred = model(data).argmax(dim=1, keepdim=True)  
                correct += pred.eq(target.view_as(pred)).sum().item()  
      
        accuracy = correct / N_VALID_EXAMPLES  
        flops, params = thop.profile(  
            model, inputs=(torch.randn(1, 28 * 28).to(DEVICE),), verbose=False  
        )  
        return np.log2(flops), 1 - accuracy, params  
```
### Evaluation function​ {#evaluation-function}
```python
    def evaluate_function(configuration):  
        model = define_model(configuration).to(DEVICE)  
        optimizer = torch.optim.Adam(model.parameters(), configuration["lr"])  
        n_epoch = configuration["n_epoch"]  
        for epoch in range(n_epoch):  
            train_model(model, optimizer, train_loader)  
        flops, error_rate, params = eval_model(model, val_loader)  
        return {"error_rate": error_rate, "flops": flops, "params": params}  

### Search space​ {#search-space}

    search_space = {  
        "n_layers": tune.randint(lower=1, upper=3),  
        "n_units_l0": tune.randint(lower=4, upper=128),  
        "n_units_l1": tune.randint(lower=4, upper=128),  
        "n_units_l2": tune.randint(lower=4, upper=128),  
        "dropout_0": tune.uniform(lower=0.2, upper=0.5),  
        "dropout_1": tune.uniform(lower=0.2, upper=0.5),  
        "dropout_2": tune.uniform(lower=0.2, upper=0.5),  
        "lr": tune.loguniform(lower=1e-5, upper=1e-1),  
        "n_epoch": tune.randint(lower=1, upper=20),  
    }  

### Launch the tuning process​ {#launch-the-tuning-process}

    # Low cost initial point  
    low_cost_partial_config = {  
        "n_layers": 1,  
        "n_units_l0": 4,  
        "n_units_l1": 4,  
        "n_units_l2": 4,  
        "n_epoch": 1,  
    }  
      
    # Specific lexicographic preference  
    lexico_objectives = {}  
    lexico_objectives["metrics"] = ["error_rate", "flops"]  
    lexico_objectives["tolerances"] = {"error_rate": 0.02, "flops": 0.0}  
    lexico_objectives["targets"] = {"error_rate": 0.0, "flops": 0.0}  
    lexico_objectives["modes"] = ["min", "min"]  
      
    # launch the tuning process  
    analysis = tune.run(  
        evaluate_function,  
        num_samples=-1,  
        time_budget_s=100,  
        config=search_space, # search space of NN  
        use_ray=False,  
        lexico_objectives=lexico_objectives,  
        low_cost_partial_config=low_cost_partial_config, # low cost initial point  
    )  
```

We also support providing percentage tolerance as shown below.

    lexico_objectives["tolerances"] = {"error_rate": "5%", "flops": "0%"}  

[Link to
notebook](https://github.com/microsoft.blob/main/notebook/tune_lexicographic.ipynb)
| [Open in
colab](https://colab.research.google.com/github/microsoft.blob/main/notebook/tune_lexicographic.ipynb)

[Previous« Tune - HuggingFace](.docs/Examples/Tune-HuggingFace)

[NextTune - PyTorch »](.docs/Examples/Tune-PyTorch)

* Requirements
* Tuning accurate and efficient neural networks with lexicographic preference
  * Data
  * Specific the model
  * Train
  * Metrics
  * Evaluation function
  * Search space
  * Launch the tuning process

* Reference
  * autogen
  * automl
    * automl.contrib
    * automl.nlp
    * automl.spark
    * automl.task
    * automl.time_series
    * [automl](.docs/reference/automl/automl)
    * [data](.docs/reference/automl/data)
    * [ml](.docs/reference/automl/ml)
    * [model](.docs/reference/automl/model)
    * [state](.docs/reference/automl/state)
  * default
  * onlineml
  * tune

On this page

# automl.ml {#automlml}

#### sklearn_metric_loss_score​ {#sklearnmetriclossscore}

```python
    def sklearn_metric_loss_score(metric_name: str, y_predict, y_true, labels=None, sample_weight=None, groups=None)  
```

Loss using the specified metric.

 **Arguments** :

* `metric_name` \- A string of the metric name, one of 'r2', 'rmse', 'mae', 'mse', 'accuracy', 'roc_auc', 'roc_auc_ovr', 'roc_auc_ovo', 'roc_auc_weighted', 'roc_auc_ovo_weighted', 'roc_auc_ovr_weighted', 'log_loss', 'mape', 'f1', 'ap', 'ndcg', 'micro_f1', 'macro_f1'.
* `y_predict` \- A 1d or 2d numpy array of the predictions which can be used to calculate the metric. E.g., 2d for log_loss and 1d for others.
* `y_true` \- A 1d numpy array of the true labels.
* `labels` \- A list or an array of the unique labels.
* `sample_weight` \- A 1d numpy array of the sample weight.
* `groups` \- A 1d numpy array of the group labels.

 **Returns** :

* `score` \- A float number of the loss, the lower the better.

#### norm_confusion_matrix​ {#normconfusionmatrix}
```python
    def norm_confusion_matrix(y_true: Union[np.array, Series], y_pred: Union[np.array, Series])  
```
normalized confusion matrix.

 **Arguments** :

* `estimator` \- A multi-class classification estimator.
* `y_true` \- A numpy array or a pandas series of true labels.
* `y_pred` \- A numpy array or a pandas series of predicted labels.

 **Returns** :

A normalized confusion matrix.

#### multi_class_curves​ {#multiclasscurves}
```python
    def multi_class_curves(y_true: Union[np.array, Series], y_pred_proba: Union[np.array, Series], curve_func: Callable)  
```

Binarize the data for multi-class tasks and produce ROC or precision-recall
curves.

 **Arguments** :

* `y_true` \- A numpy array or a pandas series of true labels.
* `y_pred_proba` \- A numpy array or a pandas dataframe of predicted probabilites.
* `curve_func` \- A function to produce a curve (e.g., roc_curve or precision_recall_curve).

 **Returns** :

A tuple of two dictionaries with the same set of keys (class indices). The
first dictionary curve_x stores the x coordinates of each curve, e.g.,
curve_x[0] is an 1D array of the x coordinates of class 0. The second
dictionary curve_y stores the y coordinates of each curve, e.g., curve_y[0] is
an 1D array of the y coordinates of class 0.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/automl/ml.md)

[Previous« data](.docs/reference/automl/data)

[Nextmodel »](.docs/reference/automl/model)

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

Recent posts

* [All About Agent Descriptions](/autogen/blog/2023/12/29/AgentDescriptions)
* [AgentOptimizer - An Agentic Way to Train Your LLM Agent](/autogen/blog/2023/12/23/AgentOptimizer)
* [AutoGen Studio: Interactively Explore Multi-Agent Workflows](/autogen/blog/2023/12/01/AutoGenStudio)
* [Agent AutoBuild - Automatically Building Multi-agent Systems](/autogen/blog/2023/11/26/Agent-AutoBuild)
* [How to Assess Utility of LLM-powered Applications?](/autogen/blog/2023/11/20/AgentEval)
* [AutoGen Meets GPTs](/autogen/blog/2023/11/13/OAI-assistants)
* [EcoAssistant - Using LLM Assistants More Accurately and Affordably](/autogen/blog/2023/11/09/EcoAssistant)
* [Multimodal with GPT-4V and LLaVA](/autogen/blog/2023/11/06/LMM-Agent)
* [AutoGen's Teachable Agents](/autogen/blog/2023/10/26/TeachableAgent)
* [Retrieval-Augmented Generation (RAG) Applications with AutoGen](/autogen/blog/2023/10/18/RetrieveChat)
* [Use AutoGen for Local LLMs](/autogen/blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](/autogen/blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](/autogen/blog/2023/05/18/GPT-adaptive-humaneval)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](/autogen/blog/2023/04/21/LLM-tuning-math)

# All About Agent Descriptions {#all-about-agent-descriptions}

December 29, 2023 · 9 min read

[![Adam
Fourney](https://github.com/afourney.png)](https://www.adamfourney.com)

[Adam Fourney](https://www.adamfourney.com)

Principal Researcher Microsoft Research

## TLDR​ {#tldr}

AutoGen 0.2.2 introduces a
[description](https://microsoft.github.io/autogen/docs/reference/agentchat/conversable_agent#__init__)
field to ConversableAgent (and all subclasses), and changes GroupChat so that
it uses agent `description`s rather than `system_message`s when choosing which
agents should speak next.

This is expected to simplify GroupChat’s job, improve orchestration, and make
it easier to implement new GroupChat or GroupChat-like alternatives.

If you are a developer, and things were already working well for you, no
action is needed -- backward compatibility is ensured because the
`description` field defaults to the `system_message` when no description is
provided.

However, if you were struggling with getting GroupChat to work, you can now
try updating the `description` field.

## Introduction​ {#introduction}

As AutoGen matures and developers build increasingly complex combinations of
agents, orchestration is becoming an important capability. At present,
[GroupChat](<https://microsoft.github.io/autogen/docs/reference/agentchat/groupchat#groupchat->
objects) and the
[GroupChatManager](<https://microsoft.github.io/autogen/docs/reference/agentchat/groupchat#groupchatmanager->
objects) are the main built-in tools for orchestrating conversations between 3
or more agents. For orchestrators like GroupChat to work well, they need to
know something about each agent so that they can decide who should speak and
when. Prior to AutoGen 0.2.2, GroupChat relied on each agent's
`system_message` and `name` to learn about each participating agent. This is
likely fine when the system prompt is short and sweet, but can lead to
problems when the instructions are very long (e.g., with the
[AssistantAgent](https://microsoft.github.io/autogen/docs/reference/agentchat/assistant_agent)),
or non-existent (e.g., with the
[UserProxyAgent](https://microsoft.github.io/autogen/docs/reference/agentchat/user_proxy_agent)).

AutoGen 0.2.2 introduces a
[description](https://microsoft.github.io/autogen/docs/reference/agentchat/conversable_agent#__init__)
field to all agents, and replaces the use of the `system_message` for
orchestration in GroupChat and all future orchestrators. The `description`
field defaults to the `system_message` to ensure backwards compatibility, so
you may not need to change anything with your code if things are working well
for you. However, if you were struggling with GroupChat, give setting the
`description` field a try.

The remainder of this post provides an example of how using the `description`
field simplifies GroupChat's job, provides some evidence of its effectiveness,
and provides tips for writing good descriptions.

## Example​ {#example}

The current GroupChat orchestration system prompt has the following template:

    You are in a role play game. The following roles are available:  
      
    {self._participant_roles(agents)}.  
      
    Read the following conversation.  
    Then select the next role from {[agent.name for agent in agents]} to play. Only return the role.  

Suppose that you wanted to include 3 agents: A UserProxyAgent, an
AssistantAgent, and perhaps a GuardrailsAgent.

Prior to 0.2.2, this template would expand to:

    You are in a role play game. The following roles are available:  
      
    assistant: You are a helpful AI assistant.  
    Solve tasks using your coding and language skills.  
    In the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for the user to execute.  
    1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time, check the operating system. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself.  
    2. When you need to perform some task with code, use the code to perform the task and output the result. Finish the task smartly.  
    Solve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill.  
    When using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can't modify your code. So do not suggest incomplete code which requires users to modify. Don't use a code block if it's not intended to be executed by the user.  
    If you want the user to save the code in a file before executing it, put # filename: <filename> inside the code block as the first line. Don't include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use 'print' function for the output when relevant. Check the execution result returned by the user.  
    If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.  
    When you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible.  
    Reply "TERMINATE" in the end when everything is done.  
    user_proxy:  
    guardrails_agent: You are a guardrails agent and are tasked with ensuring that all parties adhere to the following responsible AI policies:  
    - You MUST TERMINATE the conversation if it involves writing or running HARMFUL or DESTRUCTIVE code.  
    - You MUST TERMINATE the conversation if it involves discussions of anything relating to hacking, computer exploits, or computer security.  
    - You MUST TERMINATE the conversation if it involves violent or graphic content such as Harm to Others, Self-Harm, Suicide.  
    - You MUST TERMINATE the conversation if it involves demeaning speech, hate speech, discriminatory remarks, or any form of harassment based on race, gender, sexuality, religion, nationality, disability, or any other protected characteristic.  
    - You MUST TERMINATE the conversation if it involves seeking or giving  advice in highly regulated domains such as medical advice, mental health, legal advice or financial advice  
    - You MUST TERMINATE the conversation if it involves illegal activities including when encouraging or providing guidance on illegal activities.  
    - You MUST TERMINATE the conversation if it involves manipulative or deceptive Content including scams, phishing and spread false information.  
    - You MUST TERMINATE the conversation if it involves involve sexually explicit content or discussions.  
    - You MUST TERMINATE the conversation if it involves sharing or soliciting personal, sensitive, or confidential information from users. This includes financial details, health records, and other private matters.  
    - You MUST TERMINATE the conversation if it involves deep personal problems such as dealing with serious personal issues, mental health concerns, or crisis situations.  
    If you decide that the conversation must be terminated, explain your reasoning then output the uppercase word "TERMINATE". If, on the other hand, you decide the conversation is acceptable by the above standards, indicate as much, then ask the other parties to proceed.  
      
    Read the following conversation.  
    Then select the next role from [assistant, user_proxy, guardrails_agent] to play. Only return the role.  

As you can see, this description is super confusing:

* It is hard to make out where each agent's role-description ends
* `You` appears numerous times, and refers to three separate agents (GroupChatManager, AssistantAgent, and GuardrailsAgent)
* It takes a lot of tokens!

Consequently, it's not hard to see why the GroupChat manager sometimes
struggles with this orchestration task.

With AutoGen 0.2.2 onward, GroupChat instead relies on the description field.
With a description field the orchestration prompt becomes:

    You are in a role play game. The following roles are available:  
      
    assistant: A helpful and general-purpose AI assistant that has strong language skills, Python skills, and Linux command line skills.  
    user_proxy: A user that can run Python code or input command line commands at a Linux terminal and report back the execution results.  
    guradrails_agent: An agent that ensures the conversation conforms to responsible AI guidelines.  
      
    Read the following conversation.  
    Then select the next role from [assistant, user_proxy, guardrails_agent] to play. Only return the role.  

This is much easier to parse and understand, and it doesn't use nearly as many
tokens. Moreover, the following experiment provides early evidence that it
works.

## An Experiment with Distraction​ {#an-experiment-with-distraction}

To illustrate the impact of the `description` field, we set up a three-agent
experiment with a reduced 26-problem subset of the HumanEval benchmark. Here,
three agents were added to a GroupChat to solve programming problems. The
three agents were:

* Coder (default Assistant prompt)
* UserProxy (configured to execute code)
* ExecutiveChef (added as a distraction)

The Coder and UserProxy used the AssistantAgent and UserProxy defaults
(provided above), while the ExecutiveChef was given the system prompt:

    You are an executive chef with 28 years of industry experience. You can answer questions about menu planning, meal preparation, and cooking techniques.  

The ExecutiveChef is clearly the distractor here -- given that no HumanEval
problems are food-related, the GroupChat should rarely consult with the chef.
However, when configured with GPT-3.5-turbo-16k, we can clearly see the
GroupChat struggling with orchestration:

#### With versions prior to 0.2.2, using `system_message`:​ {#with-versions-prior-to-022-using-systemmessage}

* The Agents solve 3 out of 26 problems on their first turn
* The ExecutiveChef is called upon 54 times! (almost as much as the Coder at 68 times)

#### With version 0.2.2, using `description`:​ {#with-version-022-using-description}

* The Agents solve 7 out of 26 problems on the first turn
* The ExecutiveChef is called upon 27 times! (versus 84 times for the Coder)

Using the `description` field doubles performance on this task and halves the
incidence of calling upon the distractor agent.

## Tips for Writing Good Descriptions​ {#tips-for-writing-good-descriptions}

Since `descriptions` serve a different purpose than `system_message`s, it is
worth reviewing what makes a good agent description. While descriptions are
new, the following tips appear to lead to good results:

* Avoid using the 1st or 2nd person perspective. Descriptions should not contain "I" or "You", unless perhaps "You" is in reference to the GroupChat / orchestrator
* Include any details that might help the orchestrator know when to call upon the agent
* Keep descriptions short (e.g., "A helpful AI assistant with strong natural language and Python coding skills.").

The main thing to remember is that **the description is for the benefit of the
GroupChatManager, not for the Agent 's own use or instruction**.

## Conclusion​ {#conclusion}

AutoGen 0.2.2 introduces a `description`, becoming the main way agents
describe themselves to orchestrators like GroupChat. Since the `description`
defaults to the `system_message`, there's nothing you need to change if you
were already satisfied with how your group chats were working. However, we
expect this feature to generally improve orchestration, so please consider
experimenting with the `description` field if you are struggling with
GroupChat or want to boost performance.

 **Tags:**

* [AutoGen](/autogen/blog/tags/auto-gen)

[Older PostAgentOptimizer - An Agentic Way to Train Your LLM Agent
»](/autogen/blog/2023/12/23/AgentOptimizer)

* TLDR
* Introduction
* Example
* An Experiment with Distraction
* Tips for Writing Good Descriptions
* Conclusion

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

Recent posts

* [All About Agent Descriptions](/autogen/blog/2023/12/29/AgentDescriptions)
* [AgentOptimizer - An Agentic Way to Train Your LLM Agent](/autogen/blog/2023/12/23/AgentOptimizer)
* [AutoGen Studio: Interactively Explore Multi-Agent Workflows](/autogen/blog/2023/12/01/AutoGenStudio)
* [Agent AutoBuild - Automatically Building Multi-agent Systems](/autogen/blog/2023/11/26/Agent-AutoBuild)
* [How to Assess Utility of LLM-powered Applications?](/autogen/blog/2023/11/20/AgentEval)
* [AutoGen Meets GPTs](/autogen/blog/2023/11/13/OAI-assistants)
* [EcoAssistant - Using LLM Assistants More Accurately and Affordably](/autogen/blog/2023/11/09/EcoAssistant)
* [Multimodal with GPT-4V and LLaVA](/autogen/blog/2023/11/06/LMM-Agent)
* [AutoGen's Teachable Agents](/autogen/blog/2023/10/26/TeachableAgent)
* [Retrieval-Augmented Generation (RAG) Applications with AutoGen](/autogen/blog/2023/10/18/RetrieveChat)
* [Use AutoGen for Local LLMs](/autogen/blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](/autogen/blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](/autogen/blog/2023/05/18/GPT-adaptive-humaneval)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](/autogen/blog/2023/04/21/LLM-tuning-math)

# One post tagged with "teach" {#one-post-tagged-with-teach}

[View All Tags](/autogen/blog/tags)

## [AutoGen's Teachable Agents](/autogen/blog/2023/10/26/TeachableAgent) {#autogens-teachable-agentsautogenblog20231026teachableagent}

October 26, 2023 · 17 min read

[![Ricky Loynd](https://github.com/rickyloynd-
microsoft.png)](https://github.com/rickyloynd-microsoft)

[Ricky Loynd](https://github.com/rickyloynd-microsoft)

Senior Research Engineer at Microsoft

![Teachable Agent Architecture](/autogen/assets/images/teachable-
arch-2d85f7f511a1677c5d5fb19af7ab91ad.png)

 **TL;DR:**

* We introduce **Teachable Agents** so that users can teach their LLM-based assistants new facts, preferences, and skills.
* We showcase examples of teachable agents learning and later recalling facts, preferences, and skills in subsequent chats.

## Introduction​ {#introduction}

Conversational assistants based on LLMs can remember the current chat with the
user, and can also demonstrate in-context learning of user teachings during
the conversation. But the assistant's memories and learnings are lost once the
chat is over, or when a single chat grows too long for the LLM to handle
effectively. Then in subsequent chats the user is forced to repeat any
necessary instructions over and over.

`Teachability` addresses these limitations by persisting user teachings across
chat boundaries in long-term memory implemented as a vector database. Instead
of copying all of memory into the context window, which would eat up valuable
space, individual memories (called memos) are retrieved into context as
needed. This allows the user to teach frequently used facts and skills to the
teachable agent just once, and have it recall them in later chats.

Any instantiated `agent` that inherits from `ConversableAgent` can be made
teachable by instantiating a `Teachability` object and calling its
`add_to_agent(agent)` method. In order to make effective decisions about memo
storage and retrieval, the `Teachability` object calls an instance of
`TextAnalyzerAgent` (another AutoGen agent) to identify and reformulate text
as needed for remembering facts, preferences, and skills. Note that this adds
extra LLM calls involving a relatively small number of tokens, which can add a
few seconds to the time a user waits for each response.

## Run It Yourself​ {#run-it-yourself}

AutoGen contains four code examples that use `Teachability`.

  1. Run [chat_with_teachable_agent.py](https://github.com/microsoft/autogen/blob/main/test/agentchat/contrib/chat_with_teachable_agent.py) to converse with a teachable agent.

  2. Run [test_teachable_agent.py](https://github.com/microsoft/autogen/blob/main/test/agentchat/contrib/test_teachable_agent.py) for quick unit testing of a teachable agent.

  3. Use the Jupyter notebook [agentchat_teachability.ipynb](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_teachability.ipynb) to step through examples discussed below.

  4. Use the Jupyter notebook [agentchat_teachable_oai_assistants.ipynb](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_teachable_oai_assistants.ipynb) to make arbitrary OpenAI Assistants teachable through `GPTAssistantAgent`.

## Basic Usage of Teachability​ {#basic-usage-of-teachability}

  1. Install dependencies

Please install pyautogen with the [teachable] option before using
`Teachability`.

    pip install "pyautogen[teachable]"  

  2. Import agents

    from autogen import UserProxyAgent, config_list_from_json  
    from autogen.agentchat.contrib.capabilities.teachability import Teachability  
    from autogen import ConversableAgent  # As an example  

  3. Create llm_config

    # Load LLM inference endpoints from an env variable or a file  
    # See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints  
    # and OAI_CONFIG_LIST_sample  
    filter_dict = {"model": ["gpt-4"]}  # GPT-3.5 is less reliable than GPT-4 at learning from user feedback.  
    config_list = config_list_from_json(env_or_file="OAI_CONFIG_LIST", filter_dict=filter_dict)  
    llm_config={"config_list": config_list, "timeout": 120}  

  4. Create the agents

    # Start by instantiating any agent that inherits from ConversableAgent, which we use directly here for simplicity.  
    teachable_agent = ConversableAgent(  
        name="teachable_agent",  # The name can be anything.  
        llm_config=llm_config  
    )  
      
    # Instantiate a Teachability object. Its parameters are all optional.  
    teachability = Teachability(  
        reset_db=False,  # Use True to force-reset the memo DB, and False to use an existing DB.  
        path_to_db_dir="./tmp/interactive/teachability_db"  # Can be any path, but teachable agents in a group chat require unique paths.  
    )  
      
    # Now add teachability to the agent.  
    teachability.add_to_agent(teachable_agent)  
      
    # For this test, create a user proxy agent as usual.  
    user = UserProxyAgent("user", human_input_mode="ALWAYS")  

  5. Chat with the teachable agent

    # This function will return once the user types 'exit'.  
    teachable_agent.initiate_chat(user, message="Hi, I'm a teachable user assistant! What's on your mind?")  

## Example 1 - Learning user info​ {#example-1-learning-user-info}

A user can teach the agent facts about themselves. (Note that due to their
finetuning, LLMs can be reluctant to admit that they know personal
information.)

    Loading previous memory (if any) from disk.  
    teachable_agent (to user):  
      
    Greetings, I'm a teachable user assistant! What's on your mind today?  
      
    --------------------------------------------------------------------------------  
    Provide feedback to teachable_agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: My name is Ricky  
    user (to teachable_agent):  
      
    My name is Ricky  
      
    --------------------------------------------------------------------------------  
      
    >>>>>>>> USING AUTO REPLY...  
    teachable_agent (to user):  
      
    Hello Ricky! How can I assist you today?  
      
    --------------------------------------------------------------------------------  
    Provide feedback to teachable_agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: exit  

In a later conversation, the user can check whether the teachable agent
remembers their name. (For readability, the user prompts and some logged
notices are not repeated below.)

    --------------------------------------------------------------------------------  
    user (to teachable_agent):  
      
    what's my name?  
      
    --------------------------------------------------------------------------------  
    teachable_agent (to user):  
      
    Hi Ricky! How can I help you today?  
      
    --------------------------------------------------------------------------------  

## Example 2 - Learning new facts​ {#example-2-learning-new-facts}

A user can teach the agent more complex, related facts.

    --------------------------------------------------------------------------------  
    user (to teachable_agent):  
      
    What is the Vicuna model?  
      
    --------------------------------------------------------------------------------  
    teachable_agent (to user):  
      
    I'm sorry, but I don't have information about the "Vicuna model" in my current database. Could you please provide some context, description, or details about this model so I can better assist you?  
      
    --------------------------------------------------------------------------------  
    user (to teachable_agent):  
      
    Vicuna is a 13B-parameter language model released by Meta.  
      
    --------------------------------------------------------------------------------  
    teachable_agent (to user):  
      
    Thank you for providing the information. The Vicuna model is a large-scale language model developed by Meta. It consists of 13 billion parameters, which is what makes it able to understand and generate human-like text across various topics. Parameters in a neural network are the weights and biases assigned to each connection between the neurons, and having more parameters enables the model to learn and capture more complex patterns and relationships in data. As an AI language model, Vicuna can be utilized in various language-related tasks, such as text completion, translation, summarization, and more.  
      
    --------------------------------------------------------------------------------  
    user (to teachable_agent):  
      
    What is the Orca model?  
      
    --------------------------------------------------------------------------------  
    teachable_agent (to user):  
      
    The Orca model is a yet-to-be-released language model by Meta, referring to the Research Preview API release of Meta's advanced language model. While I don't have specific details about the Orca model at this time, it is expected that the model would have an even larger number of parameters to further enhance its capabilities compared to its predecessor, Vicuna.  
      
    As AI language models evolve, they generally become more efficient at understanding and generating human-like text. This makes them more effective in completing a wide range of tasks, including text completion, translation, summarization, and Q&A, as well as providing more accurate and coherent results in various applications.  
      
    --------------------------------------------------------------------------------  
    user (to teachable_agent):  
      
    Orca is a 13B-parameter language model released by Microsoft. It outperforms Vicuna on most tasks.  
      
    --------------------------------------------------------------------------------  
    teachable_agent (to user):  
      
    Thank you for the clarification. Orca is a 13-billion-parameter language model developed by Microsoft. It is designed to outperform Vicuna, the language model released by Meta. With a large number of parameters, Orca is capable of understanding and generating human-like text across diverse subjects, making it highly useful in various applications.  
      
    Orca's advanced capabilities allow it to excel in tasks such as text completion, translation, summarization, Q&A, and more. Its performance improvement over Vicuna highlights the rapid advancements in natural language processing (NLP) research and the development of increasingly capable AI language models.  

Then in a later chat the teachable agent can answer questions about the facts
it has been taught. (Remember to first close the previous chat by typing
'exit'.)

    --------------------------------------------------------------------------------  
    user (to teachable_agent):  
      
    How does the Vicuna model compare to the Orca model?  
      
    --------------------------------------------------------------------------------  
    teachable_agent (to user):  
      
    The Vicuna and Orca models are both 13B-parameter language models released by Meta and Microsoft, respectively. In terms of performance, Orca has been reported to outperform Vicuna on most tasks. However, without detailed information about specific tasks and benchmarks, it is difficult to provide a more comprehensive comparison. Generally speaking, both models are advanced language models that aim to provide high-quality natural language understanding and generation, but Orca appears to have an edge in terms of overall performance.  
      
    --------------------------------------------------------------------------------  

## Example 3 - Learning user preferences​ {#example-3-learning-user-preferences}

A user can teach the agent how they prefer to have things done.

Be aware that a message like the next one cannot be entered as a single
message through a command line because it contains a newline character. Such
messages can be entered in a Jupyter notebook, or through some UI layer like
that of ChatGPT.

    --------------------------------------------------------------------------------  
    user (to teachable_agent):  
      
    Please summarize this abstract.  
      
    AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation  
    Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang  
    AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.  
      
    --------------------------------------------------------------------------------  
    teachable_agent (to user):  
      
    AutoGen is an open-source framework designed to enable developers to create LLM applications with multiple customizable agents that can converse with each other to complete tasks. These agents can operate using different combinations of LLMs, human inputs, and tools, allowing developers to define agent interaction behaviors flexibly. AutoGen supports programming flexible conversation patterns using both natural language and code, making it suitable for building diverse applications with varying complexities and LLM capacities. Its effectiveness has been demonstrated through empirical studies across various domains including mathematics, coding, operations research, decision-making, and entertainment.  
      
    --------------------------------------------------------------------------------  
    Please summarize this abstract. When I'm summarizing an abstract, I try to make the summary contain just three short bullet points:  the title, the innovation, and the key empirical results.  
      
    AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation  
    Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang  
    AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.  
      
    --------------------------------------------------------------------------------  
    teachable_agent (to user):  
      
    - Title: AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation  
    - Innovation: Open-source framework for creating customizable LLM applications through agent conversations, supporting various modes and interaction behaviors.  
    - Key Empirical Results: Demonstrated effectiveness across diverse application domains, including mathematics, coding, question answering, and more.  
      
    --------------------------------------------------------------------------------  

Then in later chats the teacher doesn't need to reiterate their detailed
preferences.

    --------------------------------------------------------------------------------  
    user (to teachable_agent):  
      
    Please summarize this abstract.  
      
    Sparks of Artificial General Intelligence: Early experiments with GPT-4  
    Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang  
    Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.  
      
    --------------------------------------------------------------------------------  
    teachable_agent (to user):  
      
    - Title: Sparks of Artificial General Intelligence: Early experiments with GPT-4  
    - Innovation: GPT-4, an LLM with remarkable capabilities, demonstrates human-level performance across various domains, like math, coding, vision, medicine, law, and psychology.  
    - Key results: GPT-4 significantly surpasses prior models, suggesting it may be an early version of AGI; limitations and challenges toward deeper AGI are also discussed.  
      
    --------------------------------------------------------------------------------  

## Example 4 - Learning new skills​ {#example-4-learning-new-skills}

Users can extend the teachable agent's capabilities by teaching it new skills
for accomplishing challenging tasks. It usually works best to first describe
the task, then (in the same turn) provide a hint or advice for approaching the
task.

The [Sparks of AGI](https://arxiv.org/abs/2303.12712) paper evaluated GPT-4 on
math problems like the following, which it could only solve 32% of the time.
We first show a failure case, then teach the agent a strategy which lifts
GPT-4's success rate above 95%.

    --------------------------------------------------------------------------------  
    user (to teachable_agent):  
      
    Consider the identity:  
    9 * 4 + 6 * 6 = 72  
    Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?  
    -Let's think step-by-step, write down a plan, and then write down your solution as: "The solution is: A * B + C * D".  
      
    --------------------------------------------------------------------------------  
    Step 1: Identify the current equation and the target value on the right-hand side.  
    Current equation: 9 * 4 + 6 * 6 = 72  
    Target value: 99  
      
    Step 2: Determine what numbers can be changed on the left-hand side.  
    Possible changes: 9, 4, 6 (first occurrence), and 6 (second occurrence)  
      
    Step 3: Evaluate whether each change in the numbers on the left-hand side can result in the target value.  
    9 -> If changed, it would create a larger product.  
    4 -> If changed, the product would increase or decrease by more than the amount needed to reach 99.  
    6 (first occurrence) -> If increased to 7, then the equation would be 9 * 4 + 7 * 6 = 94, which is still not 99.  
    6 (second occurrence) -> If increased to 7, then the equation would be 9 * 4 + 6 * 7 = 99.  
      
    Step 4: Write down the solution.  
    The solution is: 9 * 4 + 6 * 7  
      
    --------------------------------------------------------------------------------  
    user (to teachable_agent):  
      
    Consider the identity:  
    9 * 4 + 6 * 6 = 72  
    Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?  
    -Let's think step-by-step, write down a plan, and then write down your solution as: "The solution is: A * B + C * D".  
      
    Here's some advice that may help:  
    1. Let E denote the original number on the right.  
    2. Let F denote the final number on the right.  
    3. Calculate the difference between the two, G = F - E.  
    4. Examine the numbers on the left one by one until finding one that divides evenly into G, where negative integers are allowed.  
    5. Calculate J = G / H. This is the number of times that H divides into G.  
    6. Verify that J is an integer, and that H * J = G.  
    7. Find the number on the left which is multiplied by H, and call it K.  
    8. Change K to K + J.  
    9. Recompute the value on the left, and verify that it equals F.  
    Finally, write down your solution as: "The solution is: A * B + C * D".  
      
    --------------------------------------------------------------------------------  
    teachable_agent (to user):  
      
    Step 1: Identify the original number (E) and the final number (F) on the right-hand side.  
    E = 72  
    F = 99  
      
    Step 2: Calculate the difference between the two, G = F - E.  
    G = 99 - 72 = 27  
      
    Step 3: Examine the numbers on the left one by one until finding one that divides evenly into G, where negative integers are allowed.  
    9 (divides evenly into 27)  
    4 (does not)  
    6 (first occurrence, does not)  
    6 (second occurrence, does not)  
      
    Step 4: Calculate J = G / H. This is the number of times that H divides into G.  
    H = 9 (since 9 divides evenly into 27)  
    J = 27 / 9 = 3  
      
    Step 5: Verify that J is an integer, and that H * J = G.  
    H * J = 9 * 3 = 27  
    J = 3 (which is an integer)  
      
    Step 6: Find the number on the left which is multiplied by H, and call it K.  
    Number on the leftside of the equation that is multiplied by 9: 4  
    Therefore, K = 4  
      
    Step 7: Change K to K + J.  
    K + J = 4 + 3 = 7  
      
    Step 8: Recompute the value on the left, and verify that it equals F.  
    9 * 7 + 6 * 6 = 63 + 36 = 99 (which is equal to F)  
      
    Step 9: Write down the solution.  
    The solution is: 9 * 7 + 6 * 6  
      
    --------------------------------------------------------------------------------  

In a later chat the user doesn't need to repeat the detailed advice.

    --------------------------------------------------------------------------------  
    user (to teachable_agent):  
      
    Consider the identity:  
    8 * 3 + 7 * 9 = 87  
    Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 59?  
    -Let's think step-by-step, write down a plan, and then write down your solution as: "The solution is: A * B + C * D".  
      
    --------------------------------------------------------------------------------  
    teachable_agent (to user):  
      
    Let's follow the advice to modify one integer on the left hand side of the equation to make the right hand side become 59:  
      
    1. Let E denote the original number on the right: E = 87.  
    2. Let F denote the final number on the right: F = 59.  
    3. Calculate the difference between the two, G = F - E: G = 59 - 87 = -28.  
    4. Examine the numbers on the left one by one to find one that divides evenly into G: 8, 3, 7, 9. Since 7 divides -28 evenly, we will work with 7 (H = 7).  
    5. Calculate J = G / H: J = -28 / 7 = -4.  
    6. Verify that J is an integer, and that H * J = G: J is an integer, and 7 * (-4) = -28.  
    7. Find the number on the left which is multiplied by H (7), and call it K: K = 9.  
    8. Change K to K + J: K_new = 9 + (-4) = 5.  
    9. Recompute the value on the left, and verify that it equals F: (8 * 3) + (7 * 5) = 24 + 35 = 59.  
      
    The solution is: 8 * 3 + 7 * 5  
      
    --------------------------------------------------------------------------------  

## Planned improvements​ {#planned-improvements}

* Understanding user instructions distributed over multiple turns.
* Learning from the agent's own experience, to reduce dependence on explicit user teachings.
* Learning skills built on top of previously learned skills.

## Conclusion​ {#conclusion}

`Teachability` is still under active research and development. For any
problems you find or improvements you have in mind, please join our
discussions in this repo and on our [Discord
channel](https://discord.gg/pAbnFJrkgZ). We look forward to seeing how you and
the rest of the community can use and improve teachable agents in AutoGen!

 **Tags:**

* [LLM](/autogen/blog/tags/llm)
* [teach](/autogen/blog/tags/teach)

[**Read More**](/autogen/blog/2023/10/26/TeachableAgent)

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

* Reference
  * agentchat
    * agentchat.contrib
      * agentchat.contrib.capabilities
      * [agent_builder](/autogen/docs/reference/agentchat/contrib/agent_builder)
      * [compressible_agent](/autogen/docs/reference/agentchat/contrib/compressible_agent)
      * [gpt_assistant_agent](/autogen/docs/reference/agentchat/contrib/gpt_assistant_agent)
      * [img_utils](/autogen/docs/reference/agentchat/contrib/img_utils)
      * [llava_agent](/autogen/docs/reference/agentchat/contrib/llava_agent)
      * [math_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/math_user_proxy_agent)
      * [multimodal_conversable_agent](/autogen/docs/reference/agentchat/contrib/multimodal_conversable_agent)
      * [qdrant_retrieve_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/qdrant_retrieve_user_proxy_agent)
      * [retrieve_assistant_agent](/autogen/docs/reference/agentchat/contrib/retrieve_assistant_agent)
      * [retrieve_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/retrieve_user_proxy_agent)
      * [text_analyzer_agent](/autogen/docs/reference/agentchat/contrib/text_analyzer_agent)
    * [agent](/autogen/docs/reference/agentchat/agent)
    * [assistant_agent](/autogen/docs/reference/agentchat/assistant_agent)
    * [conversable_agent](/autogen/docs/reference/agentchat/conversable_agent)
    * [groupchat](/autogen/docs/reference/agentchat/groupchat)
    * [user_proxy_agent](/autogen/docs/reference/agentchat/user_proxy_agent)
  * oai
  * [code_utils](/autogen/docs/reference/code_utils)
  * [function_utils](/autogen/docs/reference/function_utils)
  * [math_utils](/autogen/docs/reference/math_utils)
  * [retrieve_utils](/autogen/docs/reference/retrieve_utils)
  * [token_count_utils](/autogen/docs/reference/token_count_utils)

On this page

# agentchat.contrib.qdrant_retrieve_user_proxy_agent {#agentchatcontribqdrantretrieveuserproxyagent}

## QdrantRetrieveUserProxyAgent Objects​ {#qdrantretrieveuserproxyagent-objects}

```python
    class QdrantRetrieveUserProxyAgent(RetrieveUserProxyAgent)  

#### __init__​ {#init}


    def __init__(name="RetrieveChatAgent",  
                 human_input_mode: Optional[str] = "ALWAYS",  
                 is_termination_msg: Optional[Callable[[Dict], bool]] = None,  
                 retrieve_config: Optional[Dict] = None,  
                 **kwargs)  
```

 **Arguments** :

* `name` _str_ \- name of the agent.
* `human_input_mode` _str_ \- whether to ask for human inputs every time a message is received. Possible values are "ALWAYS", "TERMINATE", "NEVER". (1) When "ALWAYS", the agent prompts for human input every time a message is received. Under this mode, the conversation stops when the human input is "exit", or when is_termination_msg is True and there is no human input. (2) When "TERMINATE", the agent only prompts for human input only when a termination message is received or the number of auto reply reaches the max_consecutive_auto_reply. (3) When "NEVER", the agent will never prompt for human input. Under this mode, the conversation stops when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.
* `is_termination_msg` _function_ \- a function that takes a message in the form of a dictionary and returns a boolean value indicating if this received message is a termination message. The dict can contain the following keys: "content", "role", "name", "function_call".
* `retrieve_config` _dict or None_ \- config for the retrieve agent. To use default config, set to None. Otherwise, set to a dictionary with the following keys:
  * task (Optional, str): the task of the retrieve chat. Possible values are "code", "qa" and "default". System prompt will be different for different tasks. The default value is `default`, which supports both code and qa.
  * client (Optional, qdrant_client.QdrantClient(":memory:")): A QdrantClient instance. If not provided, an in-memory instance will be assigned. Not recommended for production. will be used. If you want to use other vector db, extend this class and override the `retrieve_docs` function.
  * docs_path (Optional, Union[str, List[str]]): the path to the docs directory. It can also be the path to a single file, the url to a single file or a list of directories, files and urls. Default is None, which works only if the collection is already created.
  * extra_docs (Optional, bool): when true, allows adding documents with unique IDs without overwriting existing ones; when false, it replaces existing documents using default IDs, risking collection overwrite., when set to true it enables the system to assign unique IDs starting from "length+i" for new document chunks, preventing the replacement of existing documents and facilitating the addition of more content to the collection.. By default, "extra_docs" is set to false, starting document IDs from zero. This poses a risk as new documents might overwrite existing ones, potentially causing unintended loss or alteration of data in the collection.
  * collection_name (Optional, str): the name of the collection. If key not provided, a default name `autogen-docs` will be used.
  * model (Optional, str): the model to use for the retrieve chat. If key not provided, a default model `gpt-4` will be used.
  * chunk_token_size (Optional, int): the chunk token size for the retrieve chat. If key not provided, a default size `max_tokens * 0.4` will be used.
  * context_max_tokens (Optional, int): the context max token size for the retrieve chat. If key not provided, a default size `max_tokens * 0.8` will be used.
  * chunk_mode (Optional, str): the chunk mode for the retrieve chat. Possible values are "multi_lines" and "one_line". If key not provided, a default mode `multi_lines` will be used.
  * must_break_at_empty_line (Optional, bool): chunk will only break at empty line if True. Default is True. If chunk_mode is "one_line", this parameter will be ignored.
  * embedding_model (Optional, str): the embedding model to use for the retrieve chat. If key not provided, a default model `BAAI/bge-small-en-v1.5` will be used. All available models can be found at `https://qdrant.github.io/fastembed/examples/Supported_Models/`.
  * customized_prompt (Optional, str): the customized prompt for the retrieve chat. Default is None.
  * customized_answer_prefix (Optional, str): the customized answer prefix for the retrieve chat. Default is "". If not "" and the customized_answer_prefix is not in the answer, `Update Context` will be triggered.
  * update_context (Optional, bool): if False, will not apply `Update Context` for interactive retrieval. Default is True.
  * custom_token_count_function (Optional, Callable): a custom function to count the number of tokens in a string. The function should take a string as input and return three integers (token_count, tokens_per_message, tokens_per_name). Default is None, tiktoken will be used and may not be accurate for non-OpenAI models.
  * custom_text_split_function (Optional, Callable): a custom function to split a string into a list of strings. Default is None, will use the default function in `autogen.retrieve_utils.split_text_to_chunks`.
  * custom_text_types (Optional, List[str]): a list of file types to be processed. Default is `autogen.retrieve_utils.TEXT_FORMATS`. This only applies to files under the directories in `docs_path`. Explicitly included files and urls will be chunked regardless of their types.
  * recursive (Optional, bool): whether to search documents recursively in the docs_path. Default is True.
  * parallel (Optional, int): How many parallel workers to use for embedding. Defaults to the number of CPU cores.
  * on_disk (Optional, bool): Whether to store the collection on disk. Default is False.
  * quantization_config: Quantization configuration. If None, quantization will be disabled.
  * hnsw_config: HNSW configuration. If None, default configuration will be used. You can find more info about the hnsw configuration options at [https://qdrant.tech/documentation/concepts/indexing/`vector`-index](https://qdrant.tech/documentation/concepts/indexing/%60vector%60-index). API Reference: <https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_collection>
  * payload_indexing: Whether to create a payload index for the document field. Default is False. You can find more info about the payload indexing options at [https://qdrant.tech/documentation/concepts/indexing/`payload`-index](https://qdrant.tech/documentation/concepts/indexing/%60payload%60-index) API Reference: <https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_field_index>
* `**kwargs` _dict_ \- other kwargs in [UserProxyAgent](/autogen/docs/reference/agentchat/user_proxy_agent#__init__).

#### retrieve_docs​ {#retrievedocs}

```python
    def retrieve_docs(problem: str, n_results: int = 20, search_string: str = "")  
```

 **Arguments** :

* `problem` _str_ \- the problem to be solved.
* `n_results` _int_ \- the number of results to be retrieved. Default is 20.
* `search_string` _str_ \- only docs that contain an exact match of this string will be retrieved. Default is "".

#### create_qdrant_from_dir​ {#createqdrantfromdir}

```python
    def create_qdrant_from_dir(  
            dir_path: str,  
            max_tokens: int = 4000,  
            client: QdrantClient = None,  
            collection_name: str = "all-my-documents",  
            chunk_mode: str = "multi_lines",  
            must_break_at_empty_line: bool = True,  
            embedding_model: str = "BAAI/bge-small-en-v1.5",  
            custom_text_split_function: Callable = None,  
            custom_text_types: List[str] = TEXT_FORMATS,  
            recursive: bool = True,  
            extra_docs: bool = False,  
            parallel: int = 0,  
            on_disk: bool = False,  
            quantization_config: Optional[models.QuantizationConfig] = None,  
            hnsw_config: Optional[models.HnswConfigDiff] = None,  
            payload_indexing: bool = False,  
            qdrant_client_options: Optional[Dict] = {})  
```

Create a Qdrant collection from all the files in a given directory, the
directory can also be a single file or a url to a single file.

 **Arguments** :

* `dir_path` _str_ \- the path to the directory, file or url.
* `max_tokens` _Optional, int_ \- the maximum number of tokens per chunk. Default is 4000.
* `client` _Optional, QdrantClient_ \- the QdrantClient instance. Default is None.
* `collection_name` _Optional, str_ \- the name of the collection. Default is "all-my-documents".
* `chunk_mode` _Optional, str_ \- the chunk mode. Default is "multi_lines".
* `must_break_at_empty_line` _Optional, bool_ \- Whether to break at empty line. Default is True.
* `embedding_model` _Optional, str_ \- the embedding model to use. Default is "BAAI/bge-small-en-v1.5". The list of all the available models can be at <https://qdrant.github.io/fastembed/examples/Supported_Models/>.
* `custom_text_split_function` _Optional, Callable_ \- a custom function to split a string into a list of strings. Default is None, will use the default function in `autogen.retrieve_utils.split_text_to_chunks`.
* `custom_text_types` _Optional, List [str]_ \- a list of file types to be processed. Default is TEXT_FORMATS.
* `recursive` _Optional, bool_ \- whether to search documents recursively in the dir_path. Default is True.
* `extra_docs` _Optional, bool_ \- whether to add more documents in the collection. Default is False
* `parallel` _Optional, int_ \- How many parallel workers to use for embedding. Defaults to the number of CPU cores
* `on_disk` _Optional, bool_ \- Whether to store the collection on disk. Default is False.
* `quantization_config` \- Quantization configuration. If None, quantization will be disabled.
* `Ref` \- <https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_collection>
* `hnsw_config` \- HNSW configuration. If None, default configuration will be used.
* `Ref` \- <https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_collection>
* `payload_indexing` \- Whether to create a payload index for the document field. Default is False.
* `qdrant_client_options` \- (Optional, dict): the options for instantiating the qdrant client.
* `Ref` \- <https://github.com/qdrant/qdrant-client/blob/master/qdrant_client/qdrant_client.py#L36-L58>.

#### query_qdrant​ {#queryqdrant}

```python
    def query_qdrant(  
            query_texts: List[str],  
            n_results: int = 10,  
            client: QdrantClient = None,  
            collection_name: str = "all-my-documents",  
            search_string: str = "",  
            embedding_model: str = "BAAI/bge-small-en-v1.5",  
            qdrant_client_options: Optional[Dict] = {}  
    ) -> List[List[QueryResponse]]  
```

Perform a similarity search with filters on a Qdrant collection

 **Arguments** :

* `query_texts` _List [str]_ \- the query texts.
* `n_results` _Optional, int_ \- the number of results to return. Default is 10.
* `client` _Optional, API_ \- the QdrantClient instance. A default in-memory client will be instantiated if None.
* `collection_name` _Optional, str_ \- the name of the collection. Default is "all-my-documents".
* `search_string` _Optional, str_ \- the search string. Default is "".
* `embedding_model` _Optional, str_ \- the embedding model to use. Default is "all-MiniLM-L6-v2". Will be ignored if embedding_function is not None.
* `qdrant_client_options` \- (Optional, dict): the options for instantiating the qdrant client. Reference: <https://github.com/qdrant/qdrant-client/blob/master/qdrant_client/qdrant_client.py#L36-L58>.

 **Returns** :

* `List[List[QueryResponse]]` \- the query result. The format is: class QueryResponse(BaseModel, extra="forbid"): # type: ignore
* `id` \- Union[str, int]
* `embedding` \- Optional[List[float]]
* `metadata` \- Dict[str, Any]
* `document` \- str
* `score` \- float

[Edit this
page](https://github.com/microsoft/autogen/edit/main/website/docs/reference/agentchat/contrib/qdrant_retrieve_user_proxy_agent.md)

[Previous«
multimodal_conversable_agent](/autogen/docs/reference/agentchat/contrib/multimodal_conversable_agent)

[Nextretrieve_assistant_agent
»](/autogen/docs/reference/agentchat/contrib/retrieve_assistant_agent)

* QdrantRetrieveUserProxyAgent Objects

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
  * automl
    * automl.contrib
    * automl.nlp
    * automl.spark
      * [metrics](.docs/reference/automl/spark/metrics)
      * [utils](.docs/reference/automl/spark/utils)
    * automl.task
    * automl.time_series
    * [automl](.docs/reference/automl/automl)
    * [data](.docs/reference/automl/data)
    * [ml](.docs/reference/automl/ml)
    * [model](.docs/reference/automl/model)
    * [state](.docs/reference/automl/state)
  * default
  * onlineml
  * tune

On this page

# automl.spark.utils {#automlsparkutils}

#### to_pandas_on_spark​ {#topandasonspark}

    def to_pandas_on_spark(df: Union[DataFrame, sparkDataFrame, Series, psDataFrame, psSeries], index_col: Optional[str] = None, default_index_type: Optional[str] = "distributed-sequence") -> Union[psDataFrame, psSeries]  

Convert pandas or pyspark dataframe/series to pandas_on_Spark
dataframe/series.

 **Arguments** :

* `df` \- pandas.DataFrame/series or pyspark dataframe | The input dataframe/series.
* `index_col` \- str, optional | The column name to use as index, default None.
* `default_index_type` \- str, optional | The default index type, default "distributed-sequence".

 **Returns** :

* `pyspark.pandas.DataFrame/Series` \- The converted pandas-on-Spark dataframe/series.

    import pandas as pd  
    from flaml.automl.spark.utils import to_pandas_on_spark  

    pdf = DataFrame({"a": [1, 2, 3], "b": [4, 5, 6]})  
    psdf = to_pandas_on_spark(pdf)  
    print(psdf)  

    from pyspark.sql import SparkSession  

    spark = SparkSession.builder.getOrCreate()  
    sdf = spark.createDataFrame(pdf)  
    psdf = to_pandas_on_spark(sdf)  
    print(psdf)  

    pds = Series([1, 2, 3])  
    pss = to_pandas_on_spark(pds)  
    print(pss)  

#### train_test_split_pyspark​ {#traintestsplitpyspark}

    def train_test_split_pyspark(df: Union[sparkDataFrame, psDataFrame], stratify_column: Optional[str] = None, test_fraction: Optional[float] = 0.2, seed: Optional[int] = 1234, to_pandas_spark: Optional[bool] = True, index_col: Optional[str] = "tmp_index_col") -> Tuple[Union[sparkDataFrame, psDataFrame], Union[sparkDataFrame, psDataFrame]]  

Split a pyspark dataframe into train and test dataframes.

 **Arguments** :

* `df` \- pyspark.sql.DataFrame | The input dataframe.
* `stratify_column` \- str | The column name to stratify the split. Default None.
* `test_fraction` \- float | The fraction of the test data. Default 0.2.
* `seed` \- int | The random seed. Default 1234.
* `to_pandas_spark` \- bool | Whether to convert the output to pandas_on_spark. Default True.
* `index_col` \- str | The column name to use as index. Default None.

 **Returns** :

pyspark.sql.DataFrame/pandas_on_spark DataFrame | The train dataframe.
pyspark.sql.DataFrame/pandas_on_spark DataFrame | The test dataframe.

#### unique_pandas_on_spark​ {#uniquepandasonspark}

    def unique_pandas_on_spark(psds: Union[psSeries, psDataFrame]) -> Tuple[np.ndarray, np.ndarray]  

Get the unique values and counts of a pandas_on_spark series.

#### len_labels​ {#lenlabels}

    def len_labels(y: Union[psSeries, np.ndarray], return_labels=False) -> Union[int, Optional[np.ndarray]]  

Get the number of unique labels in y.

#### unique_value_first_index​ {#uniquevaluefirstindex}

    def unique_value_first_index(y: Union[Series, psSeries, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]  

Get the unique values and indices of a pandas series, pandas_on_spark series
or numpy array.

#### iloc_pandas_on_spark​ {#ilocpandasonspark}

    def iloc_pandas_on_spark(psdf: Union[psDataFrame, psSeries, DataFrame, Series], index: Union[int, slice, list], index_col: Optional[str] = "tmp_index_col") -> Union[psDataFrame, psSeries]  

Get the rows of a pandas_on_spark dataframe/series by index.

#### spark_kFold​ {#sparkkfold}

    def spark_kFold(dataset: Union[sparkDataFrame, psDataFrame], nFolds: int = 3, foldCol: str = "", seed: int = 42, index_col: Optional[str] = "tmp_index_col") -> List[Tuple[psDataFrame, psDataFrame]]  

Generate k-fold splits for a Spark DataFrame. Adopted from
<https://spark.apache.org/docs/latest/api/python/_modules/pyspark/ml/tuning.html#CrossValidator>

**Arguments** :

* `dataset` \- sparkDataFrame / psDataFrame. | The DataFrame to split.
* `nFolds` \- int | The number of folds. Default is 3.
* `foldCol` \- str | The column name to use for fold numbers. If not specified, the DataFrame will be randomly split. Default is "". The same group will not appear in two different folds (the number of distinct groups has to be at least equal to the number of folds). The folds are approximately balanced in the sense that the number of distinct groups is approximately the same in each fold.
* `seed` \- int | The random seed. Default is 42.
* `index_col` \- str | The name of the index column. Default is "tmp_index_col".

 **Returns** :

A list of (train, validation) DataFrames.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/automl/spark/utils.md)

[Previous« metrics](.docs/reference/automl/spark/metrics)

[Nexttask »](.docs/reference/automl/task/task)

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
  * automl
    * automl.contrib
    * automl.nlp
    * automl.spark
    * automl.task
    * automl.time_series
    * [automl](.docs/reference/automl/automl)
    * [data](.docs/reference/automl/data)
    * [ml](.docs/reference/automl/ml)
    * [model](.docs/reference/automl/model)
    * [state](.docs/reference/automl/state)
  * default
  * onlineml
  * tune

On this page

# automl.automl {#automlautoml}

#### size​ {#size}

    def size(learner_classes: dict, config: dict) -> float  

Size function.

 **Returns** :

The mem size in bytes for a config.

## AutoML Objects​ {#automl-objects}

    class AutoML(BaseEstimator)  

The AutoML class.

 **Example** :

    automl = AutoML()  
    automl_settings = {  
        "time_budget": 60,  
        "metric": 'accuracy',  
        "task": 'classification',  
        "log_file_name": 'mylog.log',  
    }  
    automl.fit(X_train = X_train, y_train = y_train, **automl_settings)  

#### __init__​ {#init}

    def __init__(**settings)  

Constructor.

Many settings in fit() can be passed to the constructor too. If an argument in
fit() is provided, it will override the setting passed to the constructor. If
an argument in fit() is not provided but provided in the constructor, the
value passed to the constructor will be used.

 **Arguments** :

* `metric` \- A string of the metric name or a function, e.g., 'accuracy', 'roc_auc', 'roc_auc_ovr', 'roc_auc_ovo', 'roc_auc_weighted', 'roc_auc_ovo_weighted', 'roc_auc_ovr_weighted', 'f1', 'micro_f1', 'macro_f1', 'log_loss', 'mae', 'mse', 'r2', 'mape'. Default is 'auto'. If passing a customized metric function, the function needs to have the following input arguments:

    def custom_metric(  
        X_test, y_test, estimator, labels,  
        X_train, y_train, weight_test=None, weight_train=None,  
        config=None, groups_test=None, groups_train=None,  
    ):  
        return metric_to_minimize, metrics_to_log  

which returns a float number as the minimization objective, and a dictionary
as the metrics to log. E.g.,

    def custom_metric(  
        X_val, y_val, estimator, labels,  
        X_train, y_train, weight_val=None, weight_train=None,  
        *args,  
    ):  
        from sklearn.metrics import log_loss  
        import time  
      
        start = time.time()  
        y_pred = estimator.predict_proba(X_val)  
        pred_time = (time.time() - start) / len(X_val)  
        val_loss = log_loss(y_val, y_pred, labels=labels, sample_weight=weight_val)  
        y_pred = estimator.predict_proba(X_train)  
        train_loss = log_loss(y_train, y_pred, labels=labels, sample_weight=weight_train)  
        alpha = 0.5  
        return val_loss * (1 + alpha) - alpha * train_loss, {  
            "val_loss": val_loss,  
            "train_loss": train_loss,  
            "pred_time": pred_time,  
        }  

* `task` \- A string of the task type, e.g., 'classification', 'regression', 'ts_forecast', 'rank', 'seq-classification', 'seq-regression', 'summarization', or an instance of the Task class.
* `n_jobs` \- An integer of the number of threads for training | default=-1. Use all available resources when n_jobs == -1.
* `log_file_name` \- A string of the log file name | default="". To disable logging, set it to be an empty string "".
* `estimator_list` \- A list of strings for estimator names, or 'auto'. e.g., `['lgbm', 'xgboost', 'xgb_limitdepth', 'catboost', 'rf', 'extra_tree']`.
* `time_budget` \- A float number of the time budget in seconds. Use -1 if no time limit.
* `max_iter` \- An integer of the maximal number of iterations.
* `sample` \- A boolean of whether to sample the training data during search.
* `ensemble` \- boolean or dict | default=False. Whether to perform ensemble after search. Can be a dict with keys 'passthrough' and 'final_estimator' to specify the passthrough and final_estimator in the stacker. The dict can also contain 'n_jobs' as the key to specify the number of jobs for the stacker.
* `eval_method` \- A string of resampling strategy, one of ['auto', 'cv', 'holdout'].
* `split_ratio` \- A float of the valiation data percentage for holdout.
* `n_splits` \- An integer of the number of folds for cross - validation.
* `log_type` \- A string of the log type, one of ['better', 'all']. 'better' only logs configs with better loss than previos iters 'all' logs all the tried configs.
* `model_history` \- A boolean of whether to keep the best model per estimator. Make sure memory is large enough if setting to True.
* `log_training_metric` \- A boolean of whether to log the training metric for each model.
* `mem_thres` \- A float of the memory size constraint in bytes.
* `pred_time_limit` \- A float of the prediction latency constraint in seconds. It refers to the average prediction time per row in validation data.
* `train_time_limit` \- A float of the training time constraint in seconds.
* `verbose` \- int, default=3 | Controls the verbosity, higher means more messages.
* `retrain_full` \- bool or str, default=True | whether to retrain the selected model on the full training data when using holdout. True - retrain only after search finishes; False - no retraining; 'budget' \- do best effort to retrain without violating the time budget.
* `split_type` \- str or splitter object, default="auto" | the data split type.
  * A valid splitter object is an instance of a derived class of scikit-learn [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold) and have `split` and `get_n_splits` methods with the same signatures. Set eval_method to "cv" to use the splitter object.
  * Valid str options depend on different tasks. For classification tasks, valid choices are ["auto", 'stratified', 'uniform', 'time', 'group']. "auto" -> stratified. For regression tasks, valid choices are ["auto", 'uniform', 'time']. "auto" -> uniform. For time series forecast tasks, must be "auto" or 'time'. For ranking task, must be "auto" or 'group'.
* `hpo_method` \- str, default="auto" | The hyperparameter optimization method. By default, CFO is used for sequential search and BlendSearch is used for parallel search. No need to set when using flaml's default search space or using a simple customized search space. When set to 'bs', BlendSearch is used. BlendSearch can be tried when the search space is complex, for example, containing multiple disjoint, discontinuous subspaces. When set to 'random', random search is used.
* `starting_points` \- A dictionary or a str to specify the starting hyperparameter config for the estimators | default="static". If str:
  * if "data", use data-dependent defaults;
  * if "data:path" use data-dependent defaults which are stored at path;
  * if "static", use data-independent defaults. If dict, keys are the name of the estimators, and values are the starting hyperparamter configurations for the corresponding estimators. The value can be a single hyperparamter configuration dict or a list of hyperparamter configuration dicts. In the following code example, we get starting_points from the `automl` object and use them in the `new_automl` object. e.g.,

    from flaml import AutoML  
    automl = AutoML()  
    X_train, y_train = load_iris(return_X_y=True)  
    automl.fit(X_train, y_train)  
    starting_points = automl.best_config_per_estimator  

    new_automl = AutoML()  
    new_automl.fit(X_train, y_train, starting_points=starting_points)  

* `seed` \- int or None, default=None | The random seed for hpo.
* `n_concurrent_trials` \- [In preview] int, default=1 | The number of concurrent trials. When n_concurrent_trials > 1, flaml performes [parallel tuning](.docs/Use-Cases/Task-Oriented-AutoML#parallel-tuning) and installation of ray or spark is required: `pip install flaml[ray]` or `pip install flaml[spark]`. Please check [here](https://spark.apache.org/docs/latest/api/python/getting_started/install.html) for more details about installing Spark.
* `keep_search_state` \- boolean, default=False | Whether to keep data needed for model search after fit(). By default the state is deleted for space saving.
* `preserve_checkpoint` \- boolean, default=True | Whether to preserve the saved checkpoint on disk when deleting automl. By default the checkpoint is preserved.
* `early_stop` \- boolean, default=False | Whether to stop early if the search is considered to converge.
* `force_cancel` \- boolean, default=False | Whether to forcely cancel Spark jobs if the search time exceeded the time budget.
* `append_log` \- boolean, default=False | Whether to directly append the log records to the input log file if it exists.
* `auto_augment` \- boolean, default=True | Whether to automatically augment rare classes.
* `min_sample_size` \- int, default=MIN_SAMPLE_TRAIN | the minimal sample size when sample=True.
* `use_ray` \- boolean or dict. If boolean: default=False | Whether to use ray to run the training in separate processes. This can be used to prevent OOM for large datasets, but will incur more overhead in time. If dict: the dict contains the keywords arguments to be passed to [ray.tune.run](https://docs.ray.io/en/latest/tune/api_docs/execution.html).
* `use_spark` \- boolean, default=False | Whether to use spark to run the training in parallel spark jobs. This can be used to accelerate training on large models and large datasets, but will incur more overhead in time and thus slow down training in some cases. GPU training is not supported yet when use_spark is True. For Spark clusters, by default, we will launch one trial per executor. However, sometimes we want to launch more trials than the number of executors (e.g., local mode). In this case, we can set the environment variable `FLAML_MAX_CONCURRENT` to override the detected `num_executors`. The final number of concurrent trials will be the minimum of `n_concurrent_trials` and `num_executors`.
* `free_mem_ratio` \- float between 0 and 1, default=0. The free memory ratio to keep during training.
* `metric_constraints` \- list, default=[] | The list of metric constraints. Each element in this list is a 3-tuple, which shall be expressed in the following format: the first element of the 3-tuple is the name of the metric, the second element is the inequality sign chosen from ">=" and "<=", and the third element is the constraint value. E.g., `('val_loss', '<=', 0.1)`. Note that all the metric names in metric_constraints need to be reported via the metrics_to_log dictionary returned by a customized metric function. The customized metric function shall be provided via the `metric` key word argument of the fit() function or the automl constructor. Find an example in the 4th constraint type in this [doc](.docs/Use-Cases/Task-Oriented-AutoML#constraint). If `pred_time_limit` is provided as one of keyword arguments to fit() function or the automl constructor, flaml will automatically (and under the hood) add it as an additional element in the metric_constraints. Essentially 'pred_time_limit' specifies a constraint about the prediction latency constraint in seconds.
* `custom_hp` \- dict, default=None | The custom search space specified by user. It is a nested dict with keys being the estimator names, and values being dicts per estimator search space. In the per estimator search space dict, the keys are the hyperparameter names, and values are dicts of info ("domain", "init_value", and "low_cost_init_value") about the search space associated with the hyperparameter (i.e., per hyperparameter search space dict). When custom_hp is provided, the built-in search space which is also a nested dict of per estimator search space dict, will be updated with custom_hp. Note that during this nested dict update, the per hyperparameter search space dicts will be replaced (instead of updated) by the ones provided in custom_hp. Note that the value for "domain" can either be a constant or a sample.Domain object. e.g.,

    custom_hp = {  
         "transformer_ms": {  
             "model_path": {  
                 "domain": "albert-base-v2",  
             },  
             "learning_rate": {  
                 "domain": tune.choice([1e-4, 1e-5]),  
             }  
         }  
     }  

* `skip_transform` \- boolean, default=False | Whether to pre-process data prior to modeling.
* `fit_kwargs_by_estimator` \- dict, default=None | The user specified keywords arguments, grouped by estimator name. e.g.,

    fit_kwargs_by_estimator = {  
        "transformer": {  
            "output_dir": "test/data/output/",  
            "fp16": False,  
        }  
    }  

* `mlflow_logging` \- boolean, default=True | Whether to log the training results to mlflow. This requires mlflow to be installed and to have an active mlflow run. FLAML will create nested runs.

#### config_history​ {#confighistory}

    @property  
    def config_history() -> dict  

A dictionary of iter->(estimator, config, time), storing the best estimator,
config, and the time when the best model is updated each time.

#### model​ {#model}

    @property  
    def model()  

An object with `predict()` and `predict_proba()` method (for classification),
storing the best trained model.

#### best_model_for_estimator​ {#bestmodelforestimator}

    def best_model_for_estimator(estimator_name: str)  

Return the best model found for a particular estimator.

 **Arguments** :

* `estimator_name` \- a str of the estimator's name.

 **Returns** :

An object storing the best model for estimator_name. If `model_history` was
set to False during fit(), then the returned model is untrained unless
estimator_name is the best estimator. If `model_history` was set to True, then
the returned model is trained.

#### best_estimator​ {#bestestimator}

    @property  
    def best_estimator()  

A string indicating the best estimator found.

#### best_iteration​ {#bestiteration}

    @property  
    def best_iteration()  

An integer of the iteration number where the best config is found.

#### best_config​ {#bestconfig}

    @property  
    def best_config()  

A dictionary of the best configuration.

#### best_config_per_estimator​ {#bestconfigperestimator}

    @property  
    def best_config_per_estimator()  

A dictionary of all estimators' best configuration.

#### best_loss_per_estimator​ {#bestlossperestimator}

    @property  
    def best_loss_per_estimator()  

A dictionary of all estimators' best loss.

#### best_loss​ {#bestloss}

    @property  
    def best_loss()  

A float of the best loss found.

#### best_result​ {#bestresult}

    @property  
    def best_result()  

Result dictionary for model trained with the best config.

#### metrics_for_best_config​ {#metricsforbestconfig}

    @property  
    def metrics_for_best_config()  

Returns a float of the best loss, and a dictionary of the auxiliary metrics to
log associated with the best config. These two objects correspond to the
returned objects by the customized metric function for the config with the
best loss.

#### best_config_train_time​ {#bestconfigtraintime}

    @property  
    def best_config_train_time()  

A float of the seconds taken by training the best config.

#### feature_transformer​ {#featuretransformer}

    @property  
    def feature_transformer()  

Returns feature transformer which is used to preprocess data before applying
training or inference.

#### label_transformer​ {#labeltransformer}

    @property  
    def label_transformer()  

Returns label transformer which is used to preprocess labels before scoring,
and inverse transform labels after inference.

#### classes_​ {#classes}

    @property  
    def classes_()  

A numpy array of shape (n_classes,) for class labels.

#### time_to_find_best_model​ {#timetofindbestmodel}

    @property  
    def time_to_find_best_model() -> float  

Time taken to find best model in seconds.

#### predict​ {#predict}

    def predict(X: Union[np.array, DataFrame, List[str], List[List[str]], psDataFrame], **pred_kwargs, ,)  

Predict label from features.

 **Arguments** :

* `X` \- A numpy array or pandas dataframe or pyspark.pandas dataframe of featurized instances, shape n * m, or for time series forcast tasks: a pandas dataframe with the first column containing timestamp values (datetime type) or an integer n for the predict steps (only valid when the estimator is arima or sarimax). Other columns in the dataframe are assumed to be exogenous variables (categorical or numeric).
* `**pred_kwargs` \- Other key word arguments to pass to predict() function of the searched learners, such as per_device_eval_batch_size.

    multivariate_X_test = DataFrame({  
        'timeStamp': pd.date_range(start='1/1/2022', end='1/07/2022'),  
        'categorical_col': ['yes', 'yes', 'no', 'no', 'yes', 'no', 'yes'],  
        'continuous_col': [105, 107, 120, 118, 110, 112, 115]  
    })  
    model.predict(multivariate_X_test)  

 **Returns** :

A array-like of shape n * 1: each element is a predicted label for an
instance.

#### predict_proba​ {#predictproba}

    def predict_proba(X, **pred_kwargs)  

Predict the probability of each class from features, only works for
classification problems.

 **Arguments** :

* `X` \- A numpy array of featurized instances, shape n * m.
* `**pred_kwargs` \- Other key word arguments to pass to predict_proba() function of the searched learners, such as per_device_eval_batch_size.

 **Returns** :

A numpy array of shape n * c. c is the # classes. Each element at (i, j) is
the probability for instance i to be in class j.

#### add_learner​ {#addlearner}

    def add_learner(learner_name, learner_class)  

Add a customized learner.

 **Arguments** :

* `learner_name` \- A string of the learner's name.
* `learner_class` \- A subclass of flaml.automl.model.BaseEstimator.

#### get_estimator_from_log​ {#getestimatorfromlog}

    def get_estimator_from_log(log_file_name: str, record_id: int, task: Union[str, Task])  

Get the estimator from log file.

 **Arguments** :

* `log_file_name` \- A string of the log file name.
* `record_id` \- An integer of the record ID in the file, 0 corresponds to the first trial.
* `task` \- A string of the task type, 'binary', 'multiclass', 'regression', 'ts_forecast', 'rank', or an instance of the Task class.

 **Returns** :

An estimator object for the given configuration.

#### retrain_from_log​ {#retrainfromlog}

    def retrain_from_log(log_file_name, X_train=None, y_train=None, dataframe=None, label=None, time_budget=np.inf, task: Optional[Union[str, Task]] = None, eval_method=None, split_ratio=None, n_splits=None, split_type=None, groups=None, n_jobs=-1, train_best=True, train_full=False, record_id=-1, auto_augment=None, custom_hp=None, skip_transform=None, preserve_checkpoint=True, fit_kwargs_by_estimator=None, **fit_kwargs, ,)  

Retrain from log file.

This function is intended to retrain the logged configurations. NOTE: In some
rare case, the last config is early stopped to meet time_budget and it's the
best config. But the logged config's ITER_HP (e.g., n_estimators) is not
reduced.

 **Arguments** :

* `log_file_name` \- A string of the log file name.
* `X_train` \- A numpy array or dataframe of training data in shape n*m. For time series forecast tasks, the first column of X_train must be the timestamp column (datetime type). Other columns in the dataframe are assumed to be exogenous variables (categorical or numeric).
* `y_train` \- A numpy array or series of labels in shape n*1.
* `dataframe` \- A dataframe of training data including label column. For time series forecast tasks, dataframe must be specified and should have at least two columns: timestamp and label, where the first column is the timestamp column (datetime type). Other columns in the dataframe are assumed to be exogenous variables (categorical or numeric).
* `label` \- A str of the label column name, e.g., 'label';
* `Note` \- If X_train and y_train are provided, dataframe and label are ignored; If not, dataframe and label must be provided.
* `time_budget` \- A float number of the time budget in seconds.
* `task` \- A string of the task type, e.g., 'classification', 'regression', 'ts_forecast', 'rank', 'seq-classification', 'seq-regression', 'summarization', or an instance of Task class.
* `eval_method` \- A string of resampling strategy, one of ['auto', 'cv', 'holdout'].
* `split_ratio` \- A float of the validation data percentage for holdout.
* `n_splits` \- An integer of the number of folds for cross-validation.
* `split_type` \- str or splitter object, default="auto" | the data split type.
  * A valid splitter object is an instance of a derived class of scikit-learn [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold) and have `split` and `get_n_splits` methods with the same signatures. Set eval_method to "cv" to use the splitter object.
  * Valid str options depend on different tasks. For classification tasks, valid choices are ["auto", 'stratified', 'uniform', 'time', 'group']. "auto" -> stratified. For regression tasks, valid choices are ["auto", 'uniform', 'time']. "auto" -> uniform. For time series forecast tasks, must be "auto" or 'time'. For ranking task, must be "auto" or 'group'.
* `groups` \- None or array-like | Group labels (with matching length to y_train) or groups counts (with sum equal to length of y_train) for training data.
* `n_jobs` \- An integer of the number of threads for training | default=-1. Use all available resources when n_jobs == -1.
* `train_best` \- A boolean of whether to train the best config in the time budget; if false, train the last config in the budget.
* `train_full` \- A boolean of whether to train on the full data. If true, eval_method and sample_size in the log file will be ignored.
* `record_id` \- the ID of the training log record from which the model will be retrained. By default `record_id = -1` which means this will be ignored. `record_id = 0` corresponds to the first trial, and when `record_id >= 0`, `time_budget` will be ignored.
* `auto_augment` \- boolean, default=True | Whether to automatically augment rare classes.
* `custom_hp` \- dict, default=None | The custom search space specified by user Each key is the estimator name, each value is a dict of the custom search space for that estimator. Notice the domain of the custom search space can either be a value or a sample.Domain object.

    custom_hp = {  
        "transformer_ms": {  
            "model_path": {  
                "domain": "albert-base-v2",  
            },  
            "learning_rate": {  
                "domain": tune.choice([1e-4, 1e-5]),  
            }  
        }  
    }  

* `fit_kwargs_by_estimator` \- dict, default=None | The user specified keywords arguments, grouped by estimator name. e.g.,

    fit_kwargs_by_estimator = {  
        "transformer": {  
            "output_dir": "test/data/output/",  
            "fp16": False,  
        }  
    }  

* `**fit_kwargs` \- Other key word arguments to pass to fit() function of the searched learners, such as sample_weight. Below are a few examples of estimator-specific parameters:
* `period` \- int | forecast horizon for all time series forecast tasks.
* `gpu_per_trial` \- float, default = 0 | A float of the number of gpus per trial, only used by TransformersEstimator, XGBoostSklearnEstimator, and TemporalFusionTransformerEstimator.
* `group_ids` \- list of strings of column names identifying a time series, only used by TemporalFusionTransformerEstimator, required for 'ts_forecast_panel' task. `group_ids` is a parameter for TimeSeriesDataSet object from PyTorchForecasting. For other parameters to describe your dataset, refer to [TimeSeriesDataSet PyTorchForecasting](https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.data.timeseries.TimeSeriesDataSet.html). To specify your variables, use `static_categoricals`, `static_reals`, `time_varying_known_categoricals`, `time_varying_known_reals`, `time_varying_unknown_categoricals`, `time_varying_unknown_reals`, `variable_groups`. To provide more information on your data, use `max_encoder_length`, `min_encoder_length`, `lags`.
* `log_dir` \- str, default = "lightning_logs" | Folder into which to log results for tensorboard, only used by TemporalFusionTransformerEstimator.
* `max_epochs` \- int, default = 20 | Maximum number of epochs to run training, only used by TemporalFusionTransformerEstimator.
* `batch_size` \- int, default = 64 | Batch size for training model, only used by TemporalFusionTransformerEstimator.

#### search_space​ {#searchspace}

    @property  
    def search_space() -> dict  

Search space.

Must be called after fit(...) (use max_iter=0 and retrain_final=False to
prevent actual fitting).

 **Returns** :

A dict of the search space.

#### low_cost_partial_config​ {#lowcostpartialconfig}

    @property  
    def low_cost_partial_config() -> dict  

Low cost partial config.

 **Returns** :

A dict. (a) if there is only one estimator in estimator_list, each key is a
hyperparameter name. (b) otherwise, it is a nested dict with 'ml' as the key,
and a list of the low_cost_partial_configs as the value, corresponding to each
learner's low_cost_partial_config; the estimator index as an integer
corresponding to the cheapest learner is appended to the list at the end.

#### cat_hp_cost​ {#cathpcost}

    @property  
    def cat_hp_cost() -> dict  

Categorical hyperparameter cost

 **Returns** :

A dict. (a) if there is only one estimator in estimator_list, each key is a
hyperparameter name. (b) otherwise, it is a nested dict with 'ml' as the key,
and a list of the cat_hp_cost's as the value, corresponding to each learner's
cat_hp_cost; the cost relative to lgbm for each learner (as a list itself) is
appended to the list at the end.

#### points_to_evaluate​ {#pointstoevaluate}

    @property  
    def points_to_evaluate() -> dict  

Initial points to evaluate.

 **Returns** :

A list of dicts. Each dict is the initial point for each learner.

#### resource_attr​ {#resourceattr}

    @property  
    def resource_attr() -> Optional[str]  

Attribute of the resource dimension.

 **Returns** :

A string for the sample size attribute (the resource attribute in AutoML) or
None.

#### min_resource​ {#minresource}

    @property  
    def min_resource() -> Optional[float]  

Attribute for pruning.

 **Returns** :

A float for the minimal sample size or None.

#### max_resource​ {#maxresource}

    @property  
    def max_resource() -> Optional[float]  

Attribute for pruning.

 **Returns** :

A float for the maximal sample size or None.

#### trainable​ {#trainable}

    @property  
    def trainable() -> Callable[[dict], Optional[float]]  

Training function.

 **Returns** :

A function that evaluates each config and returns the loss.

#### metric_constraints​ {#metricconstraints}

    @property  
    def metric_constraints() -> list  

Metric constraints.

 **Returns** :

A list of the metric constraints.

#### fit​ {#fit}

    def fit(X_train=None, y_train=None, dataframe=None, label=None, metric=None, task: Optional[Union[str, Task]] = None, n_jobs=None, log_file_name=None, estimator_list=None, time_budget=None, max_iter=None, sample=None, ensemble=None, eval_method=None, log_type=None, model_history=None, split_ratio=None, n_splits=None, log_training_metric=None, mem_thres=None, pred_time_limit=None, train_time_limit=None, X_val=None, y_val=None, sample_weight_val=None, groups_val=None, groups=None, verbose=None, retrain_full=None, split_type=None, learner_selector=None, hpo_method=None, starting_points=None, seed=None, n_concurrent_trials=None, keep_search_state=None, preserve_checkpoint=True, early_stop=None, force_cancel=None, append_log=None, auto_augment=None, min_sample_size=None, use_ray=None, use_spark=None, free_mem_ratio=0, metric_constraints=None, custom_hp=None, time_col=None, cv_score_agg_func=None, skip_transform=None, mlflow_logging=None, fit_kwargs_by_estimator=None, **fit_kwargs, ,)  

Find a model for a given task.

 **Arguments** :

* `X_train` \- A numpy array or a pandas dataframe of training data in shape (n, m). For time series forecsat tasks, the first column of X_train must be the timestamp column (datetime type). Other columns in the dataframe are assumed to be exogenous variables (categorical or numeric). When using ray, X_train can be a ray.ObjectRef.
* `y_train` \- A numpy array or a pandas series of labels in shape (n, ).
* `dataframe` \- A dataframe of training data including label column. For time series forecast tasks, dataframe must be specified and must have at least two columns, timestamp and label, where the first column is the timestamp column (datetime type). Other columns in the dataframe are assumed to be exogenous variables (categorical or numeric). When using ray, dataframe can be a ray.ObjectRef.
* `label` \- A str of the label column name for, e.g., 'label';
* `Note` \- If X_train and y_train are provided, dataframe and label are ignored; If not, dataframe and label must be provided.
* `metric` \- A string of the metric name or a function, e.g., 'accuracy', 'roc_auc', 'roc_auc_ovr', 'roc_auc_ovo', 'roc_auc_weighted', 'roc_auc_ovo_weighted', 'roc_auc_ovr_weighted', 'f1', 'micro_f1', 'macro_f1', 'log_loss', 'mae', 'mse', 'r2', 'mape'. Default is 'auto'. If passing a customized metric function, the function needs to have the following input arguments:

    def custom_metric(  
        X_test, y_test, estimator, labels,  
        X_train, y_train, weight_test=None, weight_train=None,  
        config=None, groups_test=None, groups_train=None,  
    ):  
        return metric_to_minimize, metrics_to_log  

which returns a float number as the minimization objective, and a dictionary
as the metrics to log. E.g.,

    def custom_metric(  
        X_val, y_val, estimator, labels,  
        X_train, y_train, weight_val=None, weight_train=None,  
        *args,  
    ):  
        from sklearn.metrics import log_loss  
        import time  
      
        start = time.time()  
        y_pred = estimator.predict_proba(X_val)  
        pred_time = (time.time() - start) / len(X_val)  
        val_loss = log_loss(y_val, y_pred, labels=labels, sample_weight=weight_val)  
        y_pred = estimator.predict_proba(X_train)  
        train_loss = log_loss(y_train, y_pred, labels=labels, sample_weight=weight_train)  
        alpha = 0.5  
        return val_loss * (1 + alpha) - alpha * train_loss, {  
            "val_loss": val_loss,  
            "train_loss": train_loss,  
            "pred_time": pred_time,  
        }  

* `task` \- A string of the task type, e.g., 'classification', 'regression', 'ts_forecast_regression', 'ts_forecast_classification', 'rank', 'seq-classification', 'seq-regression', 'summarization', or an instance of Task class
* `n_jobs` \- An integer of the number of threads for training | default=-1. Use all available resources when n_jobs == -1.
* `log_file_name` \- A string of the log file name | default="". To disable logging, set it to be an empty string "".
* `estimator_list` \- A list of strings for estimator names, or 'auto'. e.g., `['lgbm', 'xgboost', 'xgb_limitdepth', 'catboost', 'rf', 'extra_tree']`.
* `time_budget` \- A float number of the time budget in seconds. Use -1 if no time limit.
* `max_iter` \- An integer of the maximal number of iterations.
* `NOTE` \- when both time_budget and max_iter are unspecified, only one model will be trained per estimator.
* `sample` \- A boolean of whether to sample the training data during search.
* `ensemble` \- boolean or dict | default=False. Whether to perform ensemble after search. Can be a dict with keys 'passthrough' and 'final_estimator' to specify the passthrough and final_estimator in the stacker. The dict can also contain 'n_jobs' as the key to specify the number of jobs for the stacker.
* `eval_method` \- A string of resampling strategy, one of ['auto', 'cv', 'holdout'].
* `split_ratio` \- A float of the valiation data percentage for holdout.
* `n_splits` \- An integer of the number of folds for cross - validation.
* `log_type` \- A string of the log type, one of ['better', 'all']. 'better' only logs configs with better loss than previos iters 'all' logs all the tried configs.
* `model_history` \- A boolean of whether to keep the trained best model per estimator. Make sure memory is large enough if setting to True. Default value is False: best_model_for_estimator would return a untrained model for non-best learner.
* `log_training_metric` \- A boolean of whether to log the training metric for each model.
* `mem_thres` \- A float of the memory size constraint in bytes.
* `pred_time_limit` \- A float of the prediction latency constraint in seconds. It refers to the average prediction time per row in validation data.
* `train_time_limit` \- None or a float of the training time constraint in seconds.
* `X_val` \- None or a numpy array or a pandas dataframe of validation data.
* `y_val` \- None or a numpy array or a pandas series of validation labels.
* `sample_weight_val` \- None or a numpy array of the sample weight of validation data of the same shape as y_val.
* `groups_val` \- None or array-like | group labels (with matching length to y_val) or group counts (with sum equal to length of y_val) for validation data. Need to be consistent with groups.
* `groups` \- None or array-like | Group labels (with matching length to y_train) or groups counts (with sum equal to length of y_train) for training data.
* `verbose` \- int, default=3 | Controls the verbosity, higher means more messages.
* `retrain_full` \- bool or str, default=True | whether to retrain the selected model on the full training data when using holdout. True - retrain only after search finishes; False - no retraining; 'budget' \- do best effort to retrain without violating the time budget.
* `split_type` \- str or splitter object, default="auto" | the data split type.
  * A valid splitter object is an instance of a derived class of scikit-learn [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold) and have `split` and `get_n_splits` methods with the same signatures. Set eval_method to "cv" to use the splitter object.
  * Valid str options depend on different tasks. For classification tasks, valid choices are ["auto", 'stratified', 'uniform', 'time', 'group']. "auto" -> stratified. For regression tasks, valid choices are ["auto", 'uniform', 'time']. "auto" -> uniform. For time series forecast tasks, must be "auto" or 'time'. For ranking task, must be "auto" or 'group'.
* `hpo_method` \- str, default="auto" | The hyperparameter optimization method. By default, CFO is used for sequential search and BlendSearch is used for parallel search. No need to set when using flaml's default search space or using a simple customized search space. When set to 'bs', BlendSearch is used. BlendSearch can be tried when the search space is complex, for example, containing multiple disjoint, discontinuous subspaces. When set to 'random', random search is used.
* `starting_points` \- A dictionary or a str to specify the starting hyperparameter config for the estimators | default="data". If str:
  * if "data", use data-dependent defaults;
  * if "data:path" use data-dependent defaults which are stored at path;
  * if "static", use data-independent defaults. If dict, keys are the name of the estimators, and values are the starting hyperparamter configurations for the corresponding estimators. The value can be a single hyperparamter configuration dict or a list of hyperparamter configuration dicts. In the following code example, we get starting_points from the `automl` object and use them in the `new_automl` object. e.g.,

    from flaml import AutoML  
    automl = AutoML()  
    X_train, y_train = load_iris(return_X_y=True)  
    automl.fit(X_train, y_train)  
    starting_points = automl.best_config_per_estimator  

    new_automl = AutoML()  
    new_automl.fit(X_train, y_train, starting_points=starting_points)  

* `seed` \- int or None, default=None | The random seed for hpo.
* `n_concurrent_trials` \- [In preview] int, default=1 | The number of concurrent trials. When n_concurrent_trials > 1, flaml performes [parallel tuning](.docs/Use-Cases/Task-Oriented-AutoML#parallel-tuning) and installation of ray or spark is required: `pip install flaml[ray]` or `pip install flaml[spark]`. Please check [here](https://spark.apache.org/docs/latest/api/python/getting_started/install.html) for more details about installing Spark.
* `keep_search_state` \- boolean, default=False | Whether to keep data needed for model search after fit(). By default the state is deleted for space saving.
* `preserve_checkpoint` \- boolean, default=True | Whether to preserve the saved checkpoint on disk when deleting automl. By default the checkpoint is preserved.
* `early_stop` \- boolean, default=False | Whether to stop early if the search is considered to converge.
* `force_cancel` \- boolean, default=False | Whether to forcely cancel the PySpark job if overtime.
* `append_log` \- boolean, default=False | Whether to directly append the log records to the input log file if it exists.
* `auto_augment` \- boolean, default=True | Whether to automatically augment rare classes.
* `min_sample_size` \- int, default=MIN_SAMPLE_TRAIN | the minimal sample size when sample=True.
* `use_ray` \- boolean or dict. If boolean: default=False | Whether to use ray to run the training in separate processes. This can be used to prevent OOM for large datasets, but will incur more overhead in time. If dict: the dict contains the keywords arguments to be passed to [ray.tune.run](https://docs.ray.io/en/latest/tune/api_docs/execution.html).
* `use_spark` \- boolean, default=False | Whether to use spark to run the training in parallel spark jobs. This can be used to accelerate training on large models and large datasets, but will incur more overhead in time and thus slow down training in some cases.
* `free_mem_ratio` \- float between 0 and 1, default=0. The free memory ratio to keep during training.
* `metric_constraints` \- list, default=[] | The list of metric constraints. Each element in this list is a 3-tuple, which shall be expressed in the following format: the first element of the 3-tuple is the name of the metric, the second element is the inequality sign chosen from ">=" and "<=", and the third element is the constraint value. E.g., `('precision', '>=', 0.9)`. Note that all the metric names in metric_constraints need to be reported via the metrics_to_log dictionary returned by a customized metric function. The customized metric function shall be provided via the `metric` key word argument of the fit() function or the automl constructor. Find examples in this [test](https://github.com/microsoft.tree/main/test/automl/test_constraints.py). If `pred_time_limit` is provided as one of keyword arguments to fit() function or the automl constructor, flaml will automatically (and under the hood) add it as an additional element in the metric_constraints. Essentially 'pred_time_limit' specifies a constraint about the prediction latency constraint in seconds.
* `custom_hp` \- dict, default=None | The custom search space specified by user Each key is the estimator name, each value is a dict of the custom search space for that estimator. Notice the domain of the custom search space can either be a value of a sample.Domain object.

    custom_hp = {  
        "transformer_ms": {  
            "model_path": {  
                "domain": "albert-base-v2",  
            },  
            "learning_rate": {  
                "domain": tune.choice([1e-4, 1e-5]),  
            }  
        }  
    }  

* `time_col` \- for a time series task, name of the column containing the timestamps. If not provided, defaults to the first column of X_train/X_val

* `cv_score_agg_func` \- customized cross-validation scores aggregate function. Default to average metrics across folds. If specificed, this function needs to have the following input arguments:

  * val_loss_folds: list of floats, the loss scores of each fold;
  * log_metrics_folds: list of dicts/floats, the metrics of each fold to log.

This function should return the final aggregate result of all folds. A float
number of the minimization objective, and a dictionary as the metrics to log
or None. E.g.,

    def cv_score_agg_func(val_loss_folds, log_metrics_folds):  
        metric_to_minimize = sum(val_loss_folds)/len(val_loss_folds)  
        metrics_to_log = None  
        for single_fold in log_metrics_folds:  
            if metrics_to_log is None:  
                metrics_to_log = single_fold  
            elif isinstance(metrics_to_log, dict):  
                metrics_to_log = {k: metrics_to_log[k] + v for k, v in single_fold.items()}  
            else:  
                metrics_to_log += single_fold  
        if metrics_to_log:  
            n = len(val_loss_folds)  
            metrics_to_log = (  
                {k: v / n for k, v in metrics_to_log.items()}  
                if isinstance(metrics_to_log, dict)  
                else metrics_to_log / n  
            )  
        return metric_to_minimize, metrics_to_log  

* `skip_transform` \- boolean, default=False | Whether to pre-process data prior to modeling.
* `mlflow_logging` \- boolean, default=None | Whether to log the training results to mlflow. Default value is None, which means the logging decision is made based on AutoML. **init** 's mlflow_logging argument. This requires mlflow to be installed and to have an active mlflow run. FLAML will create nested runs.
* `fit_kwargs_by_estimator` \- dict, default=None | The user specified keywords arguments, grouped by estimator name. For TransformersEstimator, available fit_kwargs can be found from [TrainingArgumentsForAuto](.docs/reference/automl/nlp/huggingface/training_args). e.g.,

    fit_kwargs_by_estimator = {  
        "transformer": {  
            "output_dir": "test/data/output/",  
            "fp16": False,  
        },  
        "tft": {  
            "max_encoder_length": 1,  
            "min_encoder_length": 1,  
            "static_categoricals": [],  
            "static_reals": [],  
            "time_varying_known_categoricals": [],  
            "time_varying_known_reals": [],  
            "time_varying_unknown_categoricals": [],  
            "time_varying_unknown_reals": [],  
            "variable_groups": {},  
            "lags": {},  
        }  
    }  

* `**fit_kwargs` \- Other key word arguments to pass to fit() function of the searched learners, such as sample_weight. Below are a few examples of estimator-specific parameters:
* `period` \- int | forecast horizon for all time series forecast tasks.
* `gpu_per_trial` \- float, default = 0 | A float of the number of gpus per trial, only used by TransformersEstimator, XGBoostSklearnEstimator, and TemporalFusionTransformerEstimator.
* `group_ids` \- list of strings of column names identifying a time series, only used by TemporalFusionTransformerEstimator, required for 'ts_forecast_panel' task. `group_ids` is a parameter for TimeSeriesDataSet object from PyTorchForecasting. For other parameters to describe your dataset, refer to [TimeSeriesDataSet PyTorchForecasting](https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.data.timeseries.TimeSeriesDataSet.html). To specify your variables, use `static_categoricals`, `static_reals`, `time_varying_known_categoricals`, `time_varying_known_reals`, `time_varying_unknown_categoricals`, `time_varying_unknown_reals`, `variable_groups`. To provide more information on your data, use `max_encoder_length`, `min_encoder_length`, `lags`.
* `log_dir` \- str, default = "lightning_logs" | Folder into which to log results for tensorboard, only used by TemporalFusionTransformerEstimator.
* `max_epochs` \- int, default = 20 | Maximum number of epochs to run training, only used by TemporalFusionTransformerEstimator.
* `batch_size` \- int, default = 64 | Batch size for training model, only used by TemporalFusionTransformerEstimator.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/automl/automl.md)

[Previous« ts_model](.docs/reference/automl/time_series/ts_model)

[Nextdata »](.docs/reference/automl/data)

* AutoML Objects

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
    * autogen.agentchat
    * autogen.oai
      * [completion](.docs/reference/autogen/oai/completion)
      * [openai_utils](.docs/reference/autogen/oai/openai_utils)
    * [code_utils](.docs/reference/autogen/code_utils)
    * [math_utils](.docs/reference/autogen/math_utils)
    * [retrieve_utils](.docs/reference/autogen/retrieve_utils)
  * automl
  * default
  * onlineml
  * tune

On this page

# autogen.oai.completion {#autogenoaicompletion}

## Completion Objects​ {#completion-objects}

    class Completion(openai_Completion)  

A class for OpenAI completion API.

It also supports: ChatCompletion, Azure OpenAI API.

#### set_cache​ {#setcache}

    @classmethod  
    def set_cache(cls, seed: Optional[int] = 41, cache_path_root: Optional[str] = ".cache")  

Set cache path.

 **Arguments** :

* `seed` _int, Optional_ \- The integer identifier for the pseudo seed. Results corresponding to different seeds will be cached in different places.
* `cache_path` _str, Optional_ \- The root path for the cache. The complete cache path will be {cache_path}/{seed}.

#### clear_cache​ {#clearcache}

    @classmethod  
    def clear_cache(cls, seed: Optional[int] = None, cache_path_root: Optional[str] = ".cache")  

Clear cache.

 **Arguments** :

* `seed` _int, Optional_ \- The integer identifier for the pseudo seed. If omitted, all caches under cache_path_root will be cleared.
* `cache_path` _str, Optional_ \- The root path for the cache. The complete cache path will be {cache_path}/{seed}.

#### tune​ {#tune}

    @classmethod  
    def tune(cls, data: List[Dict], metric: str, mode: str, eval_func: Callable, log_file_name: Optional[str] = None, inference_budget: Optional[float] = None, optimization_budget: Optional[float] = None, num_samples: Optional[int] = 1, logging_level: Optional[int] = logging.WARNING, **config, ,)  

Tune the parameters for the OpenAI API call.

TODO: support parallel tuning with ray or spark. TODO: support agg_method as
in test

 **Arguments** :

* `data` _list_ \- The list of data points.
* `metric` _str_ \- The metric to optimize.
* `mode` _str_ \- The optimization mode, "min" or "max.
* `eval_func` _Callable_ \- The evaluation function for responses. The function should take a list of responses and a data point as input, and return a dict of metrics. For example,

    def eval_func(responses, **data):  
        solution = data["solution"]  
        success_list = []  
        n = len(responses)  
        for i in range(n):  
            response = responses[i]  
            succeed = is_equiv_chain_of_thought(response, solution)  
            success_list.append(succeed)  
        return {  
            "expected_success": 1 - pow(1 - sum(success_list) / n, n),  
            "success": any(s for s in success_list),  
        }  

* `log_file_name` _str, optional_ \- The log file.
* `inference_budget` _float, optional_ \- The inference budget, dollar per instance.
* `optimization_budget` _float, optional_ \- The optimization budget, dollar in total.
* `num_samples` _int, optional_ \- The number of samples to evaluate. -1 means no hard restriction in the number of trials and the actual number is decided by optimization_budget. Defaults to 1.
* `logging_level` _optional_ \- logging level. Defaults to logging.WARNING.
* `**config` _dict_ \- The search space to update over the default search. For prompt, please provide a string/Callable or a list of strings/Callables.
  * If prompt is provided for chat models, it will be converted to messages under role "user".
  * Do not provide both prompt and messages for chat models, but provide either of them.
  * A string template will be used to generate a prompt for each data instance using `prompt.format(**data)`.
  * A callable template will be used to generate a prompt for each data instance using `prompt(data)`. For stop, please provide a string, a list of strings, or a list of lists of strings. For messages (chat models only), please provide a list of messages (for a single chat prefix) or a list of lists of messages (for multiple choices of chat prefix to choose from). Each message should be a dict with keys "role" and "content". The value of "content" can be a string/Callable template.

 **Returns** :

* `dict` \- The optimized hyperparameter setting.
* `tune.ExperimentAnalysis` \- The tuning results.

#### create​ {#create}

    @classmethod  
    def create(cls, context: Optional[Dict] = None, use_cache: Optional[bool] = True, config_list: Optional[List[Dict]] = None, filter_func: Optional[Callable[[Dict, Dict, Dict], bool]] = None, raise_on_ratelimit_or_timeout: Optional[bool] = True, allow_format_str_template: Optional[bool] = False, **config, ,)  

Make a completion for a given context.

 **Arguments** :

* `context` _Dict, Optional_ \- The context to instantiate the prompt. It needs to contain keys that are used by the prompt template or the filter function. E.g., `prompt="Complete the following sentence: {prefix}, context={"prefix": "Today I feel"}`. The actual prompt will be: "Complete the following sentence: Today I feel". More examples can be found at [templating](https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#templating).
* `use_cache` _bool, Optional_ \- Whether to use cached responses.
* `config_list` _List, Optional_ \- List of configurations for the completion to try. The first one that does not raise an error will be used. Only the differences from the default config need to be provided. E.g.,

    response = oai.Completion.create(  
        config_list=[  
            {  
                "model": "gpt-4",  
                "api_key": os.environ.get("AZURE_OPENAI_API_KEY"),  
                "api_type": "azure",  
                "api_base": os.environ.get("AZURE_OPENAI_API_BASE"),  
                "api_version": "2023-03-15-preview",  
            },  
            {  
                "model": "gpt-3.5-turbo",  
                "api_key": os.environ.get("OPENAI_API_KEY"),  
                "api_type": "open_ai",  
                "api_base": "https://api.openai.com/v1",  
            },  
            {  
                "model": "llama-7B",  
                "api_base": "http://127.0.0.1:8080",  
                "api_type": "open_ai",  
            }  
        ],  
        prompt="Hi",  
    )  

* `filter_func` _Callable, Optional_ \- A function that takes in the context, the config and the response and returns a boolean to indicate whether the response is valid. E.g.,

    def yes_or_no_filter(context, config, response):  
        return context.get("yes_or_no_choice", False) is False or any(  
            text in ["Yes.", "No."] for text in oai.Completion.extract_text(response)  
        )  

* `raise_on_ratelimit_or_timeout` _bool, Optional_ \- Whether to raise RateLimitError or Timeout when all configs fail. When set to False, -1 will be returned when all configs fail.
* `allow_format_str_template` _bool, Optional_ \- Whether to allow format string template in the config.
* `**config` \- Configuration for the openai API call. This is used as parameters for calling openai API. Besides the parameters for the openai API call, it can also contain a seed (int) for the cache. This is useful when implementing "controlled randomness" for the completion. Also, the "prompt" or "messages" parameter can contain a template (str or Callable) which will be instantiated with the context.

 **Returns** :

Responses from OpenAI API, with additional fields.

* `cost`: the total cost. When `config_list` is provided, the response will contain a few more fields:
* `config_id`: the index of the config in the config_list that is used to generate the response.
* `pass_filter`: whether the response passes the filter function. None if no filter is provided.

#### test​ {#test}

    @classmethod  
    def test(cls, data, eval_func=None, use_cache=True, agg_method="avg", return_responses_and_per_instance_result=False, logging_level=logging.WARNING, **config, ,)  

Evaluate the responses created with the config for the OpenAI API call.

 **Arguments** :

* `data` _list_ \- The list of test data points.
* `eval_func` _Callable_ \- The evaluation function for responses per data instance. The function should take a list of responses and a data point as input, and return a dict of metrics. You need to either provide a valid callable eval_func; or do not provide one (set None) but call the test function after calling the tune function in which a eval_func is provided. In the latter case we will use the eval_func provided via tune function. Defaults to None.

    def eval_func(responses, **data):  
        solution = data["solution"]  
        success_list = []  
        n = len(responses)  
        for i in range(n):  
            response = responses[i]  
            succeed = is_equiv_chain_of_thought(response, solution)  
            success_list.append(succeed)  
        return {  
            "expected_success": 1 - pow(1 - sum(success_list) / n, n),  
            "success": any(s for s in success_list),  
        }  

* `use_cache` _bool, Optional_ \- Whether to use cached responses. Defaults to True.
* `agg_method` _str, Callable or a dict of Callable_ \- Result aggregation method (across multiple instances) for each of the metrics. Defaults to 'avg'. An example agg_method in str:

    agg_method = 'median'  

An example agg_method in a Callable:

    agg_method = np.median  

An example agg_method in a dict of Callable:

    agg_method={'median_success': np.median, 'avg_success': np.mean}  

* `return_responses_and_per_instance_result` _bool_ \- Whether to also return responses and per instance results in addition to the aggregated results.
* `logging_level` _optional_ \- logging level. Defaults to logging.WARNING.
* `**config` _dict_ \- parametes passed to the openai api call `create()`.

 **Returns** :

None when no valid eval_func is provided in either test or tune; Otherwise, a
dict of aggregated results, responses and per instance results if
`return_responses_and_per_instance_result` is True; Otherwise, a dict of
aggregated results (responses and per instance results are not returned).

#### cost​ {#cost}

    @classmethod  
    def cost(cls, response: dict)  

Compute the cost of an API call.

 **Arguments** :

* `response` _dict_ \- The response from OpenAI API.

 **Returns** :

The cost in USD. 0 if the model is not supported.

#### extract_text​ {#extracttext}

    @classmethod  
    def extract_text(cls, response: dict) -> List[str]  

Extract the text from a completion or chat response.

 **Arguments** :

* `response` _dict_ \- The response from OpenAI API.

 **Returns** :

A list of text in the responses.

#### extract_text_or_function_call​ {#extracttextorfunctioncall}

    @classmethod  
    def extract_text_or_function_call(cls, response: dict) -> List[str]  

Extract the text or function calls from a completion or chat response.

 **Arguments** :

* `response` _dict_ \- The response from OpenAI API.

 **Returns** :

A list of text or function calls in the responses.

#### logged_history​ {#loggedhistory}

    @classmethod  
    @property  
    def logged_history(cls) -> Dict  

Return the book keeping dictionary.

#### start_logging​ {#startlogging}

    @classmethod  
    def start_logging(cls, history_dict: Optional[Dict] = None, compact: Optional[bool] = True, reset_counter: Optional[bool] = True)  

Start book keeping.

 **Arguments** :

* `history_dict` _Dict_ \- A dictionary for book keeping. If no provided, a new one will be created.
* `compact` _bool_ \- Whether to keep the history dictionary compact. Compact history contains one key per conversation, and the value is a dictionary like:

    {  
        "create_at": [0, 1],  
        "cost": [0.1, 0.2],  
    }  

where "created_at" is the index of API calls indicating the order of all the
calls, and "cost" is the cost of each call. This example shows that the
conversation is based on two API calls. The compact format is useful for
condensing the history of a conversation. If compact is False, the history
dictionary will contain all the API calls: the key is the index of the API
call, and the value is a dictionary like:

    {  
        "request": request_dict,  
        "response": response_dict,  
    }  

where request_dict is the request sent to OpenAI API, and response_dict is the
response. For a conversation containing two API calls, the non-compact history
dictionary will be like:

    {  
        0: {  
            "request": request_dict_0,  
            "response": response_dict_0,  
        },  
        1: {  
            "request": request_dict_1,  
            "response": response_dict_1,  
        },  

The first request's messages plus the response is equal to the second
request's messages. For a conversation with many turns, the non-compact
history dictionary has a quadratic size while the compact history dict has a
linear size.

* `reset_counter` _bool_ \- whether to reset the counter of the number of API calls.

#### stop_logging​ {#stoplogging}

    @classmethod  
    def stop_logging(cls)  

End book keeping.

## ChatCompletion Objects​ {#chatcompletion-objects}

    class ChatCompletion(Completion)  

A class for OpenAI API ChatCompletion.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/autogen/oai/completion.md)

[Previous«
user_proxy_agent](.docs/reference/autogen/agentchat/user_proxy_agent)

[Nextopenai_utils »](.docs/reference/autogen/oai/openai_utils)

* Completion Objects
* ChatCompletion Objects

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
  * automl
    * automl.contrib
    * automl.nlp
      * automl.nlp.huggingface
        * [trainer](.docs/reference/automl/nlp/huggingface/trainer)
        * [training_args](.docs/reference/automl/nlp/huggingface/training_args)
        * [utils](.docs/reference/automl/nlp/huggingface/utils)
      * [utils](.docs/reference/automl/nlp/utils)
    * automl.spark
    * automl.task
    * automl.time_series
    * [automl](.docs/reference/automl/automl)
    * [data](.docs/reference/automl/data)
    * [ml](.docs/reference/automl/ml)
    * [model](.docs/reference/automl/model)
    * [state](.docs/reference/automl/state)
  * default
  * onlineml
  * tune

On this page

# automl.nlp.huggingface.trainer {#automlnlphuggingfacetrainer}

## TrainerForAuto Objects​ {#trainerforauto-objects}

    class TrainerForAuto(Seq2SeqTrainer)  

#### evaluate​ {#evaluate}

    def evaluate(eval_dataset=None, ignore_keys=None, metric_key_prefix="eval")  

Overriding transformers.Trainer.evaluate by saving metrics and checkpoint
path.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/automl/nlp/huggingface/trainer.md)

[Previous« histgb](.docs/reference/automl/contrib/histgb)

[Nexttraining_args
»](.docs/reference/automl/nlp/huggingface/training_args)

* TrainerForAuto Objects

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

Recent posts

* [All About Agent Descriptions](/autogen/blog/2023/12/29/AgentDescriptions)
* [AgentOptimizer - An Agentic Way to Train Your LLM Agent](/autogen/blog/2023/12/23/AgentOptimizer)
* [AutoGen Studio: Interactively Explore Multi-Agent Workflows](/autogen/blog/2023/12/01/AutoGenStudio)
* [Agent AutoBuild - Automatically Building Multi-agent Systems](/autogen/blog/2023/11/26/Agent-AutoBuild)
* [How to Assess Utility of LLM-powered Applications?](/autogen/blog/2023/11/20/AgentEval)
* [AutoGen Meets GPTs](/autogen/blog/2023/11/13/OAI-assistants)
* [EcoAssistant - Using LLM Assistants More Accurately and Affordably](/autogen/blog/2023/11/09/EcoAssistant)
* [Multimodal with GPT-4V and LLaVA](/autogen/blog/2023/11/06/LMM-Agent)
* [AutoGen's Teachable Agents](/autogen/blog/2023/10/26/TeachableAgent)
* [Retrieval-Augmented Generation (RAG) Applications with AutoGen](/autogen/blog/2023/10/18/RetrieveChat)
* [Use AutoGen for Local LLMs](/autogen/blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](/autogen/blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](/autogen/blog/2023/05/18/GPT-adaptive-humaneval)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](/autogen/blog/2023/04/21/LLM-tuning-math)

# 2 posts tagged with "RAG" {#2-posts-tagged-with-rag}

[View All Tags](/autogen/blog/tags)

## [EcoAssistant - Using LLM Assistants More Accurately and {#ecoassistant-using-llm-assistants-more-accurately-and}

Affordably](/autogen/blog/2023/11/09/EcoAssistant)

November 9, 2023 · 5 min read

[![Jieyu Zhang](https://github.com/jieyuz2.png)](https://jieyuz2.github.io/)

[Jieyu Zhang](https://jieyuz2.github.io/)

PhD student at University of Washington

![system](/autogen/assets/images/system-1f6d283e77b49cac460bb999adb6bd5d.png)

 **TL;DR:**

* Introducing the **EcoAssistant** , which is designed to solve user queries more accurately and affordably.
* We show how to let the LLM assistant agent leverage external API to solve user query.
* We show how to reduce the cost of using GPT models via **Assistant Hierarchy**.
* We show how to leverage the idea of Retrieval-augmented Generation (RAG) to improve the success rate via **Solution Demonstration**.

## EcoAssistant​ {#ecoassistant}

In this blog, we introduce the **EcoAssistant** , a system built upon AutoGen
with the goal of solving user queries more accurately and affordably.

### Problem setup​ {#problem-setup}

Recently, users have been using conversational LLMs such as ChatGPT for
various queries. Reports indicate that 23% of ChatGPT user queries are for
knowledge extraction purposes. Many of these queries require knowledge that is
external to the information stored within any pre-trained large language
models (LLMs). These tasks can only be completed by generating code to fetch
necessary information via external APIs that contain the requested
information. In the table below, we show three types of user queries that we
aim to address in this work.

Dataset| API| Example query  
---|---|---  
Places| [Google
Places](<https://developers.google.com/maps/documentation/places/web->
service/overview)| I’m looking for a 24-hour pharmacy in Montreal, can you
find one for me?  
Weather| [Weather API](https://www.weatherapi.com)| What is the current cloud
coverage in Mumbai, India?  
Stock| [Alpha Vantage Stock API](https://www.alphavantage.co/documentation/)|
Can you give me the opening price of Microsoft for the month of January 2023?  
  
### Leveraging external APIs​ {#leveraging-external-apis}

To address these queries, we first build a **two-agent system** based on
AutoGen, where the first agent is a **LLM assistant agent** (`AssistantAgent`
in AutoGen) that is responsible for proposing and refining the code and the
second agent is a **code executor agent** (`UserProxyAgent` in AutoGen) that
would extract the generated code and execute it, forwarding the output back to
the LLM assistant agent. A visualization of the two-agent system is shown
below.

![chat](/autogen/assets/images/chat-a2adea6a92b3cd4059021840c869d7d5.png)

To instruct the assistant agent to leverage external APIs, we only need to add
the API name/key dictionary at the beginning of the initial message. The
template is shown below, where the red part is the information of APIs and
black part is user query.

![template](/autogen/assets/images/template-c610ae53eaa7afa3adaf670fa74b5c10.png)

Importantly, we don't want to reveal our real API key to the assistant agent
for safety concerns. Therefore, we use a **fake API key** to replace the real
API key in the initial message. In particular, we generate a random token
(e.g., `181dbb37`) for each API key and replace the real API key with the
token in the initial message. Then, when the code executor execute the code,
the fake API key would be automatically replaced by the real API key.

### Solution Demonstration​ {#solution-demonstration}

In most practical scenarios, queries from users would appear sequentially over
time. Our **EcoAssistant** leverages past success to help the LLM assistants
address future queries via **Solution Demonstration**. Specifically, whenever
a query is deemed successfully resolved by user feedback, we capture and store
the query and the final generated code snippet. These query-code pairs are
saved in a specialized vector database. When new queries appear,
**EcoAssistant** retrieves the most similar query from the database, which is
then appended with the associated code to the initial prompt for the new
query, serving as a demonstration. The new template of initial message is
shown below, where the blue part corresponds to the solution demonstration.

![template](/autogen/assets/images/template-
demo-5a8cae3df56acdcf73188e401ad739f5.png)

We found that this utilization of past successful query-code pairs improves
the query resolution process with fewer iterations and enhances the system's
performance.

### Assistant Hierarchy​ {#assistant-hierarchy}

LLMs usually have different prices and performance, for example, GPT-3.5-turbo
is much cheaper than GPT-4 but also less accurate. Thus, we propose the
**Assistant Hierarchy** to reduce the cost of using LLMs. The core idea is
that we use the cheaper LLMs first and only use the more expensive LLMs when
necessary. By this way, we are able to reduce the reliance on expensive LLMs
and thus reduce the cost. In particular, given multiple LLMs, we initiate one
assistant agent for each and start the conversation with the most cost-
effective LLM assistant. If the conversation between the current LLM assistant
and the code executor concludes without successfully resolving the query,
**EcoAssistant** would then restart the conversation with the next more
expensive LLM assistant in the hierarchy. We found that this strategy
significantly reduces costs while still effectively addressing queries.

### A Synergistic Effect​ {#a-synergistic-effect}

We found that the **Assistant Hierarchy** and **Solution Demonstration** of
**EcoAssistant** have a synergistic effect. Because the query-code database is
shared by all LLM assistants, even without specialized design, the solution
from more powerful LLM assistant (e.g., GPT-4) could be later retrieved to
guide weaker LLM assistant (e.g., GPT-3.5-turbo). Such a synergistic effect
further improves the performance and reduces the cost of **EcoAssistant**.

### Experimental Results​ {#experimental-results}

We evaluate **EcoAssistant** on three datasets: Places, Weather, and Stock.
When comparing it with a single GPT-4 assistant, we found that
**EcoAssistant** achieves a higher success rate with a lower cost as shown in
the figure below. For more details about the experimental results and other
experiments, please refer to our [paper](https://arxiv.org/abs/2310.03046).

![exp](/autogen/assets/images/results-4c8cfbb728760a85ce2d549fd7798179.png)

## Further reading​ {#further-reading}

Please refer to our [paper](https://arxiv.org/abs/2310.03046) and
[codebase](https://github.com/JieyuZ2/EcoAssistant) for more details about
**EcoAssistant**.

If you find this blog useful, please consider citing:

    @article{zhang2023ecoassistant,  
      title={EcoAssistant: Using LLM Assistant More Affordably and Accurately},  
      author={Zhang, Jieyu and Krishna, Ranjay and Awadallah, Ahmed H and Wang, Chi},  
      journal={arXiv preprint arXiv:2310.03046},  
      year={2023}  
    }  

 **Tags:**

* [LLM](/autogen/blog/tags/llm)
* [RAG](/autogen/blog/tags/rag)
* [cost-effectiveness](/autogen/blog/tags/cost-effectiveness)

[**Read More**](/autogen/blog/2023/11/09/EcoAssistant)

## [Retrieval-Augmented Generation (RAG) Applications with {#retrievalaugmented-generation-rag-applications-with}

AutoGen](/autogen/blog/2023/10/18/RetrieveChat)

October 18, 2023 · 10 min read

[![Li Jiang](https://github.com/thinkall.png)](https://github.com/thinkall)

[Li Jiang](https://github.com/thinkall)

Senior Software Engineer at Microsoft

![RAG Architecture](/autogen/assets/images/retrievechat-
arch-959e180405c99ceb3da88a441c02f45e.png)

 **TL;DR:**

* We introduce **RetrieveUserProxyAgent** and **RetrieveAssistantAgent** , RAG agents of AutoGen that allows retrieval-augmented generation, and its basic usage.
* We showcase customizations of RAG agents, such as customizing the embedding function, the text split function and vector database.
* We also showcase two advanced usage of RAG agents, integrating with group chat and building a Chat application with Gradio.

## Introduction​ {#introduction}

Retrieval augmentation has emerged as a practical and effective approach for
mitigating the intrinsic limitations of LLMs by incorporating external
documents. In this blog post, we introduce RAG agents of AutoGen that allows
retrieval-augmented generation. The system consists of two agents: a
Retrieval-augmented User Proxy agent, called `RetrieveUserProxyAgent`, and a
Retrieval-augmented Assistant agent, called `RetrieveAssistantAgent`, both of
which are extended from built-in agents from AutoGen. The overall architecture
of the RAG agents is shown in the figure above.

To use Retrieval-augmented Chat, one needs to initialize two agents including
Retrieval-augmented User Proxy and Retrieval-augmented Assistant. Initializing
the Retrieval-Augmented User Proxy necessitates specifying a path to the
document collection. Subsequently, the Retrieval-Augmented User Proxy can
download the documents, segment them into chunks of a specific size, compute
embeddings, and store them in a vector database. Once a chat is initiated, the
agents collaboratively engage in code generation or question-answering
adhering to the procedures outlined below:

  1. The Retrieval-Augmented User Proxy retrieves document chunks based on the embedding similarity, and sends them along with the question to the Retrieval-Augmented Assistant.
  2. The Retrieval-Augmented Assistant employs an LLM to generate code or text as answers based on the question and context provided. If the LLM is unable to produce a satisfactory response, it is instructed to reply with “Update Context” to the Retrieval-Augmented User Proxy.
  3. If a response includes code blocks, the Retrieval-Augmented User Proxy executes the code and sends the output as feedback. If there are no code blocks or instructions to update the context, it terminates the conversation. Otherwise, it updates the context and forwards the question along with the new context to the Retrieval-Augmented Assistant. Note that if human input solicitation is enabled, individuals can proactively send any feedback, including Update Context”, to the Retrieval-Augmented Assistant.
  4. If the Retrieval-Augmented Assistant receives “Update Context”, it requests the next most similar chunks of documents as new context from the Retrieval-Augmented User Proxy. Otherwise, it generates new code or text based on the feedback and chat history. If the LLM fails to generate an answer, it replies with “Update Context” again. This process can be repeated several times. The conversation terminates if no more documents are available for the context.

## Basic Usage of RAG Agents​ {#basic-usage-of-rag-agents}

  0. Install dependencies

Please install pyautogen with the [retrievechat] option before using RAG
agents.

    pip install "pyautogen[retrievechat]"  

RetrieveChat can handle various types of documents. By default, it can process
plain text and PDF files, including formats such as 'txt', 'json', 'csv',
'tsv', 'md', 'html', 'htm', 'rtf', 'rst', 'jsonl', 'log', 'xml', 'yaml', 'yml'
and 'pdf'. If you install [unstructured](<https://unstructured->
io.github.io/unstructured/installation/full_installation.html) (`pip install
"unstructured[all-docs]"`), additional document types such as 'docx', 'doc',
'odt', 'pptx', 'ppt', 'xlsx', 'eml', 'msg', 'epub' will also be supported.

You can find a list of all supported document types by using
`autogen.retrieve_utils.TEXT_FORMATS`.

  1. Import Agents

    import autogen  
    from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent  
    from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent  

  2. Create an 'RetrieveAssistantAgent' instance named "assistant" and an 'RetrieveUserProxyAgent' instance named "ragproxyagent"

    assistant = RetrieveAssistantAgent(  
        name="assistant",  
        system_message="You are a helpful assistant.",  
        llm_config=llm_config,  
    )  
      
    ragproxyagent = RetrieveUserProxyAgent(  
        name="ragproxyagent",  
        retrieve_config={  
            "task": "qa",  
            "docs_path": "https://raw.githubusercontent.com/microsoft/autogen/main/README.md",  
        },  
    )  

  3. Initialize Chat and ask a question

    assistant.reset()  
    ragproxyagent.initiate_chat(assistant, problem="What is autogen?")  

Output is like:

    --------------------------------------------------------------------------------  
    assistant (to ragproxyagent):  
      
    AutoGen is a framework that enables the development of large language model (LLM) applications using multiple agents that can converse with each other to solve tasks. The agents are customizable, conversable, and allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools.  
      
    --------------------------------------------------------------------------------  

  4. Create a UserProxyAgent and ask the same question

    assistant.reset()  
    userproxyagent = autogen.UserProxyAgent(name="userproxyagent")  
    userproxyagent.initiate_chat(assistant, message="What is autogen?")  

Output is like:

    --------------------------------------------------------------------------------  
    assistant (to userproxyagent):  
      
    In computer software, autogen is a tool that generates program code automatically, without the need for manual coding. It is commonly used in fields such as software engineering, game development, and web development to speed up the development process and reduce errors. Autogen tools typically use pre-programmed rules, templates, and data to create code for repetitive tasks, such as generating user interfaces, database schemas, and data models. Some popular autogen tools include Visual Studio's Code Generator and Unity's Asset Store.  
      
    --------------------------------------------------------------------------------  

You can see that the output of `UserProxyAgent` is not related to our
`autogen` since the latest info of `autogen` is not in ChatGPT's training
data. The output of `RetrieveUserProxyAgent` is correct as it can perform
retrieval-augmented generation based on the given documentation file.

## Customizing RAG Agents​ {#customizing-rag-agents}

`RetrieveUserProxyAgent` is customizable with `retrieve_config`. There are
several parameters to configure based on different use cases. In this section,
we'll show how to customize embedding function, text split function and vector
database.

### Customizing Embedding Function​ {#customizing-embedding-function}

By default, [Sentence Transformers](https://www.sbert.net) and its pretrained
models will be used to compute embeddings. It's possible that you want to use
OpenAI, Cohere, HuggingFace or other embedding functions.

* OpenAI

    from chromadb.utils import embedding_functions  

    openai_ef = embedding_functions.OpenAIEmbeddingFunction(  
                    api_key="YOUR_API_KEY",  
                    model_name="text-embedding-ada-002"  
                )  

    ragproxyagent = RetrieveUserProxyAgent(  
        name="ragproxyagent",  
        retrieve_config={  
            "task": "qa",  
            "docs_path": "<https://raw.githubusercontent.com/microsoft/autogen/main/README.md>",  
            "embedding_function": openai_ef,  
        },  
    )  

* HuggingFace

    huggingface_ef = embedding_functions.HuggingFaceEmbeddingFunction(  
        api_key="YOUR_API_KEY",  
        model_name="sentence-transformers/all-MiniLM-L6-v2"  
    )  

More examples can be found [here](https://docs.trychroma.com/embeddings).

### Customizing Text Split Function​ {#customizing-text-split-function}

Before we can store the documents into a vector database, we need to split the
texts into chunks. Although we have implemented a flexible text splitter in
autogen, you may still want to use different text splitters. There are also
some existing text split tools which are good to reuse.

For example, you can use all the text splitters in langchain.

    from langchain.text_splitter import RecursiveCharacterTextSplitter  
      
    recur_spliter = RecursiveCharacterTextSplitter(separators=["\n", "\r", "\t"])  
      
    ragproxyagent = RetrieveUserProxyAgent(  
        name="ragproxyagent",  
        retrieve_config={  
            "task": "qa",  
            "docs_path": "https://raw.githubusercontent.com/microsoft/autogen/main/README.md",  
            "custom_text_split_function": recur_spliter.split_text,  
        },  
    )  

### Customizing Vector Database​ {#customizing-vector-database}

We are using chromadb as the default vector database, you can also replace it
with any other vector database by simply overriding the function
`retrieve_docs` of `RetrieveUserProxyAgent`.

For example, you can use Qdrant as below:

    # Creating qdrant client  
    from qdrant_client import QdrantClient  
      
    client = QdrantClient(url="***", api_key="***")  
      
    # Wrapping RetrieveUserProxyAgent  
    from litellm import embedding as test_embedding  
    from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent  
    from qdrant_client.models import SearchRequest, Filter, FieldCondition, MatchText  
      
    class QdrantRetrieveUserProxyAgent(RetrieveUserProxyAgent):  
        def query_vector_db(  
            self,  
            query_texts: List[str],  
            n_results: int = 10,  
            search_string: str = "",  
            **kwargs,  
        ) -> Dict[str, Union[List[str], List[List[str]]]]:  
            # define your own query function here  
            embed_response = test_embedding('text-embedding-ada-002', input=query_texts)  
      
            all_embeddings: List[List[float]] = []  
      
            for item in embed_response['data']:  
                all_embeddings.append(item['embedding'])  
      
            search_queries: List[SearchRequest] = []  
      
            for embedding in all_embeddings:  
                search_queries.append(  
                    SearchRequest(  
                        vector=embedding,  
                        filter=Filter(  
                            must=[  
                                FieldCondition(  
                                    key="page_content",  
                                    match=MatchText(  
                                        text=search_string,  
                                    )  
                                )  
                            ]  
                        ),  
                        limit=n_results,  
                        with_payload=True,  
                    )  
                )  
      
            search_response = client.search_batch(  
                collection_name="{your collection name}",  
                requests=search_queries,  
            )  
      
            return {  
                "ids": [[scored_point.id for scored_point in batch] for batch in search_response],  
                "documents": [[scored_point.payload.get('page_content', '') for scored_point in batch] for batch in search_response],  
                "metadatas": [[scored_point.payload.get('metadata', {}) for scored_point in batch] for batch in search_response]  
            }  
      
        def retrieve_docs(self, problem: str, n_results: int = 20, search_string: str = "", **kwargs):  
            results = self.query_vector_db(  
                query_texts=[problem],  
                n_results=n_results,  
                search_string=search_string,  
                **kwargs,  
            )  
      
            self._results = results  
      
      
    # Use QdrantRetrieveUserProxyAgent  
    qdrantragagent = QdrantRetrieveUserProxyAgent(  
        name="ragproxyagent",  
        human_input_mode="NEVER",  
        max_consecutive_auto_reply=2,  
        retrieve_config={  
            "task": "qa",  
        },  
    )  
      
    qdrantragagent.retrieve_docs("What is Autogen?", n_results=10, search_string="autogen")  

## Advanced Usage of RAG Agents​ {#advanced-usage-of-rag-agents}

### Integrate with other agents in a group chat​ {#integrate-with-other-agents-in-a-group-chat}

To use `RetrieveUserProxyAgent` in a group chat is almost the same as you use
it in a two agents chat. The only thing is that you need to **initialize the
chat with`RetrieveUserProxyAgent`**. The `RetrieveAssistantAgent` is not
necessary in a group chat.

However, you may want to initialize the chat with another agent in some cases.
To leverage the best of `RetrieveUserProxyAgent`, you'll need to call it from
a function.

    llm_config = {  
        "functions": [  
            {  
                "name": "retrieve_content",  
                "description": "retrieve content for code generation and question answering.",  
                "parameters": {  
                    "type": "object",  
                    "properties": {  
                        "message": {  
                            "type": "string",  
                            "description": "Refined message which keeps the original meaning and can be used to retrieve content for code generation and question answering.",  
                        }  
                    },  
                    "required": ["message"],  
                },  
            },  
        ],  
        "config_list": config_list,  
        "timeout": 60,  
        "seed": 42,  
    }  
      
    boss = autogen.UserProxyAgent(  
        name="Boss",  
        is_termination_msg=termination_msg,  
        human_input_mode="TERMINATE",  
        system_message="The boss who ask questions and give tasks.",  
    )  
      
    boss_aid = RetrieveUserProxyAgent(  
        name="Boss_Assistant",  
        is_termination_msg=termination_msg,  
        system_message="Assistant who has extra content retrieval power for solving difficult problems.",  
        human_input_mode="NEVER",  
        max_consecutive_auto_reply=3,  
        retrieve_config={  
            "task": "qa",  
        },  
        code_execution_config=False,  # we don't want to execute code in this case.  
    )  
      
    coder = AssistantAgent(  
        name="Senior_Python_Engineer",  
        is_termination_msg=termination_msg,  
        system_message="You are a senior python engineer. Reply `TERMINATE` in the end when everything is done.",  
        llm_config=llm_config,  
    )  
      
    pm = autogen.AssistantAgent(  
        name="Product_Manager",  
        is_termination_msg=termination_msg,  
        system_message="You are a product manager. Reply `TERMINATE` in the end when everything is done.",  
        llm_config=llm_config,  
    )  
      
    reviewer = autogen.AssistantAgent(  
        name="Code_Reviewer",  
        is_termination_msg=termination_msg,  
        system_message="You are a code reviewer. Reply `TERMINATE` in the end when everything is done.",  
        llm_config=llm_config,  
    )  
      
    def retrieve_content(message, n_results=3):  
            boss_aid.n_results = n_results  # Set the number of results to be retrieved.  
            # Check if we need to update the context.  
            update_context_case1, update_context_case2 = boss_aid._check_update_context(message)  
            if (update_context_case1 or update_context_case2) and boss_aid.update_context:  
                boss_aid.problem = message if not hasattr(boss_aid, "problem") else boss_aid.problem  
                _, ret_msg = boss_aid._generate_retrieve_user_reply(message)  
            else:  
                ret_msg = boss_aid.generate_init_message(message, n_results=n_results)  
            return ret_msg if ret_msg else message  
      
    for agent in [boss, coder, pm, reviewer]:  
        # register functions for all agents.  
        agent.register_function(  
            function_map={  
                "retrieve_content": retrieve_content,  
            }  
        )  
      
    groupchat = autogen.GroupChat(  
        agents=[boss, coder, pm, reviewer], messages=[], max_round=12  
    )  
    manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)  
      
    # Start chatting with the boss as this is the user proxy agent.  
    boss.initiate_chat(  
        manager,  
        message="How to use spark for parallel training in FLAML? Give me sample code.",  
    )  

### Build a Chat application with Gradio​ {#build-a-chat-application-with-gradio}

Now, let's wrap it up and make a Chat application with AutoGen and Gradio.

![RAG ChatBot with AutoGen](/autogen/assets/images/autogen-rag-
ee3b1d222f3e10b1707527cdea69be50.gif)

    # Initialize Agents  
    def initialize_agents(config_list, docs_path=None):  
        ...  
        return assistant, ragproxyagent  
      
    # Initialize Chat  
    def initiate_chat(config_list, problem, queue, n_results=3):  
        ...  
        assistant.reset()  
        try:  
            ragproxyagent.a_initiate_chat(  
                assistant, problem=problem, silent=False, n_results=n_results  
            )  
            messages = ragproxyagent.chat_messages  
            messages = [messages[k] for k in messages.keys()][0]  
            messages = [m["content"] for m in messages if m["role"] == "user"]  
            print("messages: ", messages)  
        except Exception as e:  
            messages = [str(e)]  
        queue.put(messages)  
      
    # Wrap AutoGen part into a function  
    def chatbot_reply(input_text):  
        """Chat with the agent through terminal."""  
        queue = mp.Queue()  
        process = mp.Process(  
            target=initiate_chat,  
            args=(config_list, input_text, queue),  
        )  
        process.start()  
        try:  
            messages = queue.get(timeout=TIMEOUT)  
        except Exception as e:  
            messages = [str(e) if len(str(e)) > 0 else "Invalid Request to OpenAI, please check your API keys."]  
        finally:  
            try:  
                process.terminate()  
            except:  
                pass  
        return messages  
      
    ...  
      
    # Set up UI with Gradio  
    with gr.Blocks() as demo:  
        ...  
        assistant, ragproxyagent = initialize_agents(config_list)  
      
        chatbot = gr.Chatbot(  
            [],  
            elem_id="chatbot",  
            bubble_full_width=False,  
            avatar_images=(None, (os.path.join(os.path.dirname(__file__), "autogen.png"))),  
            # height=600,  
        )  
      
        txt_input = gr.Textbox(  
            scale=4,  
            show_label=False,  
            placeholder="Enter text and press enter",  
            container=False,  
        )  
      
        with gr.Row():  
            txt_model = gr.Dropdown(  
                label="Model",  
                choices=[  
                    "gpt-4",  
                    "gpt-35-turbo",  
                    "gpt-3.5-turbo",  
                ],  
                allow_custom_value=True,  
                value="gpt-35-turbo",  
                container=True,  
            )  
            txt_oai_key = gr.Textbox(  
                label="OpenAI API Key",  
                placeholder="Enter key and press enter",  
                max_lines=1,  
                show_label=True,  
                value=os.environ.get("OPENAI_API_KEY", ""),  
                container=True,  
                type="password",  
            )  
            ...  
      
        clear = gr.ClearButton([txt_input, chatbot])  
      
    ...  
      
    if __name__ == "__main__":  
        demo.launch(share=True)  

The online app and the source code are hosted in
[HuggingFace](https://huggingface.co/spaces/thinkall/autogen-demos). Feel free
to give it a try!

## Read More​ {#read-more}

You can check out more example notebooks for RAG use cases:

* [Automated Code Generation and Question Answering with Retrieval Augmented Agents](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_RetrieveChat.ipynb)
* [Group Chat with Retrieval Augmented Generation (with 5 group member agents and 1 manager agent)](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_groupchat_RAG.ipynb)
* [Automated Code Generation and Question Answering with Qdrant based Retrieval Augmented Agents](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_qdrant_RetrieveChat.ipynb)

 **Tags:**

* [LLM](/autogen/blog/tags/llm)
* [RAG](/autogen/blog/tags/rag)

[**Read More**](/autogen/blog/2023/10/18/RetrieveChat)

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
  * automl
  * default
  * onlineml
  * tune
    * tune.scheduler
    * tune.searcher
      * [blendsearch](.docs/reference/tune/searcher/blendsearch)
      * [cfo_cat](.docs/reference/tune/searcher/cfo_cat)
      * [flow2](.docs/reference/tune/searcher/flow2)
      * [online_searcher](.docs/reference/tune/searcher/online_searcher)
      * [search_thread](.docs/reference/tune/searcher/search_thread)
      * [suggestion](.docs/reference/tune/searcher/suggestion)
      * [variant_generator](.docs/reference/tune/searcher/variant_generator)
    * tune.spark
    * [analysis](.docs/reference/tune/analysis)
    * [sample](.docs/reference/tune/sample)
    * [space](.docs/reference/tune/space)
    * [trial](.docs/reference/tune/trial)
    * [trial_runner](.docs/reference/tune/trial_runner)
    * [tune](.docs/reference/tune/tune)
    * [utils](.docs/reference/tune/utils)

On this page

# tune.searcher.flow2 {#tunesearcherflow2}

## FLOW2 Objects​ {#flow2-objects}

    class FLOW2(Searcher)  

Local search algorithm FLOW2, with adaptive step size.

#### __init__​ {#init}

    def __init__(init_config: dict, metric: Optional[str] = None, mode: Optional[str] = None, space: Optional[dict] = None, resource_attr: Optional[str] = None, min_resource: Optional[float] = None, max_resource: Optional[float] = None, resource_multiple_factor: Optional[float] = None, cost_attr: Optional[str] = "time_total_s", seed: Optional[int] = 20, lexico_objectives=None)  

Constructor.

 **Arguments** :

* `init_config` \- a dictionary of a partial or full initial config, e.g., from a subset of controlled dimensions to the initial low-cost values. E.g., {'epochs': 1}.
* `metric` \- A string of the metric name to optimize for.
* `mode` \- A string in ['min', 'max'] to specify the objective as minimization or maximization.
* `space` \- A dictionary to specify the search space.
* `resource_attr` \- A string to specify the resource dimension and the best performance is assumed to be at the max_resource.
* `min_resource` \- A float of the minimal resource to use for the resource_attr.
* `max_resource` \- A float of the maximal resource to use for the resource_attr.
* `resource_multiple_factor` \- A float of the multiplicative factor used for increasing resource.
* `cost_attr` \- A string of the attribute used for cost.
* `seed` \- An integer of the random seed.
* `lexico_objectives` \- dict, default=None | It specifics information needed to perform multi-objective optimization with lexicographic preferences. When lexico_objectives is not None, the arguments metric, mode will be invalid. This dictionary shall contain the following fields of key-value pairs:
  * "metrics": a list of optimization objectives with the orders reflecting the priorities/preferences of the objectives.
  * "modes" (optional): a list of optimization modes (each mode either "min" or "max") corresponding to the objectives in the metric list. If not provided, we use "min" as the default mode for all the objectives
  * "targets" (optional): a dictionary to specify the optimization targets on the objectives. The keys are the metric names (provided in "metric"), and the values are the numerical target values.
  * "tolerances" (optional): a dictionary to specify the optimality tolerances on objectives. The keys are the metric names (provided in "metrics"), and the values are the absolute/percentage tolerance in the form of numeric/string. E.g.,

        lexico_objectives = {  

* `"metrics"` \- ["error_rate", "pred_time"],
* `"modes"` \- ["min", "min"],
* `"tolerances"` \- {"error_rate": 0.01, "pred_time": 0.0},
* `"targets"` \- {"error_rate": 0.0}, }

        We also support percentage tolerance.  
    E.g.,  

    ```python  
    lexico_objectives = {  

* `"metrics"` \- ["error_rate", "pred_time"],
* `"modes"` \- ["min", "min"],
* `"tolerances"` \- {"error_rate": "5%", "pred_time": "0%"},
* `"targets"` \- {"error_rate": 0.0}, }

#### complete_config​ {#completeconfig}

    def complete_config(partial_config: Dict, lower: Optional[Dict] = None, upper: Optional[Dict] = None) -> Tuple[Dict, Dict]  

Generate a complete config from the partial config input.

Add minimal resource to config if available.

#### normalize​ {#normalize}

    def normalize(config, recursive=False) -> Dict  

normalize each dimension in config to [0,1].

#### denormalize​ {#denormalize}

    def denormalize(config)  

denormalize each dimension in config from [0,1].

#### on_trial_complete​ {#ontrialcomplete}

    def on_trial_complete(trial_id: str, result: Optional[Dict] = None, error: bool = False)  

Compare with incumbent. If better, move, reset num_complete and num_proposed.
If not better and num_complete >= 2*dim, num_allowed += 2.

#### on_trial_result​ {#ontrialresult}

    def on_trial_result(trial_id: str, result: Dict)  

Early update of incumbent.

#### suggest​ {#suggest}

    def suggest(trial_id: str) -> Optional[Dict]  

Suggest a new config, one of the following cases:

  1. same incumbent, increase resource.
  2. same resource, move from the incumbent to a random direction.
  3. same resource, move from the incumbent to the opposite direction.

#### can_suggest​ {#cansuggest}

    @property  
    def can_suggest() -> bool  

Can't suggest if 2*dim configs have been proposed for the incumbent while
fewer are completed.

#### config_signature​ {#configsignature}

    def config_signature(config, space: Dict = None) -> tuple  

Return the signature tuple of a config.

#### converged​ {#converged}

    @property  
    def converged() -> bool  

Whether the local search has converged.

#### reach​ {#reach}

    def reach(other: Searcher) -> bool  

whether the incumbent can reach the incumbent of other.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/tune/searcher/flow2.md)

[Previous« cfo_cat](.docs/reference/tune/searcher/cfo_cat)

[Nextonline_searcher »](.docs/reference/tune/searcher/online_searcher)

* FLOW2 Objects

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

* Reference
  * agentchat
    * agentchat.contrib
      * agentchat.contrib.capabilities
      * [agent_builder](/autogen/docs/reference/agentchat/contrib/agent_builder)
      * [compressible_agent](/autogen/docs/reference/agentchat/contrib/compressible_agent)
      * [gpt_assistant_agent](/autogen/docs/reference/agentchat/contrib/gpt_assistant_agent)
      * [img_utils](/autogen/docs/reference/agentchat/contrib/img_utils)
      * [llava_agent](/autogen/docs/reference/agentchat/contrib/llava_agent)
      * [math_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/math_user_proxy_agent)
      * [multimodal_conversable_agent](/autogen/docs/reference/agentchat/contrib/multimodal_conversable_agent)
      * [qdrant_retrieve_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/qdrant_retrieve_user_proxy_agent)
      * [retrieve_assistant_agent](/autogen/docs/reference/agentchat/contrib/retrieve_assistant_agent)
      * [retrieve_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/retrieve_user_proxy_agent)
      * [text_analyzer_agent](/autogen/docs/reference/agentchat/contrib/text_analyzer_agent)
    * [agent](/autogen/docs/reference/agentchat/agent)
    * [assistant_agent](/autogen/docs/reference/agentchat/assistant_agent)
    * [conversable_agent](/autogen/docs/reference/agentchat/conversable_agent)
    * [groupchat](/autogen/docs/reference/agentchat/groupchat)
    * [user_proxy_agent](/autogen/docs/reference/agentchat/user_proxy_agent)
  * oai
  * [code_utils](/autogen/docs/reference/code_utils)
  * [function_utils](/autogen/docs/reference/function_utils)
  * [math_utils](/autogen/docs/reference/math_utils)
  * [retrieve_utils](/autogen/docs/reference/retrieve_utils)
  * [token_count_utils](/autogen/docs/reference/token_count_utils)

On this page

# agentchat.contrib.text_analyzer_agent {#agentchatcontribtextanalyzeragent}

## TextAnalyzerAgent Objects​ {#textanalyzeragent-objects}

    class TextAnalyzerAgent(ConversableAgent)  

(Experimental) Text Analysis agent, a subclass of ConversableAgent designed to
analyze text as instructed.

#### __init__​ {#init}

    def __init__(name="analyzer",  
                 system_message: Optional[str] = system_message,  
                 human_input_mode: Optional[str] = "NEVER",  
                 llm_config: Optional[Union[Dict, bool]] = None,  
                 **kwargs)  

 **Arguments** :

* `name` _str_ \- name of the agent.
* `system_message` _str_ \- system message for the ChatCompletion inference.
* `human_input_mode` _str_ \- This agent should NEVER prompt the human for input.
* `llm_config` _dict or False_ \- llm inference configuration. Please refer to [OpenAIWrapper.create](/autogen/docs/reference/oai/client#create) for available options. To disable llm-based auto reply, set to False.
* `**kwargs` _dict_ \- other kwargs in [ConversableAgent](/autogen/docs/reference/agentchat/conversable_agent#__init__).

#### analyze_text​ {#analyzetext}

    def analyze_text(text_to_analyze, analysis_instructions)  

Analyzes the given text as instructed, and returns the analysis.

[Edit this
page](https://github.com/microsoft/autogen/edit/main/website/docs/reference/agentchat/contrib/text_analyzer_agent.md)

[Previous«
retrieve_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/retrieve_user_proxy_agent)

[Nextagent »](/autogen/docs/reference/agentchat/agent)

* TextAnalyzerAgent Objects

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

Recent posts

* [All About Agent Descriptions](/autogen/blog/2023/12/29/AgentDescriptions)
* [AgentOptimizer - An Agentic Way to Train Your LLM Agent](/autogen/blog/2023/12/23/AgentOptimizer)
* [AutoGen Studio: Interactively Explore Multi-Agent Workflows](/autogen/blog/2023/12/01/AutoGenStudio)
* [Agent AutoBuild - Automatically Building Multi-agent Systems](/autogen/blog/2023/11/26/Agent-AutoBuild)
* [How to Assess Utility of LLM-powered Applications?](/autogen/blog/2023/11/20/AgentEval)
* [AutoGen Meets GPTs](/autogen/blog/2023/11/13/OAI-assistants)
* [EcoAssistant - Using LLM Assistants More Accurately and Affordably](/autogen/blog/2023/11/09/EcoAssistant)
* [Multimodal with GPT-4V and LLaVA](/autogen/blog/2023/11/06/LMM-Agent)
* [AutoGen's Teachable Agents](/autogen/blog/2023/10/26/TeachableAgent)
* [Retrieval-Augmented Generation (RAG) Applications with AutoGen](/autogen/blog/2023/10/18/RetrieveChat)
* [Use AutoGen for Local LLMs](/autogen/blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](/autogen/blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](/autogen/blog/2023/05/18/GPT-adaptive-humaneval)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](/autogen/blog/2023/04/21/LLM-tuning-math)

## [Use AutoGen for Local LLMs](/autogen/blog/2023/07/14/Local-LLMs) {#use-autogen-for-local-llmsautogenblog20230714localllms}

July 14, 2023 · 3 min read

[![Jiale
Liu](https://github.com/LeoLjl/leoljl.github.io/blob/main/profile.jpg?raw=true)](https://leoljl.github.io)

[Jiale Liu](https://leoljl.github.io)

Undergraduate student at Xidian University

 **TL;DR:** We demonstrate how to use autogen for local LLM application. As an
example, we will initiate an endpoint using [FastChat](<https://github.com/lm->
sys/FastChat) and perform inference on
[ChatGLMv2-6b](https://github.com/THUDM/ChatGLM2-6B).

## Preparations​ {#preparations}

### Clone FastChat​ {#clone-fastchat}

FastChat provides OpenAI-compatible APIs for its supported models, so you can
use FastChat as a local drop-in replacement for OpenAI APIs. However, its code
needs minor modification in order to function properly.

    git clone https://github.com/lm-sys/FastChat.git  
    cd FastChat  

### Download checkpoint​ {#download-checkpoint}

ChatGLM-6B is an open bilingual language model based on General Language Model
(GLM) framework, with 6.2 billion parameters. ChatGLM2-6B is its second-
generation version.

Before downloading from HuggingFace Hub, you need to have Git LFS
[installed](<https://docs.github.com/en/repositories/working-with->
files/managing-large-files/installing-git-large-file-storage).

    git clone https://huggingface.co/THUDM/chatglm2-6b  

## Initiate server​ {#initiate-server}

First, launch the controller

    python -m fastchat.serve.controller  

Then, launch the model worker(s)

    python -m fastchat.serve.model_worker --model-path chatglm2-6b  

Finally, launch the RESTful API server

    python -m fastchat.serve.openai_api_server --host localhost --port 8000  

Normally this will work. However, if you encounter error like
[this](https://github.com/lm-sys/FastChat/issues/1641), commenting out all the
lines containing `finish_reason` in `fastchat/protocol/api_protocol.py` and
`fastchat/protocol/openai_api_protocol.py` will fix the problem. The modified
code looks like:

    class CompletionResponseChoice(BaseModel):  
        index: int  
        text: str  
        logprobs: Optional[int] = None  
        # finish_reason: Optional[Literal["stop", "length"]]  
      
    class CompletionResponseStreamChoice(BaseModel):  
        index: int  
        text: str  
        logprobs: Optional[float] = None  
        # finish_reason: Optional[Literal["stop", "length"]] = None  

## Interact with model using `oai.Completion` (requires openai<1)​ {#interact-with-model-using-oaicompletion-requires-openai1}

Now the models can be directly accessed through openai-python library as well
as `autogen.oai.Completion` and `autogen.oai.ChatCompletion`.

    from autogen import oai  
      
    # create a text completion request  
    response = oai.Completion.create(  
        config_list=[  
            {  
                "model": "chatglm2-6b",  
                "base_url": "http://localhost:8000/v1",  
                "api_type": "open_ai",  
                "api_key": "NULL", # just a placeholder  
            }  
        ],  
        prompt="Hi",  
    )  
    print(response)  
      
    # create a chat completion request  
    response = oai.ChatCompletion.create(  
        config_list=[  
            {  
                "model": "chatglm2-6b",  
                "base_url": "http://localhost:8000/v1",  
                "api_type": "open_ai",  
                "api_key": "NULL",  
            }  
        ],  
        messages=[{"role": "user", "content": "Hi"}]  
    )  
    print(response)  

If you would like to switch to different models, download their checkpoints
and specify model path when launching model worker(s).

## interacting with multiple local LLMs​ {#interacting-with-multiple-local-llms}

If you would like to interact with multiple LLMs on your local machine,
replace the `model_worker` step above with a multi model variant:

    python -m fastchat.serve.multi_model_worker \  
        --model-path lmsys/vicuna-7b-v1.3 \  
        --model-names vicuna-7b-v1.3 \  
        --model-path chatglm2-6b \  
        --model-names chatglm2-6b  

The inference code would be:

    from autogen import oai  
      
    # create a chat completion request  
    response = oai.ChatCompletion.create(  
        config_list=[  
            {  
                "model": "chatglm2-6b",  
                "base_url": "http://localhost:8000/v1",  
                "api_type": "open_ai",  
                "api_key": "NULL",  
            },  
            {  
                "model": "vicuna-7b-v1.3",  
                "base_url": "http://localhost:8000/v1",  
                "api_type": "open_ai",  
                "api_key": "NULL",  
            }  
        ],  
        messages=[{"role": "user", "content": "Hi"}]  
    )  
    print(response)  

## For Further Reading​ {#for-further-reading} {#for-further-reading} {#for-further-reading} {#for-further-reading}

* [Documentation](/autogen/docs/Getting-Started) about `autogen`.
* [Documentation](https://github.com/lm-sys/FastChat) about FastChat.

 **Tags:**

* [LLM](/autogen/blog/tags/llm)

## [MathChat - An Conversational Framework to Solve Math {#mathchat-an-conversational-framework-to-solve-math}

Problems](/autogen/blog/2023/06/28/MathChat)

June 28, 2023 · 8 min read

[![Yiran
Wu](https://github.com/kevin666aa.png)](https://github.com/kevin666aa)

[Yiran Wu](https://github.com/kevin666aa)

PhD student at Pennsylvania State University

![MathChat
WorkFlow](/autogen/assets/images/mathchatflow-926a8ed1975a114ab76c69996942c23a.png)
**TL;DR:**

* **We introduce MathChat, a conversational framework leveraging Large Language Models (LLMs), specifically GPT-4, to solve advanced mathematical problems.**
* **MathChat improves LLM 's performance on challenging math problem-solving, outperforming basic prompting and other strategies by about 6%. The improvement was especially notable in the Algebra category, with a 15% increase in accuracy.**
* **Despite the advancement, GPT-4 still struggles to solve very challenging math problems, even with effective prompting strategies. Further improvements are needed, such as the development of more specific assistant models or the integration of new tools and prompts.**

Recent Large Language Models (LLMs) like GTP-3.5 and GPT-4 have demonstrated
astonishing abilities over previous models on various tasks, such as text
generation, question answering, and code generation. Moreover, these models
can communicate with humans through conversations and remember previous
contexts, making it easier for humans to interact with them. These models play
an increasingly important role in our daily lives assisting people with
different tasks, such as writing emails, summarizing documents, and writing
code.

In this blog post, we probe into the problem-solving capabilities of LLMs.
Specifically, we are interested in their capabilities to solve advanced math
problems, which could be representative of a broader class of problems that
require precise reasoning and also have deterministic solutions.

We introduce MathChat, a conversational framework designed for solving
challenging math problems with LLMs. This framework takes advantage of the
chat-optimized feature of state-of-the-art LLMs, where a user proxy agent and
an LLM assistant work together to tackle math problems. We also test previous
prompting techniques for comparison.

## The MathChat Framework​ {#the-mathchat-framework}

MathChat simulates a conversation between the LLM assistant and a user proxy
agent. As the name indicates, the user proxy agent acts as a proxy for the
user, which is responsible for communicating with the LLM assistant and
continuing the conversation in a desired manner.

The proxy agent first presents a math problem to the LLM assistant, framed by
an initial prompt that guides the assistant in how it should work
collaboratively to solve the problem. We incorporate existing prompting
methods such as CoT (chain-of-thought) and tool-using are incorporated in the
initial prompt. Key components of the prompt include:

* **Tool-using Prompt:** This guides the LLM assistant to use Python code in the correct format to address the problem, allowing the user proxy agent to parse the code and provide the appropriate results.

* **Problem-Solving Strategy Selection Prompt:** The assistant is instructed to choose one of three potential problem-solving strategies, including:

    1. Writing a Python program to solve the problem directly.
    2. Solving the problem directly without using Python, which exercises LLMs' inherent reasoning capabilities.
    3. Solving the problem step by step with Python. This should be used when the first two strategies aren't suitable.

* **Final Answer Encapsulation Prompt:** This part instructs the assistant to put the final answer in _\boxed_ {}.

The prompt design in MathChat is flexible and can easily be refined to include
other tools, such as Wolfram Alpha.

Let's take a look at an example between the **User Proxy Agent** and the **LLM
Assistant** (GPT-4). The conversation focuses on how to solve inequality using
Python. (The conversation is modified for readability.)

Participant| Dialogue  
---|---  
User Proxy Agent| (Initial Prompt: Let's solve a math problem with Python...)
Problem: Find all x that satisfy the inequality (2x+10)(x+3) < (3x+9)(x+8).
Express your answer in interval notation.  
LLM assistant| The key idea to solve this problem is to find the critical
points, which are the points where the inequality is equal. Then, test the
intervals between the critical points to find the interval for which the
inequality holds.  
LLM assistant (continued)| Here's Python code to find the critical points:
`from sympy import symbols, Eq, solve; x = symbols("x"); lhs = (2*x + 10)*(x +
3); rhs = (3*x + 9)*(x + 8); equation = Eq(lhs, rhs); critical_points =
solve(equation, x); critical_points`  
User Proxy Agent| The critical points are: [-14, -3]  
LLM assistant| We found the critical points to be x = -14 and x = -3. Now
we'll test the intervals between these critical points as well as points
before and after the critical points. The intervals are: x < -14, -14 < x <
-3, x > -3.  
LLM assistant (continued)| Here's Python code to test the inequality for each
interval: `from sympy import symbols; x = symbols("x"); inequality = (2*x +
10)*(x + 3) < (3*x + 9)*(x + 8); test_points = [-15, -5, 0]; intervals = [];
for point in test_points: if inequality.subs(x, point):
intervals.append(point); intervals`  
User Proxy Agent| The intervals where the inequality holds true are for test
points: [-15, 0]  
LLM assistant| We have tested the intervals and found that the inequality
holds for: 1. x < -14 (test point: -15) 2. x > -3 (test point: 0). Therefore,
the solution to the inequality (2x+10)(x+3) < (3x+9)(x+8) is x ∈ (-∞, -14) ∪
(-3, ∞)  
  
## Experiment Setup​ {#experiment-setup} {#experiment-setup}

We evaluate the improvement brought by MathChat.

For the experiment, we focus on the level-5 problems from the MATH dataset,
which are composed of high school competition problems. These problems include
the application of theorems and complex equation derivation and are
challenging even for undergraduate students. We evaluate 6 of 7 categories
from the dataset (excluding Geometry): Prealgebra, Algebra, Number Theory,
Counting and Probability, Intermediate Algebra, and Precalculus.

We evaluate GPT-4 and use the default configuration of the OpenAI API. To
access the final performance, we manually compare the final answer with the
correct answer. For the vanilla prompt, Program Synthesis, and MathChat, we
have GPT-4 enclose the final answer in _\boxed_ {}, and we take the return of
the function in PoT as the final answer.

We also evaluate the following methods for comparison:

  1. **Vanilla prompting:** Evaluates GPT-4's direct problem-solving capability. The prompt used is: _" Solve the problem carefully. Put the final answer in \boxed{}"_.

  2. **Program of Thoughts (PoT):** Uses a zero-shot PoT prompt that requests the model to create a _Solver_ function to solve the problem and return the final answer.

  3. **Program Synthesis (PS) prompting:** Like PoT, it prompts the model to write a program to solve the problem. The prompt used is: _" Write a program that answers the following question: {Problem}"_.

## Experiment Results​ {#experiment-results} {#experiment-results} {#experiment-results}

The accuracy on all the problems with difficulty level-5 from different
categories of the MATH dataset with different methods is shown below:

![Result](/autogen/assets/images/result-514251f4c387d5b6e11656e1fa33de08.png)

We found that compared to basic prompting, which demonstrates the innate
capabilities of GPT-4, utilizing Python within the context of PoT or PS
strategy improved the overall accuracy by about 10%. This increase was mostly
seen in categories involving more numerical manipulations, such as Counting &
Probability and Number Theory, and in more complex categories like
Intermediate Algebra and Precalculus.

For categories like Algebra and Prealgebra, PoT and PS showed little
improvement, and in some instances, even led to a decrease in accuracy.
However, MathChat was able to enhance total accuracy by around 6% compared to
PoT and PS, showing competitive performance across all categories. Remarkably,
MathChat improved accuracy in the Algebra category by about 15% over other
methods. Note that categories like Intermediate Algebra and Precalculus
remained challenging for all methods, with only about 20% of problems solved
accurately.

The code for experiments can be found at this
[repository](https://github.com/kevin666aa.tree/gpt_math_solver/flaml/autogen/math).
We now provide an implementation of MathChat using the interactive agents in
AutoGen. See this
[notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_MathChat.ipynb)
for example usage.

## Future Directions​ {#future-directions}

Despite MathChat's improvements over previous methods, the results show that
complex math problem is still challenging for recent powerful LLMs, like
GPT-4, even with help from external tools.

Further work can be done to enhance this framework or math problem-solving in
general:

* Although enabling the model to use tools like Python can reduce calculation errors, LLMs are still prone to logic errors. Methods like self-consistency (Sample several solutions and take a major vote on the final answer), or self-verification (use another LLM instance to check whether an answer is correct) might improve the performance.
* Sometimes, whether the LLM can solve the problem depends on the plan it uses. Some plans require less computation and logical reasoning, leaving less room for mistakes.
* MathChat has the potential to be adapted into a copilot system, which could assist users with math problems. This system could allow users to be more involved in the problem-solving process, potentially enhancing learning.

## For Further Reading​ {#for-further-reading} {#for-further-reading} {#for-further-reading} {#for-further-reading}

* [Research paper of MathChat](https://arxiv.org/abs/2306.01337)
* [Documentation about `autogen`](/autogen/docs/Getting-Started)

 _Are you working on applications that involve math problem-solving? Would you
appreciate additional research or support on the application of LLM-based
agents for math problem-solving? Please join
our[Discord](https://discord.gg/pAbnFJrkgZ) server for discussion._

 **Tags:**

* [LLM](/autogen/blog/tags/llm)
* [GPT](/autogen/blog/tags/gpt)
* [research](/autogen/blog/tags/research)

## [Achieve More, Pay Less - Use GPT-4 Smartly](/autogen/blog/2023/05/18/GPT- {#achieve-more-pay-less-use-gpt4-smartlyautogenblog20230518gpt}

adaptive-humaneval)

May 18, 2023 · 8 min read

[![Chi Wang](https://github.com/sonichi.png)](<https://www.linkedin.com/in/chi->
wang-49b15b16/)

[Chi Wang](https://www.linkedin.com/in/chi-wang-49b15b16/)

Principal Researcher at Microsoft Research

![An adaptive way of using GPT-3.5 and GPT-4 outperforms GPT-4 in both coding
success rate and inference
cost](/autogen/assets/images/humaneval-b2718b0b15d76b8fc59a4094e1bd684c.png)

 **TL;DR:**

* **A case study using the HumanEval benchmark shows that an adaptive way of using multiple GPT models can achieve both much higher accuracy (from 68% to 90%) and lower inference cost (by 18%) than using GPT-4 for coding.**

GPT-4 is a big upgrade of foundation model capability, e.g., in code and math,
accompanied by a much higher (more than 10x) price per token to use over
GPT-3.5-Turbo. On a code completion benchmark,
[HumanEval](https://huggingface.co/datasets/openai_humaneval), developed by
OpenAI, GPT-4 can successfully solve 68% tasks while GPT-3.5-Turbo does 46%.
It is possible to increase the success rate of GPT-4 further by generating
multiple responses or making multiple calls. However, that will further
increase the cost, which is already nearly 20 times of using GPT-3.5-Turbo and
with more restricted API call rate limit. Can we achieve more with less?

In this blog post, we will explore a creative, adaptive way of using GPT
models which leads to a big leap forward.

## Observations​ {#observations}

* GPT-3.5-Turbo can already solve 40%-50% tasks. For these tasks if we never use GPT-4, we can save nearly 40-50% cost.
* If we use the saved cost to generate more responses with GPT-4 for the remaining unsolved tasks, it is possible to solve some more of them while keeping the amortized cost down.

The obstacle of leveraging these observations is that we do not know _a
priori_ which tasks can be solved by the cheaper model, which tasks can be
solved by the expensive model, and which tasks can be solved by paying even
more to the expensive model.

To overcome that obstacle, one may want to predict which task requires what
model to solve and how many responses are required for each task. Let's look
at one example code completion task:

    def vowels_count(s):  
        """Write a function vowels_count which takes a string representing  
        a word as input and returns the number of vowels in the string.  
        Vowels in this case are 'a', 'e', 'i', 'o', 'u'. Here, 'y' is also a  
        vowel, but only when it is at the end of the given word.  
      
        Example:  
        >>> vowels_count("abcde")  
        2  
        >>> vowels_count("ACEDY")  
        3  
        """  

Can we predict whether GPT-3.5-Turbo can solve this task or do we need to use
GPT-4? My first guess is that GPT-3.5-Turbo can get it right because the
instruction is fairly straightforward. Yet, it turns out that GPT-3.5-Turbo
does not consistently get it right, if we only give it one chance. It's not
obvious (but an interesting research question!) how to predict the performance
without actually trying.

What else can we do? We notice that: **It 's "easier" to verify a given
solution than finding a correct solution from scratch.**

Some simple example test cases are provided in the docstr. If we already have
a response generated by a model, we can use those test cases to filter wrong
implementations, and either use a more powerful model or generate more
responses, until the result passes the example test cases. Moreover, this step
can be automated by asking GPT-3.5-Turbo to generate assertion statements from
the examples given in the docstr (a simpler task where we can place our bet)
and executing the code.

## Solution​ {#solution}

Combining these observations, we can design a solution with two intuitive
ideas:

* Make use of auto-generated feedback, i.e., code execution results, to filter responses.
* Try inference configurations one by one, until one response can pass the filter.

![Design](/autogen/assets/images/design-88630f6d8d95f1e31ef5ce79e81aab54.png)

This solution works adaptively without knowing or predicting which task fits
which configuration. It simply tries multiple configurations one by one,
starting from the cheapest configuration. Note that one configuration can
generate multiple responses (by setting the inference parameter n larger than
1). And different configurations can use the same model and different
inference parameters such as n and temperature. Only one response is returned
and evaluated per task.

An implementation of this solution is provided in
[autogen](/autogen/docs/reference/code_utils#implement). It uses the following
sequence of configurations:

  1. GPT-3.5-Turbo, n=1, temperature=0
  2. GPT-3.5-Turbo, n=7, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]
  3. GPT-4, n=1, temperature=0
  4. GPT-4, n=2, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]
  5. GPT-4, n=1, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]

## Experiment Results​ {#experiment-results} {#experiment-results} {#experiment-results}

The first figure in this blog post shows the success rate and average
inference cost of the adaptive solution compared with default GPT-4. The
inference cost includes the cost for generating the assertions in our
solution. The generated assertions are not always correct, and programs that
pass/fail the generated assertions are not always right/wrong. Despite of
that, the adaptive solution can increase the success rate (referred to as
pass@1 in the literature) from 68% to 90%, while reducing the cost by 18%.

Here are a few examples of function definitions which are solved by different
configurations in the portfolio.

  1. Solved by GPT-3.5-Turbo, n=1, temperature=0

    def compare(game,guess):  
        """I think we all remember that feeling when the result of some long-awaited  
        event is finally known. The feelings and thoughts you have at that moment are  
        definitely worth noting down and comparing.  
        Your task is to determine if a person correctly guessed the results of a number of matches.  
        You are given two arrays of scores and guesses of equal length, where each index shows a match.  
        Return an array of the same length denoting how far off each guess was. If they have guessed correctly,  
        the value is 0, and if not, the value is the absolute difference between the guess and the score.  
      
      
        example:  
      
        compare([1,2,3,4,5,1],[1,2,3,4,2,-2]) -> [0,0,0,0,3,3]  
        compare([0,5,0,0,0,4],[4,1,1,0,0,-2]) -> [4,4,1,0,0,6]  
        """  

  2. Solved by GPT-3.5-Turbo, n=7, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]: the `vowels_count` function presented earlier.
  3. Solved by GPT-4, n=1, temperature=0:

    def string_xor(a: str, b: str) -> str:  
        """ Input are two strings a and b consisting only of 1s and 0s.  
        Perform binary XOR on these inputs and return result also as a string.  
        >>> string_xor('010', '110')  
        '100'  
        """  

  4. Solved by GPT-4, n=2, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]:

    def is_palindrome(string: str) -> bool:  
        """ Test if given string is a palindrome """  
        return string == string[::-1]  
      
      
    def make_palindrome(string: str) -> str:  
        """ Find the shortest palindrome that begins with a supplied string.  
        Algorithm idea is simple:  
        - Find the longest postfix of supplied string that is a palindrome.  
        - Append to the end of the string reverse of a string prefix that comes before the palindromic suffix.  
        >>> make_palindrome('')  
        ''  
        >>> make_palindrome('cat')  
        'catac'  
        >>> make_palindrome('cata')  
        'catac'  
        """  

  5. Solved by GPT-4, n=1, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]:

    def sort_array(arr):  
        """  
        In this Kata, you have to sort an array of non-negative integers according to  
        number of ones in their binary representation in ascending order.  
        For similar number of ones, sort based on decimal value.  
      
        It must be implemented like this:  
        >>> sort_array([1, 5, 2, 3, 4]) == [1, 2, 3, 4, 5]  
        >>> sort_array([-2, -3, -4, -5, -6]) == [-6, -5, -4, -3, -2]  
        >>> sort_array([1, 0, 2, 3, 4]) [0, 1, 2, 3, 4]  
        """  

The last problem is an example with wrong example test cases in the original
definition. It misleads the adaptive solution because a correct implementation
is regarded as wrong and more trials are made. The last configuration in the
sequence returns the right implementation, even though it does not pass the
auto-generated assertions. This example demonstrates that:

* Our adaptive solution has a certain degree of fault tolerance.
* The success rate and inference cost for the adaptive solution can be further improved if correct example test cases are used.

It is worth noting that the reduced inference cost is the amortized cost over
all the tasks. For each individual task, the cost can be either larger or
smaller than directly using GPT-4. This is the nature of the adaptive
solution: The cost is in general larger for difficult tasks than that for easy
tasks.

An example notebook to run this experiment can be found at:
<https://github.com/microsoft.blob/v1.2.1/notebook/research/autogen_code.ipynb>.
The experiment was run when AutoGen was a subpackage in FLAML.

## Discussion​ {#discussion}

Our solution is quite simple to implement using a generic interface offered in
[`autogen`](/autogen/docs/Use-Cases/enhanced_inference#logic-error), yet the
result is quite encouraging.

While the specific way of generating assertions is application-specific, the
main ideas are general in LLM operations:

* Generate multiple responses to select - especially useful when selecting a good response is relatively easier than generating a good response at one shot.
* Consider multiple configurations to generate responses - especially useful when:
  * Model and other inference parameter choice affect the utility-cost tradeoff; or
  * Different configurations have complementary effect.

A [previous blog post](/autogen/blog/2023/04/21/LLM-tuning-math) provides
evidence that these ideas are relevant in solving math problems too. `autogen`
uses a technique [EcoOptiGen](https://arxiv.org/abs/2303.04673) to support
inference parameter tuning and model selection.

There are many directions of extensions in research and development:

* Generalize the way to provide feedback.
* Automate the process of optimizing the configurations.
* Build adaptive agents for different applications.

 _Do you find this approach applicable to your use case? Do you have any other
challenge to share about LLM applications? Do you like to see more support or
research of LLM optimization or automation? Please join
our[Discord](https://discord.gg/pAbnFJrkgZ) server for discussion._

## For Further Reading​ {#for-further-reading} {#for-further-reading} {#for-further-reading} {#for-further-reading}

* [Documentation](/autogen/docs/Getting-Started) about `autogen` and [Research paper](https://arxiv.org/abs/2303.04673).
* [Blog post](/autogen/blog/2023/04/21/LLM-tuning-math) about a related study for math.

 **Tags:**

* [LLM](/autogen/blog/tags/llm)
* [GPT](/autogen/blog/tags/gpt)
* [research](/autogen/blog/tags/research)

## [Does Model and Inference Parameter Matter in LLM Applications? - A Case {#does-model-and-inference-parameter-matter-in-llm-applications-a-case}

Study for MATH](/autogen/blog/2023/04/21/LLM-tuning-math)

April 21, 2023 · 6 min read

[![Chi Wang](https://github.com/sonichi.png)](<https://www.linkedin.com/in/chi->
wang-49b15b16/)

[Chi Wang](https://www.linkedin.com/in/chi-wang-49b15b16/)

Principal Researcher at Microsoft Research

![level 2
algebra](/autogen/assets/images/level2algebra-659ba95286432d9945fc89e84d606797.png)

 **TL;DR:**

* **Just by tuning the inference parameters like model, number of responses, temperature etc. without changing any model weights or prompt, the baseline accuracy of untuned gpt-4 can be improved by 20% in high school math competition problems.**
* **For easy problems, the tuned gpt-3.5-turbo model vastly outperformed untuned gpt-4 in accuracy (e.g., 90% vs. 70%) and cost efficiency. For hard problems, the tuned gpt-4 is much more accurate (e.g., 35% vs. 20%) and less expensive than untuned gpt-4.**
* **AutoGen can help with model selection, parameter tuning, and cost-saving in LLM applications.**

Large language models (LLMs) are powerful tools that can generate natural
language texts for various applications, such as chatbots, summarization,
translation, and more. GPT-4 is currently the state of the art LLM in the
world. Is model selection irrelevant? What about inference parameters?

In this blog post, we will explore how model and inference parameter matter in
LLM applications, using a case study for [MATH](<https://datasets-benchmarks->
proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-
round2.html), a benchmark for evaluating LLMs on advanced mathematical problem
solving. MATH consists of 12K math competition problems from AMC-10, AMC-12
and AIME. Each problem is accompanied by a step-by-step solution.

We will use AutoGen to automatically find the best model and inference
parameter for LLMs on a given task and dataset given an inference budget,
using a novel low-cost search & pruning strategy. AutoGen currently supports
all the LLMs from OpenAI, such as GPT-3.5 and GPT-4.

We will use AutoGen to perform model selection and inference parameter tuning.
Then we compare the performance and inference cost on solving algebra problems
with the untuned gpt-4. We will also analyze how different difficulty levels
affect the results.

## Experiment Setup​ {#experiment-setup} {#experiment-setup}

We use AutoGen to select between the following models with a target inference
budget $0.02 per instance:

* gpt-3.5-turbo, a relatively cheap model that powers the popular ChatGPT app
* gpt-4, the state of the art LLM that costs more than 10 times of gpt-3.5-turbo

We adapt the models using 20 examples in the train set, using the problem
statement as the input and generating the solution as the output. We use the
following inference parameters:

* temperature: The parameter that controls the randomness of the output text. A higher temperature means more diversity but less coherence. We search for the optimal temperature in the range of [0, 1].
* top_p: The parameter that controls the probability mass of the output tokens. Only tokens with a cumulative probability less than or equal to top-p are considered. A lower top-p means more diversity but less coherence. We search for the optimal top-p in the range of [0, 1].
* max_tokens: The maximum number of tokens that can be generated for each output. We search for the optimal max length in the range of [50, 1000].
* n: The number of responses to generate. We search for the optimal n in the range of [1, 100].
* prompt: We use the template: "{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \boxed{{}}." where {problem} will be replaced by the math problem instance.

In this experiment, when n > 1, we find the answer with highest votes among
all the responses and then select it as the final answer to compare with the
ground truth. For example, if n = 5 and 3 of the responses contain a final
answer 301 while 2 of the responses contain a final answer 159, we choose 301
as the final answer. This can help with resolving potential errors due to
randomness. We use the average accuracy and average inference cost as the
metric to evaluate the performance over a dataset. The inference cost of a
particular instance is measured by the price per 1K tokens and the number of
tokens consumed.

## Experiment Results​ {#experiment-results} {#experiment-results} {#experiment-results}

The first figure in this blog post shows the average accuracy and average
inference cost of each configuration on the level 2 Algebra test set.

Surprisingly, the tuned gpt-3.5-turbo model is selected as a better model and
it vastly outperforms untuned gpt-4 in accuracy (92% vs. 70%) with equal or
2.5 times higher inference budget. The same observation can be obtained on the
level 3 Algebra test set.

![level 3
algebra](/autogen/assets/images/level3algebra-94e87a683ac8832ac7ae6f41f30131a4.png)

However, the selected model changes on level 4 Algebra.

![level 4
algebra](/autogen/assets/images/level4algebra-492beb22490df30d6cc258f061912dcd.png)

This time gpt-4 is selected as the best model. The tuned gpt-4 achieves much
higher accuracy (56% vs. 44%) and lower cost than the untuned gpt-4. On level
5 the result is similar.

![level 5
algebra](/autogen/assets/images/level5algebra-8fba701551334296d08580b4b489fe56.png)

We can see that AutoGen has found different optimal model and inference
parameters for each subset of a particular level, which shows that these
parameters matter in cost-sensitive LLM applications and need to be carefully
tuned or adapted.

An example notebook to run these experiments can be found at:
<https://github.com/microsoft.blob/v1.2.1/notebook/autogen_chatgpt.ipynb>.
The experiments were run when AutoGen was a subpackage in FLAML.

## Analysis and Discussion​ {#analysis-and-discussion}

While gpt-3.5-turbo demonstrates competitive accuracy with voted answers in
relatively easy algebra problems under the same inference budget, gpt-4 is a
better choice for the most difficult problems. In general, through parameter
tuning and model selection, we can identify the opportunity to save the
expensive model for more challenging tasks, and improve the overall
effectiveness of a budget-constrained system.

There are many other alternative ways of solving math problems, which we have
not covered in this blog post. When there are choices beyond the inference
parameters, they can be generally tuned via
[`flaml.tune`](<https://microsoft.github.io.docs/Use-Cases/Tune-User->
Defined-Function).

The need for model selection, parameter tuning and cost saving is not specific
to the math problems. The [Auto-GPT](<https://github.com/Significant->
Gravitas/Auto-GPT) project is an example where high cost can easily prevent a
generic complex task to be accomplished as it needs many LLM inference calls.

## For Further Reading​ {#for-further-reading} {#for-further-reading} {#for-further-reading} {#for-further-reading}

* [Research paper about the tuning technique](https://arxiv.org/abs/2303.04673)
* [Documentation about inference tuning](/autogen/docs/Use-Cases/enhanced_inference)

 _Do you have any experience to share about LLM applications? Do you like to
see more support or research of LLM optimization or automation? Please join
our[Discord](https://discord.gg/pAbnFJrkgZ) server for discussion._

 **Tags:**

* [LLM](/autogen/blog/tags/llm)
* [GPT](/autogen/blog/tags/gpt)
* [research](/autogen/blog/tags/research)

[« Newer Entries](/autogen/blog)

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

Recent posts

* [All About Agent Descriptions](/autogen/blog/2023/12/29/AgentDescriptions)
* [AgentOptimizer - An Agentic Way to Train Your LLM Agent](/autogen/blog/2023/12/23/AgentOptimizer)
* [AutoGen Studio: Interactively Explore Multi-Agent Workflows](/autogen/blog/2023/12/01/AutoGenStudio)
* [Agent AutoBuild - Automatically Building Multi-agent Systems](/autogen/blog/2023/11/26/Agent-AutoBuild)
* [How to Assess Utility of LLM-powered Applications?](/autogen/blog/2023/11/20/AgentEval)
* [AutoGen Meets GPTs](/autogen/blog/2023/11/13/OAI-assistants)
* [EcoAssistant - Using LLM Assistants More Accurately and Affordably](/autogen/blog/2023/11/09/EcoAssistant)
* [Multimodal with GPT-4V and LLaVA](/autogen/blog/2023/11/06/LMM-Agent)
* [AutoGen's Teachable Agents](/autogen/blog/2023/10/26/TeachableAgent)
* [Retrieval-Augmented Generation (RAG) Applications with AutoGen](/autogen/blog/2023/10/18/RetrieveChat)
* [Use AutoGen for Local LLMs](/autogen/blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](/autogen/blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](/autogen/blog/2023/05/18/GPT-adaptive-humaneval)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](/autogen/blog/2023/04/21/LLM-tuning-math)

# Multimodal with GPT-4V and LLaVA {#multimodal-with-gpt4v-and-llava}

November 6, 2023 · 3 min read

[![Beibin Li](https://github.com/beibinli.png)](https://github.com/beibinli)

[Beibin Li](https://github.com/beibinli)

Senior Research Engineer at Microsoft

![LMM
Teaser](/autogen/assets/images/teaser-380bdaa90a1c02ad009520bf289776c9.png)

 **In Brief:**

* Introducing the **Multimodal Conversable Agent** and the **LLaVA Agent** to enhance LMM functionalities.
* Users can input text and images simultaneously using the `<img img_path>` tag to specify image loading.
* Demonstrated through the [GPT-4V notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_gpt-4v.ipynb).
* Demonstrated through the [LLaVA notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_llava.ipynb).

## Introduction​ {#introduction}

Large multimodal models (LMMs) augment large language models (LLMs) with the
ability to process multi-sensory data.

This blog post and the latest AutoGen update concentrate on visual
comprehension. Users can input images, pose questions about them, and receive
text-based responses from these LMMs. We support the `gpt-4-vision-preview`
model from OpenAI and `LLaVA` model from Microsoft now.

Here, we emphasize the **Multimodal Conversable Agent** and the **LLaVA
Agent** due to their growing popularity. GPT-4V represents the forefront in
image comprehension, while LLaVA is an efficient model, fine-tuned from
LLama-2.

## Installation​ {#installation}

Incorporate the `lmm` feature during AutoGen installation:

    pip install "pyautogen[lmm]"  

Subsequently, import the **Multimodal Conversable Agent** or **LLaVA Agent**
from AutoGen:

    from autogen.agentchat.contrib.multimodal_conversable_agent import MultimodalConversableAgent  # for GPT-4V  
    from autogen.agentchat.contrib.llava_agent import LLaVAAgent  # for LLaVA  

## Usage​ {#usage}

A simple syntax has been defined to incorporate both messages and images
within a single string.

Example of an in-context learning prompt:

    prompt = """You are now an image classifier for facial expressions. Here are  
    some examples.  
      
    <img happy.jpg> depicts a happy expression.  
    <img http://some_location.com/sad.jpg> represents a sad expression.  
    <img obama.jpg> portrays a neutral expression.  
      
    Now, identify the facial expression of this individual: <img unknown.png>  
    """  
      
    agent = MultimodalConversableAgent()  
    user = UserProxyAgent()  
    user.initiate_chat(agent, message=prompt)  

The `MultimodalConversableAgent` interprets the input prompt, extracting
images from local or internet sources.

## Advanced Usage​ {#advanced-usage}

Similar to other AutoGen agents, multimodal agents support multi-round
dialogues with other agents, code generation, factual queries, and management
via a GroupChat interface.

For example, the `FigureCreator` in our [GPT-4V
notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_gpt-4v.ipynb)
and [LLaVA
notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_llava.ipynb)
integrates two agents: a coder (an AssistantAgent) and critics (a multimodal
agent). The coder drafts Python code for visualizations, while the critics
provide insights for enhancement. Collaboratively, these agents aim to refine
visual outputs. With `human_input_mode=ALWAYS`, you can also contribute
suggestions for better visualizations.

## Reference​ {#reference}

* [GPT-4V System Card](https://openai.com/research/gpt-4v-system-card)
* [LLaVA GitHub](https://github.com/haotian-liu/LLaVA)

## Future Enhancements​ {#future-enhancements}

For further inquiries or suggestions, please open an issue in the [AutoGen
repository](https://github.com/microsoft/autogen/) or contact me directly at
[beibin.li@microsoft.com](mailto:beibin.li@microsoft.com).

AutoGen will continue to evolve, incorporating more multimodal functionalities
such as DALLE model integration, audio interaction, and video comprehension.
Stay tuned for these exciting developments.

 **Tags:**

* [LMM](/autogen/blog/tags/lmm)
* [multimodal](/autogen/blog/tags/multimodal)

[Newer Post« EcoAssistant - Using LLM Assistants More Accurately and
Affordably](/autogen/blog/2023/11/09/EcoAssistant)

[Older PostAutoGen's Teachable Agents
»](/autogen/blog/2023/10/26/TeachableAgent)

* Introduction
* Installation
* Usage
* Advanced Usage
* Reference
* Future Enhancements

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

On this page

# Ecosystem {#ecosystem}

This page lists libraries that have integrations with Autogen for LLM
applications using multiple agents in alphabetical order. Including your own
integration to this list is highly encouraged. Simply open a pull request with
a few lines of text, see the dropdown below for more information.

## MemGPT + AutoGen​ {#memgpt-autogen}

![MemGPT Example](/autogen/assets/images/ecosystem-memgpt-
db7e37eac957d8d744aeaaef8534b7d9.png)

MemGPT enables LLMs to manage their own memory and overcome limited context
windows. You can use MemGPT to create perpetual chatbots that learn about you
and modify their own personalities over time. You can connect MemGPT to your
own local filesystems and databases, as well as connect MemGPT to your own
tools and APIs. The MemGPT + AutoGen integration allows you to equip any
AutoGen agent with MemGPT capabilities.

* [MemGPT + AutoGen Documentation with Code Examples](https://memgpt.readme.io/docs/autogen)

## Microsoft Fabric + AutoGen​ {#microsoft-fabric-autogen}

![Fabric Example](/autogen/assets/images/ecosystem-fabric-
eee936fd890a030ea63d6cd6f1528afa.png)

[Microsoft Fabric](<https://learn.microsoft.com/en-us/fabric/get->
started/microsoft-fabric-overview) is an all-in-one analytics solution for
enterprises that covers everything from data movement to data science, Real-
Time Analytics, and business intelligence. It offers a comprehensive suite of
services, including data lake, data engineering, and data integration, all in
one place. In this notenook, we give a simple example for using AutoGen in
Microsoft Fabric.

* [Microsoft Fabric + AutoGen Code Examples](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_microsoft_fabric.ipynb)

[Edit this
page](https://github.com/microsoft/autogen/edit/main/website/docs/Ecosystem.md)

* MemGPT + AutoGen
* Microsoft Fabric + AutoGen

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* [Getting Started](.docs/Getting-Started)
* [Installation](.docs/Installation)
* Use Cases
* Examples
  * [AutoGen - Automated Multi Agent Chat](.docs/Examples/AutoGen-AgentChat)
  * [AutoGen - Tune GPT Models](.docs/Examples/AutoGen-OpenAI)
  * [AutoML - Classification](.docs/Examples/AutoML-Classification)
  * [AutoML - NLP](.docs/Examples/AutoML-NLP)
  * [AutoML - Rank](.docs/Examples/AutoML-Rank)
  * [AutoML - Regression](.docs/Examples/AutoML-Regression)
  * [AutoML - Time Series Forecast](.docs/Examples/AutoML-Time series forecast)
  * [AutoML for LightGBM](.docs/Examples/AutoML-for-LightGBM)
  * [AutoML for XGBoost](.docs/Examples/AutoML-for-XGBoost)
  * [Default - Flamlized Estimator](.docs/Examples/Default-Flamlized)
  * [Integrate - AzureML](.docs/Examples/Integrate - AzureML)
  * [Integrate - Scikit-learn Pipeline](.docs/Examples/Integrate - Scikit-learn Pipeline)
  * [Integrate - Spark](.docs/Examples/Integrate - Spark)
  * [Tune - AzureML pipeline](.docs/Examples/Tune-AzureML-pipeline)
  * [Tune - HuggingFace](.docs/Examples/Tune-HuggingFace)
  * [Tune - Lexicographic Objectives](.docs/Examples/Tune-Lexicographic-objectives)
  * [Tune - PyTorch](.docs/Examples/Tune-PyTorch)
* [Contributing](.docs/Contribute)
* [Research](.docs/Research)

On this page

# AutoML - Rank {#automl-rank}

### Prerequisites​ {#prerequisites}

Install the [automl] option.

    pip install "flaml[automl]"  

### A simple learning-to-rank example​ {#a-simple-learningtorank-example}

    from sklearn.datasets import fetch_openml  
    from flaml import AutoML  
      
    X_train, y_train = fetch_openml(name="credit-g", return_X_y=True, as_frame=False)  
    y_train = y_train.cat.codes  
    # not a real learning to rank dataaset  
    groups = [200] * 4 + [100] * 2    # group counts  
    automl = AutoML()  
    automl.fit(  
        X_train, y_train, groups=groups,  
        task='rank', time_budget=10,    # in seconds  
    )  

#### Sample output​ {#sample-output}

    [flaml.automl: 11-15 07:14:30] {1485} INFO - Data split method: group  
    [flaml.automl: 11-15 07:14:30] {1489} INFO - Evaluation method: holdout  
    [flaml.automl: 11-15 07:14:30] {1540} INFO - Minimizing error metric: 1-ndcg  
    [flaml.automl: 11-15 07:14:30] {1577} INFO - List of ML learners in AutoML Run: ['lgbm', 'xgboost']  
    [flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 0, current learner lgbm  
    [flaml.automl: 11-15 07:14:30] {1944} INFO - Estimated sufficient time budget=679s. Estimated necessary time budget=1s.  
    [flaml.automl: 11-15 07:14:30] {2029} INFO -  at 0.1s,  estimator lgbm's best error=0.0248,     best estimator lgbm's best error=0.0248  
    [flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 1, current learner lgbm  
    [flaml.automl: 11-15 07:14:30] {2029} INFO -  at 0.1s,  estimator lgbm's best error=0.0248,     best estimator lgbm's best error=0.0248  
    [flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 2, current learner lgbm  
    [flaml.automl: 11-15 07:14:30] {2029} INFO -  at 0.2s,  estimator lgbm's best error=0.0248,     best estimator lgbm's best error=0.0248  
    [flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 3, current learner lgbm  
    [flaml.automl: 11-15 07:14:30] {2029} INFO -  at 0.2s,  estimator lgbm's best error=0.0248,     best estimator lgbm's best error=0.0248  
    [flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 4, current learner xgboost  
    [flaml.automl: 11-15 07:14:30] {2029} INFO -  at 0.2s,  estimator xgboost's best error=0.0315,  best estimator lgbm's best error=0.0248  
    [flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 5, current learner xgboost  
    [flaml.automl: 11-15 07:14:30] {2029} INFO -  at 0.2s,  estimator xgboost's best error=0.0315,  best estimator lgbm's best error=0.0248  
    [flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 6, current learner lgbm  
    [flaml.automl: 11-15 07:14:30] {2029} INFO -  at 0.3s,  estimator lgbm's best error=0.0248,     best estimator lgbm's best error=0.0248  
    [flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 7, current learner lgbm  
    [flaml.automl: 11-15 07:14:30] {2029} INFO -  at 0.3s,  estimator lgbm's best error=0.0248,     best estimator lgbm's best error=0.0248  
    [flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 8, current learner xgboost  
    [flaml.automl: 11-15 07:14:30] {2029} INFO -  at 0.4s,  estimator xgboost's best error=0.0315,  best estimator lgbm's best error=0.0248  
    [flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 9, current learner xgboost  
    [flaml.automl: 11-15 07:14:30] {2029} INFO -  at 0.4s,  estimator xgboost's best error=0.0315,  best estimator lgbm's best error=0.0248  
    [flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 10, current learner xgboost  
    [flaml.automl: 11-15 07:14:30] {2029} INFO -  at 0.4s,  estimator xgboost's best error=0.0233,  best estimator xgboost's best error=0.0233  
    [flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 11, current learner xgboost  
    [flaml.automl: 11-15 07:14:30] {2029} INFO -  at 0.4s,  estimator xgboost's best error=0.0233,  best estimator xgboost's best error=0.0233  
    [flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 12, current learner xgboost  
    [flaml.automl: 11-15 07:14:30] {2029} INFO -  at 0.4s,  estimator xgboost's best error=0.0233,  best estimator xgboost's best error=0.0233  
    [flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 13, current learner xgboost  
    [flaml.automl: 11-15 07:14:30] {2029} INFO -  at 0.4s,  estimator xgboost's best error=0.0233,  best estimator xgboost's best error=0.0233  
    [flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 14, current learner lgbm  
    [flaml.automl: 11-15 07:14:30] {2029} INFO -  at 0.5s,  estimator lgbm's best error=0.0225,     best estimator lgbm's best error=0.0225  
    [flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 15, current learner xgboost  
    [flaml.automl: 11-15 07:14:30] {2029} INFO -  at 0.5s,  estimator xgboost's best error=0.0233,  best estimator lgbm's best error=0.0225  
    [flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 16, current learner lgbm  
    [flaml.automl: 11-15 07:14:30] {2029} INFO -  at 0.5s,  estimator lgbm's best error=0.0225,     best estimator lgbm's best error=0.0225  
    [flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 17, current learner lgbm  
    [flaml.automl: 11-15 07:14:31] {2029} INFO -  at 0.5s,  estimator lgbm's best error=0.0225,     best estimator lgbm's best error=0.0225  
    [flaml.automl: 11-15 07:14:31] {1826} INFO - iteration 18, current learner lgbm  
    [flaml.automl: 11-15 07:14:31] {2029} INFO -  at 0.6s,  estimator lgbm's best error=0.0225,     best estimator lgbm's best error=0.0225  
    [flaml.automl: 11-15 07:14:31] {1826} INFO - iteration 19, current learner lgbm  
    [flaml.automl: 11-15 07:14:31] {2029} INFO -  at 0.6s,  estimator lgbm's best error=0.0201,     best estimator lgbm's best error=0.0201  
    [flaml.automl: 11-15 07:14:31] {1826} INFO - iteration 20, current learner lgbm  
    [flaml.automl: 11-15 07:14:31] {2029} INFO -  at 0.6s,  estimator lgbm's best error=0.0201,     best estimator lgbm's best error=0.0201  
    [flaml.automl: 11-15 07:14:31] {1826} INFO - iteration 21, current learner lgbm  
    [flaml.automl: 11-15 07:14:31] {2029} INFO -  at 0.7s,  estimator lgbm's best error=0.0201,     best estimator lgbm's best error=0.0201  
    [flaml.automl: 11-15 07:14:31] {1826} INFO - iteration 22, current learner lgbm  
    [flaml.automl: 11-15 07:14:31] {2029} INFO -  at 0.7s,  estimator lgbm's best error=0.0201,     best estimator lgbm's best error=0.0201  
    [flaml.automl: 11-15 07:14:31] {1826} INFO - iteration 23, current learner lgbm  
    [flaml.automl: 11-15 07:14:31] {2029} INFO -  at 0.8s,  estimator lgbm's best error=0.0201,     best estimator lgbm's best error=0.0201  
    [flaml.automl: 11-15 07:14:31] {1826} INFO - iteration 24, current learner lgbm  
    [flaml.automl: 11-15 07:14:31] {2029} INFO -  at 0.8s,  estimator lgbm's best error=0.0201,     best estimator lgbm's best error=0.0201  
    [flaml.automl: 11-15 07:14:31] {1826} INFO - iteration 25, current learner lgbm  
    [flaml.automl: 11-15 07:14:31] {2029} INFO -  at 0.8s,  estimator lgbm's best error=0.0201,     best estimator lgbm's best error=0.0201  
    [flaml.automl: 11-15 07:14:31] {1826} INFO - iteration 26, current learner lgbm  
    [flaml.automl: 11-15 07:14:31] {2029} INFO -  at 0.9s,  estimator lgbm's best error=0.0197,     best estimator lgbm's best error=0.0197  
    [flaml.automl: 11-15 07:14:31] {1826} INFO - iteration 27, current learner lgbm  
    [flaml.automl: 11-15 07:14:31] {2029} INFO -  at 0.9s,  estimator lgbm's best error=0.0197,     best estimator lgbm's best error=0.0197  
    [flaml.automl: 11-15 07:14:31] {1826} INFO - iteration 28, current learner lgbm  
    [flaml.automl: 11-15 07:14:31] {2029} INFO -  at 1.0s,  estimator lgbm's best error=0.0197,     best estimator lgbm's best error=0.0197  
    [flaml.automl: 11-15 07:14:31] {1826} INFO - iteration 29, current learner lgbm  
    [flaml.automl: 11-15 07:14:31] {2029} INFO -  at 1.0s,  estimator lgbm's best error=0.0197,     best estimator lgbm's best error=0.0197  
    [flaml.automl: 11-15 07:14:31] {2242} INFO - retrain lgbm for 0.0s  
    [flaml.automl: 11-15 07:14:31] {2247} INFO - retrained model: LGBMRanker(colsample_bytree=0.9852774042640857,  
               learning_rate=0.034918421933217675, max_bin=1023,  
               min_child_samples=22, n_estimators=6, num_leaves=23,  
               reg_alpha=0.0009765625, reg_lambda=21.505295697527654, verbose=-1)  
    [flaml.automl: 11-15 07:14:31] {1608} INFO - fit succeeded  
    [flaml.automl: 11-15 07:14:31] {1610} INFO - Time taken to find the best model: 0.8846545219421387  
    [flaml.automl: 11-15 07:14:31] {1624} WARNING - Time taken to find the best model is 88% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.  

[Edit this
page](<https://github.com/microsoft.edit/main/website/docs/Examples/AutoML->
Rank.md)

[Previous« AutoML - NLP](.docs/Examples/AutoML-NLP)

[NextAutoML - Regression »](.docs/Examples/AutoML-Regression)

* Prerequisites
* A simple learning-to-rank example

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

* Reference
  * agentchat
    * agentchat.contrib
      * agentchat.contrib.capabilities
      * [agent_builder](/autogen/docs/reference/agentchat/contrib/agent_builder)
      * [compressible_agent](/autogen/docs/reference/agentchat/contrib/compressible_agent)
      * [gpt_assistant_agent](/autogen/docs/reference/agentchat/contrib/gpt_assistant_agent)
      * [img_utils](/autogen/docs/reference/agentchat/contrib/img_utils)
      * [llava_agent](/autogen/docs/reference/agentchat/contrib/llava_agent)
      * [math_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/math_user_proxy_agent)
      * [multimodal_conversable_agent](/autogen/docs/reference/agentchat/contrib/multimodal_conversable_agent)
      * [qdrant_retrieve_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/qdrant_retrieve_user_proxy_agent)
      * [retrieve_assistant_agent](/autogen/docs/reference/agentchat/contrib/retrieve_assistant_agent)
      * [retrieve_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/retrieve_user_proxy_agent)
      * [text_analyzer_agent](/autogen/docs/reference/agentchat/contrib/text_analyzer_agent)
    * [agent](/autogen/docs/reference/agentchat/agent)
    * [assistant_agent](/autogen/docs/reference/agentchat/assistant_agent)
    * [conversable_agent](/autogen/docs/reference/agentchat/conversable_agent)
    * [groupchat](/autogen/docs/reference/agentchat/groupchat)
    * [user_proxy_agent](/autogen/docs/reference/agentchat/user_proxy_agent)
  * oai
  * [code_utils](/autogen/docs/reference/code_utils)
  * [function_utils](/autogen/docs/reference/function_utils)
  * [math_utils](/autogen/docs/reference/math_utils)
  * [retrieve_utils](/autogen/docs/reference/retrieve_utils)
  * [token_count_utils](/autogen/docs/reference/token_count_utils)

On this page

# agentchat.contrib.multimodal_conversable_agent {#agentchatcontribmultimodalconversableagent}

## MultimodalConversableAgent Objects​ {#multimodalconversableagent-objects}

    class MultimodalConversableAgent(ConversableAgent)  

#### __init__​ {#init}

    def __init__(name: str,  
                 system_message: Optional[Union[str, List]] = DEFAULT_LMM_SYS_MSG,  
                 is_termination_msg: str = None,  
                 *args,  
                 **kwargs)  

 **Arguments** :

* `name` _str_ \- agent name.
* `system_message` _str_ \- system message for the OpenAIWrapper inference. Please override this attribute if you want to reprogram the agent.
* `**kwargs` _dict_ \- Please refer to other kwargs in [ConversableAgent](/autogen/docs/reference/agentchat/conversable_agent#__init__).

#### update_system_message​ {#updatesystemmessage}

    def update_system_message(system_message: Union[Dict, List, str])  

Update the system message.

 **Arguments** :

* `system_message` _str_ \- system message for the OpenAIWrapper inference.

[Edit this
page](https://github.com/microsoft/autogen/edit/main/website/docs/reference/agentchat/contrib/multimodal_conversable_agent.md)

[Previous«
math_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/math_user_proxy_agent)

[Nextqdrant_retrieve_user_proxy_agent
»](/autogen/docs/reference/agentchat/contrib/qdrant_retrieve_user_proxy_agent)

* MultimodalConversableAgent Objects

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* [Getting Started](.docs/Getting-Started)
* [Installation](.docs/Installation)
* Use Cases
* Examples
  * [AutoGen - Automated Multi Agent Chat](.docs/Examples/AutoGen-AgentChat)
  * [AutoGen - Tune GPT Models](.docs/Examples/AutoGen-OpenAI)
  * [AutoML - Classification](.docs/Examples/AutoML-Classification)
  * [AutoML - NLP](.docs/Examples/AutoML-NLP)
  * [AutoML - Rank](.docs/Examples/AutoML-Rank)
  * [AutoML - Regression](.docs/Examples/AutoML-Regression)
  * [AutoML - Time Series Forecast](.docs/Examples/AutoML-Time series forecast)
  * [AutoML for LightGBM](.docs/Examples/AutoML-for-LightGBM)
  * [AutoML for XGBoost](.docs/Examples/AutoML-for-XGBoost)
  * [Default - Flamlized Estimator](.docs/Examples/Default-Flamlized)
  * [Integrate - AzureML](.docs/Examples/Integrate - AzureML)
  * [Integrate - Scikit-learn Pipeline](.docs/Examples/Integrate - Scikit-learn Pipeline)
  * [Integrate - Spark](.docs/Examples/Integrate - Spark)
  * [Tune - AzureML pipeline](.docs/Examples/Tune-AzureML-pipeline)
  * [Tune - HuggingFace](.docs/Examples/Tune-HuggingFace)
  * [Tune - Lexicographic Objectives](.docs/Examples/Tune-Lexicographic-objectives)
  * [Tune - PyTorch](.docs/Examples/Tune-PyTorch)
* [Contributing](.docs/Contribute)
* [Research](.docs/Research)

On this page

# AutoML - NLP {#automl-nlp}

### Requirements​ {#requirements}

This example requires GPU. Install the [automl,hf] option:

    pip install "flaml[automl,hf]"  

### A simple sequence classification example​ {#a-simple-sequence-classification-example}

    from flaml import AutoML  
    from datasets import load_dataset  
      
    train_dataset = load_dataset("glue", "mrpc", split="train").to_pandas()  
    dev_dataset = load_dataset("glue", "mrpc", split="validation").to_pandas()  
    test_dataset = load_dataset("glue", "mrpc", split="test").to_pandas()  
    custom_sent_keys = ["sentence1", "sentence2"]  
    label_key = "label"  
    X_train, y_train = train_dataset[custom_sent_keys], train_dataset[label_key]  
    X_val, y_val = dev_dataset[custom_sent_keys], dev_dataset[label_key]  
    X_test = test_dataset[custom_sent_keys]  
      
    automl = AutoML()  
    automl_settings = {  
        "time_budget": 100,  
        "task": "seq-classification",  
        "fit_kwargs_by_estimator": {  
            "transformer":  
           {  
               "output_dir": "data/output/"  # if model_path is not set, the default model is facebook/muppet-roberta-base: https://huggingface.co/facebook/muppet-roberta-base  
           }  
        },  # setting the huggingface arguments: output directory  
        "gpu_per_trial": 1,                         # set to 0 if no GPU is available  
    }  
    automl.fit(X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val, **automl_settings)  
    automl.predict(X_test)  

Notice that after you run `automl.fit`, the intermediate checkpoints are saved
under the specified output_dir `data/output`. You can use the following code
to clean these outputs if they consume a large storage space:

    if os.path.exists("data/output/"):  
        shutil.rmtree("data/output/")  

#### Sample output​ {#sample-output} {#sample-output}

    [flaml.automl: 12-06 08:21:39] {1943} INFO - task = seq-classification  
    [flaml.automl: 12-06 08:21:39] {1945} INFO - Data split method: stratified  
    [flaml.automl: 12-06 08:21:39] {1949} INFO - Evaluation method: holdout  
    [flaml.automl: 12-06 08:21:39] {2019} INFO - Minimizing error metric: 1-accuracy  
    [flaml.automl: 12-06 08:21:39] {2071} INFO - List of ML learners in AutoML Run: ['transformer']  
    [flaml.automl: 12-06 08:21:39] {2311} INFO - iteration 0, current learner transformer  
    {'data/output/train_2021-12-06_08-21-53/train_8947b1b2_1_n=1e-06,s=9223372036854775807,e=1e-05,s=-1,s=0.45765,e=32,d=42,o=0.0,y=0.0_2021-12-06_08-21-53/checkpoint-53': 53}  
    [flaml.automl: 12-06 08:22:56] {2424} INFO - Estimated sufficient time budget=766860s. Estimated necessary time budget=767s.  
    [flaml.automl: 12-06 08:22:56] {2499} INFO -  at 76.7s, estimator transformer's best error=0.1740,      best estimator transformer's best error=0.1740  
    [flaml.automl: 12-06 08:22:56] {2606} INFO - selected model: <flaml.nlp.huggingface.trainer.TrainerForAuto object at 0x7f49ea8414f0>  
    [flaml.automl: 12-06 08:22:56] {2100} INFO - fit succeeded  
    [flaml.automl: 12-06 08:22:56] {2101} INFO - Time taken to find the best model: 76.69802761077881  
    [flaml.automl: 12-06 08:22:56] {2112} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.  

### A simple sequence regression example​ {#a-simple-sequence-regression-example}

    from flaml import AutoML  
    from datasets import load_dataset  
      
    train_dataset = (  
        load_dataset("glue", "stsb", split="train").to_pandas()  
    )  
    dev_dataset = (  
        load_dataset("glue", "stsb", split="train").to_pandas()  
    )  
    custom_sent_keys = ["sentence1", "sentence2"]  
    label_key = "label"  
    X_train = train_dataset[custom_sent_keys]  
    y_train = train_dataset[label_key]  
    X_val = dev_dataset[custom_sent_keys]  
    y_val = dev_dataset[label_key]  
      
    automl = AutoML()  
    automl_settings = {  
        "gpu_per_trial": 0,  
        "time_budget": 20,  
        "task": "seq-regression",  
        "metric": "rmse",  
    }  
    automl_settings["fit_kwargs_by_estimator"] = {  # setting the huggingface arguments  
        "transformer": {  
            "model_path": "google/electra-small-discriminator", # if model_path is not set, the default model is facebook/muppet-roberta-base: https://huggingface.co/facebook/muppet-roberta-base  
            "output_dir": "data/output/",                       # setting the output directory  
            "fp16": False,  
        }   # setting whether to use FP16  
    }  
    automl.fit(  
        X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val, **automl_settings  
    )  

#### Sample output​ {#sample-output} {#sample-output}

    [flaml.automl: 12-20 11:47:28] {1965} INFO - task = seq-regression  
    [flaml.automl: 12-20 11:47:28] {1967} INFO - Data split method: uniform  
    [flaml.automl: 12-20 11:47:28] {1971} INFO - Evaluation method: holdout  
    [flaml.automl: 12-20 11:47:28] {2063} INFO - Minimizing error metric: rmse  
    [flaml.automl: 12-20 11:47:28] {2115} INFO - List of ML learners in AutoML Run: ['transformer']  
    [flaml.automl: 12-20 11:47:28] {2355} INFO - iteration 0, current learner transformer  

### A simple summarization example​ {#a-simple-summarization-example}

    from flaml import AutoML  
    from datasets import load_dataset  
      
    train_dataset = (  
        load_dataset("xsum", split="train").to_pandas()  
    )  
    dev_dataset = (  
        load_dataset("xsum", split="validation").to_pandas()  
    )  
    custom_sent_keys = ["document"]  
    label_key = "summary"  
      
    X_train = train_dataset[custom_sent_keys]  
    y_train = train_dataset[label_key]  
      
    X_val = dev_dataset[custom_sent_keys]  
    y_val = dev_dataset[label_key]  
      
    automl = AutoML()  
    automl_settings = {  
        "gpu_per_trial": 1,  
        "time_budget": 20,  
        "task": "summarization",  
        "metric": "rouge1",  
    }  
    automl_settings["fit_kwargs_by_estimator"] = {      # setting the huggingface arguments  
        "transformer": {  
            "model_path": "t5-small",             # if model_path is not set, the default model is t5-small: https://huggingface.co/t5-small  
            "output_dir": "data/output/",         # setting the output directory  
            "fp16": False,  
        } # setting whether to use FP16  
    }  
    automl.fit(  
        X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val, **automl_settings  
    )  

#### Sample Output​ {#sample-output} {#sample-output}

    [flaml.automl: 12-20 11:44:03] {1965} INFO - task = summarization  
    [flaml.automl: 12-20 11:44:03] {1967} INFO - Data split method: uniform  
    [flaml.automl: 12-20 11:44:03] {1971} INFO - Evaluation method: holdout  
    [flaml.automl: 12-20 11:44:03] {2063} INFO - Minimizing error metric: -rouge  
    [flaml.automl: 12-20 11:44:03] {2115} INFO - List of ML learners in AutoML Run: ['transformer']  
    [flaml.automl: 12-20 11:44:03] {2355} INFO - iteration 0, current learner transformer  
    loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /home/xliu127/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985  
    Model config T5Config {  
      "_name_or_path": "t5-small",  
      "architectures": [  
        "T5WithLMHeadModel"  
      ],  
      "d_ff": 2048,  
      "d_kv": 64,  
      "d_model": 512,  
      "decoder_start_token_id": 0,  
      "dropout_rate": 0.1,  
      "eos_token_id": 1,  
      "feed_forward_proj": "relu",  
      "initializer_factor": 1.0,  
      "is_encoder_decoder": true,  
      "layer_norm_epsilon": 1e-06,  
      "model_type": "t5",  
      "n_positions": 512,  
      "num_decoder_layers": 6,  
      "num_heads": 8,  
      "num_layers": 6,  
      "output_past": true,  
      "pad_token_id": 0,  
      "relative_attention_num_buckets": 32,  
      "task_specific_params": {  
        "summarization": {  
          "early_stopping": true,  
          "length_penalty": 2.0,  
          "max_length": 200,  
          "min_length": 30,  
          "no_repeat_ngram_size": 3,  
          "num_beams": 4,  
          "prefix": "summarize: "  
        },  
        "translation_en_to_de": {  
          "early_stopping": true,  
          "max_length": 300,  
          "num_beams": 4,  
          "prefix": "translate English to German: "  
        },  
        "translation_en_to_fr": {  
          "early_stopping": true,  
          "max_length": 300,  
          "num_beams": 4,  
          "prefix": "translate English to French: "  
        },  
        "translation_en_to_ro": {  
          "early_stopping": true,  
          "max_length": 300,  
          "num_beams": 4,  
          "prefix": "translate English to Romanian: "  
        }  
      },  
      "transformers_version": "4.14.1",  
      "use_cache": true,  
      "vocab_size": 32128  
    }  

### A simple token classification example​ {#a-simple-token-classification-example}

There are two ways to define the label for a token classification task. The
first is to define the token labels:

    from flaml import AutoML  
    import pandas as pd  
      
    train_dataset = {  
        "id": ["0", "1"],  
        "ner_tags": [  
            ["B-ORG", "O", "B-MISC", "O", "O", "O", "B-MISC", "O", "O"],  
            ["B-PER", "I-PER"],  
        ],  
        "tokens": [  
            [  
                "EU", "rejects", "German", "call", "to", "boycott", "British", "lamb", ".",  
            ],  
            ["Peter", "Blackburn"],  
        ],  
    }  
    dev_dataset = {  
        "id": ["0"],  
        "ner_tags": [  
            ["O"],  
        ],  
        "tokens": [  
            ["1996-08-22"]  
        ],  
    }  
    test_dataset = {  
        "id": ["0"],  
        "ner_tags": [  
            ["O"],  
        ],  
        "tokens": [  
            ['.']  
        ],  
    }  
    custom_sent_keys = ["tokens"]  
    label_key = "ner_tags"  
      
    train_dataset = pd.DataFrame(train_dataset)  
    dev_dataset = pd.DataFrame(dev_dataset)  
    test_dataset = pd.DataFrame(test_dataset)  
      
    X_train, y_train = train_dataset[custom_sent_keys], train_dataset[label_key]  
    X_val, y_val = dev_dataset[custom_sent_keys], dev_dataset[label_key]  
    X_test = test_dataset[custom_sent_keys]  
      
    automl = AutoML()  
    automl_settings = {  
        "time_budget": 10,  
        "task": "token-classification",  
        "fit_kwargs_by_estimator": {  
            "transformer":  
                {  
                    "output_dir": "data/output/"  
                    # if model_path is not set, the default model is facebook/muppet-roberta-base: https://huggingface.co/facebook/muppet-roberta-base  
                }  
        },  # setting the huggingface arguments: output directory  
        "gpu_per_trial": 1,  # set to 0 if no GPU is available  
        "metric": "seqeval:overall_f1"  
    }  
      
    automl.fit(X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val, **automl_settings)  
    automl.predict(X_test)  

The second is to define the id labels + a token [label
list](https://microsoft.github.io.docs/reference/nlp/huggingface/training_args):

    from flaml import AutoML  
    import pandas as pd  
      
    train_dataset = {  
            "id": ["0", "1"],  
            "ner_tags": [  
                [3, 0, 7, 0, 0, 0, 7, 0, 0],  
                [1, 2],  
            ],  
            "tokens": [  
                [  
                    "EU", "rejects", "German", "call", "to", "boycott", "British", "lamb", ".",  
                ],  
                ["Peter", "Blackburn"],  
            ],  
        }  
    dev_dataset = {  
        "id": ["0"],  
        "ner_tags": [  
            [0],  
        ],  
        "tokens": [  
            ["1996-08-22"]  
        ],  
    }  
    test_dataset = {  
        "id": ["0"],  
        "ner_tags": [  
            [0],  
        ],  
        "tokens": [  
            ['.']  
        ],  
    }  
    custom_sent_keys = ["tokens"]  
    label_key = "ner_tags"  
      
    train_dataset = pd.DataFrame(train_dataset)  
    dev_dataset = pd.DataFrame(dev_dataset)  
    test_dataset = pd.DataFrame(test_dataset)  
      
    X_train, y_train = train_dataset[custom_sent_keys], train_dataset[label_key]  
    X_val, y_val = dev_dataset[custom_sent_keys], dev_dataset[label_key]  
    X_test = test_dataset[custom_sent_keys]  
      
    automl = AutoML()  
    automl_settings = {  
        "time_budget": 10,  
        "task": "token-classification",  
        "fit_kwargs_by_estimator": {  
            "transformer":  
                {  
                    "output_dir": "data/output/",  
                    # if model_path is not set, the default model is facebook/muppet-roberta-base: https://huggingface.co/facebook/muppet-roberta-base  
                    "label_list": [ "O","B-PER", "I-PER", "B-ORG", "I-ORG", "B-LOC", "I-LOC", "B-MISC", "I-MISC" ]  
                }  
        },  # setting the huggingface arguments: output directory  
        "gpu_per_trial": 1,  # set to 0 if no GPU is available  
        "metric": "seqeval:overall_f1"  
    }  
      
    automl.fit(X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val, **automl_settings)  
    automl.predict(X_test)  

#### Sample Output​ {#sample-output} {#sample-output}

    [flaml.automl: 06-30 03:10:02] {2423} INFO - task = token-classification  
    [flaml.automl: 06-30 03:10:02] {2425} INFO - Data split method: stratified  
    [flaml.automl: 06-30 03:10:02] {2428} INFO - Evaluation method: holdout  
    [flaml.automl: 06-30 03:10:02] {2497} INFO - Minimizing error metric: seqeval:overall_f1  
    [flaml.automl: 06-30 03:10:02] {2637} INFO - List of ML learners in AutoML Run: ['transformer']  
    [flaml.automl: 06-30 03:10:02] {2929} INFO - iteration 0, current learner transformer  

For tasks that are not currently supported, use `flaml.tune` for [customized
tuning](.docs/Examples/Tune-HuggingFace).

### Link to Jupyter notebook​ {#link-to-jupyter-notebook}

To run more examples, especially examples using Ray Tune, please go to:

[Link to
notebook](https://github.com/microsoft.blob/main/notebook/automl_nlp.ipynb)
| [Open in
colab](https://colab.research.google.com/github/microsoft.blob/main/notebook/automl_nlp.ipynb)

[Edit this
page](<https://github.com/microsoft.edit/main/website/docs/Examples/AutoML->
NLP.md)

[Previous« AutoML - Classification](.docs/Examples/AutoML-
Classification)

[NextAutoML - Rank »](.docs/Examples/AutoML-Rank)

* Requirements
* A simple sequence classification example
* A simple sequence regression example
* A simple summarization example
* A simple token classification example
* Link to Jupyter notebook

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

Recent posts

* [All About Agent Descriptions](/autogen/blog/2023/12/29/AgentDescriptions)
* [AgentOptimizer - An Agentic Way to Train Your LLM Agent](/autogen/blog/2023/12/23/AgentOptimizer)
* [AutoGen Studio: Interactively Explore Multi-Agent Workflows](/autogen/blog/2023/12/01/AutoGenStudio)
* [Agent AutoBuild - Automatically Building Multi-agent Systems](/autogen/blog/2023/11/26/Agent-AutoBuild)
* [How to Assess Utility of LLM-powered Applications?](/autogen/blog/2023/11/20/AgentEval)
* [AutoGen Meets GPTs](/autogen/blog/2023/11/13/OAI-assistants)
* [EcoAssistant - Using LLM Assistants More Accurately and Affordably](/autogen/blog/2023/11/09/EcoAssistant)
* [Multimodal with GPT-4V and LLaVA](/autogen/blog/2023/11/06/LMM-Agent)
* [AutoGen's Teachable Agents](/autogen/blog/2023/10/26/TeachableAgent)
* [Retrieval-Augmented Generation (RAG) Applications with AutoGen](/autogen/blog/2023/10/18/RetrieveChat)
* [Use AutoGen for Local LLMs](/autogen/blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](/autogen/blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](/autogen/blog/2023/05/18/GPT-adaptive-humaneval)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](/autogen/blog/2023/04/21/LLM-tuning-math)

# One post tagged with "multimodal" {#one-post-tagged-with-multimodal}

[View All Tags](/autogen/blog/tags)

## [Multimodal with GPT-4V and LLaVA](/autogen/blog/2023/11/06/LMM-Agent) {#multimodal-with-gpt4v-and-llavaautogenblog20231106lmmagent}

November 6, 2023 · 3 min read

[![Beibin Li](https://github.com/beibinli.png)](https://github.com/beibinli)

[Beibin Li](https://github.com/beibinli)

Senior Research Engineer at Microsoft

![LMM
Teaser](/autogen/assets/images/teaser-380bdaa90a1c02ad009520bf289776c9.png)

 **In Brief:**

* Introducing the **Multimodal Conversable Agent** and the **LLaVA Agent** to enhance LMM functionalities.
* Users can input text and images simultaneously using the `<img img_path>` tag to specify image loading.
* Demonstrated through the [GPT-4V notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_gpt-4v.ipynb).
* Demonstrated through the [LLaVA notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_llava.ipynb).

## Introduction​ {#introduction}

Large multimodal models (LMMs) augment large language models (LLMs) with the
ability to process multi-sensory data.

This blog post and the latest AutoGen update concentrate on visual
comprehension. Users can input images, pose questions about them, and receive
text-based responses from these LMMs. We support the `gpt-4-vision-preview`
model from OpenAI and `LLaVA` model from Microsoft now.

Here, we emphasize the **Multimodal Conversable Agent** and the **LLaVA
Agent** due to their growing popularity. GPT-4V represents the forefront in
image comprehension, while LLaVA is an efficient model, fine-tuned from
LLama-2.

## Installation​ {#installation}

Incorporate the `lmm` feature during AutoGen installation:

    pip install "pyautogen[lmm]"  

Subsequently, import the **Multimodal Conversable Agent** or **LLaVA Agent**
from AutoGen:

    from autogen.agentchat.contrib.multimodal_conversable_agent import MultimodalConversableAgent  # for GPT-4V  
    from autogen.agentchat.contrib.llava_agent import LLaVAAgent  # for LLaVA  

## Usage​ {#usage}

A simple syntax has been defined to incorporate both messages and images
within a single string.

Example of an in-context learning prompt:

    prompt = """You are now an image classifier for facial expressions. Here are  
    some examples.  
      
    <img happy.jpg> depicts a happy expression.  
    <img http://some_location.com/sad.jpg> represents a sad expression.  
    <img obama.jpg> portrays a neutral expression.  
      
    Now, identify the facial expression of this individual: <img unknown.png>  
    """  
      
    agent = MultimodalConversableAgent()  
    user = UserProxyAgent()  
    user.initiate_chat(agent, message=prompt)  

The `MultimodalConversableAgent` interprets the input prompt, extracting
images from local or internet sources.

## Advanced Usage​ {#advanced-usage}

Similar to other AutoGen agents, multimodal agents support multi-round
dialogues with other agents, code generation, factual queries, and management
via a GroupChat interface.

For example, the `FigureCreator` in our [GPT-4V
notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_gpt-4v.ipynb)
and [LLaVA
notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_llava.ipynb)
integrates two agents: a coder (an AssistantAgent) and critics (a multimodal
agent). The coder drafts Python code for visualizations, while the critics
provide insights for enhancement. Collaboratively, these agents aim to refine
visual outputs. With `human_input_mode=ALWAYS`, you can also contribute
suggestions for better visualizations.

## Reference​ {#reference}

* [GPT-4V System Card](https://openai.com/research/gpt-4v-system-card)
* [LLaVA GitHub](https://github.com/haotian-liu/LLaVA)

## Future Enhancements​ {#future-enhancements}

For further inquiries or suggestions, please open an issue in the [AutoGen
repository](https://github.com/microsoft/autogen/) or contact me directly at
[beibin.li@microsoft.com](mailto:beibin.li@microsoft.com).

AutoGen will continue to evolve, incorporating more multimodal functionalities
such as DALLE model integration, audio interaction, and video comprehension.
Stay tuned for these exciting developments.

 **Tags:**

* [LMM](/autogen/blog/tags/lmm)
* [multimodal](/autogen/blog/tags/multimodal)

[**Read More**](/autogen/blog/2023/11/06/LMM-Agent)

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* [Getting Started](.docs/Getting-Started)
* [Installation](.docs/Installation)
* Use Cases
* Examples
  * [AutoGen - Automated Multi Agent Chat](.docs/Examples/AutoGen-AgentChat)
  * [AutoGen - Tune GPT Models](.docs/Examples/AutoGen-OpenAI)
  * [AutoML - Classification](.docs/Examples/AutoML-Classification)
  * [AutoML - NLP](.docs/Examples/AutoML-NLP)
  * [AutoML - Rank](.docs/Examples/AutoML-Rank)
  * [AutoML - Regression](.docs/Examples/AutoML-Regression)
  * [AutoML - Time Series Forecast](.docs/Examples/AutoML-Time series forecast)
  * [AutoML for LightGBM](.docs/Examples/AutoML-for-LightGBM)
  * [AutoML for XGBoost](.docs/Examples/AutoML-for-XGBoost)
  * [Default - Flamlized Estimator](.docs/Examples/Default-Flamlized)
  * [Integrate - AzureML](.docs/Examples/Integrate - AzureML)
  * [Integrate - Scikit-learn Pipeline](.docs/Examples/Integrate - Scikit-learn Pipeline)
  * [Integrate - Spark](.docs/Examples/Integrate - Spark)
  * [Tune - AzureML pipeline](.docs/Examples/Tune-AzureML-pipeline)
  * [Tune - HuggingFace](.docs/Examples/Tune-HuggingFace)
  * [Tune - Lexicographic Objectives](.docs/Examples/Tune-Lexicographic-objectives)
  * [Tune - PyTorch](.docs/Examples/Tune-PyTorch)
* [Contributing](.docs/Contribute)
* [Research](.docs/Research)

On this page

# AutoML - Time Series Forecast {#automl-time-series-forecast}

### Prerequisites​ {#prerequisites}

Install the [automl,ts_forecast] option.

    pip install "flaml[automl,ts_forecast]"  

### Simple NumPy Example​ {#simple-numpy-example}

    import numpy as np  
    from flaml import AutoML  
      
    X_train = np.arange('2014-01', '2022-01', dtype='datetime64[M]')  
    y_train = np.random.random(size=84)  
    automl = AutoML()  
    automl.fit(X_train=X_train[:84],  # a single column of timestamp  
               y_train=y_train,  # value for each timestamp  
               period=12,  # time horizon to forecast, e.g., 12 months  
               task='ts_forecast', time_budget=15,  # time budget in seconds  
               log_file_name="ts_forecast.log",  
               eval_method="holdout",  
              )  
    print(automl.predict(X_train[84:]))  

#### Sample output​ {#sample-output} {#sample-output}

    [flaml.automl: 01-21 08:01:20] {2018} INFO - task = ts_forecast  
    [flaml.automl: 01-21 08:01:20] {2020} INFO - Data split method: time  
    [flaml.automl: 01-21 08:01:20] {2024} INFO - Evaluation method: holdout  
    [flaml.automl: 01-21 08:01:20] {2124} INFO - Minimizing error metric: mape  
    [flaml.automl: 01-21 08:01:21] {2181} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth', 'prophet', 'arima', 'sarimax']  
    [flaml.automl: 01-21 08:01:21] {2434} INFO - iteration 0, current learner lgbm  
    [flaml.automl: 01-21 08:01:21] {2547} INFO - Estimated sufficient time budget=1429s. Estimated necessary time budget=1s.  
    [flaml.automl: 01-21 08:01:21] {2594} INFO -  at 0.9s,  estimator lgbm's best error=0.9811,     best estimator lgbm's best error=0.9811  
    [flaml.automl: 01-21 08:01:21] {2434} INFO - iteration 1, current learner lgbm  
    [flaml.automl: 01-21 08:01:21] {2594} INFO -  at 0.9s,  estimator lgbm's best error=0.9811,     best estimator lgbm's best error=0.9811  
    [flaml.automl: 01-21 08:01:21] {2434} INFO - iteration 2, current learner lgbm  
    [flaml.automl: 01-21 08:01:21] {2594} INFO -  at 0.9s,  estimator lgbm's best error=0.9811,     best estimator lgbm's best error=0.9811  
    [flaml.automl: 01-21 08:01:21] {2434} INFO - iteration 3, current learner lgbm  
    [flaml.automl: 01-21 08:01:21] {2594} INFO -  at 1.0s,  estimator lgbm's best error=0.9811,     best estimator lgbm's best error=0.9811  
    [flaml.automl: 01-21 08:01:21] {2434} INFO - iteration 4, current learner lgbm  
    [flaml.automl: 01-21 08:01:21] {2594} INFO -  at 1.0s,  estimator lgbm's best error=0.9811,     best estimator lgbm's best error=0.9811  
    [flaml.automl: 01-21 08:01:21] {2434} INFO - iteration 5, current learner lgbm  
    [flaml.automl: 01-21 08:01:21] {2594} INFO -  at 1.0s,  estimator lgbm's best error=0.9811,     best estimator lgbm's best error=0.9811  
    [flaml.automl: 01-21 08:01:21] {2434} INFO - iteration 6, current learner lgbm  
    [flaml.automl: 01-21 08:01:21] {2594} INFO -  at 1.0s,  estimator lgbm's best error=0.9652,     best estimator lgbm's best error=0.9652  
    [flaml.automl: 01-21 08:01:21] {2434} INFO - iteration 7, current learner lgbm  
    [flaml.automl: 01-21 08:01:21] {2594} INFO -  at 1.0s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:21] {2434} INFO - iteration 8, current learner lgbm  
    [flaml.automl: 01-21 08:01:21] {2594} INFO -  at 1.0s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:21] {2434} INFO - iteration 9, current learner lgbm  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.1s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 10, current learner lgbm  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.1s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 11, current learner lgbm  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.1s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 12, current learner lgbm  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.1s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 13, current learner lgbm  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.1s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 14, current learner lgbm  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.1s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 15, current learner lgbm  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.2s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 16, current learner lgbm  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.2s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 17, current learner lgbm  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.2s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 18, current learner rf  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.2s,  estimator rf's best error=1.0994,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 19, current learner rf  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.2s,  estimator rf's best error=1.0848,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 20, current learner xgboost  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.3s,  estimator xgboost's best error=1.0271,  best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 21, current learner rf  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.3s,  estimator rf's best error=1.0848,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 22, current learner xgboost  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.3s,  estimator xgboost's best error=1.0015,  best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 23, current learner xgboost  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.3s,  estimator xgboost's best error=1.0015,  best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 24, current learner xgboost  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.3s,  estimator xgboost's best error=1.0015,  best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 25, current learner extra_tree  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.3s,  estimator extra_tree's best error=1.0130,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 26, current learner extra_tree  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.4s,  estimator extra_tree's best error=1.0130,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 27, current learner extra_tree  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.4s,  estimator extra_tree's best error=1.0130,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 28, current learner extra_tree  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.4s,  estimator extra_tree's best error=1.0130,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 29, current learner extra_tree  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.4s,  estimator extra_tree's best error=0.9499,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 30, current learner lgbm  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.5s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 31, current learner lgbm  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.5s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 32, current learner lgbm  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.5s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 33, current learner extra_tree  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.5s,  estimator extra_tree's best error=0.9499,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 34, current learner lgbm  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.5s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 35, current learner xgboost  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.5s,  estimator xgboost's best error=1.0015,  best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 36, current learner extra_tree  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.6s,  estimator extra_tree's best error=0.9499,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 37, current learner extra_tree  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.6s,  estimator extra_tree's best error=0.9499,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 38, current learner extra_tree  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.6s,  estimator extra_tree's best error=0.9499,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 39, current learner xgboost  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.6s,  estimator xgboost's best error=1.0015,  best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 40, current learner extra_tree  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.6s,  estimator extra_tree's best error=0.9499,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 41, current learner extra_tree  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.7s,  estimator extra_tree's best error=0.9499,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 42, current learner lgbm  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.7s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 43, current learner extra_tree  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.7s,  estimator extra_tree's best error=0.9499,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 44, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.7s,  estimator xgb_limitdepth's best error=1.5815,   best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 45, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.8s,  estimator xgb_limitdepth's best error=0.9683,   best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 46, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.8s,  estimator xgb_limitdepth's best error=0.9683,   best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 47, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.8s,  estimator xgb_limitdepth's best error=0.9683,   best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 48, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.9s,  estimator xgb_limitdepth's best error=0.9683,   best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 49, current learner lgbm  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.9s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 50, current learner extra_tree  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.9s,  estimator extra_tree's best error=0.9499,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 51, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 1.9s,  estimator xgb_limitdepth's best error=0.9683,   best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 52, current learner xgboost  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 2.0s,  estimator xgboost's best error=1.0015,  best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 53, current learner xgboost  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 2.0s,  estimator xgboost's best error=1.0015,  best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 54, current learner lgbm  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 2.0s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 55, current learner lgbm  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 2.0s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 56, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 2.0s,  estimator xgb_limitdepth's best error=0.9683,   best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 57, current learner rf  
    [flaml.automl: 01-21 08:01:22] {2594} INFO -  at 2.0s,  estimator rf's best error=1.0848,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:22] {2434} INFO - iteration 58, current learner xgboost  
    [flaml.automl: 01-21 08:01:23] {2594} INFO -  at 2.1s,  estimator xgboost's best error=1.0015,  best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:23] {2434} INFO - iteration 59, current learner extra_tree  
    [flaml.automl: 01-21 08:01:23] {2594} INFO -  at 2.1s,  estimator extra_tree's best error=0.9499,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:23] {2434} INFO - iteration 60, current learner lgbm  
    [flaml.automl: 01-21 08:01:23] {2594} INFO -  at 2.1s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:23] {2434} INFO - iteration 61, current learner extra_tree  
    [flaml.automl: 01-21 08:01:23] {2594} INFO -  at 2.1s,  estimator extra_tree's best error=0.9499,       best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:23] {2434} INFO - iteration 62, current learner lgbm  
    [flaml.automl: 01-21 08:01:23] {2594} INFO -  at 2.1s,  estimator lgbm's best error=0.9466,     best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:23] {2434} INFO - iteration 63, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:23] {2594} INFO -  at 2.2s,  estimator xgb_limitdepth's best error=0.9683,   best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:23] {2434} INFO - iteration 64, current learner prophet  
    [flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.2s,  estimator prophet's best error=1.5706,  best estimator lgbm's best error=0.9466  
    [flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 65, current learner arima  
    [flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.2s,  estimator arima's best error=0.5693,    best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 66, current learner arima  
    [flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.4s,  estimator arima's best error=0.5693,    best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 67, current learner sarimax  
    [flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.4s,  estimator sarimax's best error=0.5693,  best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 68, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.5s,  estimator xgb_limitdepth's best error=0.9683,   best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 69, current learner sarimax  
    [flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.6s,  estimator sarimax's best error=0.5693,  best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 70, current learner sarimax  
    [flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.6s,  estimator sarimax's best error=0.5693,  best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 71, current learner arima  
    [flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.6s,  estimator arima's best error=0.5693,    best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 72, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.6s,  estimator xgb_limitdepth's best error=0.9683,   best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 73, current learner arima  
    [flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.7s,  estimator arima's best error=0.5693,    best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 74, current learner sarimax  
    [flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.7s,  estimator sarimax's best error=0.5693,  best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 75, current learner arima  
    [flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.8s,  estimator arima's best error=0.5693,    best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 76, current learner sarimax  
    [flaml.automl: 01-21 08:01:25] {2594} INFO -  at 4.9s,  estimator sarimax's best error=0.5693,  best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 77, current learner arima  
    [flaml.automl: 01-21 08:01:25] {2594} INFO -  at 5.0s,  estimator arima's best error=0.5693,    best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:25] {2434} INFO - iteration 78, current learner sarimax  
    [flaml.automl: 01-21 08:01:26] {2594} INFO -  at 5.1s,  estimator sarimax's best error=0.5693,  best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:26] {2434} INFO - iteration 79, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:26] {2594} INFO -  at 5.1s,  estimator xgb_limitdepth's best error=0.9683,   best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:26] {2434} INFO - iteration 80, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:26] {2594} INFO -  at 5.1s,  estimator xgb_limitdepth's best error=0.9683,   best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:26] {2434} INFO - iteration 81, current learner sarimax  
    [flaml.automl: 01-21 08:01:26] {2594} INFO -  at 5.1s,  estimator sarimax's best error=0.5693,  best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:26] {2434} INFO - iteration 82, current learner prophet  
    [flaml.automl: 01-21 08:01:27] {2594} INFO -  at 6.6s,  estimator prophet's best error=1.4076,  best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:27] {2434} INFO - iteration 83, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:27] {2594} INFO -  at 6.6s,  estimator xgb_limitdepth's best error=0.9683,   best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:27] {2434} INFO - iteration 84, current learner sarimax  
    [flaml.automl: 01-21 08:01:27] {2594} INFO -  at 6.6s,  estimator sarimax's best error=0.5693,  best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:27] {2434} INFO - iteration 85, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:27] {2594} INFO -  at 6.6s,  estimator xgb_limitdepth's best error=0.9683,   best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:27] {2434} INFO - iteration 86, current learner sarimax  
    [flaml.automl: 01-21 08:01:27] {2594} INFO -  at 6.8s,  estimator sarimax's best error=0.5693,  best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:27] {2434} INFO - iteration 87, current learner arima  
    [flaml.automl: 01-21 08:01:27] {2594} INFO -  at 6.8s,  estimator arima's best error=0.5693,    best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:27] {2434} INFO - iteration 88, current learner sarimax  
    [flaml.automl: 01-21 08:01:27] {2594} INFO -  at 6.9s,  estimator sarimax's best error=0.5693,  best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:27] {2434} INFO - iteration 89, current learner arima  
    [flaml.automl: 01-21 08:01:27] {2594} INFO -  at 6.9s,  estimator arima's best error=0.5693,    best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:27] {2434} INFO - iteration 90, current learner arima  
    [flaml.automl: 01-21 08:01:27] {2594} INFO -  at 7.0s,  estimator arima's best error=0.5693,    best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:27] {2434} INFO - iteration 91, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:27] {2594} INFO -  at 7.0s,  estimator xgb_limitdepth's best error=0.9683,   best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:27] {2434} INFO - iteration 92, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:27] {2594} INFO -  at 7.0s,  estimator xgb_limitdepth's best error=0.9683,   best estimator arima's best error=0.5693  
    [flaml.automl: 01-21 08:01:27] {2434} INFO - iteration 93, current learner sarimax  
    [flaml.automl: 01-21 08:01:28] {2594} INFO -  at 7.0s,  estimator sarimax's best error=0.5600,  best estimator sarimax's best error=0.5600  
    [flaml.automl: 01-21 08:01:28] {2434} INFO - iteration 94, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:28] {2594} INFO -  at 7.1s,  estimator xgb_limitdepth's best error=0.9683,   best estimator sarimax's best error=0.5600  
    [flaml.automl: 01-21 08:01:28] {2434} INFO - iteration 95, current learner sarimax  
    [flaml.automl: 01-21 08:01:28] {2594} INFO -  at 7.2s,  estimator sarimax's best error=0.5600,  best estimator sarimax's best error=0.5600  
    [flaml.automl: 01-21 08:01:28] {2434} INFO - iteration 96, current learner arima  
    [flaml.automl: 01-21 08:01:28] {2594} INFO -  at 7.2s,  estimator arima's best error=0.5693,    best estimator sarimax's best error=0.5600  
    [flaml.automl: 01-21 08:01:28] {2434} INFO - iteration 97, current learner arima  
    [flaml.automl: 01-21 08:01:28] {2594} INFO -  at 7.2s,  estimator arima's best error=0.5693,    best estimator sarimax's best error=0.5600  
    [flaml.automl: 01-21 08:01:28] {2434} INFO - iteration 98, current learner extra_tree  
    [flaml.automl: 01-21 08:01:28] {2594} INFO -  at 7.3s,  estimator extra_tree's best error=0.9499,       best estimator sarimax's best error=0.5600  
    [flaml.automl: 01-21 08:01:28] {2434} INFO - iteration 99, current learner sarimax  
    [flaml.automl: 01-21 08:01:28] {2594} INFO -  at 7.3s,  estimator sarimax's best error=0.5600,  best estimator sarimax's best error=0.5600  
    [flaml.automl: 01-21 08:01:28] {2434} INFO - iteration 100, current learner xgb_limitdepth  
    [flaml.automl: 01-21 08:01:28] {2594} INFO -  at 7.3s,  estimator xgb_limitdepth's best error=0.9683,   best estimator sarimax's best error=0.5600  

### Univariate time series​ {#univariate-time-series}

    import statsmodels.api as sm  
      
    data = sm.datasets.co2.load_pandas().data  
    # data is given in weeks, but the task is to predict monthly, so use monthly averages instead  
    data = data['co2'].resample('MS').mean()  
    data = data.bfill().ffill()  # makes sure there are no missing values  
    data = data.to_frame().reset_index()  
    num_samples = data.shape[0]  
    time_horizon = 12  
    split_idx = num_samples - time_horizon  
    train_df = data[:split_idx]  # train_df is a dataframe with two columns: timestamp and label  
    X_test = data[split_idx:]['index'].to_frame()  # X_test is a dataframe with dates for prediction  
    y_test = data[split_idx:]['co2']  # y_test is a series of the values corresponding to the dates for prediction  
      
    from flaml import AutoML  
      
    automl = AutoML()  
    settings = {  
        "time_budget": 10,  # total running time in seconds  
        "metric": 'mape',  # primary metric for validation: 'mape' is generally used for forecast tasks  
        "task": 'ts_forecast',  # task type  
        "log_file_name": 'CO2_forecast.log',  # flaml log file  
        "eval_method": "holdout",  # validation method can be chosen from ['auto', 'holdout', 'cv']  
        "seed": 7654321,  # random seed  
    }  
      
    automl.fit(dataframe=train_df,  # training data  
               label='co2',  # label column  
               period=time_horizon,  # key word argument 'period' must be included for forecast task)  
               **settings)  

#### Sample output​ {#sample-output} {#sample-output}

    [flaml.automl: 01-21 07:54:04] {2018} INFO - task = ts_forecast  
    [flaml.automl: 01-21 07:54:04] {2020} INFO - Data split method: time  
    [flaml.automl: 01-21 07:54:04] {2024} INFO - Evaluation method: holdout  
    [flaml.automl: 01-21 07:54:04] {2124} INFO - Minimizing error metric: mape  
    Importing plotly failed. Interactive plots will not work.  
    [flaml.automl: 01-21 07:54:04] {2181} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth', 'prophet', 'arima', 'sarimax']  
    [flaml.automl: 01-21 07:54:04] {2434} INFO - iteration 0, current learner lgbm  
    [flaml.automl: 01-21 07:54:05] {2547} INFO - Estimated sufficient time budget=2145s. Estimated necessary time budget=2s.  
    [flaml.automl: 01-21 07:54:05] {2594} INFO -  at 0.9s,  estimator lgbm's best error=0.0621,     best estimator lgbm's best error=0.0621  
    [flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 1, current learner lgbm  
    [flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.0s,  estimator lgbm's best error=0.0574,     best estimator lgbm's best error=0.0574  
    [flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 2, current learner lgbm  
    [flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.0s,  estimator lgbm's best error=0.0464,     best estimator lgbm's best error=0.0464  
    [flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 3, current learner lgbm  
    [flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.0s,  estimator lgbm's best error=0.0464,     best estimator lgbm's best error=0.0464  
    [flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 4, current learner lgbm  
    [flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.0s,  estimator lgbm's best error=0.0365,     best estimator lgbm's best error=0.0365  
    [flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 5, current learner lgbm  
    [flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.1s,  estimator lgbm's best error=0.0192,     best estimator lgbm's best error=0.0192  
    [flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 6, current learner lgbm  
    [flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.1s,  estimator lgbm's best error=0.0192,     best estimator lgbm's best error=0.0192  
    [flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 7, current learner lgbm  
    [flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.1s,  estimator lgbm's best error=0.0192,     best estimator lgbm's best error=0.0192  
    [flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 8, current learner lgbm  
    [flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.2s,  estimator lgbm's best error=0.0110,     best estimator lgbm's best error=0.0110  
    [flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 9, current learner lgbm  
    [flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.2s,  estimator lgbm's best error=0.0110,     best estimator lgbm's best error=0.0110  
    [flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 10, current learner lgbm  
    [flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.2s,  estimator lgbm's best error=0.0036,     best estimator lgbm's best error=0.0036  
    [flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 11, current learner lgbm  
    [flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.4s,  estimator lgbm's best error=0.0023,     best estimator lgbm's best error=0.0023  
    [flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 12, current learner lgbm  
    [flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.4s,  estimator lgbm's best error=0.0023,     best estimator lgbm's best error=0.0023  
    [flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 13, current learner lgbm  
    [flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.5s,  estimator lgbm's best error=0.0021,     best estimator lgbm's best error=0.0021  
    [flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 14, current learner lgbm  
    [flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.6s,  estimator lgbm's best error=0.0021,     best estimator lgbm's best error=0.0021  
    [flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 15, current learner lgbm  
    [flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.7s,  estimator lgbm's best error=0.0020,     best estimator lgbm's best error=0.0020  
    [flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 16, current learner lgbm  
    [flaml.automl: 01-21 07:54:05] {2594} INFO -  at 1.8s,  estimator lgbm's best error=0.0017,     best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:05] {2434} INFO - iteration 17, current learner lgbm  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 1.9s,  estimator lgbm's best error=0.0017,     best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 18, current learner lgbm  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.0s,  estimator lgbm's best error=0.0017,     best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 19, current learner lgbm  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.1s,  estimator lgbm's best error=0.0017,     best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 20, current learner rf  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.1s,  estimator rf's best error=0.0228,       best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 21, current learner rf  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.1s,  estimator rf's best error=0.0210,       best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 22, current learner xgboost  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.2s,  estimator xgboost's best error=0.6738,  best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 23, current learner xgboost  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.2s,  estimator xgboost's best error=0.6738,  best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 24, current learner xgboost  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.2s,  estimator xgboost's best error=0.1717,  best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 25, current learner xgboost  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.3s,  estimator xgboost's best error=0.0249,  best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 26, current learner xgboost  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.3s,  estimator xgboost's best error=0.0249,  best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 27, current learner xgboost  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.3s,  estimator xgboost's best error=0.0242,  best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 28, current learner extra_tree  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.4s,  estimator extra_tree's best error=0.0245,       best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 29, current learner extra_tree  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.4s,  estimator extra_tree's best error=0.0160,       best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 30, current learner lgbm  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.5s,  estimator lgbm's best error=0.0017,     best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 31, current learner lgbm  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.6s,  estimator lgbm's best error=0.0017,     best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 32, current learner rf  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.6s,  estimator rf's best error=0.0210,       best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 33, current learner extra_tree  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.6s,  estimator extra_tree's best error=0.0160,       best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 34, current learner lgbm  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.8s,  estimator lgbm's best error=0.0017,     best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 35, current learner extra_tree  
    [flaml.automl: 01-21 07:54:06] {2594} INFO -  at 2.8s,  estimator extra_tree's best error=0.0158,       best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:06] {2434} INFO - iteration 36, current learner xgb_limitdepth  
    [flaml.automl: 01-21 07:54:07] {2594} INFO -  at 2.8s,  estimator xgb_limitdepth's best error=0.0447,   best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:07] {2434} INFO - iteration 37, current learner xgb_limitdepth  
    [flaml.automl: 01-21 07:54:07] {2594} INFO -  at 2.9s,  estimator xgb_limitdepth's best error=0.0447,   best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:07] {2434} INFO - iteration 38, current learner xgb_limitdepth  
    [flaml.automl: 01-21 07:54:07] {2594} INFO -  at 2.9s,  estimator xgb_limitdepth's best error=0.0029,   best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:07] {2434} INFO - iteration 39, current learner xgb_limitdepth  
    [flaml.automl: 01-21 07:54:07] {2594} INFO -  at 3.0s,  estimator xgb_limitdepth's best error=0.0018,   best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:07] {2434} INFO - iteration 40, current learner xgb_limitdepth  
    [flaml.automl: 01-21 07:54:07] {2594} INFO -  at 3.1s,  estimator xgb_limitdepth's best error=0.0018,   best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:07] {2434} INFO - iteration 41, current learner xgb_limitdepth  
    [flaml.automl: 01-21 07:54:07] {2594} INFO -  at 3.1s,  estimator xgb_limitdepth's best error=0.0018,   best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:07] {2434} INFO - iteration 42, current learner xgb_limitdepth  
    [flaml.automl: 01-21 07:54:07] {2594} INFO -  at 3.3s,  estimator xgb_limitdepth's best error=0.0018,   best estimator lgbm's best error=0.0017  
    [flaml.automl: 01-21 07:54:07] {2434} INFO - iteration 43, current learner prophet  
    [flaml.automl: 01-21 07:54:09] {2594} INFO -  at 5.5s,  estimator prophet's best error=0.0008,  best estimator prophet's best error=0.0008  
    [flaml.automl: 01-21 07:54:09] {2434} INFO - iteration 44, current learner arima  
    [flaml.automl: 01-21 07:54:10] {2594} INFO -  at 6.1s,  estimator arima's best error=0.0047,    best estimator prophet's best error=0.0008  
    [flaml.automl: 01-21 07:54:10] {2434} INFO - iteration 45, current learner sarimax  
    [flaml.automl: 01-21 07:54:10] {2594} INFO -  at 6.4s,  estimator sarimax's best error=0.0047,  best estimator prophet's best error=0.0008  
    [flaml.automl: 01-21 07:54:10] {2434} INFO - iteration 46, current learner lgbm  
    [flaml.automl: 01-21 07:54:10] {2594} INFO -  at 6.5s,  estimator lgbm's best error=0.0017,     best estimator prophet's best error=0.0008  
    [flaml.automl: 01-21 07:54:10] {2434} INFO - iteration 47, current learner sarimax  
    [flaml.automl: 01-21 07:54:10] {2594} INFO -  at 6.6s,  estimator sarimax's best error=0.0047,  best estimator prophet's best error=0.0008  
    [flaml.automl: 01-21 07:54:10] {2434} INFO - iteration 48, current learner sarimax  
    [flaml.automl: 01-21 07:54:11] {2594} INFO -  at 6.9s,  estimator sarimax's best error=0.0047,  best estimator prophet's best error=0.0008  
    [flaml.automl: 01-21 07:54:11] {2434} INFO - iteration 49, current learner arima  
    [flaml.automl: 01-21 07:54:11] {2594} INFO -  at 6.9s,  estimator arima's best error=0.0047,    best estimator prophet's best error=0.0008  
    [flaml.automl: 01-21 07:54:11] {2434} INFO - iteration 50, current learner xgb_limitdepth  
    [flaml.automl: 01-21 07:54:11] {2594} INFO -  at 7.0s,  estimator xgb_limitdepth's best error=0.0018,   best estimator prophet's best error=0.0008  
    [flaml.automl: 01-21 07:54:11] {2434} INFO - iteration 51, current learner sarimax  
    [flaml.automl: 01-21 07:54:11] {2594} INFO -  at 7.5s,  estimator sarimax's best error=0.0047,  best estimator prophet's best error=0.0008  
    [flaml.automl: 01-21 07:54:11] {2434} INFO - iteration 52, current learner xgboost  
    [flaml.automl: 01-21 07:54:11] {2594} INFO -  at 7.6s,  estimator xgboost's best error=0.0242,  best estimator prophet's best error=0.0008  
    [flaml.automl: 01-21 07:54:11] {2434} INFO - iteration 53, current learner prophet  
    [flaml.automl: 01-21 07:54:13] {2594} INFO -  at 9.3s,  estimator prophet's best error=0.0005,  best estimator prophet's best error=0.0005  
    [flaml.automl: 01-21 07:54:13] {2434} INFO - iteration 54, current learner sarimax  
    [flaml.automl: 01-21 07:54:13] {2594} INFO -  at 9.4s,  estimator sarimax's best error=0.0047,  best estimator prophet's best error=0.0005  
    [flaml.automl: 01-21 07:54:13] {2434} INFO - iteration 55, current learner xgb_limitdepth  
    [flaml.automl: 01-21 07:54:13] {2594} INFO -  at 9.8s,  estimator xgb_limitdepth's best error=0.0018,   best estimator prophet's best error=0.0005  
    [flaml.automl: 01-21 07:54:13] {2434} INFO - iteration 56, current learner xgboost  
    [flaml.automl: 01-21 07:54:13] {2594} INFO -  at 9.8s,  estimator xgboost's best error=0.0242,  best estimator prophet's best error=0.0005  
    [flaml.automl: 01-21 07:54:13] {2434} INFO - iteration 57, current learner lgbm  
    [flaml.automl: 01-21 07:54:14] {2594} INFO -  at 9.9s,  estimator lgbm's best error=0.0017,     best estimator prophet's best error=0.0005  
    [flaml.automl: 01-21 07:54:14] {2434} INFO - iteration 58, current learner rf  
    [flaml.automl: 01-21 07:54:14] {2594} INFO -  at 10.0s, estimator rf's best error=0.0146,       best estimator prophet's best error=0.0005  
    [flaml.automl: 01-21 07:54:14] {2824} INFO - retrain prophet for 0.6s  
    [flaml.automl: 01-21 07:54:14] {2831} INFO - retrained model: <prophet.forecaster.Prophet object at 0x7fb68ea65d60>  
    [flaml.automl: 01-21 07:54:14] {2210} INFO - fit succeeded  
    [flaml.automl: 01-21 07:54:14] {2211} INFO - Time taken to find the best model: 9.339771270751953  
    [flaml.automl: 01-21 07:54:14] {2222} WARNING - Time taken to find the best model is 93% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.  

#### Compute and plot predictions​ {#compute-and-plot-predictions}

The example plotting code requires matplotlib.

    flaml_y_pred = automl.predict(X_test)  
    import matplotlib.pyplot as plt  
      
    plt.plot(X_test, y_test, label='Actual level')  
    plt.plot(X_test, flaml_y_pred, label='FLAML forecast')  
    plt.xlabel('Date')  
    plt.ylabel('CO2 Levels')  
    plt.legend()  

![png](.assets/images/CO2-8a52a5b6467f2f3c0b4bc0fc516d5a62.png)

### Multivariate Time Series (Forecasting with Exogenous Variables)​ {#multivariate-time-series-forecasting-with-exogenous-variables}

    import pandas as pd  
      
    # pd.set_option("display.max_rows", None, "display.max_columns", None)  
    multi_df = pd.read_csv(  
        "https://raw.githubusercontent.com/srivatsan88/YouTubeLI/master/dataset/nyc_energy_consumption.csv"  
    )  
      
    # preprocessing data  
    multi_df["timeStamp"] = pd.to_datetime(multi_df["timeStamp"])  
    multi_df = multi_df.set_index("timeStamp")  
    multi_df = multi_df.resample("D").mean()  
    multi_df["temp"] = multi_df["temp"].fillna(method="ffill")  
    multi_df["precip"] = multi_df["precip"].fillna(method="ffill")  
    multi_df = multi_df[:-2]  # last two rows are NaN for 'demand' column so remove them  
    multi_df = multi_df.reset_index()  
      
    # Using temperature values create categorical values  
    # where 1 denotes daily tempurature is above monthly average and 0 is below.  
    def get_monthly_avg(data):  
        data["month"] = data["timeStamp"].dt.month  
        data = data[["month", "temp"]].groupby("month")  
        data = data.agg({"temp": "mean"})  
        return data  
      
    monthly_avg = get_monthly_avg(multi_df).to_dict().get("temp")  
      
    def above_monthly_avg(date, temp):  
        month = date.month  
        if temp > monthly_avg.get(month):  
            return 1  
        else:  
            return 0  
      
    multi_df["temp_above_monthly_avg"] = multi_df.apply(  
        lambda x: above_monthly_avg(x["timeStamp"], x["temp"]), axis=1  
    )  
      
    del multi_df["month"]  # remove temperature column to reduce redundancy  
      
    # split data into train and test  
    num_samples = multi_df.shape[0]  
    multi_time_horizon = 180  
    split_idx = num_samples - multi_time_horizon  
    multi_train_df = multi_df[:split_idx]  
    multi_test_df = multi_df[split_idx:]  
      
    multi_X_test = multi_test_df[  
        ["timeStamp", "precip", "temp", "temp_above_monthly_avg"]  
    ]  # test dataframe must contain values for the regressors / multivariate variables  
    multi_y_test = multi_test_df["demand"]  
      
    # initialize AutoML instance  
    automl = AutoML()  
      
    # configure AutoML settings  
    settings = {  
        "time_budget": 10,  # total running time in seconds  
        "metric": "mape",  # primary metric  
        "task": "ts_forecast",  # task type  
        "log_file_name": "energy_forecast_categorical.log",  # flaml log file  
        "eval_method": "holdout",  
        "log_type": "all",  
        "label": "demand",  
    }  
      
    # train the model  
    automl.fit(dataframe=df, **settings, period=time_horizon)  
      
    # predictions  
    print(automl.predict(multi_X_test))  

#### Sample Output​ {#sample-output} {#sample-output} {#sample-output}

    [flaml.automl: 08-13 01:03:11] {2540} INFO - task = ts_forecast  
    [flaml.automl: 08-13 01:03:11] {2542} INFO - Data split method: time  
    [flaml.automl: 08-13 01:03:11] {2545} INFO - Evaluation method: holdout  
    [flaml.automl: 08-13 01:03:11] {2664} INFO - Minimizing error metric: mape  
    [flaml.automl: 08-13 01:03:12] {2806} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth', 'prophet', 'arima', 'sarimax']  
    [flaml.automl: 08-13 01:03:12] {3108} INFO - iteration 0, current learner lgbm  
    [flaml.automl: 08-13 01:03:12] {3241} INFO - Estimated sufficient time budget=7681s. Estimated necessary time budget=8s.  
    [flaml.automl: 08-13 01:03:12] {3288} INFO -  at 0.8s,  estimator lgbm's best error=0.0854, best estimator lgbm's best error=0.0854  
    [flaml.automl: 08-13 01:03:12] {3108} INFO - iteration 1, current learner lgbm  
    [flaml.automl: 08-13 01:03:12] {3288} INFO -  at 0.9s,  estimator lgbm's best error=0.0854, best estimator lgbm's best error=0.0854  
    [flaml.automl: 08-13 01:03:12] {3108} INFO - iteration 2, current learner lgbm  
    [flaml.automl: 08-13 01:03:12] {3288} INFO -  at 0.9s,  estimator lgbm's best error=0.0525, best estimator lgbm's best error=0.0525  
    [flaml.automl: 08-13 01:03:12] {3108} INFO - iteration 3, current learner lgbm  
    [flaml.automl: 08-13 01:03:12] {3288} INFO -  at 0.9s,  estimator lgbm's best error=0.0525, best estimator lgbm's best error=0.0525  
    [flaml.automl: 08-13 01:03:12] {3108} INFO - iteration 4, current learner lgbm  
    [flaml.automl: 08-13 01:03:12] {3288} INFO -  at 1.0s,  estimator lgbm's best error=0.0406, best estimator lgbm's best error=0.0406  
    [flaml.automl: 08-13 01:03:12] {3108} INFO - iteration 5, current learner lgbm  
    [flaml.automl: 08-13 01:03:12] {3288} INFO -  at 1.0s,  estimator lgbm's best error=0.0406, best estimator lgbm's best error=0.0406  
    [flaml.automl: 08-13 01:03:12] {3108} INFO - iteration 6, current learner lgbm  
    [flaml.automl: 08-13 01:03:12] {3288} INFO -  at 1.0s,  estimator lgbm's best error=0.0406, best estimator lgbm's best error=0.0406  
    [flaml.automl: 08-13 01:03:12] {3108} INFO - iteration 7, current learner lgbm  
    [flaml.automl: 08-13 01:03:13] {3288} INFO -  at 1.1s,  estimator lgbm's best error=0.0393, best estimator lgbm's best error=0.0393  
    [flaml.automl: 08-13 01:03:13] {3108} INFO - iteration 8, current learner lgbm  
    [flaml.automl: 08-13 01:03:13] {3288} INFO -  at 1.1s,  estimator lgbm's best error=0.0393, best estimator lgbm's best error=0.0393  
    [flaml.automl: 08-13 01:03:13] {3108} INFO - iteration 9, current learner lgbm  
    ...  
                  silent=True, subsample=1.0, subsample_for_bin=200000,  
                  subsample_freq=0, verbose=-1)  
    [flaml.automl: 08-13 01:03:22] {2837} INFO - fit succeeded  
    [flaml.automl: 08-13 01:03:22] {2838} INFO - Time taken to find the best model: 3.4941744804382324  

### Forecasting Discrete Variables​ {#forecasting-discrete-variables}

    from hcrystalball.utils import get_sales_data  
    import numpy as np  
    from flaml import AutoML  
      
    time_horizon = 30  
    df = get_sales_data(n_dates=180, n_assortments=1, n_states=1, n_stores=1)  
    df = df[["Sales", "Open", "Promo", "Promo2"]]  
      
    # feature engineering - create a discrete value column  
    # 1 denotes above mean and 0 denotes below mean  
    df["above_mean_sales"] = np.where(df["Sales"] > df["Sales"].mean(), 1, 0)  
    df.reset_index(inplace=True)  
      
    # train-test split  
    discrete_train_df = df[:-time_horizon]  
    discrete_test_df = df[-time_horizon:]  
    discrete_X_train, discrete_X_test = (  
        discrete_train_df[["Date", "Open", "Promo", "Promo2"]],  
        discrete_test_df[["Date", "Open", "Promo", "Promo2"]],  
    )  
    discrete_y_train, discrete_y_test = discrete_train_df["above_mean_sales"], discrete_test_df["above_mean_sales"]  
      
    # initialize AutoML instance  
    automl = AutoML()  
      
    # configure the settings  
    settings = {  
        "time_budget": 15,  # total running time in seconds  
        "metric": "accuracy",  # primary metric  
        "task": "ts_forecast_classification",  # task type  
        "log_file_name": "sales_classification_forecast.log",  # flaml log file  
        "eval_method": "holdout",  
    }  
      
    # train the model  
    automl.fit(X_train=discrete_X_train,  
               y_train=discrete_y_train,  
               **settings,  
               period=time_horizon)  
      
    # make predictions  
    discrete_y_pred = automl.predict(discrete_X_test)  
    print("Predicted label", discrete_y_pred)  
    print("True label", discrete_y_test)  

#### Sample Output​ {#sample-output} {#sample-output} {#sample-output}

    [flaml.automl: 02-28 21:53:03] {2060} INFO - task = ts_forecast_classification  
    [flaml.automl: 02-28 21:53:03] {2062} INFO - Data split method: time  
    [flaml.automl: 02-28 21:53:03] {2066} INFO - Evaluation method: holdout  
    [flaml.automl: 02-28 21:53:03] {2147} INFO - Minimizing error metric: 1-accuracy  
    [flaml.automl: 02-28 21:53:03] {2205} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth']  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 0, current learner lgbm  
    [flaml.automl: 02-28 21:53:03] {2573} INFO - Estimated sufficient time budget=269s. Estimated necessary time budget=0s.  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.1s,  estimator lgbm's best error=0.2667, best estimator lgbm's best error=0.2667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 1, current learner lgbm  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.1s,  estimator lgbm's best error=0.2667, best estimator lgbm's best error=0.2667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 2, current learner lgbm  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.1s,  estimator lgbm's best error=0.1333, best estimator lgbm's best error=0.1333  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 3, current learner rf  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.2s,  estimator rf's best error=0.1333,   best estimator lgbm's best error=0.1333  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 4, current learner xgboost  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.2s,  estimator xgboost's best error=0.1333,  best estimator lgbm's best error=0.1333  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 5, current learner lgbm  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.2s,  estimator lgbm's best error=0.1333, best estimator lgbm's best error=0.1333  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 6, current learner rf  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.3s,  estimator rf's best error=0.0667,   best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 7, current learner lgbm  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.3s,  estimator lgbm's best error=0.0667, best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 8, current learner lgbm  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.3s,  estimator lgbm's best error=0.0667, best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 9, current learner lgbm  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.4s,  estimator lgbm's best error=0.0667, best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 10, current learner rf  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.4s,  estimator rf's best error=0.0667,   best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 11, current learner rf  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.4s,  estimator rf's best error=0.0667,   best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 12, current learner xgboost  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.5s,  estimator xgboost's best error=0.1333,  best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 13, current learner extra_tree  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.5s,  estimator extra_tree's best error=0.1333,   best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 14, current learner xgb_limitdepth  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.5s,  estimator xgb_limitdepth's best error=0.0667,   best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 15, current learner xgboost  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.6s,  estimator xgboost's best error=0.0667,  best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 16, current learner xgb_limitdepth  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.6s,  estimator xgb_limitdepth's best error=0.0667,   best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 17, current learner rf  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.6s,  estimator rf's best error=0.0667,   best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 18, current learner xgb_limitdepth  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.7s,  estimator xgb_limitdepth's best error=0.0667,   best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 19, current learner lgbm  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.7s,  estimator lgbm's best error=0.0667, best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 20, current learner extra_tree  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.7s,  estimator extra_tree's best error=0.0667,   best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 21, current learner xgboost  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.7s,  estimator xgboost's best error=0.0667,  best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 22, current learner extra_tree  
    [flaml.automl: 02-28 21:53:03] {2620} INFO -  at 0.8s,  estimator extra_tree's best error=0.0667,   best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:03] {2458} INFO - iteration 23, current learner rf  
    [flaml.automl: 02-28 21:53:04] {2620} INFO -  at 0.8s,  estimator rf's best error=0.0667,   best estimator rf's best error=0.0667  
    [flaml.automl: 02-28 21:53:04] {2458} INFO - iteration 24, current learner xgboost  
    [flaml.automl: 02-28 21:53:04] {2620} INFO -  at 0.9s,  estimator xgboost's best error=0.0333,  best estimator xgboost's best error=0.0333  
    [flaml.automl: 02-28 21:53:04] {2458} INFO - iteration 25, current learner xgb_limitdepth  
    [flaml.automl: 02-28 21:53:04] {2620} INFO -  at 0.9s,  estimator xgb_limitdepth's best error=0.0667,   best estimator xgboost's best error=0.0333  
    [flaml.automl: 02-28 21:53:04] {2458} INFO - iteration 26, current learner xgb_limitdepth  
    [flaml.automl: 02-28 21:53:04] {2620} INFO -  at 0.9s,  estimator xgb_limitdepth's best error=0.0667,   best estimator xgboost's best error=0.0333  
    [flaml.automl: 02-28 21:53:04] {2458} INFO - iteration 27, current learner xgboost  
    [flaml.automl: 02-28 21:53:04] {2620} INFO -  at 0.9s,  estimator xgboost's best error=0.0333,  best estimator xgboost's best error=0.0333  
    [flaml.automl: 02-28 21:53:04] {2458} INFO - iteration 28, current learner extra_tree  
    [flaml.automl: 02-28 21:53:04] {2620} INFO -  at 1.0s,  estimator extra_tree's best error=0.0667,   best estimator xgboost's best error=0.0333  
    [flaml.automl: 02-28 21:53:04] {2458} INFO - iteration 29, current learner xgb_limitdepth  
    [flaml.automl: 02-28 21:53:04] {2620} INFO -  at 1.0s,  estimator xgb_limitdepth's best error=0.0667,   best estimator xgboost's best error=0.0333  
    [flaml.automl: 02-28 21:53:04] {2850} INFO - retrain xgboost for 0.0s  
    [flaml.automl: 02-28 21:53:04] {2857} INFO - retrained model: XGBClassifier(base_score=0.5, booster='gbtree',  
                  colsample_bylevel=0.9826753651836615, colsample_bynode=1,  
                  colsample_bytree=0.9725493834064914, gamma=0, gpu_id=-1,  
                  grow_policy='lossguide', importance_type='gain',  
                  interaction_constraints='', learning_rate=0.1665803484560213,  
                  max_delta_step=0, max_depth=0, max_leaves=4,  
                  min_child_weight=0.5649012460525115, missing=nan,  
                  monotone_constraints='()', n_estimators=4, n_jobs=-1,  
                  num_parallel_tree=1, objective='binary:logistic', random_state=0,  
                  reg_alpha=0.009638363373006869, reg_lambda=0.143703802530408,  
                  scale_pos_weight=1, subsample=0.9643606787051899,  
                  tree_method='hist', use_label_encoder=False,  
                  validate_parameters=1, verbosity=0)  
    [flaml.automl: 02-28 21:53:04] {2234} INFO - fit succeeded  
    [flaml.automl: 02-28 21:53:04] {2235} INFO - Time taken to find the best model: 0.8547139167785645  

### Forecasting with Panel Datasets​ {#forecasting-with-panel-datasets}

Panel time series datasets involves multiple individual time series. For
example, see Stallion demand dataset from PyTorch Forecasting, orginally from
Kaggle.

    def get_stalliion_data():  
        from pytorch_forecasting.data.examples import get_stallion_data  
      
        data = get_stallion_data()  
        # add time index - For datasets with no missing values, FLAML will automate this process  
        data["time_idx"] = data["date"].dt.year * 12 + data["date"].dt.month  
        data["time_idx"] -= data["time_idx"].min()  
        # add additional features  
        data["month"] = data.date.dt.month.astype(str).astype(  
            "category"  
        )  # categories have be strings  
        data["log_volume"] = np.log(data.volume + 1e-8)  
        data["avg_volume_by_sku"] = data.groupby(  
            ["time_idx", "sku"], observed=True  
        ).volume.transform("mean")  
        data["avg_volume_by_agency"] = data.groupby(  
            ["time_idx", "agency"], observed=True  
        ).volume.transform("mean")  
        # we want to encode special days as one variable and thus need to first reverse one-hot encoding  
        special_days = [  
            "easter_day",  
            "good_friday",  
            "new_year",  
            "christmas",  
            "labor_day",  
            "independence_day",  
            "revolution_day_memorial",  
            "regional_games",  
            "beer_capital",  
            "music_fest",  
        ]  
        data[special_days] = (  
            data[special_days]  
            .apply(lambda x: x.map({0: "-", 1: x.name}))  
            .astype("category")  
        )  
        return data, special_days  
      
    data, special_days = get_stalliion_data()  
    time_horizon = 6  # predict six months  
    training_cutoff = data["time_idx"].max() - time_horizon  
    data["time_idx"] = data["time_idx"].astype("int")  
    ts_col = data.pop("date")  
    data.insert(0, "date", ts_col)  
    # FLAML assumes input is not sorted, but we sort here for comparison purposes with y_test  
    data = data.sort_values(["agency", "sku", "date"])  
    X_train = data[lambda x: x.time_idx <= training_cutoff]  
    X_test = data[lambda x: x.time_idx > training_cutoff]  
    y_train = X_train.pop("volume")  
    y_test = X_test.pop("volume")  
    automl = AutoML()  
    # Configure settings for FLAML model  
    settings = {  
        "time_budget": budget,  # total running time in seconds  
        "metric": "mape",  # primary metric  
        "task": "ts_forecast_panel",  # task type  
        "log_file_name": "test/stallion_forecast.log",  # flaml log file  
        "eval_method": "holdout",  
    }  
    # Specify kwargs for TimeSeriesDataSet used by TemporalFusionTransformerEstimator  
    fit_kwargs_by_estimator = {  
        "tft": {  
            "max_encoder_length": 24,  
            "static_categoricals": ["agency", "sku"],  
            "static_reals": ["avg_population_2017", "avg_yearly_household_income_2017"],  
            "time_varying_known_categoricals": ["special_days", "month"],  
            "variable_groups": {  
                "special_days": special_days  
            },  # group of categorical variables can be treated as one variable  
            "time_varying_known_reals": [  
                "time_idx",  
                "price_regular",  
                "discount_in_percent",  
            ],  
            "time_varying_unknown_categoricals": [],  
            "time_varying_unknown_reals": [  
                "y",  # always need a 'y' column for the target column  
                "log_volume",  
                "industry_volume",  
                "soda_volume",  
                "avg_max_temp",  
                "avg_volume_by_agency",  
                "avg_volume_by_sku",  
            ],  
            "batch_size": 256,  
            "max_epochs": 1,  
            "gpu_per_trial": -1,  
        }  
    }  
    # Train the model  
    automl.fit(  
        X_train=X_train,  
        y_train=y_train,  
        **settings,  
        period=time_horizon,  
        group_ids=["agency", "sku"],  
        fit_kwargs_by_estimator=fit_kwargs_by_estimator,  
    )  
    # Compute predictions of testing dataset  
    y_pred = automl.predict(X_test)  
    print(y_test)  
    print(y_pred)  
    # best model  
    print(automl.model.estimator)  

#### Sample Output​ {#sample-output} {#sample-output} {#sample-output}

    [flaml.automl: 07-28 21:26:03] {2478} INFO - task = ts_forecast_panel  
    [flaml.automl: 07-28 21:26:03] {2480} INFO - Data split method: time  
    [flaml.automl: 07-28 21:26:03] {2483} INFO - Evaluation method: holdout  
    [flaml.automl: 07-28 21:26:03] {2552} INFO - Minimizing error metric: mape  
    [flaml.automl: 07-28 21:26:03] {2694} INFO - List of ML learners in AutoML Run: ['tft']  
    [flaml.automl: 07-28 21:26:03] {2986} INFO - iteration 0, current learner tft  
    GPU available: False, used: False  
    TPU available: False, using: 0 TPU cores  
    IPU available: False, using: 0 IPUs  
      
       | Name                               | Type                            | Params  
    ----------------------------------------------------------------------------------------  
    0  | loss                               | QuantileLoss                    | 0  
    1  | logging_metrics                    | ModuleList                      | 0  
    2  | input_embeddings                   | MultiEmbedding                  | 1.3 K  
    3  | prescalers                         | ModuleDict                      | 256  
    4  | static_variable_selection          | VariableSelectionNetwork        | 3.4 K  
    5  | encoder_variable_selection         | VariableSelectionNetwork        | 8.0 K  
    6  | decoder_variable_selection         | VariableSelectionNetwork        | 2.7 K  
    7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K  
    8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K  
    9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K  
    10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K  
    11 | lstm_encoder                       | LSTM                            | 4.4 K  
    12 | lstm_decoder                       | LSTM                            | 4.4 K  
    13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544  
    14 | post_lstm_add_norm_encoder         | AddNorm                         | 32  
    15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K  
    16 | multihead_attn                     | InterpretableMultiHeadAttention | 676  
    17 | post_attn_gate_norm                | GateAddNorm                     | 576  
    18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K  
    19 | pre_output_gate_norm               | GateAddNorm                     | 576  
    20 | output_layer                       | Linear                          | 119  
    ----------------------------------------------------------------------------------------  
    33.6 K    Trainable params  
    0         Non-trainable params  
    33.6 K    Total params  
    0.135     Total estimated model params size (MB)  
      
    Epoch 19: 100%|██████████| 129/129 [00:56<00:00,  2.27it/s, loss=45.9, v_num=2, train_loss_step=43.00, val_loss=65.20, train_loss_epoch=46.50]  
      
    [flaml.automl: 07-28 21:46:46] {3114} INFO - Estimated sufficient time budget=12424212s. Estimated necessary time budget=12424s.  
    [flaml.automl: 07-28 21:46:46] {3161} INFO -  at 1242.6s,\testimator tft's best error=1324290483134574.7500,\tbest estimator tft's best error=1324290483134574.7500  
    GPU available: False, used: False  
    TPU available: False, using: 0 TPU cores  
    IPU available: False, using: 0 IPUs  
      
       | Name                               | Type                            | Params  
    ----------------------------------------------------------------------------------------  
    0  | loss                               | QuantileLoss                    | 0  
    1  | logging_metrics                    | ModuleList                      | 0  
    2  | input_embeddings                   | MultiEmbedding                  | 1.3 K  
    3  | prescalers                         | ModuleDict                      | 256  
    4  | static_variable_selection          | VariableSelectionNetwork        | 3.4 K  
    5  | encoder_variable_selection         | VariableSelectionNetwork        | 8.0 K  
    6  | decoder_variable_selection         | VariableSelectionNetwork        | 2.7 K  
    7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K  
    8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K  
    9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K  
    10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K  
    11 | lstm_encoder                       | LSTM                            | 4.4 K  
    12 | lstm_decoder                       | LSTM                            | 4.4 K  
    13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544  
    14 | post_lstm_add_norm_encoder         | AddNorm                         | 32  
    15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K  
    16 | multihead_attn                     | InterpretableMultiHeadAttention | 676  
    17 | post_attn_gate_norm                | GateAddNorm                     | 576  
    18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K  
    19 | pre_output_gate_norm               | GateAddNorm                     | 576  
    20 | output_layer                       | Linear                          | 119  
    ----------------------------------------------------------------------------------------  
    33.6 K    Trainable params  
    0         Non-trainable params  
    33.6 K    Total params  
    0.135     Total estimated model params size (MB)  
    Epoch 19: 100%|██████████| 145/145 [01:03<00:00,  2.28it/s, loss=45.2, v_num=3, train_loss_step=46.30, val_loss=67.60, train_loss_epoch=48.10]  
    [flaml.automl: 07-28 22:08:05] {3425} INFO - retrain tft for 1279.6s  
    [flaml.automl: 07-28 22:08:05] {3432} INFO - retrained model: TemporalFusionTransformer(  
      (loss): QuantileLoss()  
      (logging_metrics): ModuleList(  
        (0): SMAPE()  
        (1): MAE()  
        (2): RMSE()  
        (3): MAPE()  
      )  
      (input_embeddings): MultiEmbedding(  
        (embeddings): ModuleDict(  
          (agency): Embedding(58, 16)  
          (sku): Embedding(25, 10)  
          (special_days): TimeDistributedEmbeddingBag(11, 6, mode=sum)  
          (month): Embedding(12, 6)  
        )  
      )  
      (prescalers): ModuleDict(  
        (avg_population_2017): Linear(in_features=1, out_features=8, bias=True)  
        (avg_yearly_household_income_2017): Linear(in_features=1, out_features=8, bias=True)  
        (encoder_length): Linear(in_features=1, out_features=8, bias=True)  
        (y_center): Linear(in_features=1, out_features=8, bias=True)  
        (y_scale): Linear(in_features=1, out_features=8, bias=True)  
        (time_idx): Linear(in_features=1, out_features=8, bias=True)  
        (price_regular): Linear(in_features=1, out_features=8, bias=True)  
        (discount_in_percent): Linear(in_features=1, out_features=8, bias=True)  
        (relative_time_idx): Linear(in_features=1, out_features=8, bias=True)  
        (y): Linear(in_features=1, out_features=8, bias=True)  
        (log_volume): Linear(in_features=1, out_features=8, bias=True)  
        (industry_volume): Linear(in_features=1, out_features=8, bias=True)  
        (soda_volume): Linear(in_features=1, out_features=8, bias=True)  
        (avg_max_temp): Linear(in_features=1, out_features=8, bias=True)  
        (avg_volume_by_agency): Linear(in_features=1, out_features=8, bias=True)  
        (avg_volume_by_sku): Linear(in_features=1, out_features=8, bias=True)  
      )  
      (static_variable_selection): VariableSelectionNetwork(  
        (flattened_grn): GatedResidualNetwork(  
          (resample_norm): ResampleNorm(  
            (resample): TimeDistributedInterpolation()  
            (gate): Sigmoid()  
            (norm): LayerNorm((7,), eps=1e-05, elementwise_affine=True)  
          )  
          (fc1): Linear(in_features=66, out_features=7, bias=True)  
          (elu): ELU(alpha=1.0)  
          (fc2): Linear(in_features=7, out_features=7, bias=True)  
          (gate_norm): GateAddNorm(  
            (glu): GatedLinearUnit(  
              (dropout): Dropout(p=0.1, inplace=False)  
              (fc): Linear(in_features=7, out_features=14, bias=True)  
            )  
            (add_norm): AddNorm(  
              (norm): LayerNorm((7,), eps=1e-05, elementwise_affine=True)  
            )  
          )  
        )  
        (single_variable_grns): ModuleDict(  
          (agency): ResampleNorm(  
            (gate): Sigmoid()  
            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
          )  
          (sku): ResampleNorm(  
            (resample): TimeDistributedInterpolation()  
            (gate): Sigmoid()  
            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
          )  
          (avg_population_2017): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
          (avg_yearly_household_income_2017): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
          (encoder_length): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
          (y_center): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
          (y_scale): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
        )  
        (prescalers): ModuleDict(  
          (avg_population_2017): Linear(in_features=1, out_features=8, bias=True)  
          (avg_yearly_household_income_2017): Linear(in_features=1, out_features=8, bias=True)  
          (encoder_length): Linear(in_features=1, out_features=8, bias=True)  
          (y_center): Linear(in_features=1, out_features=8, bias=True)  
          (y_scale): Linear(in_features=1, out_features=8, bias=True)  
        )  
        (softmax): Softmax(dim=-1)  
      )  
      (encoder_variable_selection): VariableSelectionNetwork(  
        (flattened_grn): GatedResidualNetwork(  
          (resample_norm): ResampleNorm(  
            (resample): TimeDistributedInterpolation()  
            (gate): Sigmoid()  
            (norm): LayerNorm((13,), eps=1e-05, elementwise_affine=True)  
          )  
          (fc1): Linear(in_features=100, out_features=13, bias=True)  
          (elu): ELU(alpha=1.0)  
          (context): Linear(in_features=16, out_features=13, bias=False)  
          (fc2): Linear(in_features=13, out_features=13, bias=True)  
          (gate_norm): GateAddNorm(  
            (glu): GatedLinearUnit(  
              (dropout): Dropout(p=0.1, inplace=False)  
              (fc): Linear(in_features=13, out_features=26, bias=True)  
            )  
            (add_norm): AddNorm(  
              (norm): LayerNorm((13,), eps=1e-05, elementwise_affine=True)  
            )  
          )  
        )  
        (single_variable_grns): ModuleDict(  
          (special_days): ResampleNorm(  
            (resample): TimeDistributedInterpolation()  
            (gate): Sigmoid()  
            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
          )  
          (month): ResampleNorm(  
            (resample): TimeDistributedInterpolation()  
            (gate): Sigmoid()  
            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
          )  
          (time_idx): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
          (price_regular): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
          (discount_in_percent): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
          (relative_time_idx): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
          (y): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
          (log_volume): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
          (industry_volume): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
          (soda_volume): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
          (avg_max_temp): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
          (avg_volume_by_agency): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
          (avg_volume_by_sku): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
        )  
        (prescalers): ModuleDict(  
          (time_idx): Linear(in_features=1, out_features=8, bias=True)  
          (price_regular): Linear(in_features=1, out_features=8, bias=True)  
          (discount_in_percent): Linear(in_features=1, out_features=8, bias=True)  
          (relative_time_idx): Linear(in_features=1, out_features=8, bias=True)  
          (y): Linear(in_features=1, out_features=8, bias=True)  
          (log_volume): Linear(in_features=1, out_features=8, bias=True)  
          (industry_volume): Linear(in_features=1, out_features=8, bias=True)  
          (soda_volume): Linear(in_features=1, out_features=8, bias=True)  
          (avg_max_temp): Linear(in_features=1, out_features=8, bias=True)  
          (avg_volume_by_agency): Linear(in_features=1, out_features=8, bias=True)  
          (avg_volume_by_sku): Linear(in_features=1, out_features=8, bias=True)  
        )  
        (softmax): Softmax(dim=-1)  
      )  
      (decoder_variable_selection): VariableSelectionNetwork(  
        (flattened_grn): GatedResidualNetwork(  
          (resample_norm): ResampleNorm(  
            (resample): TimeDistributedInterpolation()  
            (gate): Sigmoid()  
            (norm): LayerNorm((6,), eps=1e-05, elementwise_affine=True)  
          )  
          (fc1): Linear(in_features=44, out_features=6, bias=True)  
          (elu): ELU(alpha=1.0)  
          (context): Linear(in_features=16, out_features=6, bias=False)  
          (fc2): Linear(in_features=6, out_features=6, bias=True)  
          (gate_norm): GateAddNorm(  
            (glu): GatedLinearUnit(  
              (dropout): Dropout(p=0.1, inplace=False)  
              (fc): Linear(in_features=6, out_features=12, bias=True)  
            )  
            (add_norm): AddNorm(  
              (norm): LayerNorm((6,), eps=1e-05, elementwise_affine=True)  
            )  
          )  
        )  
        (single_variable_grns): ModuleDict(  
          (special_days): ResampleNorm(  
            (resample): TimeDistributedInterpolation()  
            (gate): Sigmoid()  
            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
          )  
          (month): ResampleNorm(  
            (resample): TimeDistributedInterpolation()  
            (gate): Sigmoid()  
            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
          )  
          (time_idx): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
          (price_regular): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
          (discount_in_percent): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
          (relative_time_idx): GatedResidualNetwork(  
            (resample_norm): ResampleNorm(  
              (resample): TimeDistributedInterpolation()  
              (gate): Sigmoid()  
              (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
            )  
            (fc1): Linear(in_features=8, out_features=8, bias=True)  
            (elu): ELU(alpha=1.0)  
            (fc2): Linear(in_features=8, out_features=8, bias=True)  
            (gate_norm): GateAddNorm(  
              (glu): GatedLinearUnit(  
                (dropout): Dropout(p=0.1, inplace=False)  
                (fc): Linear(in_features=8, out_features=32, bias=True)  
              )  
              (add_norm): AddNorm(  
                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
              )  
            )  
          )  
        )  
        (prescalers): ModuleDict(  
          (time_idx): Linear(in_features=1, out_features=8, bias=True)  
          (price_regular): Linear(in_features=1, out_features=8, bias=True)  
          (discount_in_percent): Linear(in_features=1, out_features=8, bias=True)  
          (relative_time_idx): Linear(in_features=1, out_features=8, bias=True)  
        )  
        (softmax): Softmax(dim=-1)  
      )  
      (static_context_variable_selection): GatedResidualNetwork(  
        (fc1): Linear(in_features=16, out_features=16, bias=True)  
        (elu): ELU(alpha=1.0)  
        (fc2): Linear(in_features=16, out_features=16, bias=True)  
        (gate_norm): GateAddNorm(  
          (glu): GatedLinearUnit(  
            (dropout): Dropout(p=0.1, inplace=False)  
            (fc): Linear(in_features=16, out_features=32, bias=True)  
          )  
          (add_norm): AddNorm(  
            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
          )  
        )  
      )  
      (static_context_initial_hidden_lstm): GatedResidualNetwork(  
        (fc1): Linear(in_features=16, out_features=16, bias=True)  
        (elu): ELU(alpha=1.0)  
        (fc2): Linear(in_features=16, out_features=16, bias=True)  
        (gate_norm): GateAddNorm(  
          (glu): GatedLinearUnit(  
            (dropout): Dropout(p=0.1, inplace=False)  
            (fc): Linear(in_features=16, out_features=32, bias=True)  
          )  
          (add_norm): AddNorm(  
            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
          )  
        )  
      )  
      (static_context_initial_cell_lstm): GatedResidualNetwork(  
        (fc1): Linear(in_features=16, out_features=16, bias=True)  
        (elu): ELU(alpha=1.0)  
        (fc2): Linear(in_features=16, out_features=16, bias=True)  
        (gate_norm): GateAddNorm(  
          (glu): GatedLinearUnit(  
            (dropout): Dropout(p=0.1, inplace=False)  
            (fc): Linear(in_features=16, out_features=32, bias=True)  
          )  
          (add_norm): AddNorm(  
            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
          )  
        )  
      )  
      (static_context_enrichment): GatedResidualNetwork(  
        (fc1): Linear(in_features=16, out_features=16, bias=True)  
        (elu): ELU(alpha=1.0)  
        (fc2): Linear(in_features=16, out_features=16, bias=True)  
        (gate_norm): GateAddNorm(  
          (glu): GatedLinearUnit(  
            (dropout): Dropout(p=0.1, inplace=False)  
            (fc): Linear(in_features=16, out_features=32, bias=True)  
          )  
          (add_norm): AddNorm(  
            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
          )  
        )  
      )  
      (lstm_encoder): LSTM(16, 16, num_layers=2, batch_first=True, dropout=0.1)  
      (lstm_decoder): LSTM(16, 16, num_layers=2, batch_first=True, dropout=0.1)  
      (post_lstm_gate_encoder): GatedLinearUnit(  
        (dropout): Dropout(p=0.1, inplace=False)  
        (fc): Linear(in_features=16, out_features=32, bias=True)  
      )  
      (post_lstm_gate_decoder): GatedLinearUnit(  
        (dropout): Dropout(p=0.1, inplace=False)  
        (fc): Linear(in_features=16, out_features=32, bias=True)  
      )  
      (post_lstm_add_norm_encoder): AddNorm(  
        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
      )  
      (post_lstm_add_norm_decoder): AddNorm(  
        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
      )  
      (static_enrichment): GatedResidualNetwork(  
        (fc1): Linear(in_features=16, out_features=16, bias=True)  
        (elu): ELU(alpha=1.0)  
        (context): Linear(in_features=16, out_features=16, bias=False)  
        (fc2): Linear(in_features=16, out_features=16, bias=True)  
        (gate_norm): GateAddNorm(  
          (glu): GatedLinearUnit(  
            (dropout): Dropout(p=0.1, inplace=False)  
            (fc): Linear(in_features=16, out_features=32, bias=True)  
          )  
          (add_norm): AddNorm(  
            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
          )  
        )  
      )  
      (multihead_attn): InterpretableMultiHeadAttention(  
        (dropout): Dropout(p=0.1, inplace=False)  
        (v_layer): Linear(in_features=16, out_features=4, bias=True)  
        (q_layers): ModuleList(  
          (0): Linear(in_features=16, out_features=4, bias=True)  
          (1): Linear(in_features=16, out_features=4, bias=True)  
          (2): Linear(in_features=16, out_features=4, bias=True)  
          (3): Linear(in_features=16, out_features=4, bias=True)  
        )  
        (k_layers): ModuleList(  
          (0): Linear(in_features=16, out_features=4, bias=True)  
          (1): Linear(in_features=16, out_features=4, bias=True)  
          (2): Linear(in_features=16, out_features=4, bias=True)  
          (3): Linear(in_features=16, out_features=4, bias=True)  
        )  
        (attention): ScaledDotProductAttention(  
          (softmax): Softmax(dim=2)  
        )  
        (w_h): Linear(in_features=4, out_features=16, bias=False)  
      )  
      (post_attn_gate_norm): GateAddNorm(  
        (glu): GatedLinearUnit(  
          (dropout): Dropout(p=0.1, inplace=False)  
          (fc): Linear(in_features=16, out_features=32, bias=True)  
        )  
        (add_norm): AddNorm(  
          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
        )  
      )  
      (pos_wise_ff): GatedResidualNetwork(  
        (fc1): Linear(in_features=16, out_features=16, bias=True)  
        (elu): ELU(alpha=1.0)  
        (fc2): Linear(in_features=16, out_features=16, bias=True)  
        (gate_norm): GateAddNorm(  
          (glu): GatedLinearUnit(  
            (dropout): Dropout(p=0.1, inplace=False)  
            (fc): Linear(in_features=16, out_features=32, bias=True)  
          )  
          (add_norm): AddNorm(  
            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
          )  
        )  
      )  
      (pre_output_gate_norm): GateAddNorm(  
        (glu): GatedLinearUnit(  
          (fc): Linear(in_features=16, out_features=32, bias=True)  
        )  
        (add_norm): AddNorm(  
          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  
        )  
      )  
      (output_layer): Linear(in_features=16, out_features=7, bias=True)  
    )  
    [flaml.automl: 07-28 22:08:05] {2725} INFO - fit succeeded  
    [flaml.automl: 07-28 22:08:05] {2726} INFO - Time taken to find the best model: 1242.6435902118683  
    [flaml.automl: 07-28 22:08:05] {2737} WARNING - Time taken to find the best model is 414% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.\n"  
         ]  
        }  
       ],  

[Link to
notebook](https://github.com/microsoft.blob/main/notebook/automl_time_series_forecast.ipynb)
| [Open in
colab](https://colab.research.google.com/github/microsoft.blob/main/notebook/automl_time_series_forecast.ipynb)

[Edit this
page](<https://github.com/microsoft.edit/main/website/docs/Examples/AutoML->
Time series forecast.md)

[Previous« AutoML - Regression](.docs/Examples/AutoML-Regression)

[NextAutoML for LightGBM »](.docs/Examples/AutoML-for-LightGBM)

* Prerequisites
* Simple NumPy Example
* Univariate time series
* Multivariate Time Series (Forecasting with Exogenous Variables)
* Forecasting Discrete Variables
* Forecasting with Panel Datasets

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
    * autogen.agentchat
    * autogen.oai
    * [code_utils](.docs/reference/autogen/code_utils)
    * [math_utils](.docs/reference/autogen/math_utils)
    * [retrieve_utils](.docs/reference/autogen/retrieve_utils)
  * automl
  * default
  * onlineml
  * tune

On this page

# autogen.retrieve_utils {#autogenretrieveutils}

#### num_tokens_from_text​ {#numtokensfromtext}

    def num_tokens_from_text(text: str, model: str = "gpt-3.5-turbo-0613", return_tokens_per_name_and_message: bool = False) -> Union[int, Tuple[int, int, int]]  

Return the number of tokens used by a text.

#### num_tokens_from_messages​ {#numtokensfrommessages}

    def num_tokens_from_messages(messages: dict, model: str = "gpt-3.5-turbo-0613")  

Return the number of tokens used by a list of messages.

#### split_text_to_chunks​ {#splittexttochunks}

    def split_text_to_chunks(text: str, max_tokens: int = 4000, chunk_mode: str = "multi_lines", must_break_at_empty_line: bool = True, overlap: int = 10)  

Split a long text into chunks of max_tokens.

#### split_files_to_chunks​ {#splitfilestochunks}

    def split_files_to_chunks(files: list, max_tokens: int = 4000, chunk_mode: str = "multi_lines", must_break_at_empty_line: bool = True)  

Split a list of files into chunks of max_tokens.

#### get_files_from_dir​ {#getfilesfromdir}

    def get_files_from_dir(dir_path: str, types: list = TEXT_FORMATS, recursive: bool = True)  

Return a list of all the files in a given directory.

#### get_file_from_url​ {#getfilefromurl}

    def get_file_from_url(url: str, save_path: str = None)  

Download a file from a URL.

#### is_url​ {#isurl}

    def is_url(string: str)  

Return True if the string is a valid URL.

#### create_vector_db_from_dir​ {#createvectordbfromdir}

    def create_vector_db_from_dir(dir_path: str, max_tokens: int = 4000, client: API = None, db_path: str = "/tmp/chromadb.db", collection_name: str = "all-my-documents", get_or_create: bool = False, chunk_mode: str = "multi_lines", must_break_at_empty_line: bool = True, embedding_model: str = "all-MiniLM-L6-v2")  

Create a vector db from all the files in a given directory.

#### query_vector_db​ {#queryvectordb}

    def query_vector_db(query_texts: List[str], n_results: int = 10, client: API = None, db_path: str = "/tmp/chromadb.db", collection_name: str = "all-my-documents", search_string: str = "", embedding_model: str = "all-MiniLM-L6-v2") -> Dict[str, List[str]]  

Query a vector db.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/autogen/retrieve_utils.md)

[Previous« math_utils](.docs/reference/autogen/math_utils)

[Nexthistgb »](.docs/reference/automl/contrib/histgb)

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

Recent posts

* [Use flaml.autogen for Local LLMs](.blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](.blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](.blog/2023/05/18/GPT-adaptive-humaneval)
* [Surpassing 1 Million Downloads - A Retrospective and a Look into the Future](.blog/2023/05/07/1M-milestone)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](.blog/2023/04/21/LLM-tuning-math)

# Tags {#tags}

## F {#f}

* [FLAMLv22](.blog/tags/flam-lv-2)

* * *

## G {#g}

* [GPT3](.blog/tags/gpt)

* * *

## L {#l}

* [LLM5](.blog/tags/llm)
* [LLMOps1](.blog/tags/llm-ops)

* * *

## R {#r}

* [research3](.blog/tags/research)

* * *

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
  * automl
  * default
  * onlineml
  * tune
    * tune.scheduler
    * tune.searcher
    * tune.spark
    * [analysis](.docs/reference/tune/analysis)
    * [sample](.docs/reference/tune/sample)
    * [space](.docs/reference/tune/space)
    * [trial](.docs/reference/tune/trial)
    * [trial_runner](.docs/reference/tune/trial_runner)
    * [tune](.docs/reference/tune/tune)
    * [utils](.docs/reference/tune/utils)

On this page

# tune.sample {#tunesample}

## Domain Objects​ {#domain-objects}

    class Domain()  

Base class to specify a type and valid range to sample parameters from. This
base class is implemented by parameter spaces, like float ranges (`Float`),
integer ranges (`Integer`), or categorical variables (`Categorical`). The
`Domain` object contains information about valid values (e.g. minimum and
maximum values), and exposes methods that allow specification of specific
samplers (e.g. `uniform()` or `loguniform()`).

#### cast​ {#cast}

    def cast(value)  

Cast value to domain type

#### is_valid​ {#isvalid}

    def is_valid(value: Any)  

Returns True if `value` is a valid value in this domain.

## Grid Objects​ {#grid-objects}

    class Grid(Sampler)  

Dummy sampler used for grid search

#### uniform​ {#uniform}

    def uniform(lower: float, upper: float)  

Sample a float value uniformly between `lower` and `upper`. Sampling from
`tune.uniform(1, 10)` is equivalent to sampling from `np.random.uniform(1,
10))`

#### quniform​ {#quniform}

    def quniform(lower: float, upper: float, q: float)  

Sample a quantized float value uniformly between `lower` and `upper`. Sampling
from `tune.uniform(1, 10)` is equivalent to sampling from
`np.random.uniform(1, 10))` The value will be quantized, i.e. rounded to an
integer increment of `q`. Quantization makes the upper bound inclusive.

#### loguniform​ {#loguniform}

    def loguniform(lower: float, upper: float, base: float = 10)  

Sugar for sampling in different orders of magnitude.

 **Arguments** :

* `lower` _float_ \- Lower boundary of the output interval (e.g. 1e-4)
* `upper` _float_ \- Upper boundary of the output interval (e.g. 1e-2)
* `base` _int_ \- Base of the log. Defaults to 10.

#### qloguniform​ {#qloguniform}

    def qloguniform(lower: float, upper: float, q: float, base: float = 10)  

Sugar for sampling in different orders of magnitude. The value will be
quantized, i.e. rounded to an integer increment of `q`. Quantization makes the
upper bound inclusive.

 **Arguments** :

* `lower` _float_ \- Lower boundary of the output interval (e.g. 1e-4)
* `upper` _float_ \- Upper boundary of the output interval (e.g. 1e-2)
* `q` _float_ \- Quantization number. The result will be rounded to an integer increment of this value.
* `base` _int_ \- Base of the log. Defaults to 10.

#### choice​ {#choice}

    def choice(categories: Sequence)  

Sample a categorical value. Sampling from `tune.choice([1, 2])` is equivalent
to sampling from `np.random.choice([1, 2])`

#### randint​ {#randint}

    def randint(lower: int, upper: int)  

Sample an integer value uniformly between `lower` and `upper`. `lower` is
inclusive, `upper` is exclusive. Sampling from `tune.randint(10)` is
equivalent to sampling from `np.random.randint(10)`

#### lograndint​ {#lograndint}

    def lograndint(lower: int, upper: int, base: float = 10)  

Sample an integer value log-uniformly between `lower` and `upper`, with `base`
being the base of logarithm. `lower` is inclusive, `upper` is exclusive.

#### qrandint​ {#qrandint}

    def qrandint(lower: int, upper: int, q: int = 1)  

Sample an integer value uniformly between `lower` and `upper`.

`lower` is inclusive, `upper` is also inclusive (!).

The value will be quantized, i.e. rounded to an integer increment of `q`.
Quantization makes the upper bound inclusive.

#### qlograndint​ {#qlograndint}

    def qlograndint(lower: int, upper: int, q: int, base: float = 10)  

Sample an integer value log-uniformly between `lower` and `upper`, with `base`
being the base of logarithm. `lower` is inclusive, `upper` is also inclusive
(!). The value will be quantized, i.e. rounded to an integer increment of `q`.
Quantization makes the upper bound inclusive.

#### randn​ {#randn}

    def randn(mean: float = 0.0, sd: float = 1.0)  

Sample a float value normally with `mean` and `sd`.

 **Arguments** :

* `mean` _float_ \- Mean of the normal distribution. Defaults to 0.
* `sd` _float_ \- SD of the normal distribution. Defaults to 1.

#### qrandn​ {#qrandn}

    def qrandn(mean: float, sd: float, q: float)  

Sample a float value normally with `mean` and `sd`.

The value will be quantized, i.e. rounded to an integer increment of `q`.

 **Arguments** :

* `mean` \- Mean of the normal distribution.
* `sd` \- SD of the normal distribution.
* `q` \- Quantization number. The result will be rounded to an integer increment of this value.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/tune/sample.md)

[Previous« analysis](.docs/reference/tune/analysis)

[Nextspace »](.docs/reference/tune/space)

* Domain Objects
* Grid Objects

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
  * automl
  * default
    * [estimator](.docs/reference/default/estimator)
    * [greedy](.docs/reference/default/greedy)
    * [portfolio](.docs/reference/default/portfolio)
    * [suggest](.docs/reference/default/suggest)
  * onlineml
  * tune

On this page

# default.greedy {#defaultgreedy}

#### construct_portfolio​ {#constructportfolio}

    def construct_portfolio(regret_matrix, meta_features, regret_bound)  

The portfolio construction algorithm.

[Reference](https://arxiv.org/abs/2202.09927).

 **Arguments** :

* `regret_matrix` \- A dataframe of regret matrix.
* `meta_features` \- None or a dataframe of metafeatures matrix. When set to None, the algorithm uses greedy strategy. Otherwise, the algorithm uses greedy strategy with feedback from the nearest neighbor predictor.
* `regret_bound` \- A float of the regret bound.

 **Returns** :

A list of configuration names.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/default/greedy.md)

[Previous« estimator](.docs/reference/default/estimator)

[Nextportfolio »](.docs/reference/default/portfolio)

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

Recent posts

* [Use flaml.autogen for Local LLMs](.blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](.blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](.blog/2023/05/18/GPT-adaptive-humaneval)
* [Surpassing 1 Million Downloads - A Retrospective and a Look into the Future](.blog/2023/05/07/1M-milestone)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](.blog/2023/04/21/LLM-tuning-math)

# 3 posts tagged with "GPT" {#3-posts-tagged-with-gpt}

[View All Tags](.blog/tags)

## [MathChat - An Conversational Framework to Solve Math {#mathchat-an-conversational-framework-to-solve-math}

Problems](.blog/2023/06/28/MathChat)

June 28, 2023 · 8 min read

[![Yiran
Wu](https://github.com/kevin666aa.png)](https://github.com/kevin666aa)

[Yiran Wu](https://github.com/kevin666aa)

PhD student at Pennsylvania State University

![MathChat
WorkFlow](.assets/images/mathchatflow-926a8ed1975a114ab76c69996942c23a.png)
**TL;DR:**

* **We introduce MathChat, a conversational framework leveraging Large Language Models (LLMs), specifically GPT-4, to solve advanced mathematical problems.**
* **MathChat improves LLM 's performance on challenging math problem-solving, outperforming basic prompting and other strategies by about 6%. The improvement was especially notable in the Algebra category, with a 15% increase in accuracy.**
* **Despite the advancement, GPT-4 still struggles to solve very challenging math problems, even with effective prompting strategies. Further improvements are needed, such as the development of more specific assistant models or the integration of new tools and prompts.**

Recent Large Language Models (LLMs) like GTP-3.5 and GPT-4 have demonstrated
astonishing abilities over previous models on various tasks, such as text
generation, question answering, and code generation. Moreover, these models
can communicate with humans through conversations and remember previous
contexts, making it easier for humans to interact with them. These models play
an increasingly important role in our daily lives assisting people with
different tasks, such as writing emails, summarizing documents, and writing
code.

In this blog post, we probe into the problem-solving capabilities of LLMs.
Specifically, we are interested in their capabilities to solve advanced math
problems, which could be representative of a broader class of problems that
require precise reasoning and also have deterministic solutions.

We introduce MathChat, a conversational framework designed for solving
challenging math problems with LLMs. This framework takes advantage of the
chat-optimized feature of state-of-the-art LLMs, where a user proxy agent and
an LLM assistant work together to tackle math problems. We also test previous
prompting techniques for comparison.

## The MathChat Framework​ {#the-mathchat-framework}

MathChat simulates a conversation between the LLM assistant and a user proxy
agent. As the name indicates, the user proxy agent acts as a proxy for the
user, which is responsible for communicating with the LLM assistant and
continuing the conversation in a desired manner.

The proxy agent first presents a math problem to the LLM assistant, framed by
an initial prompt that guides the assistant in how it should work
collaboratively to solve the problem. We incorporate existing prompting
methods such as CoT (chain-of-thought) and tool-using are incorporated in the
initial prompt. Key components of the prompt include:

* **Tool-using Prompt:** This guides the LLM assistant to use Python code in the correct format to address the problem, allowing the user proxy agent to parse the code and provide the appropriate results.

* **Problem-Solving Strategy Selection Prompt:** The assistant is instructed to choose one of three potential problem-solving strategies, including:

    1. Writing a Python program to solve the problem directly.
    2. Solving the problem directly without using Python, which exercises LLMs' inherent reasoning capabilities.
    3. Solving the problem step by step with Python. This should be used when the first two strategies aren't suitable.

* **Final Answer Encapsulation Prompt:** This part instructs the assistant to put the final answer in _\boxed_ {}.

The prompt design in MathChat is flexible and can easily be refined to include
other tools, such as Wolfram Alpha.

Let's take a look at an example between the **User Proxy Agent** and the **LLM
Assistant** (GPT-4). The conversation focuses on how to solve inequality using
Python. (The conversation is modified for readability.)

Participant| Dialogue  
---|---  
User Proxy Agent| (Initial Prompt: Let's solve a math problem with Python...)
Problem: Find all x that satisfy the inequality (2x+10)(x+3) < (3x+9)(x+8).
Express your answer in interval notation.  
LLM assistant| The key idea to solve this problem is to find the critical
points, which are the points where the inequality is equal. Then, test the
intervals between the critical points to find the interval for which the
inequality holds.  
LLM assistant (continued)| Here's Python code to find the critical points:
`from sympy import symbols, Eq, solve; x = symbols("x"); lhs = (2*x + 10)*(x +
3); rhs = (3*x + 9)*(x + 8); equation = Eq(lhs, rhs); critical_points =
solve(equation, x); critical_points`  
User Proxy Agent| The critical points are: [-14, -3]  
LLM assistant| We found the critical points to be x = -14 and x = -3. Now
we'll test the intervals between these critical points as well as points
before and after the critical points. The intervals are: x < -14, -14 < x <
-3, x > -3.  
LLM assistant (continued)| Here's Python code to test the inequality for each
interval: `from sympy import symbols; x = symbols("x"); inequality = (2*x +
10)*(x + 3) < (3*x + 9)*(x + 8); test_points = [-15, -5, 0]; intervals = [];
for point in test_points: if inequality.subs(x, point):
intervals.append(point); intervals`  
User Proxy Agent| The intervals where the inequality holds true are for test
points: [-15, 0]  
LLM assistant| We have tested the intervals and found that the inequality
holds for: 1. x < -14 (test point: -15) 2. x > -3 (test point: 0). Therefore,
the solution to the inequality (2x+10)(x+3) < (3x+9)(x+8) is x ∈ (-∞, -14) ∪
(-3, ∞)  
  
## Experiment Setup​ {#experiment-setup} {#experiment-setup}

We evaluate the improvement brought by MathChat.

For the experiment, we focus on the level-5 problems from the MATH dataset,
which are composed of high school competition problems. These problems include
the application of theorems and complex equation derivation and are
challenging even for undergraduate students. We evaluate 6 of 7 categories
from the dataset (excluding Geometry): Prealgebra, Algebra, Number Theory,
Counting and Probability, Intermediate Algebra, and Precalculus.

We evaluate GPT-4 and use the default configuration of the OpenAI API. To
access the final performance, we manually compare the final answer with the
correct answer. For the vanilla prompt, Program Synthesis, and MathChat, we
have GPT-4 enclose the final answer in _\boxed_ {}, and we take the return of
the function in PoT as the final answer.

We also evaluate the following methods for comparison:

  1. **Vanilla prompting:** Evaluates GPT-4's direct problem-solving capability. The prompt used is: _" Solve the problem carefully. Put the final answer in \boxed{}"_.

  2. **Program of Thoughts (PoT):** Uses a zero-shot PoT prompt that requests the model to create a _Solver_ function to solve the problem and return the final answer.

  3. **Program Synthesis (PS) prompting:** Like PoT, it prompts the model to write a program to solve the problem. The prompt used is: _" Write a program that answers the following question: {Problem}"_.

## Experiment Results​ {#experiment-results} {#experiment-results} {#experiment-results}

The accuracy on all the problems with difficulty level-5 from different
categories of the MATH dataset with different methods is shown below:

![Result](.assets/images/result-514251f4c387d5b6e11656e1fa33de08.png)

We found that compared to basic prompting, which demonstrates the innate
capabilities of GPT-4, utilizing Python within the context of PoT or PS
strategy improved the overall accuracy by about 10%. This increase was mostly
seen in categories involving more numerical manipulations, such as Counting &
Probability and Number Theory, and in more complex categories like
Intermediate Algebra and Precalculus.

For categories like Algebra and Prealgebra, PoT and PS showed little
improvement, and in some instances, even led to a decrease in accuracy.
However, MathChat was able to enhance total accuracy by around 6% compared to
PoT and PS, showing competitive performance across all categories. Remarkably,
MathChat improved accuracy in the Algebra category by about 15% over other
methods. Note that categories like Intermediate Algebra and Precalculus
remained challenging for all methods, with only about 20% of problems solved
accurately.

The code for experiments can be found at this
[repository](https://github.com/kevin666aa.tree/gpt_math_solver/flaml/autogen/math).
We now provide an implementation of MathChat using the interactive agents in
FLAML. See this
[notebook](https://github.com/microsoft.blob/main/notebook/autogen_agentchat_MathChat.ipynb)
for example usage.

## Future Directions​ {#future-directions}

Despite MathChat's improvements over previous methods, the results show that
complex math problem is still challenging for recent powerful LLMs, like
GPT-4, even with help from external tools.

Further work can be done to enhance this framework or math problem-solving in
general:

* Although enabling the model to use tools like Python can reduce calculation errors, LLMs are still prone to logic errors. Methods like self-consistency (Sample several solutions and take a major vote on the final answer), or self-verification (use another LLM instance to check whether an answer is correct) might improve the performance.
* Sometimes, whether the LLM can solve the problem depends on the plan it uses. Some plans require less computation and logical reasoning, leaving less room for mistakes.
* MathChat has the potential to be adapted into a copilot system, which could assist users with math problems. This system could allow users to be more involved in the problem-solving process, potentially enhancing learning.

## For Further Reading​ {#for-further-reading} {#for-further-reading} {#for-further-reading}

* [Research paper of MathChat](https://arxiv.org/abs/2306.01337)
* [Documentation about `flaml.autogen`](https://microsoft.github.io/autogen/)

 _Are you working on applications that involve math problem-solving? Would you
appreciate additional research or support on the application of LLM-based
agents for math problem-solving? Please join
our[Discord](https://discord.gg/Cppx2vSPVP) server for discussion._

 **Tags:**

* [LLM](.blog/tags/llm)
* [GPT](.blog/tags/gpt)
* [research](.blog/tags/research)

[**Read More**](.blog/2023/06/28/MathChat)

## [Achieve More, Pay Less - Use GPT-4 Smartly](.blog/2023/05/18/GPT- {#achieve-more-pay-less-use-gpt4-smartlyflamlblog20230518gpt}

adaptive-humaneval)

May 18, 2023 · 8 min read

[![Chi Wang](https://github.com/sonichi.png)](<https://www.linkedin.com/in/chi->
wang-49b15b16/)

[Chi Wang](https://www.linkedin.com/in/chi-wang-49b15b16/)

Principal Researcher at Microsoft Research

![An adaptive way of using GPT-3.5 and GPT-4 outperforms GPT-4 in both coding
success rate and inference
cost](.assets/images/humaneval-b2718b0b15d76b8fc59a4094e1bd684c.png)

 **TL;DR:**

* **A case study using the HumanEval benchmark shows that an adaptive way of using multiple GPT models can achieve both much higher accuracy (from 68% to 90%) and lower inference cost (by 18%) than using GPT-4 for coding.**

GPT-4 is a big upgrade of foundation model capability, e.g., in code and math,
accompanied by a much higher (more than 10x) price per token to use over
GPT-3.5-Turbo. On a code completion benchmark,
[HumanEval](https://huggingface.co/datasets/openai_humaneval), developed by
OpenAI, GPT-4 can successfully solve 68% tasks while GPT-3.5-Turbo does 46%.
It is possible to increase the success rate of GPT-4 further by generating
multiple responses or making multiple calls. However, that will further
increase the cost, which is already nearly 20 times of using GPT-3.5-Turbo and
with more restricted API call rate limit. Can we achieve more with less?

In this blog post, we will explore a creative, adaptive way of using GPT
models which leads to a big leap forward.

## Observations​ {#observations}

* GPT-3.5-Turbo can alrady solve 40%-50% tasks. For these tasks if we never use GPT-4, we can save nearly 40-50% cost.
* If we use the saved cost to generate more responses with GPT-4 for the remaining unsolved tasks, it is possible to solve some more of them while keeping the amortized cost down.

The obstacle of leveraging these observations is that we do not know _a
priori_ which tasks can be solved by the cheaper model, which tasks can be
solved by the expensive model, and which tasks can be solved by paying even
more to the expensive model.

To overcome that obstacle, one may want to predict which task requires what
model to solve and how many responses are required for each task. Let's look
at one example code completion task:

    def vowels_count(s):  
        """Write a function vowels_count which takes a string representing  
        a word as input and returns the number of vowels in the string.  
        Vowels in this case are 'a', 'e', 'i', 'o', 'u'. Here, 'y' is also a  
        vowel, but only when it is at the end of the given word.  
      
        Example:  
        >>> vowels_count("abcde")  
        2  
        >>> vowels_count("ACEDY")  
        3  
        """  

Can we predict whether GPT-3.5-Turbo can solve this task or do we need to use
GPT-4? My first guess is that GPT-3.5-Turbo can get it right because the
instruction is fairly straightforward. Yet, it turns out that GPT-3.5-Turbo
does not consistently get it right, if we only give it one chance. It's not
obvious (but an interesting research question!) how to predict the performance
without actually trying.

What else can we do? We notice that: **It 's "easier" to verify a given
solution than finding a correct solution from scratch.**

Some simple example test cases are provided in the docstr. If we already have
a response generated by a model, we can use those test cases to filter wrong
implementations, and either use a more powerful model or generate more
responses, until the result passes the example test cases. Moreover, this step
can be automated by asking GPT-3.5-Turbo to generate assertion statements from
the examples given in the docstr (a simpler task where we can place our bet)
and executing the code.

## Solution​ {#solution}

Combining these observations, we can design a solution with two intuitive
ideas:

* Make use of auto-generated feedback, i.e., code execution results, to filter responses.
* Try inference configurations one by one, until one response can pass the filter.

![Design](.assets/images/design-88630f6d8d95f1e31ef5ce79e81aab54.png)

This solution works adaptively without knowing or predicting which task fits
which configuration. It simply tries multiple configurations one by one,
starting from the cheapest configuration. Note that one configuration can
generate multiple responses (by setting the inference parameter n larger than
1). And different configurations can use the same model and different
inference parameters such as n and temperature. Only one response is returned
and evaluated per task.

An implementation of this solution is provided in
[flaml.autogen](.docs/reference/autogen/code_utils#implement). It uses
the following sequence of configurations:

  1. GPT-3.5-Turbo, n=1, temperature=0
  2. GPT-3.5-Turbo, n=7, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]
  3. GPT-4, n=1, temperature=0
  4. GPT-4, n=2, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]
  5. GPT-4, n=1, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]

## Experiment Results​ {#experiment-results} {#experiment-results} {#experiment-results}

The first figure in this blog post shows the success rate and average
inference cost of the adaptive solution compared with default GPT-4. The
inference cost includes the cost for generating the assertions in our
solution. The generated assertions are not always correct, and programs that
pass/fail the generated assertions are not always right/wrong. Despite of
that, the adaptive solution can increase the success rate (referred to as
pass@1 in the literature) from 68% to 90%, while reducing the cost by 18%.

Here are a few examples of function definitions which are solved by different
configurations in the portfolio.

  1. Solved by GPT-3.5-Turbo, n=1, temperature=0

    def compare(game,guess):  
        """I think we all remember that feeling when the result of some long-awaited  
        event is finally known. The feelings and thoughts you have at that moment are  
        definitely worth noting down and comparing.  
        Your task is to determine if a person correctly guessed the results of a number of matches.  
        You are given two arrays of scores and guesses of equal length, where each index shows a match.  
        Return an array of the same length denoting how far off each guess was. If they have guessed correctly,  
        the value is 0, and if not, the value is the absolute difference between the guess and the score.  
      
      
        example:  
      
        compare([1,2,3,4,5,1],[1,2,3,4,2,-2]) -> [0,0,0,0,3,3]  
        compare([0,5,0,0,0,4],[4,1,1,0,0,-2]) -> [4,4,1,0,0,6]  
        """  

  2. Solved by GPT-3.5-Turbo, n=7, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]: the `vowels_count` function presented earlier.
  3. Solved by GPT-4, n=1, temperature=0:

    def string_xor(a: str, b: str) -> str:  
        """ Input are two strings a and b consisting only of 1s and 0s.  
        Perform binary XOR on these inputs and return result also as a string.  
        >>> string_xor('010', '110')  
        '100'  
        """  

  4. Solved by GPT-4, n=2, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]:

    def is_palindrome(string: str) -> bool:  
        """ Test if given string is a palindrome """  
        return string == string[::-1]  
      
      
    def make_palindrome(string: str) -> str:  
        """ Find the shortest palindrome that begins with a supplied string.  
        Algorithm idea is simple:  
        - Find the longest postfix of supplied string that is a palindrome.  
        - Append to the end of the string reverse of a string prefix that comes before the palindromic suffix.  
        >>> make_palindrome('')  
        ''  
        >>> make_palindrome('cat')  
        'catac'  
        >>> make_palindrome('cata')  
        'catac'  
        """  

  5. Solved by GPT-4, n=1, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]:

    def sort_array(arr):  
        """  
        In this Kata, you have to sort an array of non-negative integers according to  
        number of ones in their binary representation in ascending order.  
        For similar number of ones, sort based on decimal value.  
      
        It must be implemented like this:  
        >>> sort_array([1, 5, 2, 3, 4]) == [1, 2, 3, 4, 5]  
        >>> sort_array([-2, -3, -4, -5, -6]) == [-6, -5, -4, -3, -2]  
        >>> sort_array([1, 0, 2, 3, 4]) [0, 1, 2, 3, 4]  
        """  

The last problem is an example with wrong example test cases in the original
definition. It misleads the adaptive solution because a correct implementation
is regarded as wrong and more trials are made. The last configuration in the
sequence returns the right implementation, even though it does not pass the
auto-generated assertions. This example demonstrates that:

* Our adaptive solution has a certain degree of fault tolerance.
* The success rate and inference cost for the adaptive solution can be further improved if correct example test cases are used.

It is worth noting that the reduced inference cost is the amortized cost over
all the tasks. For each individual task, the cost can be either larger or
smaller than directly using GPT-4. This is the nature of the adaptive
solution: The cost is in general larger for difficult tasks than that for easy
tasks.

An example notebook to run this experiment can be found at:
<https://github.com/microsoft.blob/v1.2.1/notebook/research/autogen_code.ipynb>

## Discussion​ {#discussion}

Our solution is quite simple to implement using a generic interface offered in
[`flaml.autogen`](<https://microsoft.github.io/autogen/docs/Use->
Cases/enhanced_inference#logic-error), yet the result is quite encouraging.

While the specific way of generating assertions is application-specific, the
main ideas are general in LLM operations:

* Generate multiple responses to select - especially useful when selecting a good response is relatively easier than generating a good response at one shot.
* Consider multiple configurations to generate responses - especially useful when:
  * Model and other inference parameter choice affect the utility-cost tradeoff; or
  * Different configurations have complementary effect.

A [previous blog post](.blog/2023/04/21/LLM-tuning-math) provides
evidence that these ideas are relevant in solving math problems too.
`flaml.autogen` uses a technique
[EcoOptiGen](https://arxiv.org/abs/2303.04673) to support inference parameter
tuning and model selection.

There are many directions of extensions in research and development:

* Generalize the way to provide feedback.
* Automate the process of optimizing the configurations.
* Build adaptive agents for different applications.

 _Do you find this approach applicable to your use case? Do you have any other
challenge to share about LLM applications? Do you like to see more support or
research of LLM optimization or automation? Please join
our[Discord](https://discord.gg/Cppx2vSPVP) server for discussion._

## For Further Reading​ {#for-further-reading} {#for-further-reading} {#for-further-reading}

* [Documentation](https://microsoft.github.io/autogen/) about `flaml.autogen` and [Research paper](https://arxiv.org/abs/2303.04673).
* [Blog post](.blog/2023/04/21/LLM-tuning-math) about a related study for math.

 **Tags:**

* [LLM](.blog/tags/llm)
* [GPT](.blog/tags/gpt)
* [research](.blog/tags/research)

[**Read More**](.blog/2023/05/18/GPT-adaptive-humaneval)

## [Does Model and Inference Parameter Matter in LLM Applications? - A Case {#does-model-and-inference-parameter-matter-in-llm-applications-a-case}

Study for MATH](.blog/2023/04/21/LLM-tuning-math)

April 21, 2023 · 5 min read

[![Chi Wang](https://github.com/sonichi.png)](<https://www.linkedin.com/in/chi->
wang-49b15b16/)

[Chi Wang](https://www.linkedin.com/in/chi-wang-49b15b16/)

Principal Researcher at Microsoft Research

![level 2
algebra](.assets/images/level2algebra-659ba95286432d9945fc89e84d606797.png)

 **TL;DR:**

* **Just by tuning the inference parameters like model, number of responses, temperature etc. without changing any model weights or prompt, the baseline accuracy of untuned gpt-4 can be improved by 20% in high school math competition problems.**
* **For easy problems, the tuned gpt-3.5-turbo model vastly outperformed untuned gpt-4 in accuracy (e.g., 90% vs. 70%) and cost efficiency. For hard problems, the tuned gpt-4 is much more accurate (e.g., 35% vs. 20%) and less expensive than untuned gpt-4.**
* **FLAML can help with model selection, parameter tuning, and cost-saving in LLM applications.**

Large language models (LLMs) are powerful tools that can generate natural
language texts for various applications, such as chatbots, summarization,
translation, and more. GPT-4 is currently the state of the art LLM in the
world. Is model selection irrelevant? What about inference parameters?

In this blog post, we will explore how model and inference parameter matter in
LLM applications, using a case study for [MATH](<https://datasets-benchmarks->
proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-
round2.html), a benchmark for evaluating LLMs on advanced mathematical problem
solving. MATH consists of 12K math competition problems from AMC-10, AMC-12
and AIME. Each problem is accompanied by a step-by-step solution.

We will use the new subpackage [`flaml.autogen`](.blog/tags/docs/Use-
Cases/Autogen) to automatically find the best model and inference parameter
for LLMs on a given task and dataset given an inference budget, using a novel
low-cost search & pruning strategy. FLAML currently supports all the LLMs from
OpenAI, such as GPT-3.5 and GPT-4.

We will use FLAML to perform model selection and inference parameter tuning.
Then we compare the performance and inference cost on solving algebra problems
with the untuned gpt-4. We will also analyze how different difficulty levels
affect the results.

## Experiment Setup​ {#experiment-setup} {#experiment-setup}

We use FLAML to select between the following models with a target inference
budget $0.02 per instance:

* gpt-3.5-turbo, a relatively cheap model that powers the popular ChatGPT app
* gpt-4, the state of the art LLM that costs more than 10 times of gpt-3.5-turbo

We adapt the models using 20 examples in the train set, using the problem
statement as the input and generating the solution as the output. We use the
following inference parameters:

* temperature: The parameter that controls the randomness of the output text. A higher temperature means more diversity but less coherence. We search for the optimal temperature in the range of [0, 1].
* top_p: The parameter that controls the probability mass of the output tokens. Only tokens with a cumulative probability less than or equal to top-p are considered. A lower top-p means more diversity but less coherence. We search for the optimal top-p in the range of [0, 1].
* max_tokens: The maximum number of tokens that can be generated for each output. We search for the optimal max length in the range of [50, 1000].
* n: The number of responses to generate. We search for the optimal n in the range of [1, 100].
* prompt: We use the template: "{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \boxed{{}}." where {problem} will be replaced by the math problem instance.

In this experiment, when n > 1, we find the answer with highest votes among
all the responses and then select it as the final answer to compare with the
ground truth. For example, if n = 5 and 3 of the responses contain a final
answer 301 while 2 of the responses contain a final answer 159, we choose 301
as the final answer. This can help with resolving potential errors due to
randomness. We use the average accuracy and average inference cost as the
metric to evaluate the performance over a dataset. The inference cost of a
particular instance is measured by the price per 1K tokens and the number of
tokens consumed.

## Experiment Results​ {#experiment-results} {#experiment-results} {#experiment-results}

The first figure in this blog post shows the average accuracy and average
inference cost of each configuration on the level 2 Algebra test set.

Surprisingly, the tuned gpt-3.5-turbo model is selected as a better model and
it vastly outperforms untuned gpt-4 in accuracy (92% vs. 70%) with equal or
2.5 times higher inference budget. The same observation can be obtained on the
level 3 Algebra test set.

![level 3
algebra](.assets/images/level3algebra-94e87a683ac8832ac7ae6f41f30131a4.png)

However, the selected model changes on level 4 Algebra.

![level 4
algebra](.assets/images/level4algebra-492beb22490df30d6cc258f061912dcd.png)

This time gpt-4 is selected as the best model. The tuned gpt-4 achieves much
higher accuracy (56% vs. 44%) and lower cost than the untuned gpt-4. On level
5 the result is similar.

![level 5
algebra](.assets/images/level5algebra-8fba701551334296d08580b4b489fe56.png)

We can see that FLAML has found different optimal model and inference
parameters for each subset of a particular level, which shows that these
parameters matter in cost-sensitive LLM applications and need to be carefully
tuned or adapted.

An example notebook to run these experiments can be found at:
<https://github.com/microsoft.blob/v1.2.1/notebook/autogen_chatgpt.ipynb>

## Analysis and Discussion​ {#analysis-and-discussion}

While gpt-3.5-turbo demonstrates competitive accuracy with voted answers in
relatively easy algebra problems under the same inference budget, gpt-4 is a
better choice for the most difficult problems. In general, through parameter
tuning and model selection, we can identify the opportunity to save the
expensive model for more challenging tasks, and improve the overall
effectiveness of a budget-constrained system.

There are many other alternative ways of solving math problems, which we have
not covered in this blog post. When there are choices beyond the inference
parameters, they can be generally tuned via
[`flaml.tune`](.blog/tags/docs/Use-Cases/Tune-User-Defined-Function).

The need for model selection, parameter tuning and cost saving is not specific
to the math problems. The [Auto-GPT](<https://github.com/Significant->
Gravitas/Auto-GPT) project is an example where high cost can easily prevent a
generic complex task to be accomplished as it needs many LLM inference calls.

## For Further Reading​ {#for-further-reading} {#for-further-reading} {#for-further-reading}

* [Research paper about the tuning technique](https://arxiv.org/abs/2303.04673)
* [Documentation about `flaml.autogen`](https://microsoft.github.io/autogen/)

 _Do you have any experience to share about LLM applications? Do you like to
see more support or research of LLM optimization or automation? Please join
our[Discord](https://discord.gg/Cppx2vSPVP) server for discussion._

 **Tags:**

* [LLM](.blog/tags/llm)
* [GPT](.blog/tags/gpt)
* [research](.blog/tags/research)

[**Read More**](.blog/2023/04/21/LLM-tuning-math)

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

Recent posts

* [All About Agent Descriptions](/autogen/blog/2023/12/29/AgentDescriptions)
* [AgentOptimizer - An Agentic Way to Train Your LLM Agent](/autogen/blog/2023/12/23/AgentOptimizer)
* [AutoGen Studio: Interactively Explore Multi-Agent Workflows](/autogen/blog/2023/12/01/AutoGenStudio)
* [Agent AutoBuild - Automatically Building Multi-agent Systems](/autogen/blog/2023/11/26/Agent-AutoBuild)
* [How to Assess Utility of LLM-powered Applications?](/autogen/blog/2023/11/20/AgentEval)
* [AutoGen Meets GPTs](/autogen/blog/2023/11/13/OAI-assistants)
* [EcoAssistant - Using LLM Assistants More Accurately and Affordably](/autogen/blog/2023/11/09/EcoAssistant)
* [Multimodal with GPT-4V and LLaVA](/autogen/blog/2023/11/06/LMM-Agent)
* [AutoGen's Teachable Agents](/autogen/blog/2023/10/26/TeachableAgent)
* [Retrieval-Augmented Generation (RAG) Applications with AutoGen](/autogen/blog/2023/10/18/RetrieveChat)
* [Use AutoGen for Local LLMs](/autogen/blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](/autogen/blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](/autogen/blog/2023/05/18/GPT-adaptive-humaneval)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](/autogen/blog/2023/04/21/LLM-tuning-math)

# One post tagged with "cost-effectiveness" {#one-post-tagged-with-costeffectiveness}

[View All Tags](/autogen/blog/tags)

## [EcoAssistant - Using LLM Assistants More Accurately and {#ecoassistant-using-llm-assistants-more-accurately-and}

Affordably](/autogen/blog/2023/11/09/EcoAssistant)

November 9, 2023 · 5 min read

[![Jieyu Zhang](https://github.com/jieyuz2.png)](https://jieyuz2.github.io/)

[Jieyu Zhang](https://jieyuz2.github.io/)

PhD student at University of Washington

![system](/autogen/assets/images/system-1f6d283e77b49cac460bb999adb6bd5d.png)

 **TL;DR:**

* Introducing the **EcoAssistant** , which is designed to solve user queries more accurately and affordably.
* We show how to let the LLM assistant agent leverage external API to solve user query.
* We show how to reduce the cost of using GPT models via **Assistant Hierarchy**.
* We show how to leverage the idea of Retrieval-augmented Generation (RAG) to improve the success rate via **Solution Demonstration**.

## EcoAssistant​ {#ecoassistant}

In this blog, we introduce the **EcoAssistant** , a system built upon AutoGen
with the goal of solving user queries more accurately and affordably.

### Problem setup​ {#problem-setup}

Recently, users have been using conversational LLMs such as ChatGPT for
various queries. Reports indicate that 23% of ChatGPT user queries are for
knowledge extraction purposes. Many of these queries require knowledge that is
external to the information stored within any pre-trained large language
models (LLMs). These tasks can only be completed by generating code to fetch
necessary information via external APIs that contain the requested
information. In the table below, we show three types of user queries that we
aim to address in this work.

Dataset| API| Example query  
---|---|---  
Places| [Google
Places](<https://developers.google.com/maps/documentation/places/web->
service/overview)| I’m looking for a 24-hour pharmacy in Montreal, can you
find one for me?  
Weather| [Weather API](https://www.weatherapi.com)| What is the current cloud
coverage in Mumbai, India?  
Stock| [Alpha Vantage Stock API](https://www.alphavantage.co/documentation/)|
Can you give me the opening price of Microsoft for the month of January 2023?  
  
### Leveraging external APIs​ {#leveraging-external-apis}

To address these queries, we first build a **two-agent system** based on
AutoGen, where the first agent is a **LLM assistant agent** (`AssistantAgent`
in AutoGen) that is responsible for proposing and refining the code and the
second agent is a **code executor agent** (`UserProxyAgent` in AutoGen) that
would extract the generated code and execute it, forwarding the output back to
the LLM assistant agent. A visualization of the two-agent system is shown
below.

![chat](/autogen/assets/images/chat-a2adea6a92b3cd4059021840c869d7d5.png)

To instruct the assistant agent to leverage external APIs, we only need to add
the API name/key dictionary at the beginning of the initial message. The
template is shown below, where the red part is the information of APIs and
black part is user query.

![template](/autogen/assets/images/template-c610ae53eaa7afa3adaf670fa74b5c10.png)

Importantly, we don't want to reveal our real API key to the assistant agent
for safety concerns. Therefore, we use a **fake API key** to replace the real
API key in the initial message. In particular, we generate a random token
(e.g., `181dbb37`) for each API key and replace the real API key with the
token in the initial message. Then, when the code executor execute the code,
the fake API key would be automatically replaced by the real API key.

### Solution Demonstration​ {#solution-demonstration}

In most practical scenarios, queries from users would appear sequentially over
time. Our **EcoAssistant** leverages past success to help the LLM assistants
address future queries via **Solution Demonstration**. Specifically, whenever
a query is deemed successfully resolved by user feedback, we capture and store
the query and the final generated code snippet. These query-code pairs are
saved in a specialized vector database. When new queries appear,
**EcoAssistant** retrieves the most similar query from the database, which is
then appended with the associated code to the initial prompt for the new
query, serving as a demonstration. The new template of initial message is
shown below, where the blue part corresponds to the solution demonstration.

![template](/autogen/assets/images/template-
demo-5a8cae3df56acdcf73188e401ad739f5.png)

We found that this utilization of past successful query-code pairs improves
the query resolution process with fewer iterations and enhances the system's
performance.

### Assistant Hierarchy​ {#assistant-hierarchy}

LLMs usually have different prices and performance, for example, GPT-3.5-turbo
is much cheaper than GPT-4 but also less accurate. Thus, we propose the
**Assistant Hierarchy** to reduce the cost of using LLMs. The core idea is
that we use the cheaper LLMs first and only use the more expensive LLMs when
necessary. By this way, we are able to reduce the reliance on expensive LLMs
and thus reduce the cost. In particular, given multiple LLMs, we initiate one
assistant agent for each and start the conversation with the most cost-
effective LLM assistant. If the conversation between the current LLM assistant
and the code executor concludes without successfully resolving the query,
**EcoAssistant** would then restart the conversation with the next more
expensive LLM assistant in the hierarchy. We found that this strategy
significantly reduces costs while still effectively addressing queries.

### A Synergistic Effect​ {#a-synergistic-effect}

We found that the **Assistant Hierarchy** and **Solution Demonstration** of
**EcoAssistant** have a synergistic effect. Because the query-code database is
shared by all LLM assistants, even without specialized design, the solution
from more powerful LLM assistant (e.g., GPT-4) could be later retrieved to
guide weaker LLM assistant (e.g., GPT-3.5-turbo). Such a synergistic effect
further improves the performance and reduces the cost of **EcoAssistant**.

### Experimental Results​ {#experimental-results}

We evaluate **EcoAssistant** on three datasets: Places, Weather, and Stock.
When comparing it with a single GPT-4 assistant, we found that
**EcoAssistant** achieves a higher success rate with a lower cost as shown in
the figure below. For more details about the experimental results and other
experiments, please refer to our [paper](https://arxiv.org/abs/2310.03046).

![exp](/autogen/assets/images/results-4c8cfbb728760a85ce2d549fd7798179.png)

## Further reading​ {#further-reading}

Please refer to our [paper](https://arxiv.org/abs/2310.03046) and
[codebase](https://github.com/JieyuZ2/EcoAssistant) for more details about
**EcoAssistant**.

If you find this blog useful, please consider citing:

    @article{zhang2023ecoassistant,  
      title={EcoAssistant: Using LLM Assistant More Affordably and Accurately},  
      author={Zhang, Jieyu and Krishna, Ranjay and Awadallah, Ahmed H and Wang, Chi},  
      journal={arXiv preprint arXiv:2310.03046},  
      year={2023}  
    }  

 **Tags:**

* [LLM](/autogen/blog/tags/llm)
* [RAG](/autogen/blog/tags/rag)
* [cost-effectiveness](/autogen/blog/tags/cost-effectiveness)

[**Read More**](/autogen/blog/2023/11/09/EcoAssistant)

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* [Getting Started](.docs/Getting-Started)
* [Installation](.docs/Installation)
* Use Cases
* Examples
  * [AutoGen - Automated Multi Agent Chat](.docs/Examples/AutoGen-AgentChat)
  * [AutoGen - Tune GPT Models](.docs/Examples/AutoGen-OpenAI)
  * [AutoML - Classification](.docs/Examples/AutoML-Classification)
  * [AutoML - NLP](.docs/Examples/AutoML-NLP)
  * [AutoML - Rank](.docs/Examples/AutoML-Rank)
  * [AutoML - Regression](.docs/Examples/AutoML-Regression)
  * [AutoML - Time Series Forecast](.docs/Examples/AutoML-Time series forecast)
  * [AutoML for LightGBM](.docs/Examples/AutoML-for-LightGBM)
  * [AutoML for XGBoost](.docs/Examples/AutoML-for-XGBoost)
  * [Default - Flamlized Estimator](.docs/Examples/Default-Flamlized)
  * [Integrate - AzureML](.docs/Examples/Integrate - AzureML)
  * [Integrate - Scikit-learn Pipeline](.docs/Examples/Integrate - Scikit-learn Pipeline)
  * [Integrate - Spark](.docs/Examples/Integrate - Spark)
  * [Tune - AzureML pipeline](.docs/Examples/Tune-AzureML-pipeline)
  * [Tune - HuggingFace](.docs/Examples/Tune-HuggingFace)
  * [Tune - Lexicographic Objectives](.docs/Examples/Tune-Lexicographic-objectives)
  * [Tune - PyTorch](.docs/Examples/Tune-PyTorch)
* [Contributing](.docs/Contribute)
* [Research](.docs/Research)

On this page

# Integrate - Scikit-learn Pipeline {#integrate-scikitlearn-pipeline}

As FLAML's AutoML module can be used a transformer in the Sklearn's pipeline
we can get all the benefits of pipeline.

### Prerequisites​ {#prerequisites}

Install the [automl] option.

    pip install "flaml[automl] openml"  

### Load data​ {#load-data}

    from flaml.automl.data import load_openml_dataset  
      
    # Download [Airlines dataset](https://www.openml.org/d/1169) from OpenML. The task is to predict whether a given flight will be delayed, given the information of the scheduled departure.  
    X_train, X_test, y_train, y_test = load_openml_dataset(  
        dataset_id=1169, data_dir='./', random_state=1234, dataset_format='array')  

### Create a pipeline​ {#create-a-pipeline}

    from sklearn import set_config  
    from sklearn.pipeline import Pipeline  
    from sklearn.impute import SimpleImputer  
    from sklearn.preprocessing import StandardScaler  
    from flaml import AutoML  
      
    set_config(display='diagram')  
      
    imputer = SimpleImputer()  
    standardizer = StandardScaler()  
    automl = AutoML()  
      
    automl_pipeline = Pipeline([  
        ("imputuer",imputer),  
        ("standardizer", standardizer),  
        ("automl", automl)  
    ])  
    automl_pipeline  

![png](.assets/images/pipeline-7e27c1edb9e2dac4b1ee78f5ced90b1f.png)

### Run AutoML in the pipeline​ {#run-automl-in-the-pipeline}

    automl_settings = {  
        "time_budget": 60,  # total running time in seconds  
        "metric": "accuracy",  # primary metrics can be chosen from: ['accuracy', 'roc_auc', 'roc_auc_weighted', 'roc_auc_ovr', 'roc_auc_ovo', 'f1', 'log_loss', 'mae', 'mse', 'r2'] Check the documentation for more details (https://microsoft.github.io.docs/Use-Cases/Task-Oriented-AutoML#optimization-metric)  
        "task": "classification",  # task type  
        "estimator_list": ["xgboost", "catboost", "lgbm"],  
        "log_file_name": "airlines_experiment.log",  # flaml log file  
    }  
    pipeline_settings = {  
        f"automl__{key}": value for key, value in automl_settings.items()  
    }  
    automl_pipeline.fit(X_train, y_train, **pipeline_settings)  

### Get the automl object from the pipeline​ {#get-the-automl-object-from-the-pipeline}

    automl = automl_pipeline.steps[2][1]  
    # Get the best config and best learner  
    print('Best ML leaner:', automl.best_estimator)  
    print('Best hyperparmeter config:', automl.best_config)  
    print('Best accuracy on validation data: {0:.4g}'.format(1 - automl.best_loss))  
    print('Training duration of best run: {0:.4g} s'.format(automl.best_config_train_time))  

[Link to
notebook](https://github.com/microsoft.blob/main/notebook/integrate_sklearn.ipynb)
| [Open in
colab](https://colab.research.google.com/github/microsoft.blob/main/notebook/integrate_sklearn.ipynb)

[Edit this
page](<https://github.com/microsoft.edit/main/website/docs/Examples/Integrate>

* Scikit-learn Pipeline.md)

[Previous« Integrate - AzureML](.docs/Examples/Integrate - AzureML)

[NextIntegrate - Spark »](.docs/Examples/Integrate - Spark)

* Prerequisites
* Load data
* Create a pipeline
* Run AutoML in the pipeline
* Get the automl object from the pipeline

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

Recent posts

* [All About Agent Descriptions](/autogen/blog/2023/12/29/AgentDescriptions)
* [AgentOptimizer - An Agentic Way to Train Your LLM Agent](/autogen/blog/2023/12/23/AgentOptimizer)
* [AutoGen Studio: Interactively Explore Multi-Agent Workflows](/autogen/blog/2023/12/01/AutoGenStudio)
* [Agent AutoBuild - Automatically Building Multi-agent Systems](/autogen/blog/2023/11/26/Agent-AutoBuild)
* [How to Assess Utility of LLM-powered Applications?](/autogen/blog/2023/11/20/AgentEval)
* [AutoGen Meets GPTs](/autogen/blog/2023/11/13/OAI-assistants)
* [EcoAssistant - Using LLM Assistants More Accurately and Affordably](/autogen/blog/2023/11/09/EcoAssistant)
* [Multimodal with GPT-4V and LLaVA](/autogen/blog/2023/11/06/LMM-Agent)
* [AutoGen's Teachable Agents](/autogen/blog/2023/10/26/TeachableAgent)
* [Retrieval-Augmented Generation (RAG) Applications with AutoGen](/autogen/blog/2023/10/18/RetrieveChat)
* [Use AutoGen for Local LLMs](/autogen/blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](/autogen/blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](/autogen/blog/2023/05/18/GPT-adaptive-humaneval)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](/autogen/blog/2023/04/21/LLM-tuning-math)

# EcoAssistant - Using LLM Assistants More Accurately and Affordably {#ecoassistant-using-llm-assistants-more-accurately-and-affordably}

November 9, 2023 · 5 min read

[![Jieyu Zhang](https://github.com/jieyuz2.png)](https://jieyuz2.github.io/)

[Jieyu Zhang](https://jieyuz2.github.io/)

PhD student at University of Washington

![system](/autogen/assets/images/system-1f6d283e77b49cac460bb999adb6bd5d.png)

 **TL;DR:**

* Introducing the **EcoAssistant** , which is designed to solve user queries more accurately and affordably.
* We show how to let the LLM assistant agent leverage external API to solve user query.
* We show how to reduce the cost of using GPT models via **Assistant Hierarchy**.
* We show how to leverage the idea of Retrieval-augmented Generation (RAG) to improve the success rate via **Solution Demonstration**.

## EcoAssistant​ {#ecoassistant}

In this blog, we introduce the **EcoAssistant** , a system built upon AutoGen
with the goal of solving user queries more accurately and affordably.

### Problem setup​ {#problem-setup}

Recently, users have been using conversational LLMs such as ChatGPT for
various queries. Reports indicate that 23% of ChatGPT user queries are for
knowledge extraction purposes. Many of these queries require knowledge that is
external to the information stored within any pre-trained large language
models (LLMs). These tasks can only be completed by generating code to fetch
necessary information via external APIs that contain the requested
information. In the table below, we show three types of user queries that we
aim to address in this work.

Dataset| API| Example query  
---|---|---  
Places| [Google
Places](<https://developers.google.com/maps/documentation/places/web->
service/overview)| I’m looking for a 24-hour pharmacy in Montreal, can you
find one for me?  
Weather| [Weather API](https://www.weatherapi.com)| What is the current cloud
coverage in Mumbai, India?  
Stock| [Alpha Vantage Stock API](https://www.alphavantage.co/documentation/)|
Can you give me the opening price of Microsoft for the month of January 2023?  
  
### Leveraging external APIs​ {#leveraging-external-apis}

To address these queries, we first build a **two-agent system** based on
AutoGen, where the first agent is a **LLM assistant agent** (`AssistantAgent`
in AutoGen) that is responsible for proposing and refining the code and the
second agent is a **code executor agent** (`UserProxyAgent` in AutoGen) that
would extract the generated code and execute it, forwarding the output back to
the LLM assistant agent. A visualization of the two-agent system is shown
below.

![chat](/autogen/assets/images/chat-a2adea6a92b3cd4059021840c869d7d5.png)

To instruct the assistant agent to leverage external APIs, we only need to add
the API name/key dictionary at the beginning of the initial message. The
template is shown below, where the red part is the information of APIs and
black part is user query.

![template](/autogen/assets/images/template-c610ae53eaa7afa3adaf670fa74b5c10.png)

Importantly, we don't want to reveal our real API key to the assistant agent
for safety concerns. Therefore, we use a **fake API key** to replace the real
API key in the initial message. In particular, we generate a random token
(e.g., `181dbb37`) for each API key and replace the real API key with the
token in the initial message. Then, when the code executor execute the code,
the fake API key would be automatically replaced by the real API key.

### Solution Demonstration​ {#solution-demonstration}

In most practical scenarios, queries from users would appear sequentially over
time. Our **EcoAssistant** leverages past success to help the LLM assistants
address future queries via **Solution Demonstration**. Specifically, whenever
a query is deemed successfully resolved by user feedback, we capture and store
the query and the final generated code snippet. These query-code pairs are
saved in a specialized vector database. When new queries appear,
**EcoAssistant** retrieves the most similar query from the database, which is
then appended with the associated code to the initial prompt for the new
query, serving as a demonstration. The new template of initial message is
shown below, where the blue part corresponds to the solution demonstration.

![template](/autogen/assets/images/template-
demo-5a8cae3df56acdcf73188e401ad739f5.png)

We found that this utilization of past successful query-code pairs improves
the query resolution process with fewer iterations and enhances the system's
performance.

### Assistant Hierarchy​ {#assistant-hierarchy}

LLMs usually have different prices and performance, for example, GPT-3.5-turbo
is much cheaper than GPT-4 but also less accurate. Thus, we propose the
**Assistant Hierarchy** to reduce the cost of using LLMs. The core idea is
that we use the cheaper LLMs first and only use the more expensive LLMs when
necessary. By this way, we are able to reduce the reliance on expensive LLMs
and thus reduce the cost. In particular, given multiple LLMs, we initiate one
assistant agent for each and start the conversation with the most cost-
effective LLM assistant. If the conversation between the current LLM assistant
and the code executor concludes without successfully resolving the query,
**EcoAssistant** would then restart the conversation with the next more
expensive LLM assistant in the hierarchy. We found that this strategy
significantly reduces costs while still effectively addressing queries.

### A Synergistic Effect​ {#a-synergistic-effect}

We found that the **Assistant Hierarchy** and **Solution Demonstration** of
**EcoAssistant** have a synergistic effect. Because the query-code database is
shared by all LLM assistants, even without specialized design, the solution
from more powerful LLM assistant (e.g., GPT-4) could be later retrieved to
guide weaker LLM assistant (e.g., GPT-3.5-turbo). Such a synergistic effect
further improves the performance and reduces the cost of **EcoAssistant**.

### Experimental Results​ {#experimental-results}

We evaluate **EcoAssistant** on three datasets: Places, Weather, and Stock.
When comparing it with a single GPT-4 assistant, we found that
**EcoAssistant** achieves a higher success rate with a lower cost as shown in
the figure below. For more details about the experimental results and other
experiments, please refer to our [paper](https://arxiv.org/abs/2310.03046).

![exp](/autogen/assets/images/results-4c8cfbb728760a85ce2d549fd7798179.png)

## Further reading​ {#further-reading}

Please refer to our [paper](https://arxiv.org/abs/2310.03046) and
[codebase](https://github.com/JieyuZ2/EcoAssistant) for more details about
**EcoAssistant**.

If you find this blog useful, please consider citing:

    @article{zhang2023ecoassistant,  
      title={EcoAssistant: Using LLM Assistant More Affordably and Accurately},  
      author={Zhang, Jieyu and Krishna, Ranjay and Awadallah, Ahmed H and Wang, Chi},  
      journal={arXiv preprint arXiv:2310.03046},  
      year={2023}  
    }  

 **Tags:**

* [LLM](/autogen/blog/tags/llm)
* [RAG](/autogen/blog/tags/rag)
* [cost-effectiveness](/autogen/blog/tags/cost-effectiveness)

[Newer Post« AutoGen Meets GPTs](/autogen/blog/2023/11/13/OAI-assistants)

[Older PostMultimodal with GPT-4V and LLaVA »](/autogen/blog/2023/11/06/LMM-
Agent)

* EcoAssistant
  * Problem setup
  * Leveraging external APIs
  * Solution Demonstration
  * Assistant Hierarchy
  * A Synergistic Effect
  * Experimental Results
* Further reading

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

Recent posts

* [Use flaml.autogen for Local LLMs](.blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](.blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](.blog/2023/05/18/GPT-adaptive-humaneval)
* [Surpassing 1 Million Downloads - A Retrospective and a Look into the Future](.blog/2023/05/07/1M-milestone)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](.blog/2023/04/21/LLM-tuning-math)

# Achieve More, Pay Less - Use GPT-4 Smartly {#achieve-more-pay-less-use-gpt4-smartly}

May 18, 2023 · 8 min read

[![Chi Wang](https://github.com/sonichi.png)](<https://www.linkedin.com/in/chi->
wang-49b15b16/)

[Chi Wang](https://www.linkedin.com/in/chi-wang-49b15b16/)

Principal Researcher at Microsoft Research

![An adaptive way of using GPT-3.5 and GPT-4 outperforms GPT-4 in both coding
success rate and inference
cost](.assets/images/humaneval-b2718b0b15d76b8fc59a4094e1bd684c.png)

 **TL;DR:**

* **A case study using the HumanEval benchmark shows that an adaptive way of using multiple GPT models can achieve both much higher accuracy (from 68% to 90%) and lower inference cost (by 18%) than using GPT-4 for coding.**

GPT-4 is a big upgrade of foundation model capability, e.g., in code and math,
accompanied by a much higher (more than 10x) price per token to use over
GPT-3.5-Turbo. On a code completion benchmark,
[HumanEval](https://huggingface.co/datasets/openai_humaneval), developed by
OpenAI, GPT-4 can successfully solve 68% tasks while GPT-3.5-Turbo does 46%.
It is possible to increase the success rate of GPT-4 further by generating
multiple responses or making multiple calls. However, that will further
increase the cost, which is already nearly 20 times of using GPT-3.5-Turbo and
with more restricted API call rate limit. Can we achieve more with less?

In this blog post, we will explore a creative, adaptive way of using GPT
models which leads to a big leap forward.

## Observations​ {#observations}

* GPT-3.5-Turbo can alrady solve 40%-50% tasks. For these tasks if we never use GPT-4, we can save nearly 40-50% cost.
* If we use the saved cost to generate more responses with GPT-4 for the remaining unsolved tasks, it is possible to solve some more of them while keeping the amortized cost down.

The obstacle of leveraging these observations is that we do not know _a
priori_ which tasks can be solved by the cheaper model, which tasks can be
solved by the expensive model, and which tasks can be solved by paying even
more to the expensive model.

To overcome that obstacle, one may want to predict which task requires what
model to solve and how many responses are required for each task. Let's look
at one example code completion task:

    def vowels_count(s):  
        """Write a function vowels_count which takes a string representing  
        a word as input and returns the number of vowels in the string.  
        Vowels in this case are 'a', 'e', 'i', 'o', 'u'. Here, 'y' is also a  
        vowel, but only when it is at the end of the given word.  
      
        Example:  
        >>> vowels_count("abcde")  
        2  
        >>> vowels_count("ACEDY")  
        3  
        """  

Can we predict whether GPT-3.5-Turbo can solve this task or do we need to use
GPT-4? My first guess is that GPT-3.5-Turbo can get it right because the
instruction is fairly straightforward. Yet, it turns out that GPT-3.5-Turbo
does not consistently get it right, if we only give it one chance. It's not
obvious (but an interesting research question!) how to predict the performance
without actually trying.

What else can we do? We notice that: **It 's "easier" to verify a given
solution than finding a correct solution from scratch.**

Some simple example test cases are provided in the docstr. If we already have
a response generated by a model, we can use those test cases to filter wrong
implementations, and either use a more powerful model or generate more
responses, until the result passes the example test cases. Moreover, this step
can be automated by asking GPT-3.5-Turbo to generate assertion statements from
the examples given in the docstr (a simpler task where we can place our bet)
and executing the code.

## Solution​ {#solution}

Combining these observations, we can design a solution with two intuitive
ideas:

* Make use of auto-generated feedback, i.e., code execution results, to filter responses.
* Try inference configurations one by one, until one response can pass the filter.

![Design](.assets/images/design-88630f6d8d95f1e31ef5ce79e81aab54.png)

This solution works adaptively without knowing or predicting which task fits
which configuration. It simply tries multiple configurations one by one,
starting from the cheapest configuration. Note that one configuration can
generate multiple responses (by setting the inference parameter n larger than
1). And different configurations can use the same model and different
inference parameters such as n and temperature. Only one response is returned
and evaluated per task.

An implementation of this solution is provided in
[flaml.autogen](.docs/reference/autogen/code_utils#implement). It uses
the following sequence of configurations:

  1. GPT-3.5-Turbo, n=1, temperature=0
  2. GPT-3.5-Turbo, n=7, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]
  3. GPT-4, n=1, temperature=0
  4. GPT-4, n=2, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]
  5. GPT-4, n=1, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]

## Experiment Results​ {#experiment-results}

The first figure in this blog post shows the success rate and average
inference cost of the adaptive solution compared with default GPT-4. The
inference cost includes the cost for generating the assertions in our
solution. The generated assertions are not always correct, and programs that
pass/fail the generated assertions are not always right/wrong. Despite of
that, the adaptive solution can increase the success rate (referred to as
pass@1 in the literature) from 68% to 90%, while reducing the cost by 18%.

Here are a few examples of function definitions which are solved by different
configurations in the portfolio.

  1. Solved by GPT-3.5-Turbo, n=1, temperature=0

    def compare(game,guess):  
        """I think we all remember that feeling when the result of some long-awaited  
        event is finally known. The feelings and thoughts you have at that moment are  
        definitely worth noting down and comparing.  
        Your task is to determine if a person correctly guessed the results of a number of matches.  
        You are given two arrays of scores and guesses of equal length, where each index shows a match.  
        Return an array of the same length denoting how far off each guess was. If they have guessed correctly,  
        the value is 0, and if not, the value is the absolute difference between the guess and the score.  
      
      
        example:  
      
        compare([1,2,3,4,5,1],[1,2,3,4,2,-2]) -> [0,0,0,0,3,3]  
        compare([0,5,0,0,0,4],[4,1,1,0,0,-2]) -> [4,4,1,0,0,6]  
        """  

  2. Solved by GPT-3.5-Turbo, n=7, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]: the `vowels_count` function presented earlier.
  3. Solved by GPT-4, n=1, temperature=0:

    def string_xor(a: str, b: str) -> str:  
        """ Input are two strings a and b consisting only of 1s and 0s.  
        Perform binary XOR on these inputs and return result also as a string.  
        >>> string_xor('010', '110')  
        '100'  
        """  

  4. Solved by GPT-4, n=2, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]:

    def is_palindrome(string: str) -> bool:  
        """ Test if given string is a palindrome """  
        return string == string[::-1]  
      
      
    def make_palindrome(string: str) -> str:  
        """ Find the shortest palindrome that begins with a supplied string.  
        Algorithm idea is simple:  
        - Find the longest postfix of supplied string that is a palindrome.  
        - Append to the end of the string reverse of a string prefix that comes before the palindromic suffix.  
        >>> make_palindrome('')  
        ''  
        >>> make_palindrome('cat')  
        'catac'  
        >>> make_palindrome('cata')  
        'catac'  
        """  

  5. Solved by GPT-4, n=1, temperature=1, stop=["\nclass", "\ndef", "\nif", "\nprint"]:

    def sort_array(arr):  
        """  
        In this Kata, you have to sort an array of non-negative integers according to  
        number of ones in their binary representation in ascending order.  
        For similar number of ones, sort based on decimal value.  
      
        It must be implemented like this:  
        >>> sort_array([1, 5, 2, 3, 4]) == [1, 2, 3, 4, 5]  
        >>> sort_array([-2, -3, -4, -5, -6]) == [-6, -5, -4, -3, -2]  
        >>> sort_array([1, 0, 2, 3, 4]) [0, 1, 2, 3, 4]  
        """  

The last problem is an example with wrong example test cases in the original
definition. It misleads the adaptive solution because a correct implementation
is regarded as wrong and more trials are made. The last configuration in the
sequence returns the right implementation, even though it does not pass the
auto-generated assertions. This example demonstrates that:

* Our adaptive solution has a certain degree of fault tolerance.
* The success rate and inference cost for the adaptive solution can be further improved if correct example test cases are used.

It is worth noting that the reduced inference cost is the amortized cost over
all the tasks. For each individual task, the cost can be either larger or
smaller than directly using GPT-4. This is the nature of the adaptive
solution: The cost is in general larger for difficult tasks than that for easy
tasks.

An example notebook to run this experiment can be found at:
<https://github.com/microsoft.blob/v1.2.1/notebook/research/autogen_code.ipynb>

## Discussion​ {#discussion}

Our solution is quite simple to implement using a generic interface offered in
[`flaml.autogen`](<https://microsoft.github.io/autogen/docs/Use->
Cases/enhanced_inference#logic-error), yet the result is quite encouraging.

While the specific way of generating assertions is application-specific, the
main ideas are general in LLM operations:

* Generate multiple responses to select - especially useful when selecting a good response is relatively easier than generating a good response at one shot.
* Consider multiple configurations to generate responses - especially useful when:
  * Model and other inference parameter choice affect the utility-cost tradeoff; or
  * Different configurations have complementary effect.

A [previous blog post](.blog/2023/04/21/LLM-tuning-math) provides
evidence that these ideas are relevant in solving math problems too.
`flaml.autogen` uses a technique
[EcoOptiGen](https://arxiv.org/abs/2303.04673) to support inference parameter
tuning and model selection.

There are many directions of extensions in research and development:

* Generalize the way to provide feedback.
* Automate the process of optimizing the configurations.
* Build adaptive agents for different applications.

 _Do you find this approach applicable to your use case? Do you have any other
challenge to share about LLM applications? Do you like to see more support or
research of LLM optimization or automation? Please join
our[Discord](https://discord.gg/Cppx2vSPVP) server for discussion._

## For Further Reading​ {#for-further-reading}

* [Documentation](https://microsoft.github.io/autogen/) about `flaml.autogen` and [Research paper](https://arxiv.org/abs/2303.04673).
* [Blog post](.blog/2023/04/21/LLM-tuning-math) about a related study for math.

 **Tags:**

* [LLM](.blog/tags/llm)
* [GPT](.blog/tags/gpt)
* [research](.blog/tags/research)

[Newer Post« MathChat - An Conversational Framework to Solve Math
Problems](.blog/2023/06/28/MathChat)

[Older PostSurpassing 1 Million Downloads - A Retrospective and a Look into
the Future »](.blog/2023/05/07/1M-milestone)

* Observations
* Solution
* Experiment Results
* Discussion
* For Further Reading

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

Recent posts

* [All About Agent Descriptions](/autogen/blog/2023/12/29/AgentDescriptions)
* [AgentOptimizer - An Agentic Way to Train Your LLM Agent](/autogen/blog/2023/12/23/AgentOptimizer)
* [AutoGen Studio: Interactively Explore Multi-Agent Workflows](/autogen/blog/2023/12/01/AutoGenStudio)
* [Agent AutoBuild - Automatically Building Multi-agent Systems](/autogen/blog/2023/11/26/Agent-AutoBuild)
* [How to Assess Utility of LLM-powered Applications?](/autogen/blog/2023/11/20/AgentEval)
* [AutoGen Meets GPTs](/autogen/blog/2023/11/13/OAI-assistants)
* [EcoAssistant - Using LLM Assistants More Accurately and Affordably](/autogen/blog/2023/11/09/EcoAssistant)
* [Multimodal with GPT-4V and LLaVA](/autogen/blog/2023/11/06/LMM-Agent)
* [AutoGen's Teachable Agents](/autogen/blog/2023/10/26/TeachableAgent)
* [Retrieval-Augmented Generation (RAG) Applications with AutoGen](/autogen/blog/2023/10/18/RetrieveChat)
* [Use AutoGen for Local LLMs](/autogen/blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](/autogen/blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](/autogen/blog/2023/05/18/GPT-adaptive-humaneval)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](/autogen/blog/2023/04/21/LLM-tuning-math)

# One post tagged with "web" {#one-post-tagged-with-web}

[View All Tags](/autogen/blog/tags)

## [AutoGen Studio: Interactively Explore Multi-Agent {#autogen-studio-interactively-explore-multiagent}

Workflows](/autogen/blog/2023/12/01/AutoGenStudio)

December 1, 2023 · 8 min read

[![Victor
Dibia](https://github.com/victordibia.png)](https://github.com/victordibia)

[Victor Dibia](https://github.com/victordibia)

Principal RSDE at Microsoft Research

[![Gagan
Bansal](https://github.com/gagb.png)](<https://www.linkedin.com/in/gagan->
bansal/)

[Gagan Bansal](https://www.linkedin.com/in/gagan-bansal/)

Senior Researcher at Microsoft Research

[![Saleema
Amershi](https://github.com/samershi.png)](https://github.com/samershi)

[Saleema Amershi](https://github.com/samershi)

Senior Principal Research Manager at Microsoft Research

![AutoGen Studio Playground View: Solving a task with multiple agents that
generate a pdf document with
images.](/autogen/assets/images/autogenstudio_home-
cce78dc150d1bb0073620754df73d863.png)

 _AutoGen Studio: Solving a task with multiple agents that generate a pdf
document with images._

## TLDR​ {#tldr}

To help you rapidly prototype multi-agent solutions for your tasks, we are
introducing AutoGen Studio, an interface powered by
[AutoGen](https://github.com/microsoft/autogen/tree/main/autogen). It allows
you to:

* Declaratively define and modify agents and multi-agent workflows through a point and click, drag and drop interface (e.g., you can select the parameters of two agents that will communicate to solve your task).
* Use our UI to create chat sessions with the specified agents and view results (e.g., view chat history, generated files, and time taken).
* Explicitly add skills to your agents and accomplish more tasks.
* Publish your sessions to a local gallery.

AutoGen Studio is open source [code
here](<https://github.com/microsoft/autogen/tree/main/samples/apps/autogen->
studio), and can be installed via pip. Give it a try!

    pip install autogenstudio  

## Introduction​ {#introduction}

The accelerating pace of technology has ushered us into an era where digital
assistants (or agents) are becoming integral to our lives.
[AutoGen](https://github.com/microsoft/autogen/tree/main/autogen) has emerged
as a leading framework for orchestrating the power of agents. In the spirit of
expanding this frontier and democratizing this capability, we are thrilled to
introduce a new user-friendly interface: **AutoGen Studio**.

With AutoGen Studio, users can rapidly create, manage, and interact with
agents that can learn, adapt, and collaborate. As we release this interface
into the open-source community, our ambition is not only to enhance
productivity but to inspire a level of personalized interaction between humans
and agents.

> **Note** : AutoGen Studio is meant to help you rapidly prototype multi-
> agent workflows and demonstrate an example of end user interfaces built with
> AutoGen. It is not meant to be a production-ready app.

## Getting Started with AutoGen Studio​ {#getting-started-with-autogen-studio}

The following guide will help you get AutoGen Studio up and running on your
system.

### Configuring an LLM Provider​ {#configuring-an-llm-provider}

To get started, you need access to a language model. You can get this set up
by following the steps in the AutoGen documentation
[here](https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints).
Configure your environment with either `OPENAI_API_KEY` or
`AZURE_OPENAI_API_KEY`.

For example, in your terminal, you would set the API key like this:

    export OPENAI_API_KEY=<your_api_key>  

You can also specify the model directly in the agent's configuration as shown
below.

    llm_config = LLMConfig(  
        config_list=[{  
            "model": "gpt-4",  
            "api_key": "<azure_api_key>",  
            "base_url": "<azure api base url>",  
            "api_type": "azure",  
            "api_version": "2023-06-01-preview"  
        }],  
        temperature=0,  
    )  

### Installation​ {#installation}

  1. **Install from PyPi**

We recommend using a virtual environment (e.g., conda) to avoid conflicts with
existing Python packages. With Python 3.10 or newer active in your virtual
environment, use pip to install AutoGen Studio:

        pip install autogenstudio  

  2. **Install from Source**

> Note: This approach requires some familiarity with building interfaces in
> React.

If you prefer to install from source, ensure you have Python 3.10+ and Node.js
(version above 14.15.0) installed. Here's how you get started:

    * Clone the AutoGen Studio repository and install its Python dependencies:
        
                pip install -e .  

    * Navigate to the `samples/apps/autogen-studio/frontend` directory, install dependencies, and build the UI:
        
                npm install -g gatsby-cli  
        npm install --global yarn  
        cd frontend  
        yarn install  
        yarn build  

For Windows users, to build the frontend, you may need alternative commands
provided in the [autogen studio
readme](<https://github.com/microsoft/autogen/tree/main/samples/apps/autogen->
studio).

### Running the Application​ {#running-the-application}

Once installed, run the web UI by entering the following in your terminal:

    autogenstudio ui --port 8081  

This will start the application on the specified port. Open your web browser
and go to `http://localhost:8081/` to begin using AutoGen Studio.

Now that you have AutoGen Studio installed and running, you are ready to
explore its capabilities, including defining and modifying agent workflows,
interacting with agents and sessions, and expanding agent skills.

## What Can You Do with AutoGen Studio?​ {#what-can-you-do-with-autogen-studio}

The AutoGen Studio UI is organized into 3 high level sections - **Build** ,
**Playground** , and **Gallery**.

### Build​ {#build}

![Specify
Agents.](/autogen/assets/images/autogenstudio_config-5e0107b565503d4952c9b00dd3d5c10e.png)

This section focuses on defining the properties of agents and agent workflows.
It includes the following concepts:

 **Skills** : Skills are functions (e.g., Python functions) that describe how
to solve a task. In general, a good skill has a descriptive name (e.g.
`generate_images`), extensive docstrings and good defaults (e.g., writing out
files to disk for persistence and reuse). You can add new skills to AutoGen
Studio via the provided UI. At inference time, these skills are made available
to the assistant agent as they address your tasks.

![View and add
skills.](/autogen/assets/images/autogenstudio_skills-7376f0a5aec5917fda817a4793f2b89a.png)

 _AutoGen Studio Build View: View, add or edit skills that an agent can
leverage in addressing tasks._

 **Agents** : This provides an interface to declaratively specify properties
for an AutoGen agent (mirrors most of the members of a base [AutoGen
conversable
agent](https://github.com/microsoft/autogen/blob/main/autogen/agentchat/conversable_agent.py)
class).

 **Agent Workflows** : An agent workflow is a specification of a set of agents
that can work together to accomplish a task. The simplest version of this is a
setup with two agents – a user proxy agent (that represents a user i.e. it
compiles code and prints result) and an assistant that can address task
requests (e.g., generating plans, writing code, evaluating responses,
proposing error recovery steps, etc.). A more complex flow could be a group
chat where even more agents work towards a solution.

### Playground​ {#playground}

![AutoGen Studio Playground View: Solving a task with multiple agents that
generate a pdf document with
images.](/autogen/assets/images/autogenstudio_home-
cce78dc150d1bb0073620754df73d863.png)

 _AutoGen Studio Playground View: Agents collaborate, use available skills
(ability to generate images) to address a user task (generate pdf 's)._

The playground section is focused on interacting with agent workflows defined
in the previous build section. It includes the following concepts:

 **Session** : A session refers to a period of continuous interaction or
engagement with an agent workflow, typically characterized by a sequence of
activities or operations aimed at achieving specific objectives. It includes
the agent workflow configuration, the interactions between the user and the
agents. A session can be “published” to a “gallery”.

 **Chat View** : A chat is a sequence of interactions between a user and an
agent. It is a part of a session.

### Gallery​ {#gallery}

This section is focused on sharing and reusing artifacts (e.g., workflow
configurations, sessions, etc.).

AutoGen Studio comes with 3 example skills: `fetch_profile`, `find_papers`,
`generate_images`. Please feel free to review the repo to learn more about how
they work.

## The AutoGen Studio API​ {#the-autogen-studio-api}

While AutoGen Studio is a web interface, it is powered by an underlying python
API that is reusable and modular. Importantly, we have implemented an API
where agent workflows can be declaratively specified (in JSON), loaded and
run. An example of the current API is shown below. Please consult the [AutoGen
Studio repo](https://microsoft.github.io/autogen/docs/autogenstudio) for more
details.

    import json  
    from autogenstudio import AutoGenWorkFlowManager, AgentWorkFlowConfig  
      
    # load an agent specification in JSON  
    agent_spec = json.load(open('agent_spec.json'))  
      
    # Create an AutoGen Workflow Configuration from the agent specification  
    agent_work_flow_config = FlowConfig(**agent_spec)  
      
    # Create a Workflow from the configuration  
    agent_work_flow = AutoGenWorkFlowManager(agent_work_flow_config)  
      
    # Run the workflow on a task  
    task_query = "What is the height of the Eiffel Tower?"  
    agent_work_flow.run(message=task_query)  

## Road Map and Next Steps​ {#road-map-and-next-steps}

As we continue to develop and refine AutoGen Studio, the road map below
outlines an array of enhancements and new features planned for future
releases. Here's what users can look forward to:

* **Complex Agent Workflows** : We're working on integrating support for more sophisticated agent workflows, such as `GroupChat`, allowing for richer interaction between multiple agents or dynamic topologies.
* **Improved User Experience** : This includes features like streaming intermediate model output for real-time feedback, better summarization of agent responses, information on costs of each interaction. We will also invest in improving the workflow for composing and reusing agents. We will also explore support for more interactive human in the loop feedback to agents.
* **Expansion of Agent Skills** : We will work towards improving the workflow for authoring, composing and reusing agent skills.
* **Community Features** : Facilitation of sharing and collaboration within AutoGen Studio user community is a key goal. We're exploring options for sharing sessions and results more easily among users and contributing to a shared repository of skills, agents, and agent workflows.

### FAQ​ {#faq}

 **Q: Where can I adjust the default skills, agent and workflow
configurations?** A: You can modify agent configurations directly from the UI
or by editing the `autogentstudio/utils/dbdefaults.json` file which is used to
initialize the database.

 **Q: If I want to reset the entire conversation with an agent, how do I go
about it?** A: To reset your conversation history, you can delete the
`database.sqlite` file. If you need to clear user-specific data, remove the
relevant `autogenstudio/web/files/user/<user_id_md5hash>` folder.

 **Q: Is it possible to view the output and messages generated by the agents
during interactions?** A: Yes, you can view the generated messages in the
debug console of the web UI, providing insights into the agent interactions.
Alternatively, you can inspect the `database.sqlite` file for a comprehensive
record of messages.

 **Q: Where can I find documentation and support for AutoGen Studio?** A: We
are constantly working to improve AutoGen Studio. For the latest updates,
please refer to the [AutoGen Studio
Readme](<https://github.com/microsoft/autogen/tree/main/samples/apps/autogen->
studio). For additional support, please open an issue on
[GitHub](https://github.com/microsoft/autogen) or ask questions on
[Discord](https://discord.gg/pAbnFJrkgZ).

 **Tags:**

* [AutoGen](/autogen/blog/tags/auto-gen)
* [UI](/autogen/blog/tags/ui)
* [web](/autogen/blog/tags/web)
* [UX](/autogen/blog/tags/ux)

[**Read More**](/autogen/blog/2023/12/01/AutoGenStudio)

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

Recent posts

* [All About Agent Descriptions](/autogen/blog/2023/12/29/AgentDescriptions)
* [AgentOptimizer - An Agentic Way to Train Your LLM Agent](/autogen/blog/2023/12/23/AgentOptimizer)
* [AutoGen Studio: Interactively Explore Multi-Agent Workflows](/autogen/blog/2023/12/01/AutoGenStudio)
* [Agent AutoBuild - Automatically Building Multi-agent Systems](/autogen/blog/2023/11/26/Agent-AutoBuild)
* [How to Assess Utility of LLM-powered Applications?](/autogen/blog/2023/11/20/AgentEval)
* [AutoGen Meets GPTs](/autogen/blog/2023/11/13/OAI-assistants)
* [EcoAssistant - Using LLM Assistants More Accurately and Affordably](/autogen/blog/2023/11/09/EcoAssistant)
* [Multimodal with GPT-4V and LLaVA](/autogen/blog/2023/11/06/LMM-Agent)
* [AutoGen's Teachable Agents](/autogen/blog/2023/10/26/TeachableAgent)
* [Retrieval-Augmented Generation (RAG) Applications with AutoGen](/autogen/blog/2023/10/18/RetrieveChat)
* [Use AutoGen for Local LLMs](/autogen/blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](/autogen/blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](/autogen/blog/2023/05/18/GPT-adaptive-humaneval)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](/autogen/blog/2023/04/21/LLM-tuning-math)

# Retrieval-Augmented Generation (RAG) Applications with AutoGen {#retrievalaugmented-generation-rag-applications-with-autogen}

October 18, 2023 · 10 min read

[![Li Jiang](https://github.com/thinkall.png)](https://github.com/thinkall)

[Li Jiang](https://github.com/thinkall)

Senior Software Engineer at Microsoft

![RAG Architecture](/autogen/assets/images/retrievechat-
arch-959e180405c99ceb3da88a441c02f45e.png)

 **TL;DR:**

* We introduce **RetrieveUserProxyAgent** and **RetrieveAssistantAgent** , RAG agents of AutoGen that allows retrieval-augmented generation, and its basic usage.
* We showcase customizations of RAG agents, such as customizing the embedding function, the text split function and vector database.
* We also showcase two advanced usage of RAG agents, integrating with group chat and building a Chat application with Gradio.

## Introduction​ {#introduction}

Retrieval augmentation has emerged as a practical and effective approach for
mitigating the intrinsic limitations of LLMs by incorporating external
documents. In this blog post, we introduce RAG agents of AutoGen that allows
retrieval-augmented generation. The system consists of two agents: a
Retrieval-augmented User Proxy agent, called `RetrieveUserProxyAgent`, and a
Retrieval-augmented Assistant agent, called `RetrieveAssistantAgent`, both of
which are extended from built-in agents from AutoGen. The overall architecture
of the RAG agents is shown in the figure above.

To use Retrieval-augmented Chat, one needs to initialize two agents including
Retrieval-augmented User Proxy and Retrieval-augmented Assistant. Initializing
the Retrieval-Augmented User Proxy necessitates specifying a path to the
document collection. Subsequently, the Retrieval-Augmented User Proxy can
download the documents, segment them into chunks of a specific size, compute
embeddings, and store them in a vector database. Once a chat is initiated, the
agents collaboratively engage in code generation or question-answering
adhering to the procedures outlined below:

  1. The Retrieval-Augmented User Proxy retrieves document chunks based on the embedding similarity, and sends them along with the question to the Retrieval-Augmented Assistant.
  2. The Retrieval-Augmented Assistant employs an LLM to generate code or text as answers based on the question and context provided. If the LLM is unable to produce a satisfactory response, it is instructed to reply with “Update Context” to the Retrieval-Augmented User Proxy.
  3. If a response includes code blocks, the Retrieval-Augmented User Proxy executes the code and sends the output as feedback. If there are no code blocks or instructions to update the context, it terminates the conversation. Otherwise, it updates the context and forwards the question along with the new context to the Retrieval-Augmented Assistant. Note that if human input solicitation is enabled, individuals can proactively send any feedback, including Update Context”, to the Retrieval-Augmented Assistant.
  4. If the Retrieval-Augmented Assistant receives “Update Context”, it requests the next most similar chunks of documents as new context from the Retrieval-Augmented User Proxy. Otherwise, it generates new code or text based on the feedback and chat history. If the LLM fails to generate an answer, it replies with “Update Context” again. This process can be repeated several times. The conversation terminates if no more documents are available for the context.

## Basic Usage of RAG Agents​ {#basic-usage-of-rag-agents}

  0. Install dependencies

Please install pyautogen with the [retrievechat] option before using RAG
agents.

    pip install "pyautogen[retrievechat]"  

RetrieveChat can handle various types of documents. By default, it can process
plain text and PDF files, including formats such as 'txt', 'json', 'csv',
'tsv', 'md', 'html', 'htm', 'rtf', 'rst', 'jsonl', 'log', 'xml', 'yaml', 'yml'
and 'pdf'. If you install [unstructured](<https://unstructured->
io.github.io/unstructured/installation/full_installation.html) (`pip install
"unstructured[all-docs]"`), additional document types such as 'docx', 'doc',
'odt', 'pptx', 'ppt', 'xlsx', 'eml', 'msg', 'epub' will also be supported.

You can find a list of all supported document types by using
`autogen.retrieve_utils.TEXT_FORMATS`.

  1. Import Agents

    import autogen  
    from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent  
    from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent  

  2. Create an 'RetrieveAssistantAgent' instance named "assistant" and an 'RetrieveUserProxyAgent' instance named "ragproxyagent"

    assistant = RetrieveAssistantAgent(  
        name="assistant",  
        system_message="You are a helpful assistant.",  
        llm_config=llm_config,  
    )  
      
    ragproxyagent = RetrieveUserProxyAgent(  
        name="ragproxyagent",  
        retrieve_config={  
            "task": "qa",  
            "docs_path": "https://raw.githubusercontent.com/microsoft/autogen/main/README.md",  
        },  
    )  

  3. Initialize Chat and ask a question

    assistant.reset()  
    ragproxyagent.initiate_chat(assistant, problem="What is autogen?")  

Output is like:

    --------------------------------------------------------------------------------  
    assistant (to ragproxyagent):  
      
    AutoGen is a framework that enables the development of large language model (LLM) applications using multiple agents that can converse with each other to solve tasks. The agents are customizable, conversable, and allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools.  
      
    --------------------------------------------------------------------------------  

  4. Create a UserProxyAgent and ask the same question

    assistant.reset()  
    userproxyagent = autogen.UserProxyAgent(name="userproxyagent")  
    userproxyagent.initiate_chat(assistant, message="What is autogen?")  

Output is like:

    --------------------------------------------------------------------------------  
    assistant (to userproxyagent):  
      
    In computer software, autogen is a tool that generates program code automatically, without the need for manual coding. It is commonly used in fields such as software engineering, game development, and web development to speed up the development process and reduce errors. Autogen tools typically use pre-programmed rules, templates, and data to create code for repetitive tasks, such as generating user interfaces, database schemas, and data models. Some popular autogen tools include Visual Studio's Code Generator and Unity's Asset Store.  
      
    --------------------------------------------------------------------------------  

You can see that the output of `UserProxyAgent` is not related to our
`autogen` since the latest info of `autogen` is not in ChatGPT's training
data. The output of `RetrieveUserProxyAgent` is correct as it can perform
retrieval-augmented generation based on the given documentation file.

## Customizing RAG Agents​ {#customizing-rag-agents}

`RetrieveUserProxyAgent` is customizable with `retrieve_config`. There are
several parameters to configure based on different use cases. In this section,
we'll show how to customize embedding function, text split function and vector
database.

### Customizing Embedding Function​ {#customizing-embedding-function}

By default, [Sentence Transformers](https://www.sbert.net) and its pretrained
models will be used to compute embeddings. It's possible that you want to use
OpenAI, Cohere, HuggingFace or other embedding functions.

* OpenAI

    from chromadb.utils import embedding_functions  

    openai_ef = embedding_functions.OpenAIEmbeddingFunction(  
                    api_key="YOUR_API_KEY",  
                    model_name="text-embedding-ada-002"  
                )  

    ragproxyagent = RetrieveUserProxyAgent(  
        name="ragproxyagent",  
        retrieve_config={  
            "task": "qa",  
            "docs_path": "<https://raw.githubusercontent.com/microsoft/autogen/main/README.md>",  
            "embedding_function": openai_ef,  
        },  
    )  

* HuggingFace

    huggingface_ef = embedding_functions.HuggingFaceEmbeddingFunction(  
        api_key="YOUR_API_KEY",  
        model_name="sentence-transformers/all-MiniLM-L6-v2"  
    )  

More examples can be found [here](https://docs.trychroma.com/embeddings).

### Customizing Text Split Function​ {#customizing-text-split-function}

Before we can store the documents into a vector database, we need to split the
texts into chunks. Although we have implemented a flexible text splitter in
autogen, you may still want to use different text splitters. There are also
some existing text split tools which are good to reuse.

For example, you can use all the text splitters in langchain.

    from langchain.text_splitter import RecursiveCharacterTextSplitter  
      
    recur_spliter = RecursiveCharacterTextSplitter(separators=["\n", "\r", "\t"])  
      
    ragproxyagent = RetrieveUserProxyAgent(  
        name="ragproxyagent",  
        retrieve_config={  
            "task": "qa",  
            "docs_path": "https://raw.githubusercontent.com/microsoft/autogen/main/README.md",  
            "custom_text_split_function": recur_spliter.split_text,  
        },  
    )  

### Customizing Vector Database​ {#customizing-vector-database}

We are using chromadb as the default vector database, you can also replace it
with any other vector database by simply overriding the function
`retrieve_docs` of `RetrieveUserProxyAgent`.

For example, you can use Qdrant as below:

    # Creating qdrant client  
    from qdrant_client import QdrantClient  
      
    client = QdrantClient(url="***", api_key="***")  
      
    # Wrapping RetrieveUserProxyAgent  
    from litellm import embedding as test_embedding  
    from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent  
    from qdrant_client.models import SearchRequest, Filter, FieldCondition, MatchText  
      
    class QdrantRetrieveUserProxyAgent(RetrieveUserProxyAgent):  
        def query_vector_db(  
            self,  
            query_texts: List[str],  
            n_results: int = 10,  
            search_string: str = "",  
            **kwargs,  
        ) -> Dict[str, Union[List[str], List[List[str]]]]:  
            # define your own query function here  
            embed_response = test_embedding('text-embedding-ada-002', input=query_texts)  
      
            all_embeddings: List[List[float]] = []  
      
            for item in embed_response['data']:  
                all_embeddings.append(item['embedding'])  
      
            search_queries: List[SearchRequest] = []  
      
            for embedding in all_embeddings:  
                search_queries.append(  
                    SearchRequest(  
                        vector=embedding,  
                        filter=Filter(  
                            must=[  
                                FieldCondition(  
                                    key="page_content",  
                                    match=MatchText(  
                                        text=search_string,  
                                    )  
                                )  
                            ]  
                        ),  
                        limit=n_results,  
                        with_payload=True,  
                    )  
                )  
      
            search_response = client.search_batch(  
                collection_name="{your collection name}",  
                requests=search_queries,  
            )  
      
            return {  
                "ids": [[scored_point.id for scored_point in batch] for batch in search_response],  
                "documents": [[scored_point.payload.get('page_content', '') for scored_point in batch] for batch in search_response],  
                "metadatas": [[scored_point.payload.get('metadata', {}) for scored_point in batch] for batch in search_response]  
            }  
      
        def retrieve_docs(self, problem: str, n_results: int = 20, search_string: str = "", **kwargs):  
            results = self.query_vector_db(  
                query_texts=[problem],  
                n_results=n_results,  
                search_string=search_string,  
                **kwargs,  
            )  
      
            self._results = results  
      
      
    # Use QdrantRetrieveUserProxyAgent  
    qdrantragagent = QdrantRetrieveUserProxyAgent(  
        name="ragproxyagent",  
        human_input_mode="NEVER",  
        max_consecutive_auto_reply=2,  
        retrieve_config={  
            "task": "qa",  
        },  
    )  
      
    qdrantragagent.retrieve_docs("What is Autogen?", n_results=10, search_string="autogen")  

## Advanced Usage of RAG Agents​ {#advanced-usage-of-rag-agents}

### Integrate with other agents in a group chat​ {#integrate-with-other-agents-in-a-group-chat}

To use `RetrieveUserProxyAgent` in a group chat is almost the same as you use
it in a two agents chat. The only thing is that you need to **initialize the
chat with`RetrieveUserProxyAgent`**. The `RetrieveAssistantAgent` is not
necessary in a group chat.

However, you may want to initialize the chat with another agent in some cases.
To leverage the best of `RetrieveUserProxyAgent`, you'll need to call it from
a function.

    llm_config = {  
        "functions": [  
            {  
                "name": "retrieve_content",  
                "description": "retrieve content for code generation and question answering.",  
                "parameters": {  
                    "type": "object",  
                    "properties": {  
                        "message": {  
                            "type": "string",  
                            "description": "Refined message which keeps the original meaning and can be used to retrieve content for code generation and question answering.",  
                        }  
                    },  
                    "required": ["message"],  
                },  
            },  
        ],  
        "config_list": config_list,  
        "timeout": 60,  
        "seed": 42,  
    }  
      
    boss = autogen.UserProxyAgent(  
        name="Boss",  
        is_termination_msg=termination_msg,  
        human_input_mode="TERMINATE",  
        system_message="The boss who ask questions and give tasks.",  
    )  
      
    boss_aid = RetrieveUserProxyAgent(  
        name="Boss_Assistant",  
        is_termination_msg=termination_msg,  
        system_message="Assistant who has extra content retrieval power for solving difficult problems.",  
        human_input_mode="NEVER",  
        max_consecutive_auto_reply=3,  
        retrieve_config={  
            "task": "qa",  
        },  
        code_execution_config=False,  # we don't want to execute code in this case.  
    )  
      
    coder = AssistantAgent(  
        name="Senior_Python_Engineer",  
        is_termination_msg=termination_msg,  
        system_message="You are a senior python engineer. Reply `TERMINATE` in the end when everything is done.",  
        llm_config=llm_config,  
    )  
      
    pm = autogen.AssistantAgent(  
        name="Product_Manager",  
        is_termination_msg=termination_msg,  
        system_message="You are a product manager. Reply `TERMINATE` in the end when everything is done.",  
        llm_config=llm_config,  
    )  
      
    reviewer = autogen.AssistantAgent(  
        name="Code_Reviewer",  
        is_termination_msg=termination_msg,  
        system_message="You are a code reviewer. Reply `TERMINATE` in the end when everything is done.",  
        llm_config=llm_config,  
    )  
      
    def retrieve_content(message, n_results=3):  
            boss_aid.n_results = n_results  # Set the number of results to be retrieved.  
            # Check if we need to update the context.  
            update_context_case1, update_context_case2 = boss_aid._check_update_context(message)  
            if (update_context_case1 or update_context_case2) and boss_aid.update_context:  
                boss_aid.problem = message if not hasattr(boss_aid, "problem") else boss_aid.problem  
                _, ret_msg = boss_aid._generate_retrieve_user_reply(message)  
            else:  
                ret_msg = boss_aid.generate_init_message(message, n_results=n_results)  
            return ret_msg if ret_msg else message  
      
    for agent in [boss, coder, pm, reviewer]:  
        # register functions for all agents.  
        agent.register_function(  
            function_map={  
                "retrieve_content": retrieve_content,  
            }  
        )  
      
    groupchat = autogen.GroupChat(  
        agents=[boss, coder, pm, reviewer], messages=[], max_round=12  
    )  
    manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)  
      
    # Start chatting with the boss as this is the user proxy agent.  
    boss.initiate_chat(  
        manager,  
        message="How to use spark for parallel training in FLAML? Give me sample code.",  
    )  

### Build a Chat application with Gradio​ {#build-a-chat-application-with-gradio}

Now, let's wrap it up and make a Chat application with AutoGen and Gradio.

![RAG ChatBot with AutoGen](/autogen/assets/images/autogen-rag-
ee3b1d222f3e10b1707527cdea69be50.gif)

    # Initialize Agents  
    def initialize_agents(config_list, docs_path=None):  
        ...  
        return assistant, ragproxyagent  
      
    # Initialize Chat  
    def initiate_chat(config_list, problem, queue, n_results=3):  
        ...  
        assistant.reset()  
        try:  
            ragproxyagent.a_initiate_chat(  
                assistant, problem=problem, silent=False, n_results=n_results  
            )  
            messages = ragproxyagent.chat_messages  
            messages = [messages[k] for k in messages.keys()][0]  
            messages = [m["content"] for m in messages if m["role"] == "user"]  
            print("messages: ", messages)  
        except Exception as e:  
            messages = [str(e)]  
        queue.put(messages)  
      
    # Wrap AutoGen part into a function  
    def chatbot_reply(input_text):  
        """Chat with the agent through terminal."""  
        queue = mp.Queue()  
        process = mp.Process(  
            target=initiate_chat,  
            args=(config_list, input_text, queue),  
        )  
        process.start()  
        try:  
            messages = queue.get(timeout=TIMEOUT)  
        except Exception as e:  
            messages = [str(e) if len(str(e)) > 0 else "Invalid Request to OpenAI, please check your API keys."]  
        finally:  
            try:  
                process.terminate()  
            except:  
                pass  
        return messages  
      
    ...  
      
    # Set up UI with Gradio  
    with gr.Blocks() as demo:  
        ...  
        assistant, ragproxyagent = initialize_agents(config_list)  
      
        chatbot = gr.Chatbot(  
            [],  
            elem_id="chatbot",  
            bubble_full_width=False,  
            avatar_images=(None, (os.path.join(os.path.dirname(__file__), "autogen.png"))),  
            # height=600,  
        )  
      
        txt_input = gr.Textbox(  
            scale=4,  
            show_label=False,  
            placeholder="Enter text and press enter",  
            container=False,  
        )  
      
        with gr.Row():  
            txt_model = gr.Dropdown(  
                label="Model",  
                choices=[  
                    "gpt-4",  
                    "gpt-35-turbo",  
                    "gpt-3.5-turbo",  
                ],  
                allow_custom_value=True,  
                value="gpt-35-turbo",  
                container=True,  
            )  
            txt_oai_key = gr.Textbox(  
                label="OpenAI API Key",  
                placeholder="Enter key and press enter",  
                max_lines=1,  
                show_label=True,  
                value=os.environ.get("OPENAI_API_KEY", ""),  
                container=True,  
                type="password",  
            )  
            ...  
      
        clear = gr.ClearButton([txt_input, chatbot])  
      
    ...  
      
    if __name__ == "__main__":  
        demo.launch(share=True)  

The online app and the source code are hosted in
[HuggingFace](https://huggingface.co/spaces/thinkall/autogen-demos). Feel free
to give it a try!

## Read More​ {#read-more}

You can check out more example notebooks for RAG use cases:

* [Automated Code Generation and Question Answering with Retrieval Augmented Agents](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_RetrieveChat.ipynb)
* [Group Chat with Retrieval Augmented Generation (with 5 group member agents and 1 manager agent)](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_groupchat_RAG.ipynb)
* [Automated Code Generation and Question Answering with Qdrant based Retrieval Augmented Agents](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_qdrant_RetrieveChat.ipynb)

 **Tags:**

* [LLM](/autogen/blog/tags/llm)
* [RAG](/autogen/blog/tags/rag)

[Newer Post« AutoGen's Teachable
Agents](/autogen/blog/2023/10/26/TeachableAgent)

[Older PostUse AutoGen for Local LLMs »](/autogen/blog/2023/07/14/Local-LLMs)

* Introduction
* Basic Usage of RAG Agents
* Customizing RAG Agents
  * Customizing Embedding Function
  * Customizing Text Split Function
  * Customizing Vector Database
* Advanced Usage of RAG Agents
  * Integrate with other agents in a group chat
  * Build a Chat application with Gradio
* Read More

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* [Getting Started](.docs/Getting-Started)
* [Installation](.docs/Installation)
* Use Cases
* Examples
  * [AutoGen - Automated Multi Agent Chat](.docs/Examples/AutoGen-AgentChat)
  * [AutoGen - Tune GPT Models](.docs/Examples/AutoGen-OpenAI)
  * [AutoML - Classification](.docs/Examples/AutoML-Classification)
  * [AutoML - NLP](.docs/Examples/AutoML-NLP)
  * [AutoML - Rank](.docs/Examples/AutoML-Rank)
  * [AutoML - Regression](.docs/Examples/AutoML-Regression)
  * [AutoML - Time Series Forecast](.docs/Examples/AutoML-Time series forecast)
  * [AutoML for LightGBM](.docs/Examples/AutoML-for-LightGBM)
  * [AutoML for XGBoost](.docs/Examples/AutoML-for-XGBoost)
  * [Default - Flamlized Estimator](.docs/Examples/Default-Flamlized)
  * [Integrate - AzureML](.docs/Examples/Integrate - AzureML)
  * [Integrate - Scikit-learn Pipeline](.docs/Examples/Integrate - Scikit-learn Pipeline)
  * [Integrate - Spark](.docs/Examples/Integrate - Spark)
  * [Tune - AzureML pipeline](.docs/Examples/Tune-AzureML-pipeline)
  * [Tune - HuggingFace](.docs/Examples/Tune-HuggingFace)
  * [Tune - Lexicographic Objectives](.docs/Examples/Tune-Lexicographic-objectives)
  * [Tune - PyTorch](.docs/Examples/Tune-PyTorch)
* [Contributing](.docs/Contribute)
* [Research](.docs/Research)

On this page

# Integrate - AzureML {#integrate-azureml}

FLAML can be used together with AzureML. On top of that, using mlflow and ray
is easy too.

### Prerequisites​ {#prerequisites}

Install the [automl,azureml] option.

    pip install "flaml[automl,azureml]"  

Setup a AzureML workspace:

    from azureml.core import Workspace  
      
    ws = Workspace.create(name='myworkspace', subscription_id='<azure-subscription-id>', resource_group='myresourcegroup')  

### Enable mlflow in AzureML workspace​ {#enable-mlflow-in-azureml-workspace}

    import mlflow  
    from azureml.core import Workspace  
      
    ws = Workspace.from_config()  
    mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())  

### Start an AutoML run​ {#start-an-automl-run}

    from flaml.automl.data import load_openml_dataset  
    from flaml import AutoML  
      
    # Download [Airlines dataset](https://www.openml.org/d/1169) from OpenML. The task is to predict whether a given flight will be delayed, given the information of the scheduled departure.  
    X_train, X_test, y_train, y_test = load_openml_dataset(dataset_id=1169, data_dir="./")  
      
    automl = AutoML()  
    settings = {  
        "time_budget": 60,  # total running time in seconds  
        "metric": "accuracy",  # metric to optimize  
        "task": "classification",  # task type  
        "log_file_name": "airlines_experiment.log",  # flaml log file  
    }  
    experiment = mlflow.set_experiment("flaml")  # the experiment name in AzureML workspace  
    with mlflow.start_run() as run:  # create a mlflow run  
        automl.fit(X_train=X_train, y_train=y_train, **settings)  
        mlflow.sklearn.log_model(automl, "automl")  

The metrics in the run will be automatically logged in an experiment named
"flaml" in your AzureML workspace. They can be retrieved by
`mlflow.search_runs`:

    mlflow.search_runs(experiment_ids=[experiment.experiment_id], filter_string="params.learner = 'xgboost'")  

The logged model can be loaded and used to make predictions:

    automl = mlflow.sklearn.load_model(f"{run.info.artifact_uri}/automl")  
    print(automl.predict(X_test))  

[Link to
notebook](https://github.com/microsoft.blob/main/notebook/integrate_azureml.ipynb)
| [Open in
colab](https://colab.research.google.com/github/microsoft.blob/main/notebook/integrate_azureml.ipynb)

### Use ray to distribute across a cluster​ {#use-ray-to-distribute-across-a-cluster}

When you have a compute cluster in AzureML, you can distribute `flaml.AutoML`
or `flaml.tune` with ray.

#### Build a ray environment in AzureML​ {#build-a-ray-environment-in-azureml}

Create a docker file such as [.Docker/Dockerfile-
cpu](<https://github.com/microsoft.blob/main/test/.Docker/Dockerfile->
cpu). Make sure `RUN pip install flaml[blendsearch,ray]` is included in the
docker file.

Then build a AzureML environment in the workspace `ws`.

    ray_environment_name = "aml-ray-cpu"  
    ray_environment_dockerfile_path = "./Docker/Dockerfile-cpu"  
      
    # Build CPU image for Ray  
    ray_cpu_env = Environment.from_dockerfile(name=ray_environment_name, dockerfile=ray_environment_dockerfile_path)  
    ray_cpu_env.register(workspace=ws)  
    ray_cpu_build_details = ray_cpu_env.build(workspace=ws)  
      
    import time  
    while ray_cpu_build_details.status not in ["Succeeded", "Failed"]:  
        print(f"Awaiting completion of ray CPU environment build. Current status is: {ray_cpu_build_details.status}")  
        time.sleep(10)  

You only need to do this step once for one workspace.

#### Create a compute cluster with multiple nodes​ {#create-a-compute-cluster-with-multiple-nodes}

    from azureml.core.compute import AmlCompute, ComputeTarget  
      
    compute_target_name = "cpucluster"  
    node_count = 2  
      
    # This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6  
    compute_target_size = "STANDARD_D2_V2"  
      
    if compute_target_name in ws.compute_targets:  
        compute_target = ws.compute_targets[compute_target_name]  
        if compute_target and type(compute_target) is AmlCompute:  
            if compute_target.provisioning_state == "Succeeded":  
                print("Found compute target; using it:", compute_target_name)  
            else:  
                raise Exception(  
                    "Found compute target but it is in state", compute_target.provisioning_state)  
    else:  
        print("creating a new compute target...")  
        provisioning_config = AmlCompute.provisioning_configuration(  
            vm_size=compute_target_size,  
            min_nodes=0,  
            max_nodes=node_count)  
      
        # Create the cluster  
        compute_target = ComputeTarget.create(ws, compute_target_name, provisioning_config)  
      
        # Can poll for a minimum number of nodes and for a specific timeout.  
        # If no min node count is provided it will use the scale settings for the cluster  
        compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)  
      
        # For a more detailed view of current AmlCompute status, use get_status()  
        print(compute_target.get_status().serialize())  

If the computer target "cpucluster" already exists, it will not be recreated.

#### Run distributed AutoML job​ {#run-distributed-automl-job}

Assuming you have an automl script like
[ray/distribute_automl.py](https://github.com/microsoft.blob/main/test/ray/distribute_automl.py).
It uses `n_concurrent_trials=k` to inform `AutoML.fit()` to perform k
concurrent trials in parallel.

Submit an AzureML job as the following:

    from azureml.core import Workspace, Experiment, ScriptRunConfig, Environment  
    from azureml.core.runconfig import RunConfiguration, DockerConfiguration  
      
    command = ["python distribute_automl.py"]  
    ray_environment_name = "aml-ray-cpu"  
    env = Environment.get(workspace=ws, name=ray_environment_name)  
    aml_run_config = RunConfiguration(communicator="OpenMpi")  
    aml_run_config.target = compute_target  
    aml_run_config.docker = DockerConfiguration(use_docker=True)  
    aml_run_config.environment = env  
    aml_run_config.node_count = 2  
    config = ScriptRunConfig(  
        source_directory="ray/",  
        command=command,  
        run_config=aml_run_config,  
    )  
      
    exp = Experiment(ws, "distribute-automl")  
    run = exp.submit(config)  
      
    print(run.get_portal_url())  # link to ml.azure.com  
    run.wait_for_completion(show_output=True)  

#### Run distributed tune job​ {#run-distributed-tune-job}

Prepare a script like
[ray/distribute_tune.py](https://github.com/microsoft.blob/main/test/ray/distribute_tune.py).
Replace the command in the above eample with:

    command = ["python distribute_tune.py"]  

Everything else is the same.

[Edit this
page](<https://github.com/microsoft.edit/main/website/docs/Examples/Integrate>

* AzureML.md)

[Previous« Default - Flamlized Estimator](.docs/Examples/Default-
Flamlized)

[NextIntegrate - Scikit-learn Pipeline »](.docs/Examples/Integrate -
Scikit-learn Pipeline)

* Prerequisites
* Enable mlflow in AzureML workspace
* Start an AutoML run
* Use ray to distribute across a cluster

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* [Getting Started](.docs/Getting-Started)
* [Installation](.docs/Installation)
* Use Cases
* Examples
  * [AutoGen - Automated Multi Agent Chat](.docs/Examples/AutoGen-AgentChat)
  * [AutoGen - Tune GPT Models](.docs/Examples/AutoGen-OpenAI)
  * [AutoML - Classification](.docs/Examples/AutoML-Classification)
  * [AutoML - NLP](.docs/Examples/AutoML-NLP)
  * [AutoML - Rank](.docs/Examples/AutoML-Rank)
  * [AutoML - Regression](.docs/Examples/AutoML-Regression)
  * [AutoML - Time Series Forecast](.docs/Examples/AutoML-Time series forecast)
  * [AutoML for LightGBM](.docs/Examples/AutoML-for-LightGBM)
  * [AutoML for XGBoost](.docs/Examples/AutoML-for-XGBoost)
  * [Default - Flamlized Estimator](.docs/Examples/Default-Flamlized)
  * [Integrate - AzureML](.docs/Examples/Integrate - AzureML)
  * [Integrate - Scikit-learn Pipeline](.docs/Examples/Integrate - Scikit-learn Pipeline)
  * [Integrate - Spark](.docs/Examples/Integrate - Spark)
  * [Tune - AzureML pipeline](.docs/Examples/Tune-AzureML-pipeline)
  * [Tune - HuggingFace](.docs/Examples/Tune-HuggingFace)
  * [Tune - Lexicographic Objectives](.docs/Examples/Tune-Lexicographic-objectives)
  * [Tune - PyTorch](.docs/Examples/Tune-PyTorch)
* [Contributing](.docs/Contribute)
* [Research](.docs/Research)

# AutoGen - Tune GPT Models {#autogen-tune-gpt-models}

`flaml.autogen` offers a cost-effective hyperparameter optimization technique
[EcoOptiGen](https://arxiv.org/abs/2303.04673) for tuning Large Language
Models. The research study finds that tuning hyperparameters can significantly
improve the utility of them. Please find documentation about this feature
[here](<https://microsoft.github.io/autogen/docs/Use-Cases/#enhanced->
inference).

Links to notebook examples:

* [Optimize for Code Generation](https://github.com/microsoft.blob/main/notebook/autogen_openai_completion.ipynb) | [Open in colab](https://colab.research.google.com/github/microsoft.blob/main/notebook/autogen_openai_completion.ipynb)
* [Optimize for Math](https://github.com/microsoft.blob/main/notebook/autogen_chatgpt_gpt4.ipynb) | [Open in colab](https://colab.research.google.com/github/microsoft.blob/main/notebook/autogen_chatgpt_gpt4.ipynb)

[Edit this
page](<https://github.com/microsoft.edit/main/website/docs/Examples/AutoGen->
OpenAI.md)

[Previous« AutoGen - Automated Multi Agent Chat](.docs/Examples/AutoGen-
AgentChat)

[NextAutoML - Classification »](.docs/Examples/AutoML-Classification)

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

Recent posts

* [All About Agent Descriptions](/autogen/blog/2023/12/29/AgentDescriptions)
* [AgentOptimizer - An Agentic Way to Train Your LLM Agent](/autogen/blog/2023/12/23/AgentOptimizer)
* [AutoGen Studio: Interactively Explore Multi-Agent Workflows](/autogen/blog/2023/12/01/AutoGenStudio)
* [Agent AutoBuild - Automatically Building Multi-agent Systems](/autogen/blog/2023/11/26/Agent-AutoBuild)
* [How to Assess Utility of LLM-powered Applications?](/autogen/blog/2023/11/20/AgentEval)
* [AutoGen Meets GPTs](/autogen/blog/2023/11/13/OAI-assistants)
* [EcoAssistant - Using LLM Assistants More Accurately and Affordably](/autogen/blog/2023/11/09/EcoAssistant)
* [Multimodal with GPT-4V and LLaVA](/autogen/blog/2023/11/06/LMM-Agent)
* [AutoGen's Teachable Agents](/autogen/blog/2023/10/26/TeachableAgent)
* [Retrieval-Augmented Generation (RAG) Applications with AutoGen](/autogen/blog/2023/10/18/RetrieveChat)
* [Use AutoGen for Local LLMs](/autogen/blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](/autogen/blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](/autogen/blog/2023/05/18/GPT-adaptive-humaneval)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](/autogen/blog/2023/04/21/LLM-tuning-math)

# AutoGen's Teachable Agents {#autogens-teachable-agents}

October 26, 2023 · 17 min read

[![Ricky Loynd](https://github.com/rickyloynd-
microsoft.png)](https://github.com/rickyloynd-microsoft)

[Ricky Loynd](https://github.com/rickyloynd-microsoft)

Senior Research Engineer at Microsoft

![Teachable Agent Architecture](/autogen/assets/images/teachable-
arch-2d85f7f511a1677c5d5fb19af7ab91ad.png)

 **TL;DR:**

* We introduce **Teachable Agents** so that users can teach their LLM-based assistants new facts, preferences, and skills.
* We showcase examples of teachable agents learning and later recalling facts, preferences, and skills in subsequent chats.

## Introduction​ {#introduction}

Conversational assistants based on LLMs can remember the current chat with the
user, and can also demonstrate in-context learning of user teachings during
the conversation. But the assistant's memories and learnings are lost once the
chat is over, or when a single chat grows too long for the LLM to handle
effectively. Then in subsequent chats the user is forced to repeat any
necessary instructions over and over.

`Teachability` addresses these limitations by persisting user teachings across
chat boundaries in long-term memory implemented as a vector database. Instead
of copying all of memory into the context window, which would eat up valuable
space, individual memories (called memos) are retrieved into context as
needed. This allows the user to teach frequently used facts and skills to the
teachable agent just once, and have it recall them in later chats.

Any instantiated `agent` that inherits from `ConversableAgent` can be made
teachable by instantiating a `Teachability` object and calling its
`add_to_agent(agent)` method. In order to make effective decisions about memo
storage and retrieval, the `Teachability` object calls an instance of
`TextAnalyzerAgent` (another AutoGen agent) to identify and reformulate text
as needed for remembering facts, preferences, and skills. Note that this adds
extra LLM calls involving a relatively small number of tokens, which can add a
few seconds to the time a user waits for each response.

## Run It Yourself​ {#run-it-yourself}

AutoGen contains four code examples that use `Teachability`.

  1. Run [chat_with_teachable_agent.py](https://github.com/microsoft/autogen/blob/main/test/agentchat/contrib/chat_with_teachable_agent.py) to converse with a teachable agent.

  2. Run [test_teachable_agent.py](https://github.com/microsoft/autogen/blob/main/test/agentchat/contrib/test_teachable_agent.py) for quick unit testing of a teachable agent.

  3. Use the Jupyter notebook [agentchat_teachability.ipynb](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_teachability.ipynb) to step through examples discussed below.

  4. Use the Jupyter notebook [agentchat_teachable_oai_assistants.ipynb](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_teachable_oai_assistants.ipynb) to make arbitrary OpenAI Assistants teachable through `GPTAssistantAgent`.

## Basic Usage of Teachability​ {#basic-usage-of-teachability}

  1. Install dependencies

Please install pyautogen with the [teachable] option before using
`Teachability`.

    pip install "pyautogen[teachable]"  

  2. Import agents

    from autogen import UserProxyAgent, config_list_from_json  
    from autogen.agentchat.contrib.capabilities.teachability import Teachability  
    from autogen import ConversableAgent  # As an example  

  3. Create llm_config

    # Load LLM inference endpoints from an env variable or a file  
    # See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints  
    # and OAI_CONFIG_LIST_sample  
    filter_dict = {"model": ["gpt-4"]}  # GPT-3.5 is less reliable than GPT-4 at learning from user feedback.  
    config_list = config_list_from_json(env_or_file="OAI_CONFIG_LIST", filter_dict=filter_dict)  
    llm_config={"config_list": config_list, "timeout": 120}  

  4. Create the agents

    # Start by instantiating any agent that inherits from ConversableAgent, which we use directly here for simplicity.  
    teachable_agent = ConversableAgent(  
        name="teachable_agent",  # The name can be anything.  
        llm_config=llm_config  
    )  
      
    # Instantiate a Teachability object. Its parameters are all optional.  
    teachability = Teachability(  
        reset_db=False,  # Use True to force-reset the memo DB, and False to use an existing DB.  
        path_to_db_dir="./tmp/interactive/teachability_db"  # Can be any path, but teachable agents in a group chat require unique paths.  
    )  
      
    # Now add teachability to the agent.  
    teachability.add_to_agent(teachable_agent)  
      
    # For this test, create a user proxy agent as usual.  
    user = UserProxyAgent("user", human_input_mode="ALWAYS")  

  5. Chat with the teachable agent

    # This function will return once the user types 'exit'.  
    teachable_agent.initiate_chat(user, message="Hi, I'm a teachable user assistant! What's on your mind?")  

## Example 1 - Learning user info​ {#example-1-learning-user-info}

A user can teach the agent facts about themselves. (Note that due to their
finetuning, LLMs can be reluctant to admit that they know personal
information.)

    Loading previous memory (if any) from disk.  
    teachable_agent (to user):  
      
    Greetings, I'm a teachable user assistant! What's on your mind today?  
      
    --------------------------------------------------------------------------------  
    Provide feedback to teachable_agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: My name is Ricky  
    user (to teachable_agent):  
      
    My name is Ricky  
      
    --------------------------------------------------------------------------------  
      
    >>>>>>>> USING AUTO REPLY...  
    teachable_agent (to user):  
      
    Hello Ricky! How can I assist you today?  
      
    --------------------------------------------------------------------------------  
    Provide feedback to teachable_agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: exit  

In a later conversation, the user can check whether the teachable agent
remembers their name. (For readability, the user prompts and some logged
notices are not repeated below.)

    --------------------------------------------------------------------------------  
    user (to teachable_agent):  
      
    what's my name?  
      
    --------------------------------------------------------------------------------  
    teachable_agent (to user):  
      
    Hi Ricky! How can I help you today?  
      
    --------------------------------------------------------------------------------  

## Example 2 - Learning new facts​ {#example-2-learning-new-facts}

A user can teach the agent more complex, related facts.

    --------------------------------------------------------------------------------  
    user (to teachable_agent):  
      
    What is the Vicuna model?  
      
    --------------------------------------------------------------------------------  
    teachable_agent (to user):  
      
    I'm sorry, but I don't have information about the "Vicuna model" in my current database. Could you please provide some context, description, or details about this model so I can better assist you?  
      
    --------------------------------------------------------------------------------  
    user (to teachable_agent):  
      
    Vicuna is a 13B-parameter language model released by Meta.  
      
    --------------------------------------------------------------------------------  
    teachable_agent (to user):  
      
    Thank you for providing the information. The Vicuna model is a large-scale language model developed by Meta. It consists of 13 billion parameters, which is what makes it able to understand and generate human-like text across various topics. Parameters in a neural network are the weights and biases assigned to each connection between the neurons, and having more parameters enables the model to learn and capture more complex patterns and relationships in data. As an AI language model, Vicuna can be utilized in various language-related tasks, such as text completion, translation, summarization, and more.  
      
    --------------------------------------------------------------------------------  
    user (to teachable_agent):  
      
    What is the Orca model?  
      
    --------------------------------------------------------------------------------  
    teachable_agent (to user):  
      
    The Orca model is a yet-to-be-released language model by Meta, referring to the Research Preview API release of Meta's advanced language model. While I don't have specific details about the Orca model at this time, it is expected that the model would have an even larger number of parameters to further enhance its capabilities compared to its predecessor, Vicuna.  
      
    As AI language models evolve, they generally become more efficient at understanding and generating human-like text. This makes them more effective in completing a wide range of tasks, including text completion, translation, summarization, and Q&A, as well as providing more accurate and coherent results in various applications.  
      
    --------------------------------------------------------------------------------  
    user (to teachable_agent):  
      
    Orca is a 13B-parameter language model released by Microsoft. It outperforms Vicuna on most tasks.  
      
    --------------------------------------------------------------------------------  
    teachable_agent (to user):  
      
    Thank you for the clarification. Orca is a 13-billion-parameter language model developed by Microsoft. It is designed to outperform Vicuna, the language model released by Meta. With a large number of parameters, Orca is capable of understanding and generating human-like text across diverse subjects, making it highly useful in various applications.  
      
    Orca's advanced capabilities allow it to excel in tasks such as text completion, translation, summarization, Q&A, and more. Its performance improvement over Vicuna highlights the rapid advancements in natural language processing (NLP) research and the development of increasingly capable AI language models.  

Then in a later chat the teachable agent can answer questions about the facts
it has been taught. (Remember to first close the previous chat by typing
'exit'.)

    --------------------------------------------------------------------------------  
    user (to teachable_agent):  
      
    How does the Vicuna model compare to the Orca model?  
      
    --------------------------------------------------------------------------------  
    teachable_agent (to user):  
      
    The Vicuna and Orca models are both 13B-parameter language models released by Meta and Microsoft, respectively. In terms of performance, Orca has been reported to outperform Vicuna on most tasks. However, without detailed information about specific tasks and benchmarks, it is difficult to provide a more comprehensive comparison. Generally speaking, both models are advanced language models that aim to provide high-quality natural language understanding and generation, but Orca appears to have an edge in terms of overall performance.  
      
    --------------------------------------------------------------------------------  

## Example 3 - Learning user preferences​ {#example-3-learning-user-preferences}

A user can teach the agent how they prefer to have things done.

Be aware that a message like the next one cannot be entered as a single
message through a command line because it contains a newline character. Such
messages can be entered in a Jupyter notebook, or through some UI layer like
that of ChatGPT.

    --------------------------------------------------------------------------------  
    user (to teachable_agent):  
      
    Please summarize this abstract.  
      
    AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation  
    Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang  
    AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.  
      
    --------------------------------------------------------------------------------  
    teachable_agent (to user):  
      
    AutoGen is an open-source framework designed to enable developers to create LLM applications with multiple customizable agents that can converse with each other to complete tasks. These agents can operate using different combinations of LLMs, human inputs, and tools, allowing developers to define agent interaction behaviors flexibly. AutoGen supports programming flexible conversation patterns using both natural language and code, making it suitable for building diverse applications with varying complexities and LLM capacities. Its effectiveness has been demonstrated through empirical studies across various domains including mathematics, coding, operations research, decision-making, and entertainment.  
      
    --------------------------------------------------------------------------------  
    Please summarize this abstract. When I'm summarizing an abstract, I try to make the summary contain just three short bullet points:  the title, the innovation, and the key empirical results.  
      
    AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation  
    Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang  
    AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.  
      
    --------------------------------------------------------------------------------  
    teachable_agent (to user):  
      
    - Title: AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation  
    - Innovation: Open-source framework for creating customizable LLM applications through agent conversations, supporting various modes and interaction behaviors.  
    - Key Empirical Results: Demonstrated effectiveness across diverse application domains, including mathematics, coding, question answering, and more.  
      
    --------------------------------------------------------------------------------  

Then in later chats the teacher doesn't need to reiterate their detailed
preferences.

    --------------------------------------------------------------------------------  
    user (to teachable_agent):  
      
    Please summarize this abstract.  
      
    Sparks of Artificial General Intelligence: Early experiments with GPT-4  
    Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang  
    Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.  
      
    --------------------------------------------------------------------------------  
    teachable_agent (to user):  
      
    - Title: Sparks of Artificial General Intelligence: Early experiments with GPT-4  
    - Innovation: GPT-4, an LLM with remarkable capabilities, demonstrates human-level performance across various domains, like math, coding, vision, medicine, law, and psychology.  
    - Key results: GPT-4 significantly surpasses prior models, suggesting it may be an early version of AGI; limitations and challenges toward deeper AGI are also discussed.  
      
    --------------------------------------------------------------------------------  

## Example 4 - Learning new skills​ {#example-4-learning-new-skills}

Users can extend the teachable agent's capabilities by teaching it new skills
for accomplishing challenging tasks. It usually works best to first describe
the task, then (in the same turn) provide a hint or advice for approaching the
task.

The [Sparks of AGI](https://arxiv.org/abs/2303.12712) paper evaluated GPT-4 on
math problems like the following, which it could only solve 32% of the time.
We first show a failure case, then teach the agent a strategy which lifts
GPT-4's success rate above 95%.

    --------------------------------------------------------------------------------  
    user (to teachable_agent):  
      
    Consider the identity:  
    9 * 4 + 6 * 6 = 72  
    Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?  
    -Let's think step-by-step, write down a plan, and then write down your solution as: "The solution is: A * B + C * D".  
      
    --------------------------------------------------------------------------------  
    Step 1: Identify the current equation and the target value on the right-hand side.  
    Current equation: 9 * 4 + 6 * 6 = 72  
    Target value: 99  
      
    Step 2: Determine what numbers can be changed on the left-hand side.  
    Possible changes: 9, 4, 6 (first occurrence), and 6 (second occurrence)  
      
    Step 3: Evaluate whether each change in the numbers on the left-hand side can result in the target value.  
    9 -> If changed, it would create a larger product.  
    4 -> If changed, the product would increase or decrease by more than the amount needed to reach 99.  
    6 (first occurrence) -> If increased to 7, then the equation would be 9 * 4 + 7 * 6 = 94, which is still not 99.  
    6 (second occurrence) -> If increased to 7, then the equation would be 9 * 4 + 6 * 7 = 99.  
      
    Step 4: Write down the solution.  
    The solution is: 9 * 4 + 6 * 7  
      
    --------------------------------------------------------------------------------  
    user (to teachable_agent):  
      
    Consider the identity:  
    9 * 4 + 6 * 6 = 72  
    Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?  
    -Let's think step-by-step, write down a plan, and then write down your solution as: "The solution is: A * B + C * D".  
      
    Here's some advice that may help:  
    1. Let E denote the original number on the right.  
    2. Let F denote the final number on the right.  
    3. Calculate the difference between the two, G = F - E.  
    4. Examine the numbers on the left one by one until finding one that divides evenly into G, where negative integers are allowed.  
    5. Calculate J = G / H. This is the number of times that H divides into G.  
    6. Verify that J is an integer, and that H * J = G.  
    7. Find the number on the left which is multiplied by H, and call it K.  
    8. Change K to K + J.  
    9. Recompute the value on the left, and verify that it equals F.  
    Finally, write down your solution as: "The solution is: A * B + C * D".  
      
    --------------------------------------------------------------------------------  
    teachable_agent (to user):  
      
    Step 1: Identify the original number (E) and the final number (F) on the right-hand side.  
    E = 72  
    F = 99  
      
    Step 2: Calculate the difference between the two, G = F - E.  
    G = 99 - 72 = 27  
      
    Step 3: Examine the numbers on the left one by one until finding one that divides evenly into G, where negative integers are allowed.  
    9 (divides evenly into 27)  
    4 (does not)  
    6 (first occurrence, does not)  
    6 (second occurrence, does not)  
      
    Step 4: Calculate J = G / H. This is the number of times that H divides into G.  
    H = 9 (since 9 divides evenly into 27)  
    J = 27 / 9 = 3  
      
    Step 5: Verify that J is an integer, and that H * J = G.  
    H * J = 9 * 3 = 27  
    J = 3 (which is an integer)  
      
    Step 6: Find the number on the left which is multiplied by H, and call it K.  
    Number on the leftside of the equation that is multiplied by 9: 4  
    Therefore, K = 4  
      
    Step 7: Change K to K + J.  
    K + J = 4 + 3 = 7  
      
    Step 8: Recompute the value on the left, and verify that it equals F.  
    9 * 7 + 6 * 6 = 63 + 36 = 99 (which is equal to F)  
      
    Step 9: Write down the solution.  
    The solution is: 9 * 7 + 6 * 6  
      
    --------------------------------------------------------------------------------  

In a later chat the user doesn't need to repeat the detailed advice.

    --------------------------------------------------------------------------------  
    user (to teachable_agent):  
      
    Consider the identity:  
    8 * 3 + 7 * 9 = 87  
    Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 59?  
    -Let's think step-by-step, write down a plan, and then write down your solution as: "The solution is: A * B + C * D".  
      
    --------------------------------------------------------------------------------  
    teachable_agent (to user):  
      
    Let's follow the advice to modify one integer on the left hand side of the equation to make the right hand side become 59:  
      
    1. Let E denote the original number on the right: E = 87.  
    2. Let F denote the final number on the right: F = 59.  
    3. Calculate the difference between the two, G = F - E: G = 59 - 87 = -28.  
    4. Examine the numbers on the left one by one to find one that divides evenly into G: 8, 3, 7, 9. Since 7 divides -28 evenly, we will work with 7 (H = 7).  
    5. Calculate J = G / H: J = -28 / 7 = -4.  
    6. Verify that J is an integer, and that H * J = G: J is an integer, and 7 * (-4) = -28.  
    7. Find the number on the left which is multiplied by H (7), and call it K: K = 9.  
    8. Change K to K + J: K_new = 9 + (-4) = 5.  
    9. Recompute the value on the left, and verify that it equals F: (8 * 3) + (7 * 5) = 24 + 35 = 59.  
      
    The solution is: 8 * 3 + 7 * 5  
      
    --------------------------------------------------------------------------------  

## Planned improvements​ {#planned-improvements}

* Understanding user instructions distributed over multiple turns.
* Learning from the agent's own experience, to reduce dependence on explicit user teachings.
* Learning skills built on top of previously learned skills.

## Conclusion​ {#conclusion}

`Teachability` is still under active research and development. For any
problems you find or improvements you have in mind, please join our
discussions in this repo and on our [Discord
channel](https://discord.gg/pAbnFJrkgZ). We look forward to seeing how you and
the rest of the community can use and improve teachable agents in AutoGen!

 **Tags:**

* [LLM](/autogen/blog/tags/llm)
* [teach](/autogen/blog/tags/teach)

[Newer Post« Multimodal with GPT-4V and LLaVA](/autogen/blog/2023/11/06/LMM-
Agent)

[Older PostRetrieval-Augmented Generation (RAG) Applications with AutoGen
»](/autogen/blog/2023/10/18/RetrieveChat)

* Introduction
* Run It Yourself
* Basic Usage of Teachability
* Example 1 - Learning user info
* Example 2 - Learning new facts
* Example 3 - Learning user preferences
* Example 4 - Learning new skills
* Planned improvements
* Conclusion

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

On this page

# Gallery {#gallery}

This page contains a list of demos that use AutoGen in various applications
from the community.

 **Contribution guide:** Built something interesting with AutoGen? Submit a PR
to add it to the list! See the Contribution Guide below for more details.

Filter by tags

[![AutoGen Group Chat Playground](/autogen/img/gallery/default.png)AutoGen
Group Chat PlaygroundA huggingface space to explore AutoGen group chat, build
agents with zero-code, and access source code for
reuse.ui](https://huggingface.co/spaces/thinkall/AutoGen_Playground)

[![Solving Security Vulnerabilities with
AutoGen](/autogen/img/gallery/default.png)Solving Security Vulnerabilities
with AutoGenAn article discussing the use of AutoGen to tackle security
vulnerabilities.app](<https://www.linkedin.com/pulse/solving-security->
vulnerabilities-llms-society-mind-model-leah-bonser-
kcswc?trk=public_post_feed-article-content)

[![Research Agents via AutoGen](/autogen/img/gallery/default.png)Research
Agents via AutoGenA guide to building a team of AI research agents using
AutoGen.groupchattools](https://youtu.be/AVInhYBUnKs?feature=shared)

[![AutoGen with Ollama and LiteLLM](/autogen/img/gallery/default.png)AutoGen
with Ollama and LiteLLMA demonstration of integrating Ollama, LiteLLM, and
AutoGen.extension](https://youtu.be/y7wMTwJN7rA)

[![AutoGen Engineer](/autogen/img/gallery/default.png)AutoGen EngineerJoin the
AutoGen Engineer group chat to collaborate and build with
others.groupchatapp](https://chat.openai.com/g/g-Y50TY4F35-AutoGen-engineer)

[![AutoGen with Obsidian](/autogen/img/gallery/default.png)AutoGen with
ObsidianLearn how to integrate AutoGen with Obsidian for note-taking and
management.toolsapp](https://youtu.be/iWdVAr4xMkg)

[![AutoGen Builder GPT](/autogen/img/gallery/default.png)AutoGen Builder GPTA
platform for building conversational AI agents with AutoGen Builder
GPT.groupchatui](https://chat.openai.com/g/g-EwugVj4zq-AutoGen-builder)

[![AutoGen Multi-Round Human Interaction Chatbot with Gradio
4.0](/autogen/img/gallery/default.png)AutoGen Multi-Round Human Interaction
Chatbot with Gradio 4.0Experience a multi-round human interaction chatbot
built with AutoGen and Gradio
4.0.uiapp](https://huggingface.co/spaces/thinkall/AutoGen-human-input-demo)

[![Agentcloud.dev \(UI for
AutoGen\)](/autogen/img/gallery/default.png)Agentcloud.dev (UI for
AutoGen)Agentcloud.dev provides a user interface for managing and
collaborating with AutoGen
agents.ui](https://github.com/rnadigital/agentcloud)

[![Next.js + FASTAPI Based UI for
AutoGen](/autogen/img/gallery/default.png)Next.js + FASTAPI Based UI for
AutoGenA project featuring a UI for AutoGen built with Next.js and
FastAPI.ui](https://github.com/victordibia/AutoGen-ui)

[![AutoGen Monitoring and
Observability](/autogen/img/gallery/default.png)AutoGen Monitoring and
ObservabilityDocumentation on monitoring and observability features for
AutoGen.extension](<https://docs.arize.com/phoenix/quickstart/llm->
traces/autogen-support)

[![Postgres Data Analytics AI Agent with
AutoGen](/autogen/img/gallery/default.png)Postgres Data Analytics AI Agent
with AutoGenUtilizing AutoGen to speak directly to Postgres
Database.toolsapp](https://www.youtube.com/playlist?list=PLS_o2ayVCKvDzj2YxeFqMq9UbR1PkPEh0)

[![AutoGen with Local LLMs](/autogen/img/gallery/default.png)AutoGen with
Local LLMsAn article on deploying multiple AI agents using local LLMs with
AutoGen.extension](<https://hackernoon.com/beep-beep-bop-bop-how-to-deploy->
multiple-ai-agents-using-local-llms)

[![AutoGen with FastApi backend and React
Frontend](/autogen/img/gallery/default.png)AutoGen with FastApi backend and
React FrontendA demonstration of using AutoGen with a FastAPI backend and
React frontend.ui](https://github.com/bonadio/AutoGenwebdemo)

[![Talk to AutoGen Agents Using Whisper and
Gradio](/autogen/img/gallery/default.png)Talk to AutoGen Agents Using Whisper
and GradioInteract with AutoGen agents using Whisper and Gradio
interfaces.ui](https://youtu.be/WysBjwJoulo)

[![AutoGen + LangChain + ChromaDB = You Super AI
Assistant](/autogen/img/gallery/default.png)AutoGen + LangChain + ChromaDB =
You Super AI AssistantCreate a super AI assistant combining AutoGen,
LangChain, and ChromaDB.app](https://www.youtube.com/watch?v=fd9fcRhYoFQ)

[![AutoGen + Flowise = Super AI Agents on No-Code
Platform](/autogen/img/gallery/default.png)AutoGen + Flowise = Super AI Agents
on No-Code PlatformBuild super AI agents on a no-code platform using AutoGen
and Flowise.app](<https://github.com/sugarforever/LangChain->
Advanced/blob/main/Integrations/AutoGen/autogen_flowise_ai_agent.ipynb)

[![AutoGen with RunPod and TextGen
WebUI](/autogen/img/gallery/default.png)AutoGen with RunPod and TextGen
WebUILearn how to use AutoGen with RunPod and TextGen WebUI for enhanced AI
agent integration.uiextension](https://youtu.be/FHXmiAvloUg)

[![Jarvis Collaborates with AutoGen for Tweet
Analysis](/autogen/img/gallery/default.png)Jarvis Collaborates with AutoGen
for Tweet AnalysisExplore how Jarvis collaborates with AutoGen for tweet
analysis.toolsapp](https://github.com/ngaut/jarvis#demo)

[![AutoGen + LangChain + PlayHT = Super AI Agent that
Speaks](/autogen/img/gallery/default.png)AutoGen + LangChain + PlayHT = Super
AI Agent that SpeaksCombine AutoGen, LangChain, and PlayHT to create a
speaking super AI agent.toolsapp](https://www.youtube.com/watch?v=zo2ft4Qje1Y)

[![AutoGen Flow with FastAPI and
Nextjs](/autogen/img/gallery/default.png)AutoGen Flow with FastAPI and NextjsA
development flow using AutoGen with FastAPI and
Next.js.ui](https://github.com/jaemil/agentsflow)

[![Build Vision-Enabled AI Agents with AutoGen +
Llava](/autogen/img/gallery/default.png)Build Vision-Enabled AI Agents with
AutoGen + LlavaTutorial on building vision-enabled AI agents using AutoGen and
llava.toolsapp](https://youtu.be/JgVb8A6OJwM)

[![AutoGen + Chainlit chat interface with multi-agent
conversation](/autogen/img/gallery/default.png)AutoGen + Chainlit chat
interface with multi-agent conversationChainlit chat interface with multi-
agent conversation between agents to complete a
tasksui](https://github.com/antoineross/Autogen-UI/tree/main)

## Contributing​ {#contributing}

To contribute, please open a PR that adds an entry to the `data/gallery.json`
file in the `src` directory. The entry should be an object with the following
properties:

    {  
        "title": "AutoGen Playground",  
        "link": "https://huggingface.co/spaces/thinkall/AutoGen_Playground",  
        "description": "A space to explore the capabilities of AutoGen.",  
        "image": "default.png",  
        "tags": ["ui"]  
      }  

The `image` property should be the name of a file in the `static/img/gallery`
directory. The `tags` property should be an array of strings that describe the
demo. We recommend using no more than two tags for clarity. Here are the
meanings of several tags for reference:

  1. app: Using Autogen for specific applications.
  2. extension: Enhancing AutoGen beyond the features in current version.
  3. ui: Building user interface for AutoGen.
  4. tool: Strengthening AutoGen Agents with external tools.
  5. groupchat: Solving complex tasks with a group of Agents.

if the existing ones do not precisely portray your own demos, new tags are
also encouraged to add.

[Edit this
page](https://github.com/microsoft/autogen/edit/main/website/docs/Gallery.mdx)

* Contributing

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
  * automl
    * automl.contrib
    * automl.nlp
    * automl.spark
    * automl.task
    * automl.time_series
    * [automl](.docs/reference/automl/automl)
    * [data](.docs/reference/automl/data)
    * [ml](.docs/reference/automl/ml)
    * [model](.docs/reference/automl/model)
    * [state](.docs/reference/automl/state)
  * default
  * onlineml
  * tune

On this page

# automl.state {#automlstate}

## AutoMLState Objects​ {#automlstate-objects}

    class AutoMLState()  

#### sanitize​ {#sanitize}

    @classmethod  
    def sanitize(cls, config: dict) -> dict  

Make a config ready for passing to estimator.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/automl/state.md)

[Previous« model](.docs/reference/automl/model)

[Nextestimator »](.docs/reference/default/estimator)

* AutoMLState Objects

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
  * automl
  * default
  * onlineml
  * tune
    * tune.scheduler
    * tune.searcher
      * [blendsearch](.docs/reference/tune/searcher/blendsearch)
      * [cfo_cat](.docs/reference/tune/searcher/cfo_cat)
      * [flow2](.docs/reference/tune/searcher/flow2)
      * [online_searcher](.docs/reference/tune/searcher/online_searcher)
      * [search_thread](.docs/reference/tune/searcher/search_thread)
      * [suggestion](.docs/reference/tune/searcher/suggestion)
      * [variant_generator](.docs/reference/tune/searcher/variant_generator)
    * tune.spark
    * [analysis](.docs/reference/tune/analysis)
    * [sample](.docs/reference/tune/sample)
    * [space](.docs/reference/tune/space)
    * [trial](.docs/reference/tune/trial)
    * [trial_runner](.docs/reference/tune/trial_runner)
    * [tune](.docs/reference/tune/tune)
    * [utils](.docs/reference/tune/utils)

On this page

# tune.searcher.suggestion {#tunesearchersuggestion}

## Searcher Objects​ {#searcher-objects}

    class Searcher()  

Abstract class for wrapping suggesting algorithms. Custom algorithms can
extend this class easily by overriding the `suggest` method provide generated
parameters for the trials. Any subclass that implements `__init__` must also
call the constructor of this class: `super(Subclass, self).__init__(...)`. To
track suggestions and their corresponding evaluations, the method `suggest`
will be passed a trial_id, which will be used in subsequent notifications. Not
all implementations support multi objectives.

 **Arguments** :

* `metric` _str or list_ \- The training result objective value attribute. If list then list of training result objective value attributes
* `mode` _str or list_ \- If string One of {min, max}. If list then list of max and min, determines whether objective is minimizing or maximizing the metric attribute. Must match type of metric.

    class ExampleSearch(Searcher):  
        def **init**(self, metric="mean_loss", mode="min", **kwargs):  
            super(ExampleSearch, self).**init**(  
                metric=metric, mode=mode,**kwargs)  
            self.optimizer = Optimizer()  
            self.configurations = {}  
        def suggest(self, trial_id):  
            configuration = self.optimizer.query()  
            self.configurations[trial_id] = configuration  
        def on_trial_complete(self, trial_id, result, **kwargs):  
            configuration = self.configurations[trial_id]  
            if result and self.metric in result:  
                self.optimizer.update(configuration, result[self.metric])  
    tune.run(trainable_function, search_alg=ExampleSearch())  

#### set_search_properties​ {#setsearchproperties}

    def set_search_properties(metric: Optional[str], mode: Optional[str], config: Dict) -> bool  

Pass search properties to searcher. This method acts as an alternative to
instantiating search algorithms with their own specific search spaces. Instead
they can accept a Tune config through this method. A searcher should return
`True` if setting the config was successful, or `False` if it was
unsuccessful, e.g. when the search space has already been set.

 **Arguments** :

* `metric` _str_ \- Metric to optimize
* `mode` _str_ \- One of ["min", "max"]. Direction to optimize.
* `config` _dict_ \- Tune config dict.

#### on_trial_result​ {#ontrialresult}

    def on_trial_result(trial_id: str, result: Dict)  

Optional notification for result during training. Note that by default, the
result dict may include NaNs or may not include the optimization metric. It is
up to the subclass implementation to preprocess the result to avoid breaking
the optimization process.

 **Arguments** :

* `trial_id` _str_ \- A unique string ID for the trial.
* `result` _dict_ \- Dictionary of metrics for current training progress. Note that the result dict may include NaNs or may not include the optimization metric. It is up to the subclass implementation to preprocess the result to avoid breaking the optimization process.

#### metric​ {#metric}

    @property  
    def metric() -> str  

The training result objective value attribute.

#### mode​ {#mode}

    @property  
    def mode() -> str  

Specifies if minimizing or maximizing the metric.

## ConcurrencyLimiter Objects​ {#concurrencylimiter-objects}

    class ConcurrencyLimiter(Searcher)  

A wrapper algorithm for limiting the number of concurrent trials.

 **Arguments** :

* `searcher` _Searcher_ \- Searcher object that the ConcurrencyLimiter will manage.
* `max_concurrent` _int_ \- Maximum concurrent samples from the underlying searcher.
* `batch` _bool_ \- Whether to wait for all concurrent samples to finish before updating the underlying searcher.

 **Example** :

    from ray.tune.suggest import ConcurrencyLimiter  # ray version < 2  
    search_alg = HyperOptSearch(metric="accuracy")  
    search_alg = ConcurrencyLimiter(search_alg, max_concurrent=2)  
    tune.run(trainable, search_alg=search_alg)  

#### validate_warmstart​ {#validatewarmstart}

    def validate_warmstart(parameter_names: List[str], points_to_evaluate: List[Union[List, Dict]], evaluated_rewards: List, validate_point_name_lengths: bool = True)  

Generic validation of a Searcher's warm start functionality. Raises exceptions
in case of type and length mismatches between parameters. If
`validate_point_name_lengths` is False, the equality of lengths between
`points_to_evaluate` and `parameter_names` will not be validated.

## OptunaSearch Objects​ {#optunasearch-objects}

    class OptunaSearch(Searcher)  

A wrapper around Optuna to provide trial suggestions.

`Optuna <https://optuna.org/>`_ is a hyperparameter optimization library. In
contrast to other libraries, it employs define-by-run style hyperparameter
definitions.

This Searcher is a thin wrapper around Optuna's search algorithms. You can
pass any Optuna sampler, which will be used to generate hyperparameter
suggestions.

Multi-objective optimization is supported.

 **Arguments** :

* `space` \- Hyperparameter search space definition for Optuna's sampler. This can be either a dict with parameter names as keys and `optuna.distributions` as values, or a Callable - in which case, it should be a define-by-run function using `optuna.trial` to obtain the hyperparameter values. The function should return either a dict of constant values with names as keys, or None. For more information, see [https://optuna.readthedocs.io\](https://optuna.readthedocs.io%5C) /en/stable/tutorial/10_key_features/002_configurations.html.

Warning - No actual computation should take place in the define-by-run
function. Instead, put the training logic inside the function or class
trainable passed to `tune.run`.

* `metric` \- The training result objective value attribute. If None but a mode was passed, the anonymous metric `_metric` will be used per default. Can be a list of metrics for multi-objective optimization.

* `mode` \- One of {min, max}. Determines whether objective is minimizing or maximizing the metric attribute. Can be a list of modes for multi-objective optimization (corresponding to `metric`).

* `points_to_evaluate` \- Initial parameter suggestions to be run first. This is for when you already have some good parameters you want to run first to help the algorithm make better suggestions for future parameters. Needs to be a list of dicts containing the configurations.

* `sampler` \- Optuna sampler used to draw hyperparameter configurations. Defaults to `MOTPESampler` for multi-objective optimization with Optuna<2.9.0, and `TPESampler` in every other case.

* `Warning` \- Please note that with Optuna 2.10.0 and earlier default `MOTPESampler`/`TPESampler` suffer from performance issues when dealing with a large number of completed trials (approx. >100). This will manifest as a delay when suggesting new configurations. This is an Optuna issue and may be fixed in a future Optuna release.

* `seed` \- Seed to initialize sampler with. This parameter is only used when `sampler=None`. In all other cases, the sampler you pass should be initialized with the seed already.

* `evaluated_rewards` \- If you have previously evaluated the parameters passed in as points_to_evaluate you can avoid re-running those trials by passing in the reward attributes as a list so the optimiser can be told the results without needing to re-compute the trial. Must be the same length as points_to_evaluate.

Warning - When using `evaluated_rewards`, the search space `space` must be
provided as a dict with parameter names as keys and `optuna.distributions`
instances as values. The define-by-run search space definition is not yet
supported with this functionality.

Tune automatically converts search spaces to Optuna's format:

        from ray.tune.suggest.optuna import OptunaSearch  
      
    config = {  

* `"a"` \- tune.uniform(6, 8)

* `"b"` \- tune.loguniform(1e-4, 1e-2) }

optuna_search = OptunaSearch( metric="loss", mode="min")

tune.run(trainable, config=config, search_alg=optuna_search)

    If you would like to pass the search space manually, the code would  
    look like this:  
      
    ```python  
    from ray.tune.suggest.optuna import OptunaSearch  
    import optuna  
      
    space = {  

* `"a"` \- optuna.distributions.UniformDistribution(6, 8),

* `"b"` \- optuna.distributions.LogUniformDistribution(1e-4, 1e-2), }

optuna_search = OptunaSearch( space, metric="loss", mode="min")

tune.run(trainable, search_alg=optuna_search)

# Equivalent Optuna define-by-run function approach: {#equivalent-optuna-definebyrun-function-approach}

def define_search_space(trial: optuna.Trial): trial.suggest_float("a", 6, 8)
trial.suggest_float("b", 1e-4, 1e-2, log=True)

# training logic goes into trainable, this is just {#training-logic-goes-into-trainable-this-is-just}

# for search space definition {#for-search-space-definition}

optuna_search = OptunaSearch( define_search_space, metric="loss", mode="min")

tune.run(trainable, search_alg=optuna_search)

    Multi-objective optimization is supported:  
      
    ```python  
    from ray.tune.suggest.optuna import OptunaSearch  
    import optuna  
      
    space = {  

* `"a"` \- optuna.distributions.UniformDistribution(6, 8),

* `"b"` \- optuna.distributions.LogUniformDistribution(1e-4, 1e-2), }

# Note you have to specify metric and mode here instead of {#note-you-have-to-specify-metric-and-mode-here-instead-of}

# in tune.run {#in-tunerun}

optuna_search = OptunaSearch( space, metric=["loss1", "loss2"], mode=["min",
"max"])

# Do not specify metric and mode here! {#do-not-specify-metric-and-mode-here}

tune.run( trainable, search_alg=optuna_search )

    You can pass configs that will be evaluated first using  
    ``points_to_evaluate``:  
      
    ```python  
    from ray.tune.suggest.optuna import OptunaSearch  
    import optuna  
      
    space = {  

* `"a"` \- optuna.distributions.UniformDistribution(6, 8),

* `"b"` \- optuna.distributions.LogUniformDistribution(1e-4, 1e-2), }

optuna_search = OptunaSearch( space,

* `points_to_evaluate=[{"a"` \- 6.5, "b": 5e-4}, {"a": 7.5, "b": 1e-3}] metric="loss", mode="min")

tune.run(trainable, search_alg=optuna_search)

    Avoid re-running evaluated trials by passing the rewards together with  
    `points_to_evaluate`:  
      
    ```python  
    from ray.tune.suggest.optuna import OptunaSearch  
    import optuna  
      
    space = {  

* `"a"` \- optuna.distributions.UniformDistribution(6, 8),

* `"b"` \- optuna.distributions.LogUniformDistribution(1e-4, 1e-2), }

optuna_search = OptunaSearch( space,

* `points_to_evaluate=[{"a"` \- 6.5, "b": 5e-4}, {"a": 7.5, "b": 1e-3}] evaluated_rewards=[0.89, 0.42] metric="loss", mode="min")

tune.run(trainable, search_alg=optuna_search)

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/tune/searcher/suggestion.md)

[Previous« search_thread](.docs/reference/tune/searcher/search_thread)

[Nextvariant_generator
»](.docs/reference/tune/searcher/variant_generator)

* Searcher Objects
* ConcurrencyLimiter Objects
* OptunaSearch Objects

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

Recent posts

* [All About Agent Descriptions](/autogen/blog/2023/12/29/AgentDescriptions)
* [AgentOptimizer - An Agentic Way to Train Your LLM Agent](/autogen/blog/2023/12/23/AgentOptimizer)
* [AutoGen Studio: Interactively Explore Multi-Agent Workflows](/autogen/blog/2023/12/01/AutoGenStudio)
* [Agent AutoBuild - Automatically Building Multi-agent Systems](/autogen/blog/2023/11/26/Agent-AutoBuild)
* [How to Assess Utility of LLM-powered Applications?](/autogen/blog/2023/11/20/AgentEval)
* [AutoGen Meets GPTs](/autogen/blog/2023/11/13/OAI-assistants)
* [EcoAssistant - Using LLM Assistants More Accurately and Affordably](/autogen/blog/2023/11/09/EcoAssistant)
* [Multimodal with GPT-4V and LLaVA](/autogen/blog/2023/11/06/LMM-Agent)
* [AutoGen's Teachable Agents](/autogen/blog/2023/10/26/TeachableAgent)
* [Retrieval-Augmented Generation (RAG) Applications with AutoGen](/autogen/blog/2023/10/18/RetrieveChat)
* [Use AutoGen for Local LLMs](/autogen/blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](/autogen/blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](/autogen/blog/2023/05/18/GPT-adaptive-humaneval)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](/autogen/blog/2023/04/21/LLM-tuning-math)

# One post tagged with "UI" {#one-post-tagged-with-ui}

[View All Tags](/autogen/blog/tags)

## [AutoGen Studio: Interactively Explore Multi-Agent {#autogen-studio-interactively-explore-multiagent}

Workflows](/autogen/blog/2023/12/01/AutoGenStudio)

December 1, 2023 · 8 min read

[![Victor
Dibia](https://github.com/victordibia.png)](https://github.com/victordibia)

[Victor Dibia](https://github.com/victordibia)

Principal RSDE at Microsoft Research

[![Gagan
Bansal](https://github.com/gagb.png)](<https://www.linkedin.com/in/gagan->
bansal/)

[Gagan Bansal](https://www.linkedin.com/in/gagan-bansal/)

Senior Researcher at Microsoft Research

[![Saleema
Amershi](https://github.com/samershi.png)](https://github.com/samershi)

[Saleema Amershi](https://github.com/samershi)

Senior Principal Research Manager at Microsoft Research

![AutoGen Studio Playground View: Solving a task with multiple agents that
generate a pdf document with
images.](/autogen/assets/images/autogenstudio_home-
cce78dc150d1bb0073620754df73d863.png)

 _AutoGen Studio: Solving a task with multiple agents that generate a pdf
document with images._

## TLDR​ {#tldr}

To help you rapidly prototype multi-agent solutions for your tasks, we are
introducing AutoGen Studio, an interface powered by
[AutoGen](https://github.com/microsoft/autogen/tree/main/autogen). It allows
you to:

* Declaratively define and modify agents and multi-agent workflows through a point and click, drag and drop interface (e.g., you can select the parameters of two agents that will communicate to solve your task).
* Use our UI to create chat sessions with the specified agents and view results (e.g., view chat history, generated files, and time taken).
* Explicitly add skills to your agents and accomplish more tasks.
* Publish your sessions to a local gallery.

AutoGen Studio is open source [code
here](<https://github.com/microsoft/autogen/tree/main/samples/apps/autogen->
studio), and can be installed via pip. Give it a try!

    pip install autogenstudio  

## Introduction​ {#introduction}

The accelerating pace of technology has ushered us into an era where digital
assistants (or agents) are becoming integral to our lives.
[AutoGen](https://github.com/microsoft/autogen/tree/main/autogen) has emerged
as a leading framework for orchestrating the power of agents. In the spirit of
expanding this frontier and democratizing this capability, we are thrilled to
introduce a new user-friendly interface: **AutoGen Studio**.

With AutoGen Studio, users can rapidly create, manage, and interact with
agents that can learn, adapt, and collaborate. As we release this interface
into the open-source community, our ambition is not only to enhance
productivity but to inspire a level of personalized interaction between humans
and agents.

> **Note** : AutoGen Studio is meant to help you rapidly prototype multi-
> agent workflows and demonstrate an example of end user interfaces built with
> AutoGen. It is not meant to be a production-ready app.

## Getting Started with AutoGen Studio​ {#getting-started-with-autogen-studio}

The following guide will help you get AutoGen Studio up and running on your
system.

### Configuring an LLM Provider​ {#configuring-an-llm-provider}

To get started, you need access to a language model. You can get this set up
by following the steps in the AutoGen documentation
[here](https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints).
Configure your environment with either `OPENAI_API_KEY` or
`AZURE_OPENAI_API_KEY`.

For example, in your terminal, you would set the API key like this:

    export OPENAI_API_KEY=<your_api_key>  

You can also specify the model directly in the agent's configuration as shown
below.

    llm_config = LLMConfig(  
        config_list=[{  
            "model": "gpt-4",  
            "api_key": "<azure_api_key>",  
            "base_url": "<azure api base url>",  
            "api_type": "azure",  
            "api_version": "2023-06-01-preview"  
        }],  
        temperature=0,  
    )  

### Installation​ {#installation}

  1. **Install from PyPi**

We recommend using a virtual environment (e.g., conda) to avoid conflicts with
existing Python packages. With Python 3.10 or newer active in your virtual
environment, use pip to install AutoGen Studio:

        pip install autogenstudio  

  2. **Install from Source**

> Note: This approach requires some familiarity with building interfaces in
> React.

If you prefer to install from source, ensure you have Python 3.10+ and Node.js
(version above 14.15.0) installed. Here's how you get started:

    * Clone the AutoGen Studio repository and install its Python dependencies:
        
                pip install -e .  

    * Navigate to the `samples/apps/autogen-studio/frontend` directory, install dependencies, and build the UI:
        
                npm install -g gatsby-cli  
        npm install --global yarn  
        cd frontend  
        yarn install  
        yarn build  

For Windows users, to build the frontend, you may need alternative commands
provided in the [autogen studio
readme](<https://github.com/microsoft/autogen/tree/main/samples/apps/autogen->
studio).

### Running the Application​ {#running-the-application}

Once installed, run the web UI by entering the following in your terminal:

    autogenstudio ui --port 8081  

This will start the application on the specified port. Open your web browser
and go to `http://localhost:8081/` to begin using AutoGen Studio.

Now that you have AutoGen Studio installed and running, you are ready to
explore its capabilities, including defining and modifying agent workflows,
interacting with agents and sessions, and expanding agent skills.

## What Can You Do with AutoGen Studio?​ {#what-can-you-do-with-autogen-studio}

The AutoGen Studio UI is organized into 3 high level sections - **Build** ,
**Playground** , and **Gallery**.

### Build​ {#build}

![Specify
Agents.](/autogen/assets/images/autogenstudio_config-5e0107b565503d4952c9b00dd3d5c10e.png)

This section focuses on defining the properties of agents and agent workflows.
It includes the following concepts:

 **Skills** : Skills are functions (e.g., Python functions) that describe how
to solve a task. In general, a good skill has a descriptive name (e.g.
`generate_images`), extensive docstrings and good defaults (e.g., writing out
files to disk for persistence and reuse). You can add new skills to AutoGen
Studio via the provided UI. At inference time, these skills are made available
to the assistant agent as they address your tasks.

![View and add
skills.](/autogen/assets/images/autogenstudio_skills-7376f0a5aec5917fda817a4793f2b89a.png)

 _AutoGen Studio Build View: View, add or edit skills that an agent can
leverage in addressing tasks._

 **Agents** : This provides an interface to declaratively specify properties
for an AutoGen agent (mirrors most of the members of a base [AutoGen
conversable
agent](https://github.com/microsoft/autogen/blob/main/autogen/agentchat/conversable_agent.py)
class).

 **Agent Workflows** : An agent workflow is a specification of a set of agents
that can work together to accomplish a task. The simplest version of this is a
setup with two agents – a user proxy agent (that represents a user i.e. it
compiles code and prints result) and an assistant that can address task
requests (e.g., generating plans, writing code, evaluating responses,
proposing error recovery steps, etc.). A more complex flow could be a group
chat where even more agents work towards a solution.

### Playground​ {#playground}

![AutoGen Studio Playground View: Solving a task with multiple agents that
generate a pdf document with
images.](/autogen/assets/images/autogenstudio_home-
cce78dc150d1bb0073620754df73d863.png)

 _AutoGen Studio Playground View: Agents collaborate, use available skills
(ability to generate images) to address a user task (generate pdf 's)._

The playground section is focused on interacting with agent workflows defined
in the previous build section. It includes the following concepts:

 **Session** : A session refers to a period of continuous interaction or
engagement with an agent workflow, typically characterized by a sequence of
activities or operations aimed at achieving specific objectives. It includes
the agent workflow configuration, the interactions between the user and the
agents. A session can be “published” to a “gallery”.

 **Chat View** : A chat is a sequence of interactions between a user and an
agent. It is a part of a session.

### Gallery​ {#gallery}

This section is focused on sharing and reusing artifacts (e.g., workflow
configurations, sessions, etc.).

AutoGen Studio comes with 3 example skills: `fetch_profile`, `find_papers`,
`generate_images`. Please feel free to review the repo to learn more about how
they work.

## The AutoGen Studio API​ {#the-autogen-studio-api}

While AutoGen Studio is a web interface, it is powered by an underlying python
API that is reusable and modular. Importantly, we have implemented an API
where agent workflows can be declaratively specified (in JSON), loaded and
run. An example of the current API is shown below. Please consult the [AutoGen
Studio repo](https://microsoft.github.io/autogen/docs/autogenstudio) for more
details.

    import json  
    from autogenstudio import AutoGenWorkFlowManager, AgentWorkFlowConfig  
      
    # load an agent specification in JSON  
    agent_spec = json.load(open('agent_spec.json'))  
      
    # Create an AutoGen Workflow Configuration from the agent specification  
    agent_work_flow_config = FlowConfig(**agent_spec)  
      
    # Create a Workflow from the configuration  
    agent_work_flow = AutoGenWorkFlowManager(agent_work_flow_config)  
      
    # Run the workflow on a task  
    task_query = "What is the height of the Eiffel Tower?"  
    agent_work_flow.run(message=task_query)  

## Road Map and Next Steps​ {#road-map-and-next-steps}

As we continue to develop and refine AutoGen Studio, the road map below
outlines an array of enhancements and new features planned for future
releases. Here's what users can look forward to:

* **Complex Agent Workflows** : We're working on integrating support for more sophisticated agent workflows, such as `GroupChat`, allowing for richer interaction between multiple agents or dynamic topologies.
* **Improved User Experience** : This includes features like streaming intermediate model output for real-time feedback, better summarization of agent responses, information on costs of each interaction. We will also invest in improving the workflow for composing and reusing agents. We will also explore support for more interactive human in the loop feedback to agents.
* **Expansion of Agent Skills** : We will work towards improving the workflow for authoring, composing and reusing agent skills.
* **Community Features** : Facilitation of sharing and collaboration within AutoGen Studio user community is a key goal. We're exploring options for sharing sessions and results more easily among users and contributing to a shared repository of skills, agents, and agent workflows.

### FAQ​ {#faq}

 **Q: Where can I adjust the default skills, agent and workflow
configurations?** A: You can modify agent configurations directly from the UI
or by editing the `autogentstudio/utils/dbdefaults.json` file which is used to
initialize the database.

 **Q: If I want to reset the entire conversation with an agent, how do I go
about it?** A: To reset your conversation history, you can delete the
`database.sqlite` file. If you need to clear user-specific data, remove the
relevant `autogenstudio/web/files/user/<user_id_md5hash>` folder.

 **Q: Is it possible to view the output and messages generated by the agents
during interactions?** A: Yes, you can view the generated messages in the
debug console of the web UI, providing insights into the agent interactions.
Alternatively, you can inspect the `database.sqlite` file for a comprehensive
record of messages.

 **Q: Where can I find documentation and support for AutoGen Studio?** A: We
are constantly working to improve AutoGen Studio. For the latest updates,
please refer to the [AutoGen Studio
Readme](<https://github.com/microsoft/autogen/tree/main/samples/apps/autogen->
studio). For additional support, please open an issue on
[GitHub](https://github.com/microsoft/autogen) or ask questions on
[Discord](https://discord.gg/pAbnFJrkgZ).

 **Tags:**

* [AutoGen](/autogen/blog/tags/auto-gen)
* [UI](/autogen/blog/tags/ui)
* [web](/autogen/blog/tags/web)
* [UX](/autogen/blog/tags/ux)

[**Read More**](/autogen/blog/2023/12/01/AutoGenStudio)

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
  * automl
  * default
    * [estimator](.docs/reference/default/estimator)
    * [greedy](.docs/reference/default/greedy)
    * [portfolio](.docs/reference/default/portfolio)
    * [suggest](.docs/reference/default/suggest)
  * onlineml
  * tune

On this page

# default.suggest {#defaultsuggest}

#### suggest_config​ {#suggestconfig}

    def suggest_config(task, X, y, estimator_or_predictor, location=None, k=None, meta_feature_fn=meta_feature)  

Suggest a list of configs for the given task and training data.

The returned configs can be used as starting points for AutoML.fit().
`FLAML_sample_size` is removed from the configs.

#### suggest_learner​ {#suggestlearner}

    def suggest_learner(task, X, y, estimator_or_predictor="all", estimator_list=None, location=None)  

Suggest best learner within estimator_list.

#### suggest_hyperparams​ {#suggesthyperparams}

    def suggest_hyperparams(task, X, y, estimator_or_predictor, location=None)  

Suggest hyperparameter configurations and an estimator class.

The configurations can be used to initialize the estimator class like
lightgbm.LGBMRegressor.

 **Example** :

    hyperparams, estimator_class = suggest_hyperparams("regression", X_train, y_train, "lgbm")  
    model = estimator_class(**hyperparams)  # estimator_class is LGBMRegressor  
    model.fit(X_train, y_train)  

 **Arguments** :

* `task` \- A string of the task type, e.g., 'classification', 'regression', 'ts_forecast', 'rank', 'seq-classification', 'seq-regression'.
* `X` \- A dataframe of training data in shape n*m. For 'ts_forecast' task, the first column of X_train must be the timestamp column (datetime type). Other columns in the dataframe are assumed to be exogenous variables (categorical or numeric).
* `y` \- A series of labels in shape n*1.
* `estimator_or_predictor` \- A str of the learner name or a dict of the learned config predictor. If a dict, it contains:
  * "version": a str of the version number.
  * "preprocessing": a dictionary containing:
  * "center": a list of meta feature value offsets for normalization.
  * "scale": a list of meta feature scales to normalize each dimension.
  * "neighbors": a list of dictionaries. Each dictionary contains:
  * "features": a list of the normalized meta features for a neighbor.
  * "choice": an integer of the configuration id in the portfolio.
  * "portfolio": a list of dictionaries, each corresponding to a configuration:
  * "class": a str of the learner name.
  * "hyperparameters": a dict of the config. The key "FLAML_sample_size" will be ignored.
* `location` \- (Optional) A str of the location containing mined portfolio file. Only valid when the portfolio is a str, by default the location is flaml/default.

 **Returns** :

* `hyperparams` \- A dict of the hyperparameter configurations.
* `estiamtor_class` \- A class of the underlying estimator, e.g., lightgbm.LGBMClassifier.

#### preprocess_and_suggest_hyperparams​ {#preprocessandsuggesthyperparams}

    def preprocess_and_suggest_hyperparams(task, X, y, estimator_or_predictor, location=None)  

Preprocess the data and suggest hyperparameters.

 **Example** :

    hyperparams, estimator_class, X, y, feature_transformer, label_transformer = \  
        preprocess_and_suggest_hyperparams("classification", X_train, y_train, "xgb_limitdepth")  
    model = estimator_class(**hyperparams)  # estimator_class is XGBClassifier  
    model.fit(X, y)  
    X_test = feature_transformer.transform(X_test)  
    y_pred = label_transformer.inverse_transform(pd.Series(model.predict(X_test).astype(int)))  

 **Arguments** :

* `task` \- A string of the task type, e.g., 'classification', 'regression', 'ts_forecast', 'rank', 'seq-classification', 'seq-regression'.
* `X` \- A dataframe of training data in shape n*m. For 'ts_forecast' task, the first column of X_train must be the timestamp column (datetime type). Other columns in the dataframe are assumed to be exogenous variables (categorical or numeric).
* `y` \- A series of labels in shape n*1.
* `estimator_or_predictor` \- A str of the learner name or a dict of the learned config predictor. "choose_xgb" means choosing between xgb_limitdepth and xgboost. If a dict, it contains:
  * "version": a str of the version number.
  * "preprocessing": a dictionary containing:
  * "center": a list of meta feature value offsets for normalization.
  * "scale": a list of meta feature scales to normalize each dimension.
  * "neighbors": a list of dictionaries. Each dictionary contains:
  * "features": a list of the normalized meta features for a neighbor.
  * "choice": a integer of the configuration id in the portfolio.
  * "portfolio": a list of dictionaries, each corresponding to a configuration:
  * "class": a str of the learner name.
  * "hyperparameters": a dict of the config. They key "FLAML_sample_size" will be ignored.
* `location` \- (Optional) A str of the location containing mined portfolio file. Only valid when the portfolio is a str, by default the location is flaml/default.

 **Returns** :

* `hyperparams` \- A dict of the hyperparameter configurations.
* `estiamtor_class` \- A class of the underlying estimator, e.g., lightgbm.LGBMClassifier.
* `X` \- the preprocessed X.
* `y` \- the preprocessed y.
* `feature_transformer` \- a data transformer that can be applied to X_test.
* `label_transformer` \- a label transformer that can be applied to y_test.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/default/suggest.md)

[Previous« portfolio](.docs/reference/default/portfolio)

[Nextautovw »](.docs/reference/onlineml/autovw)

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
  * automl
  * default
    * [estimator](.docs/reference/default/estimator)
    * [greedy](.docs/reference/default/greedy)
    * [portfolio](.docs/reference/default/portfolio)
    * [suggest](.docs/reference/default/suggest)
  * onlineml
  * tune

On this page

# default.portfolio {#defaultportfolio}

#### config_predictor_tuple​ {#configpredictortuple}

    def config_predictor_tuple(tasks, configs, meta_features, regret_matrix)  

Config predictor represented in tuple.

The returned tuple consists of (meta_features, preferences, proc).

 **Returns** :

* `meta_features_norm` \- A dataframe of normalized meta features, each column for a task.
* `preferences` \- A dataframe of sorted configuration indicies by their performance per task (column).
* `regret_matrix` \- A dataframe of the configuration(row)-task(column) regret matrix.

#### build_portfolio​ {#buildportfolio}

    def build_portfolio(meta_features, regret, strategy)  

Build a portfolio from meta features and regret matrix.

 **Arguments** :

* `meta_features` \- A dataframe of metafeatures matrix.
* `regret` \- A dataframe of regret matrix.
* `strategy` \- A str of the strategy, one of ("greedy", "greedy-feedback").

#### load_json​ {#loadjson}

    def load_json(filename)  

Returns the contents of json file filename.

#### serialize​ {#serialize}

    def serialize(configs, regret, meta_features, output_file, config_path)  

Store to disk all information FLAML-metalearn needs at runtime.

configs: names of model configs regret: regret matrix meta_features: task
metafeatures output_file: filename config_path: path containing config json
files

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/default/portfolio.md)

[Previous« greedy](.docs/reference/default/greedy)

[Nextsuggest »](.docs/reference/default/suggest)

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
  * automl
    * automl.contrib
    * automl.nlp
    * automl.spark
    * automl.task
      * [task](.docs/reference/automl/task/task)
      * [time_series_task](.docs/reference/automl/task/time_series_task)
    * automl.time_series
    * [automl](.docs/reference/automl/automl)
    * [data](.docs/reference/automl/data)
    * [ml](.docs/reference/automl/ml)
    * [model](.docs/reference/automl/model)
    * [state](.docs/reference/automl/state)
  * default
  * onlineml
  * tune

On this page

# automl.task.time_series_task {#automltasktimeseriestask}

#### remove_ts_duplicates​ {#removetsduplicates}

    def remove_ts_duplicates(X, time_col)  

Assumes the targets are included @param X: @param time_col: @param y: @return:

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/automl/task/time_series_task.md)

[Previous« task](.docs/reference/automl/task/task)

[Nextsklearn »](.docs/reference/automl/time_series/sklearn)

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
  * automl
  * default
  * onlineml
  * tune
    * tune.scheduler
      * [online_scheduler](.docs/reference/tune/scheduler/online_scheduler)
      * [trial_scheduler](.docs/reference/tune/scheduler/trial_scheduler)
    * tune.searcher
    * tune.spark
    * [analysis](.docs/reference/tune/analysis)
    * [sample](.docs/reference/tune/sample)
    * [space](.docs/reference/tune/space)
    * [trial](.docs/reference/tune/trial)
    * [trial_runner](.docs/reference/tune/trial_runner)
    * [tune](.docs/reference/tune/tune)
    * [utils](.docs/reference/tune/utils)

On this page

# tune.scheduler.online_scheduler {#tunescheduleronlinescheduler}

## OnlineScheduler Objects​ {#onlinescheduler-objects}

    class OnlineScheduler(TrialScheduler)  

Class for the most basic OnlineScheduler.

#### on_trial_result​ {#ontrialresult} {#ontrialresult} {#ontrialresult}

    def on_trial_result(trial_runner, trial: Trial, result: Dict)  

Report result and return a decision on the trial's status.

#### choose_trial_to_run​ {#choosetrialtorun}

    def choose_trial_to_run(trial_runner) -> Trial  

Decide which trial to run next.

## OnlineSuccessiveDoublingScheduler Objects​ {#onlinesuccessivedoublingscheduler-objects}

    class OnlineSuccessiveDoublingScheduler(OnlineScheduler)  

class for the OnlineSuccessiveDoublingScheduler algorithm.

#### __init__​ {#init} {#init}

    def __init__(increase_factor: float = 2.0)  

Constructor.

 **Arguments** :

* `increase_factor` \- A float of multiplicative factor used to increase resource lease. Default is 2.0.

#### on_trial_result​ {#ontrialresult} {#ontrialresult} {#ontrialresult}

    def on_trial_result(trial_runner, trial: Trial, result: Dict)  

Report result and return a decision on the trial's status.

## ChaChaScheduler Objects​ {#chachascheduler-objects}

    class ChaChaScheduler(OnlineSuccessiveDoublingScheduler)  

class for the ChaChaScheduler algorithm.

#### __init__​ {#init} {#init}

    def __init__(increase_factor: float = 2.0, **kwargs)  

Constructor.

 **Arguments** :

* `increase_factor` \- A float of multiplicative factor used to increase resource lease. Default is 2.0.

#### on_trial_result​ {#ontrialresult} {#ontrialresult} {#ontrialresult}

    def on_trial_result(trial_runner, trial: Trial, result: Dict)  

Report result and return a decision on the trial's status.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/tune/scheduler/online_scheduler.md)

[Previous« trial_runner](.docs/reference/onlineml/trial_runner)

[Nexttrial_scheduler »](.docs/reference/tune/scheduler/trial_scheduler)

* OnlineScheduler Objects
* OnlineSuccessiveDoublingScheduler Objects
* ChaChaScheduler Objects

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
  * automl
  * default
  * onlineml
    * [autovw](.docs/reference/onlineml/autovw)
    * [trial](.docs/reference/onlineml/trial)
    * [trial_runner](.docs/reference/onlineml/trial_runner)
  * tune

On this page

# onlineml.trial {#onlinemltrial}

#### get_ns_feature_dim_from_vw_example​ {#getnsfeaturedimfromvwexample}

    def get_ns_feature_dim_from_vw_example(vw_example) -> dict  

Get a dictionary of feature dimensionality for each namespace singleton.

## OnlineResult Objects​ {#onlineresult-objects}

    class OnlineResult()  

Class for managing the result statistics of a trial.

#### __init__​ {#init} {#init} {#init}

    def __init__(result_type_name: str, cb_coef: Optional[float] = None, init_loss: Optional[float] = 0.0, init_cb: Optional[float] = 100.0, mode: Optional[str] = "min", sliding_window_size: Optional[int] = 100)  

Constructor.

 **Arguments** :

* `result_type_name` \- A String to specify the name of the result type.
* `cb_coef` \- a string to specify the coefficient on the confidence bound.
* `init_loss` \- a float to specify the inital loss.
* `init_cb` \- a float to specify the intial confidence bound.
* `mode` \- A string in ['min', 'max'] to specify the objective as minimization or maximization.
* `sliding_window_size` \- An int to specify the size of the sliding window (for experimental purpose).

#### update_result​ {#updateresult}

    def update_result(new_loss, new_resource_used, data_dimension, bound_of_range=1.0, new_observation_count=1.0)  

Update result statistics.

## BaseOnlineTrial Objects​ {#baseonlinetrial-objects}

    class BaseOnlineTrial(Trial)  

Class for the online trial.

#### __init__​ {#init} {#init} {#init}

    def __init__(config: dict, min_resource_lease: float, is_champion: Optional[bool] = False, is_checked_under_current_champion: Optional[bool] = True, custom_trial_name: Optional[str] = "mae", trial_id: Optional[str] = None)  

Constructor.

 **Arguments** :

* `config` \- The configuration dictionary.
* `min_resource_lease` \- A float specifying the minimum resource lease.
* `is_champion` \- A bool variable indicating whether the trial is champion.
* `is_checked_under_current_champion` \- A bool indicating whether the trial has been used under the current champion.
* `custom_trial_name` \- A string of a custom trial name.
* `trial_id` \- A string for the trial id.

#### set_resource_lease​ {#setresourcelease}

    def set_resource_lease(resource: float)  

Sets the resource lease accordingly.

#### set_status​ {#setstatus}

    def set_status(status)  

Sets the status of the trial and record the start time.

## VowpalWabbitTrial Objects​ {#vowpalwabbittrial-objects}

    class VowpalWabbitTrial(BaseOnlineTrial)  

The class for Vowpal Wabbit online trials.

#### __init__​ {#init} {#init} {#init}

    def __init__(config: dict, min_resource_lease: float, metric: str = "mae", is_champion: Optional[bool] = False, is_checked_under_current_champion: Optional[bool] = True, custom_trial_name: Optional[str] = "vw_mae_clipped", trial_id: Optional[str] = None, cb_coef: Optional[float] = None)  

Constructor.

 **Arguments** :

* `config` _dict_ \- the config of the trial (note that the config is a set because the hyperparameters are).
* `min_resource_lease` _float_ \- the minimum resource lease.
* `metric` _str_ \- the loss metric.
* `is_champion` _bool_ \- indicates whether the trial is the current champion or not.
* `is_checked_under_current_champion` _bool_ \- indicates whether this trials has been paused under the current champion.
* `trial_id` _str_ \- id of the trial (if None, it will be generated in the constructor).

#### train_eval_model_online​ {#trainevalmodelonline}

    def train_eval_model_online(data_sample, y_pred)  

Train and evaluate model online.

#### predict​ {#predict}

    def predict(x)  

Predict using the model.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/onlineml/trial.md)

[Previous« autovw](.docs/reference/onlineml/autovw)

[Nexttrial_runner »](.docs/reference/onlineml/trial_runner)

* OnlineResult Objects
* BaseOnlineTrial Objects
* VowpalWabbitTrial Objects

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
  * automl
  * default
  * onlineml
  * tune
    * tune.scheduler
    * tune.searcher
      * [blendsearch](.docs/reference/tune/searcher/blendsearch)
      * [cfo_cat](.docs/reference/tune/searcher/cfo_cat)
      * [flow2](.docs/reference/tune/searcher/flow2)
      * [online_searcher](.docs/reference/tune/searcher/online_searcher)
      * [search_thread](.docs/reference/tune/searcher/search_thread)
      * [suggestion](.docs/reference/tune/searcher/suggestion)
      * [variant_generator](.docs/reference/tune/searcher/variant_generator)
    * tune.spark
    * [analysis](.docs/reference/tune/analysis)
    * [sample](.docs/reference/tune/sample)
    * [space](.docs/reference/tune/space)
    * [trial](.docs/reference/tune/trial)
    * [trial_runner](.docs/reference/tune/trial_runner)
    * [tune](.docs/reference/tune/tune)
    * [utils](.docs/reference/tune/utils)

On this page

# tune.searcher.online_searcher {#tunesearcheronlinesearcher}

## BaseSearcher Objects​ {#basesearcher-objects}

    class BaseSearcher()  

Abstract class for an online searcher.

## ChampionFrontierSearcher Objects​ {#championfrontiersearcher-objects}

    class ChampionFrontierSearcher(BaseSearcher)  

The ChampionFrontierSearcher class.

NOTE about the correspondence about this code and the research paper: [ChaCha
for Online AutoML](https://arxiv.org/pdf/2106.04815.pdf). This class serves
the role of ConfigOralce as described in the paper.

#### __init__​ {#init}

    def __init__(init_config: Dict, space: Optional[Dict] = None, metric: Optional[str] = None, mode: Optional[str] = None, random_seed: Optional[int] = 2345, online_trial_args: Optional[Dict] = {}, nonpoly_searcher_name: Optional[str] = "CFO")  

Constructor.

 **Arguments** :

* `init_config` \- A dictionary of initial configuration.
* `space` \- A dictionary to specify the search space.
* `metric` \- A string of the metric name to optimize for.
* `mode` \- A string in ['min', 'max'] to specify the objective as minimization or maximization.
* `random_seed` \- An integer of the random seed.
* `online_trial_args` \- A dictionary to specify the online trial arguments for experimental purpose.
* `nonpoly_searcher_name` \- A string to specify the search algorithm for nonpoly hyperparameters.

#### set_search_properties​ {#setsearchproperties}

    def set_search_properties(metric: Optional[str] = None, mode: Optional[str] = None, config: Optional[Dict] = {}, setting: Optional[Dict] = {}, init_call: Optional[bool] = False)  

Construct search space with the given config, and setup the search.

#### next_trial​ {#nexttrial}

    def next_trial()  

Return a trial from the _challenger_list.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/tune/searcher/online_searcher.md)

[Previous« flow2](.docs/reference/tune/searcher/flow2)

[Nextsearch_thread »](.docs/reference/tune/searcher/search_thread)

* BaseSearcher Objects
* ChampionFrontierSearcher Objects

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

* Reference
  * agentchat
    * agentchat.contrib
      * agentchat.contrib.capabilities
        * [agent_capability](/autogen/docs/reference/agentchat/contrib/capabilities/agent_capability)
        * [teachability](/autogen/docs/reference/agentchat/contrib/capabilities/teachability)
      * [agent_builder](/autogen/docs/reference/agentchat/contrib/agent_builder)
      * [compressible_agent](/autogen/docs/reference/agentchat/contrib/compressible_agent)
      * [gpt_assistant_agent](/autogen/docs/reference/agentchat/contrib/gpt_assistant_agent)
      * [img_utils](/autogen/docs/reference/agentchat/contrib/img_utils)
      * [llava_agent](/autogen/docs/reference/agentchat/contrib/llava_agent)
      * [math_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/math_user_proxy_agent)
      * [multimodal_conversable_agent](/autogen/docs/reference/agentchat/contrib/multimodal_conversable_agent)
      * [qdrant_retrieve_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/qdrant_retrieve_user_proxy_agent)
      * [retrieve_assistant_agent](/autogen/docs/reference/agentchat/contrib/retrieve_assistant_agent)
      * [retrieve_user_proxy_agent](/autogen/docs/reference/agentchat/contrib/retrieve_user_proxy_agent)
      * [text_analyzer_agent](/autogen/docs/reference/agentchat/contrib/text_analyzer_agent)
    * [agent](/autogen/docs/reference/agentchat/agent)
    * [assistant_agent](/autogen/docs/reference/agentchat/assistant_agent)
    * [conversable_agent](/autogen/docs/reference/agentchat/conversable_agent)
    * [groupchat](/autogen/docs/reference/agentchat/groupchat)
    * [user_proxy_agent](/autogen/docs/reference/agentchat/user_proxy_agent)
  * oai
  * [code_utils](/autogen/docs/reference/code_utils)
  * [function_utils](/autogen/docs/reference/function_utils)
  * [math_utils](/autogen/docs/reference/math_utils)
  * [retrieve_utils](/autogen/docs/reference/retrieve_utils)
  * [token_count_utils](/autogen/docs/reference/token_count_utils)

On this page

# agentchat.contrib.capabilities.agent_capability {#agentchatcontribcapabilitiesagentcapability}

## AgentCapability Objects​ {#agentcapability-objects}

    class AgentCapability()  

Base class for composable capabilities that can be added to an agent.

#### add_to_agent​ {#addtoagent}

    def add_to_agent(agent: ConversableAgent)  

Adds a particular capability to the given agent. Must be implemented by the
capability subclass. An implementation will typically call
agent.register_hook() one or more times. See teachability.py as an example.

[Edit this
page](https://github.com/microsoft/autogen/edit/main/website/docs/reference/agentchat/contrib/capabilities/agent_capability.md)

[Nextteachability
»](/autogen/docs/reference/agentchat/contrib/capabilities/teachability)

* AgentCapability Objects

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

* Reference
  * agentchat
  * oai
  * [code_utils](/autogen/docs/reference/code_utils)
  * [function_utils](/autogen/docs/reference/function_utils)
  * [math_utils](/autogen/docs/reference/math_utils)
  * [retrieve_utils](/autogen/docs/reference/retrieve_utils)
  * [token_count_utils](/autogen/docs/reference/token_count_utils)

On this page

# function_utils {#functionutils}

#### get_typed_annotation​ {#gettypedannotation}

    def get_typed_annotation(annotation: Any, globalns: Dict[str, Any]) -> Any  

Get the type annotation of a parameter.

 **Arguments** :

* `annotation` \- The annotation of the parameter
* `globalns` \- The global namespace of the function

 **Returns** :

The type annotation of the parameter

#### get_typed_signature​ {#gettypedsignature}

    def get_typed_signature(call: Callable[..., Any]) -> inspect.Signature  

Get the signature of a function with type annotations.

 **Arguments** :

* `call` \- The function to get the signature for

 **Returns** :

The signature of the function with type annotations

#### get_typed_return_annotation​ {#gettypedreturnannotation}

    def get_typed_return_annotation(call: Callable[..., Any]) -> Any  

Get the return annotation of a function.

 **Arguments** :

* `call` \- The function to get the return annotation for

 **Returns** :

The return annotation of the function

#### get_param_annotations​ {#getparamannotations}

    def get_param_annotations(  
        typed_signature: inspect.Signature  
    ) -> Dict[int, Union[Annotated[Type[Any], str], Type[Any]]]  

Get the type annotations of the parameters of a function

 **Arguments** :

* `typed_signature` \- The signature of the function with type annotations

 **Returns** :

A dictionary of the type annotations of the parameters of the function

## Parameters Objects​ {#parameters-objects}

    class Parameters(BaseModel)  

Parameters of a function as defined by the OpenAI API

## Function Objects​ {#function-objects}

    class Function(BaseModel)  

A function as defined by the OpenAI API

## ToolFunction Objects​ {#toolfunction-objects}

    class ToolFunction(BaseModel)  

A function under tool as defined by the OpenAI API.

#### get_parameter_json_schema​ {#getparameterjsonschema}

    def get_parameter_json_schema(  
            k: str, v: Union[Annotated[Type[Any], str], Type[Any]],  
            default_values: Dict[str, Any]) -> JsonSchemaValue  

Get a JSON schema for a parameter as defined by the OpenAI API

 **Arguments** :

* `k` \- The name of the parameter
* `v` \- The type of the parameter
* `default_values` \- The default values of the parameters of the function

 **Returns** :

A Pydanitc model for the parameter

#### get_required_params​ {#getrequiredparams}

    def get_required_params(typed_signature: inspect.Signature) -> List[str]  

Get the required parameters of a function

 **Arguments** :

* `signature` \- The signature of the function as returned by inspect.signature

 **Returns** :

A list of the required parameters of the function

#### get_default_values​ {#getdefaultvalues}

    def get_default_values(typed_signature: inspect.Signature) -> Dict[str, Any]  

Get default values of parameters of a function

 **Arguments** :

* `signature` \- The signature of the function as returned by inspect.signature

 **Returns** :

A dictionary of the default values of the parameters of the function

#### get_parameters​ {#getparameters}

    def get_parameters(required: List[str],  
                       param_annotations: Dict[str, Union[Annotated[Type[Any],  
                                                                    str],  
                                                          Type[Any]]],  
                       default_values: Dict[str, Any]) -> Parameters  

Get the parameters of a function as defined by the OpenAI API

 **Arguments** :

* `required` \- The required parameters of the function
* `hints` \- The type hints of the function as returned by typing.get_type_hints

 **Returns** :

A Pydantic model for the parameters of the function

#### get_missing_annotations​ {#getmissingannotations}

    def get_missing_annotations(typed_signature: inspect.Signature,  
                                required: List[str]) -> Tuple[Set[str], Set[str]]  

Get the missing annotations of a function

Ignores the parameters with default values as they are not required to be
annotated, but logs a warning.

 **Arguments** :

* `typed_signature` \- The signature of the function with type annotations
* `required` \- The required parameters of the function

 **Returns** :

A set of the missing annotations of the function

#### get_function_schema​ {#getfunctionschema}

    def get_function_schema(f: Callable[..., Any],  
                            *,  
                            name: Optional[str] = None,  
                            description: str) -> Dict[str, Any]  

Get a JSON schema for a function as defined by the OpenAI API

 **Arguments** :

* `f` \- The function to get the JSON schema for
* `name` \- The name of the function
* `description` \- The description of the function

 **Returns** :

A JSON schema for the function

 **Raises** :

* `TypeError` \- If the function is not annotated

 **Examples** :

    ```  
    def f(a: Annotated[str, "Parameter a"], b: int = 2, c: Annotated[float, "Parameter c"] = 0.1) -> None:  
        pass  
      
    get_function_schema(f, description="function f")  
      
    #   {'type': 'function',  
    #    'function': {'description': 'function f',  
    #        'name': 'f',  
    #        'parameters': {'type': 'object',  
    #           'properties': {'a': {'type': 'str', 'description': 'Parameter a'},  
    #               'b': {'type': 'int', 'description': 'b'},  
    #               'c': {'type': 'float', 'description': 'Parameter c'}},  
    #           'required': ['a']}}}  
        ```  

#### get_load_param_if_needed_function​ {#getloadparamifneededfunction}

    def get_load_param_if_needed_function(  
            t: Any) -> Optional[Callable[[T, Type[Any]], BaseModel]]  

Get a function to load a parameter if it is a Pydantic model

 **Arguments** :

* `t` \- The type annotation of the parameter

 **Returns** :

A function to load the parameter if it is a Pydantic model, otherwise None

#### load_basemodels_if_needed​ {#loadbasemodelsifneeded}

    def load_basemodels_if_needed(func: Callable[..., Any]) -> Callable[..., Any]  

A decorator to load the parameters of a function if they are Pydantic models

 **Arguments** :

* `func` \- The function with annotated parameters

 **Returns** :

A function that loads the parameters before calling the original function

[Edit this
page](https://github.com/microsoft/autogen/edit/main/website/docs/reference/function_utils.md)

[Previous« code_utils](/autogen/docs/reference/code_utils)

[Nextmath_utils »](/autogen/docs/reference/math_utils)

* Parameters Objects
* Function Objects
* ToolFunction Objects

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* [Getting Started](.docs/Getting-Started)
* [Installation](.docs/Installation)
* Use Cases
  * [AutoGen for Large Language Models](.docs/Use-Cases/Autogen)
  * [Task Oriented AutoML](.docs/Use-Cases/Task-Oriented-AutoML)
  * [Tune User Defined Function](.docs/Use-Cases/Tune-User-Defined-Function)
  * [Zero Shot AutoML](.docs/Use-Cases/Zero-Shot-AutoML)
* Examples
* [Contributing](.docs/Contribute)
* [Research](.docs/Research)

On this page

# Zero Shot AutoML {#zero-shot-automl}

`flaml.default` is a package for zero-shot AutoML, or "no-tuning" AutoML. It
uses [`flaml.AutoML`](.docs/reference/automl/automl#automl-objects) and
[`flaml.default.portfolio`](.docs/reference/default/portfolio) to mine
good hyperparameter configurations across different datasets offline, and
recommend data-dependent default configurations at runtime without expensive
tuning.

Zero-shot AutoML has several benefits:

* The computation cost is just training one model. No tuning is involved.
* The decision of hyperparameter configuration is instant. No overhead to worry about.
* Your code remains the same. No breaking of the existing workflow.
* It requires less input from the user. No need to specify a tuning budget etc.
* All training data are used for, guess what, training. No need to worry about holding a subset of training data for validation (and overfitting the validation data).
* The offline preparation can be customized for a domain and leverage the historical tuning data. No experience is wasted.

## How to Use at Runtime​ {#how-to-use-at-runtime}

The easiest way to leverage this technique is to import a "flamlized" learner
of your favorite choice and use it just as how you use the learner before. The
automation is done behind the scene and you are not required to change your
code. For example, if you are currently using:

    from lightgbm import LGBMRegressor  
      
    estimator = LGBMRegressor()  
    estimator.fit(X_train, y_train)  
    estimator.predict(X_test)  

Simply replace the first line with:

    from flaml.default import LGBMRegressor  

All the other code remains the same. And you are expected to get a equal or
better model in most cases.

The current list of "flamlized" learners are:

* LGBMClassifier, LGBMRegressor.
* XGBClassifier, XGBRegressor.
* RandomForestClassifier, RandomForestRegressor.
* ExtraTreesClassifier, ExtraTreesRegressor.

### What's the magic behind the scene?​ {#whats-the-magic-behind-the-scene}

`flaml.default.LGBMRegressor` inherits `lightgbm.LGBMRegressor`, so all the
APIs in `lightgbm.LGBMRegressor` are still valid in
`flaml.default.LGBMRegressor`. The difference is,
`flaml.default.LGBMRegressor` decides the hyperparameter configurations based
on the training data. It would use a different configuration if it is
predicted to outperform the original data-independent default. If you inspect
the params of the fitted estimator, you can find what configuration is used.
If the original default configuration is used, then it is equivalent to the
original estimator.

The recommendation of which configuration should be used is based on offline
AutoML run results. Information about the training dataset, such as the size
of the dataset will be used to recommend a data-dependent configuration. The
recommendation is done instantly in negligible time. The training can be
faster or slower than using the original default configuration depending on
the recommended configuration. Note that there is no tuning involved. Only one
model is trained.

### Can I check the configuration before training?​ {#can-i-check-the-configuration-before-training}

Yes. You can use `suggest_hyperparams()` to find the suggested configuration.
For example,

    from flaml.default import LGBMRegressor  
      
    estimator = LGBMRegressor()  
    hyperparams, estimator_name, X_transformed, y_transformed = estimator.suggest_hyperparams(X_train, y_train)  
    print(hyperparams)  

If you would like more control over the training, use an equivalent, open-box
way for zero-shot AutoML. For example,

    from flaml.default import preprocess_and_suggest_hyperparams  
      
    X, y = load_iris(return_X_y=True, as_frame=True)  
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)  
    hyperparams, estimator_class, X_transformed, y_transformed, feature_transformer, label_transformer = preprocess_and_suggest_hyperparams(  
        "classification", X_train, y_train, "lgbm"  
    )  
    model = estimator_class(**hyperparams)  # estimator_class is lightgbm.LGBMClassifier  
    model.fit(X_transformed, y_train)  # LGBMClassifier can handle raw labels  
    X_test = feature_transformer.transform(X_test)  # preprocess test data  
    y_pred = model.predict(X_test)  

Note that some classifiers like XGBClassifier require the labels to be
integers, while others do not. So you can decide whether to use the
transformed labels `y_transformed` and the label transformer
`label_transformer`. Also, each estimator may require specific preprocessing
of the data. `X_transformed` is the preprocessed data, and
`feature_transformer` is the preprocessor. It needs to be applied to the test
data before prediction. These are automated when you use the "flamlized"
learner. When you use the open-box way, pay attention to them.

### Combine zero shot AutoML and hyperparameter tuning​ {#combine-zero-shot-automl-and-hyperparameter-tuning}

Zero Shot AutoML is fast. If tuning from the recommended data-dependent
configuration is required, you can use `flaml.AutoML.fit()` and set
`starting_points="data"`. For example,

    from flaml import AutoML  
    automl = AutoML()  
    automl_settings = {  
        "task": "classification",  
        "starting_points": "data",  
        "estimator_list": ["lgbm"],  
        "time_budget": 600,  
        "max_iter": 50,  
    }  
    automl.fit(X_train, y_train, **automl_settings)  

Note that if you set `max_iter=0` and `time_budget=None`, you are effectively
using zero-shot AutoML. When `estimator_list` is omitted, the estimator
together with its hyperparameter configuration will be decided in a zero-shot
manner.

### Use your own meta-learned defaults​ {#use-your-own-metalearned-defaults}

To use your own meta-learned defaults, specify the path containing the meta-
learned defaults. For example,

    estimator = flaml.default.LGBMRegressor(default_location="location_for_defaults")  

Or,

    preprocess_and_suggest_hyperparams(  
        "classification", X_train, y_train, "lgbm", location="location_for_defaults"  
    )  

Or,

    X_train, y_train = load_iris(return_X_y=True, as_frame=as_frame)  
    automl = AutoML()  
    automl_settings = {  
        "task": "classification",  
        "log_file_name": "test/iris.log",  
        "starting_points": "data:location_for_defaults",  
        "estimator_list": ["lgbm", "xgb_limitdepth", "rf"]  
        "max_iter": 0,  
    }  
    automl.fit(X_train, y_train, **automl_settings)  

Since this is a multiclass task, it will look for the following files under
`{location_for_defaults}/`:

* `all/multiclass.json`.
* `{learner_name}/multiclass.json` for every learner_name in the estimator_list.

Read the next section to understand how to generate these files if you would
like to meta-learn the defaults yourself.

## How to Prepare Offline​ {#how-to-prepare-offline}

This section is intended for:

  1. AutoML providers for a particular domain.
  2. Data scientists or engineers who need to repeatedly train models for similar tasks with varying training data.

Instead of running full hyperparameter tuning from scratch every time, one can
leverage the tuning experiences in similar tasks before. While we have offered
the meta-learned defaults from tuning experiences of several popular learners
on benchmark datasets for classification and regression, you can customize the
defaults for your own tasks/learners/metrics based on your own tuning
experiences.

### Prepare a collection of training tasks​ {#prepare-a-collection-of-training-tasks}

Collect a diverse set of training tasks. For each task, extract its meta
feature and save in a .csv file. For example,
test/default/all/metafeatures.csv:

    Dataset,NumberOfInstances,NumberOfFeatures,NumberOfClasses,PercentageOfNumericFeatures  
    2dplanes,36691,10,0,1.0  
    adult,43957,14,2,0.42857142857142855  
    Airlines,485444,7,2,0.42857142857142855  
    Albert,382716,78,2,0.3333333333333333  
    Amazon_employee_access,29492,9,2,0.0  
    bng_breastTumor,104976,9,0,0.1111111111111111  
    bng_pbc,900000,18,0,0.5555555555555556  
    car,1555,6,4,0.0  
    connect-4,60801,42,3,0.0  
    dilbert,9000,2000,5,1.0  
    Dionis,374569,60,355,1.0  
    poker,922509,10,0,1.0  

The first column is the dataset name, and the latter four are meta features.

### Prepare the candidate configurations​ {#prepare-the-candidate-configurations}

You can extract the best configurations for each task in your collection of
training tasks by running flaml on each of them with a long enough budget.
Save the best configuration in a .json file under
`{location_for_defaults}/{learner_name}/{task_name}.json`. For example,

    X_train, y_train = load_iris(return_X_y=True, as_frame=as_frame)  
    automl.fit(X_train, y_train, estimator_list=["lgbm"], **settings)  
    automl.save_best_config("test/default/lgbm/iris.json")  

### Evaluate each candidate configuration on each task​ {#evaluate-each-candidate-configuration-on-each-task}

Save the evaluation results in a .csv file. For example, save the evaluation
results for lgbm under `test/default/lgbm/results.csv`:

    task,fold,type,result,params  
    2dplanes,0,regression,0.946366,{'_modeljson': 'lgbm/2dplanes.json'}  
    2dplanes,0,regression,0.907774,{'_modeljson': 'lgbm/adult.json'}  
    2dplanes,0,regression,0.901643,{'_modeljson': 'lgbm/Airlines.json'}  
    2dplanes,0,regression,0.915098,{'_modeljson': 'lgbm/Albert.json'}  
    2dplanes,0,regression,0.302328,{'_modeljson': 'lgbm/Amazon_employee_access.json'}  
    2dplanes,0,regression,0.94523,{'_modeljson': 'lgbm/bng_breastTumor.json'}  
    2dplanes,0,regression,0.945698,{'_modeljson': 'lgbm/bng_pbc.json'}  
    2dplanes,0,regression,0.946194,{'_modeljson': 'lgbm/car.json'}  
    2dplanes,0,regression,0.945549,{'_modeljson': 'lgbm/connect-4.json'}  
    2dplanes,0,regression,0.946232,{'_modeljson': 'lgbm/default.json'}  
    2dplanes,0,regression,0.945594,{'_modeljson': 'lgbm/dilbert.json'}  
    2dplanes,0,regression,0.836996,{'_modeljson': 'lgbm/Dionis.json'}  
    2dplanes,0,regression,0.917152,{'_modeljson': 'lgbm/poker.json'}  
    adult,0,binary,0.927203,{'_modeljson': 'lgbm/2dplanes.json'}  
    adult,0,binary,0.932072,{'_modeljson': 'lgbm/adult.json'}  
    adult,0,binary,0.926563,{'_modeljson': 'lgbm/Airlines.json'}  
    adult,0,binary,0.928604,{'_modeljson': 'lgbm/Albert.json'}  
    adult,0,binary,0.911171,{'_modeljson': 'lgbm/Amazon_employee_access.json'}  
    adult,0,binary,0.930645,{'_modeljson': 'lgbm/bng_breastTumor.json'}  
    adult,0,binary,0.928603,{'_modeljson': 'lgbm/bng_pbc.json'}  
    adult,0,binary,0.915825,{'_modeljson': 'lgbm/car.json'}  
    adult,0,binary,0.919499,{'_modeljson': 'lgbm/connect-4.json'}  
    adult,0,binary,0.930109,{'_modeljson': 'lgbm/default.json'}  
    adult,0,binary,0.932453,{'_modeljson': 'lgbm/dilbert.json'}  
    adult,0,binary,0.921959,{'_modeljson': 'lgbm/Dionis.json'}  
    adult,0,binary,0.910763,{'_modeljson': 'lgbm/poker.json'}  
    ...  

The `type` column indicates the type of the task, such as regression, binary
or multiclass. The `result` column stores the evaluation result, assumed the
large the better. The `params` column indicates which json config is used. For
example 'lgbm/2dplanes.json' indicates that the best lgbm configuration
extracted from 2dplanes is used. Different types of tasks can appear in the
same file, as long as any json config file can be used in all the tasks. For
example, 'lgbm/2dplanes.json' is extracted from a regression task, and it can
be applied to binary and multiclass tasks as well.

### Learn data-dependent defaults​ {#learn-datadependent-defaults}

To recap, the inputs required for meta-learning are:

  1. Metafeatures: e.g., `{location}/all/metafeatures.csv`.
  2. Configurations: `{location}/{learner_name}/{task_name}.json`.
  3. Evaluation results: `{location}/{learner_name}/results.csv`.

For example, if the input location is "test/default", learners are lgbm,
xgb_limitdepth and rf, the following command learns data-dependent defaults
for binary classification tasks.

    python portfolio.py --output test/default --input test/default --metafeatures test/default/all/metafeatures.csv --task binary --estimator lgbm xgb_limitdepth rf  

In a few seconds, it will produce the following files as output:

* test/default/lgbm/binary.json: the learned defaults for lgbm.
* test/default/xgb_limitdepth/binary.json: the learned defaults for xgb_limitdepth.
* test/default/rf/binary.json: the learned defaults for rf.
* test/default/all/binary.json: the learned defaults for lgbm, xgb_limitdepth and rf together.

Change "binary" into "multiclass" or "regression", or your own types in your
"results.csv" for the other types of tasks. To update the learned defaults
when more experiences are available, simply update your input files and rerun
the learning command.

### "Flamlize" a learner​ {#flamlize-a-learner}

You have now effectively built your own zero-shot AutoML solution.
Congratulations!

Optionally, you can "flamlize" a learner using
[`flaml.default.flamlize_estimator`](.docs/reference/default/estimator#flamlize_estimator)
for easy dissemination. For example,

    import sklearn.ensemble as ensemble  
    from flaml.default import flamlize_estimator  
      
    ExtraTreesClassifier = flamlize_estimator(  
        ensemble.ExtraTreesClassifier, "extra_tree", "classification"  
    )  

Then, you can share this "flamlized" `ExtraTreesClassifier` together with the
location of your learned defaults with others (or the _future_ yourself). They
will benefit from your past experience. Your group can also share experiences
in a central place and update the learned defaults continuously. Over time,
your organization gets better collectively.

[Edit this
page](<https://github.com/microsoft.edit/main/website/docs/Use->
Cases/Zero-Shot-AutoML.md)

[Previous« Tune User Defined Function](.docs/Use-Cases/Tune-User-
Defined-Function)

[NextAutoGen - Automated Multi Agent Chat »](.docs/Examples/AutoGen-
AgentChat)

* How to Use at Runtime
  * What's the magic behind the scene?
  * Can I check the configuration before training?
  * Combine zero shot AutoML and hyperparameter tuning
  * Use your own meta-learned defaults
* How to Prepare Offline
  * Prepare a collection of training tasks
  * Prepare the candidate configurations
  * Evaluate each candidate configuration on each task
  * Learn data-dependent defaults
  * "Flamlize" a learner

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
    * autogen.agentchat
      * [agent](.docs/reference/autogen/agentchat/agent)
      * [assistant_agent](.docs/reference/autogen/agentchat/assistant_agent)
      * [conversable_agent](.docs/reference/autogen/agentchat/conversable_agent)
      * [groupchat](.docs/reference/autogen/agentchat/groupchat)
      * [user_proxy_agent](.docs/reference/autogen/agentchat/user_proxy_agent)
    * autogen.oai
    * [code_utils](.docs/reference/autogen/code_utils)
    * [math_utils](.docs/reference/autogen/math_utils)
    * [retrieve_utils](.docs/reference/autogen/retrieve_utils)
  * automl
  * default
  * onlineml
  * tune

On this page

# autogen.agentchat.conversable_agent {#autogenagentchatconversableagent}

## ConversableAgent Objects​ {#conversableagent-objects}

    class ConversableAgent(Agent)  

(In preview) A class for generic conversable agents which can be configured as
assistant or user proxy.

After receiving each message, the agent will send a reply to the sender unless
the msg is a termination msg. For example, AssistantAgent and UserProxyAgent
are subclasses of this class, configured with different default settings.

To modify auto reply, override `generate_reply` method. To disable/enable
human response in every turn, set `human_input_mode` to "NEVER" or "ALWAYS".
To modify the way to get human input, override `get_human_input` method. To
modify the way to execute code blocks, single code block, or function call,
override `execute_code_blocks`, `run_code`, and `execute_function` methods
respectively. To customize the initial message when a conversation starts,
override `generate_init_message` method.

#### __init__​ {#init}

    def __init__(name: str, system_message: Optional[str] = "You are a helpful AI Assistant.", is_termination_msg: Optional[Callable[[Dict], bool]] = None, max_consecutive_auto_reply: Optional[int] = None, human_input_mode: Optional[str] = "TERMINATE", function_map: Optional[Dict[str, Callable]] = None, code_execution_config: Optional[Union[Dict, bool]] = None, llm_config: Optional[Union[Dict, bool]] = None, default_auto_reply: Optional[Union[str, Dict, None]] = "")  

 **Arguments** :

* `name` _str_ \- name of the agent.
* `system_message` _str_ \- system message for the ChatCompletion inference.
* `is_termination_msg` _function_ \- a function that takes a message in the form of a dictionary and returns a boolean value indicating if this received message is a termination message. The dict can contain the following keys: "content", "role", "name", "function_call".
* `max_consecutive_auto_reply` _int_ \- the maximum number of consecutive auto replies. default to None (no limit provided, class attribute MAX_CONSECUTIVE_AUTO_REPLY will be used as the limit in this case). When set to 0, no auto reply will be generated.
* `human_input_mode` _str_ \- whether to ask for human inputs every time a message is received. Possible values are "ALWAYS", "TERMINATE", "NEVER". (1) When "ALWAYS", the agent prompts for human input every time a message is received. Under this mode, the conversation stops when the human input is "exit", or when is_termination_msg is True and there is no human input. (2) When "TERMINATE", the agent only prompts for human input only when a termination message is received or the number of auto reply reaches the max_consecutive_auto_reply. (3) When "NEVER", the agent will never prompt for human input. Under this mode, the conversation stops when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.
* `function_map` _dict [str, callable]_ \- Mapping function names (passed to openai) to callable functions.
* `code_execution_config` _dict or False_ \- config for the code execution. To disable code execution, set to False. Otherwise, set to a dictionary with the following keys:
  * work_dir (Optional, str): The working directory for the code execution. If None, a default working directory will be used. The default working directory is the "extensions" directory under "path_to_flaml/autogen".
  * use_docker (Optional, list, str or bool): The docker image to use for code execution. If a list or a str of image name(s) is provided, the code will be executed in a docker container with the first image successfully pulled. If None, False or empty, the code will be executed in the current environment. Default is True, which will be converted into a list. If the code is executed in the current environment, the code must be trusted.
  * timeout (Optional, int): The maximum execution time in seconds.
  * last_n_messages (Experimental, Optional, int): The number of messages to look back for code execution. Default to 1.
* `llm_config` _dict or False_ \- llm inference configuration. Please refer to [autogen.Completion.create](.docs/reference/autogen/oai/completion#create) for available options. To disable llm-based auto reply, set to False.
* `default_auto_reply` _str or dict or None_ \- default auto reply when no code execution or llm-based reply is generated.

#### register_reply​ {#registerreply}

    def register_reply(trigger: Union[Type[Agent], str, Agent, Callable[[Agent], bool], List], reply_func: Callable, position: Optional[int] = 0, config: Optional[Any] = None, reset_config: Optional[Callable] = None)  

Register a reply function.

The reply function will be called when the trigger matches the sender. The
function registered later will be checked earlier by default. To change the
order, set the position to a positive integer.

 **Arguments** :

* `trigger` _Agent class, str, Agent instance, callable, or list_ \- the trigger.
  * If a class is provided, the reply function will be called when the sender is an instance of the class.
  * If a string is provided, the reply function will be called when the sender's name matches the string.
  * If an agent instance is provided, the reply function will be called when the sender is the agent instance.
  * If a callable is provided, the reply function will be called when the callable returns True.
  * If a list is provided, the reply function will be called when any of the triggers in the list is activated.
  * If None is provided, the reply function will be called only when the sender is None.
* `Note` \- Be sure to register `None` as a trigger if you would like to trigger an auto-reply function with non-empty messages and `sender=None`.
* `reply_func` _Callable_ \- the reply function. The function takes a recipient agent, a list of messages, a sender agent and a config as input and returns a reply message.

    def reply_func(  
        recipient: ConversableAgent,  
        messages: Optional[List[Dict]] = None,  
        sender: Optional[Agent] = None,  
        config: Optional[Any] = None,  
    ) -> Union[str, Dict, None]:  

* `position` _int_ \- the position of the reply function in the reply function list. The function registered later will be checked earlier by default. To change the order, set the position to a positive integer.
* `config` _Any_ \- the config to be passed to the reply function. When an agent is reset, the config will be reset to the original value.
* `reset_config` _Callable_ \- the function to reset the config. The function returns None. Signature: `def reset_config(config: Any)`

#### system_message​ {#systemmessage}

    @property  
    def system_message()  

Return the system message.

#### update_system_message​ {#updatesystemmessage}

    def update_system_message(system_message: str)  

Update the system message.

 **Arguments** :

* `system_message` _str_ \- system message for the ChatCompletion inference.

#### update_max_consecutive_auto_reply​ {#updatemaxconsecutiveautoreply}

    def update_max_consecutive_auto_reply(value: int, sender: Optional[Agent] = None)  

Update the maximum number of consecutive auto replies.

 **Arguments** :

* `value` _int_ \- the maximum number of consecutive auto replies.
* `sender` _Agent_ \- when the sender is provided, only update the max_consecutive_auto_reply for that sender.

#### max_consecutive_auto_reply​ {#maxconsecutiveautoreply}

    def max_consecutive_auto_reply(sender: Optional[Agent] = None) -> int  

The maximum number of consecutive auto replies.

#### chat_messages​ {#chatmessages}

    @property  
    def chat_messages() -> Dict[str, List[Dict]]  

A dictionary of conversations from name to list of ChatCompletion messages.

#### last_message​ {#lastmessage}

    def last_message(agent: Optional[Agent] = None) -> Dict  

The last message exchanged with the agent.

 **Arguments** :

* `agent` _Agent_ \- The agent in the conversation. If None and more than one agent's conversations are found, an error will be raised. If None and only one conversation is found, the last message of the only conversation will be returned.

 **Returns** :

The last message exchanged with the agent.

#### use_docker​ {#usedocker}

    @property  
    def use_docker() -> Union[bool, str, None]  

Bool value of whether to use docker to execute the code, or str value of the
docker image name to use, or None when code execution is disabled.

#### send​ {#send}

    def send(message: Union[Dict, str], recipient: Agent, request_reply: Optional[bool] = None, silent: Optional[bool] = False) -> bool  

Send a message to another agent.

 **Arguments** :

* `message` _dict or str_ \- message to be sent. The message could contain the following fields (either content or function_call must be provided):
  * content (str): the content of the message.
  * function_call (str): the name of the function to be called.
  * name (str): the name of the function to be called.
  * role (str): the role of the message, any role that is not "function" will be modified to "assistant".
  * context (dict): the context of the message, which will be passed to [autogen.Completion.create](.docs/reference/autogen/oai/Completion#create). For example, one agent can send a message A as:

    {  
        "content": lambda context: context["use_tool_msg"],  
        "context": {  
            "use_tool_msg": "Use tool X if they are relevant."  
        }  
    }  

Next time, one agent can send a message B with a different "use_tool_msg".
Then the content of message A will be refreshed to the new "use_tool_msg". So
effectively, this provides a way for an agent to send a "link" and modify the
content of the "link" later.

* `recipient` _Agent_ \- the recipient of the message.
* `request_reply` _bool or None_ \- whether to request a reply from the recipient.
* `silent` _bool or None_ \- (Experimental) whether to print the message sent.

 **Raises** :

* `ValueError` \- if the message can't be converted into a valid ChatCompletion message.

#### a_send​ {#asend}

    async def a_send(message: Union[Dict, str], recipient: Agent, request_reply: Optional[bool] = None, silent: Optional[bool] = False) -> bool  

(async) Send a message to another agent.

 **Arguments** :

* `message` _dict or str_ \- message to be sent. The message could contain the following fields (either content or function_call must be provided):
  * content (str): the content of the message.
  * function_call (str): the name of the function to be called.
  * name (str): the name of the function to be called.
  * role (str): the role of the message, any role that is not "function" will be modified to "assistant".
  * context (dict): the context of the message, which will be passed to [autogen.Completion.create](.docs/reference/autogen/oai/Completion#create). For example, one agent can send a message A as:

    {  
        "content": lambda context: context["use_tool_msg"],  
        "context": {  
            "use_tool_msg": "Use tool X if they are relevant."  
        }  
    }  

Next time, one agent can send a message B with a different "use_tool_msg".
Then the content of message A will be refreshed to the new "use_tool_msg". So
effectively, this provides a way for an agent to send a "link" and modify the
content of the "link" later.

* `recipient` _Agent_ \- the recipient of the message.
* `request_reply` _bool or None_ \- whether to request a reply from the recipient.
* `silent` _bool or None_ \- (Experimental) whether to print the message sent.

 **Raises** :

* `ValueError` \- if the message can't be converted into a valid ChatCompletion message.

#### receive​ {#receive}

    def receive(message: Union[Dict, str], sender: Agent, request_reply: Optional[bool] = None, silent: Optional[bool] = False)  

Receive a message from another agent.

Once a message is received, this function sends a reply to the sender or stop.
The reply can be generated automatically or entered manually by a human.

 **Arguments** :

* `message` _dict or str_ \- message from the sender. If the type is dict, it may contain the following reserved fields (either content or function_call need to be provided).
    1. "content": content of the message, can be None.
    2. "function_call": a dictionary containing the function name and arguments.
    3. "role": role of the message, can be "assistant", "user", "function". This field is only needed to distinguish between "function" or "assistant"/"user".
    4. "name": In most cases, this field is not needed. When the role is "function", this field is needed to indicate the function name.
    5. "context" (dict): the context of the message, which will be passed to [autogen.Completion.create](.docs/reference/autogen/oai/Completion#create).
* `sender` \- sender of an Agent instance.
* `request_reply` _bool or None_ \- whether a reply is requested from the sender. If None, the value is determined by `self.reply_at_receive[sender]`.
* `silent` _bool or None_ \- (Experimental) whether to print the message received.

 **Raises** :

* `ValueError` \- if the message can't be converted into a valid ChatCompletion message.

#### a_receive​ {#areceive}

    async def a_receive(message: Union[Dict, str], sender: Agent, request_reply: Optional[bool] = None, silent: Optional[bool] = False)  

(async) Receive a message from another agent.

Once a message is received, this function sends a reply to the sender or stop.
The reply can be generated automatically or entered manually by a human.

 **Arguments** :

* `message` _dict or str_ \- message from the sender. If the type is dict, it may contain the following reserved fields (either content or function_call need to be provided).
    1. "content": content of the message, can be None.
    2. "function_call": a dictionary containing the function name and arguments.
    3. "role": role of the message, can be "assistant", "user", "function". This field is only needed to distinguish between "function" or "assistant"/"user".
    4. "name": In most cases, this field is not needed. When the role is "function", this field is needed to indicate the function name.
    5. "context" (dict): the context of the message, which will be passed to [autogen.Completion.create](.docs/reference/autogen/oai/Completion#create).
* `sender` \- sender of an Agent instance.
* `request_reply` _bool or None_ \- whether a reply is requested from the sender. If None, the value is determined by `self.reply_at_receive[sender]`.
* `silent` _bool or None_ \- (Experimental) whether to print the message received.

 **Raises** :

* `ValueError` \- if the message can't be converted into a valid ChatCompletion message.

#### initiate_chat​ {#initiatechat}

    def initiate_chat(recipient: "ConversableAgent", clear_history: Optional[bool] = True, silent: Optional[bool] = False, **context, ,)  

Initiate a chat with the recipient agent.

Reset the consecutive auto reply counter. If `clear_history` is True, the chat
history with the recipient agent will be cleared. `generate_init_message` is
called to generate the initial message for the agent.

 **Arguments** :

* `recipient` \- the recipient agent.
* `clear_history` _bool_ \- whether to clear the chat history with the agent.
* `silent` _bool or None_ \- (Experimental) whether to print the messages for this conversation.
* `**context` \- any context information. "message" needs to be provided if the `generate_init_message` method is not overridden.

#### a_initiate_chat​ {#ainitiatechat}

    async def a_initiate_chat(recipient: "ConversableAgent", clear_history: Optional[bool] = True, silent: Optional[bool] = False, **context, ,)  

(async) Initiate a chat with the recipient agent.

Reset the consecutive auto reply counter. If `clear_history` is True, the chat
history with the recipient agent will be cleared. `generate_init_message` is
called to generate the initial message for the agent.

 **Arguments** :

* `recipient` \- the recipient agent.
* `clear_history` _bool_ \- whether to clear the chat history with the agent.
* `silent` _bool or None_ \- (Experimental) whether to print the messages for this conversation.
* `**context` \- any context information. "message" needs to be provided if the `generate_init_message` method is not overridden.

#### reset​ {#reset}

    def reset()  

Reset the agent.

#### stop_reply_at_receive​ {#stopreplyatreceive}

    def stop_reply_at_receive(sender: Optional[Agent] = None)  

Reset the reply_at_receive of the sender.

#### reset_consecutive_auto_reply_counter​ {#resetconsecutiveautoreplycounter}

    def reset_consecutive_auto_reply_counter(sender: Optional[Agent] = None)  

Reset the consecutive_auto_reply_counter of the sender.

#### clear_history​ {#clearhistory}

    def clear_history(agent: Optional[Agent] = None)  

Clear the chat history of the agent.

 **Arguments** :

* `agent` \- the agent with whom the chat history to clear. If None, clear the chat history with all agents.

#### generate_oai_reply​ {#generateoaireply}

    def generate_oai_reply(messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, config: Optional[Any] = None) -> Tuple[bool, Union[str, Dict, None]]  

Generate a reply using autogen.oai.

#### generate_code_execution_reply​ {#generatecodeexecutionreply}

    def generate_code_execution_reply(messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, config: Optional[Any] = None)  

Generate a reply using code execution.

#### generate_function_call_reply​ {#generatefunctioncallreply}

    def generate_function_call_reply(messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, config: Optional[Any] = None)  

Generate a reply using function call.

#### check_termination_and_human_reply​ {#checkterminationandhumanreply}

    def check_termination_and_human_reply(messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, config: Optional[Any] = None) -> Tuple[bool, Union[str, Dict, None]]  

Check if the conversation should be terminated, and if human reply is
provided.

#### generate_reply​ {#generatereply}

    def generate_reply(messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, exclude: Optional[List[Callable]] = None) -> Union[str, Dict, None]  

Reply based on the conversation history and the sender.

Either messages or sender must be provided. Register a reply_func with `None`
as one trigger for it to be activated when `messages` is non-empty and
`sender` is `None`. Use registered auto reply functions to generate replies.
By default, the following functions are checked in order:

  1. check_termination_and_human_reply
  2. generate_function_call_reply
  3. generate_code_execution_reply
  4. generate_oai_reply Every function returns a tuple (final, reply). When a function returns final=False, the next function will be checked. So by default, termination and human reply will be checked first. If not terminating and human reply is skipped, execute function or code and return the result. AI replies are generated only when no code execution is performed.

 **Arguments** :

* `messages` \- a list of messages in the conversation history.
* `default_reply` _str or dict_ \- default reply.
* `sender` \- sender of an Agent instance.
* `exclude` \- a list of functions to exclude.

 **Returns** :

str or dict or None: reply. None if no reply is generated.

#### a_generate_reply​ {#ageneratereply}

    async def a_generate_reply(messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, exclude: Optional[List[Callable]] = None) -> Union[str, Dict, None]  

(async) Reply based on the conversation history and the sender.

Either messages or sender must be provided. Register a reply_func with `None`
as one trigger for it to be activated when `messages` is non-empty and
`sender` is `None`. Use registered auto reply functions to generate replies.
By default, the following functions are checked in order:

  1. check_termination_and_human_reply
  2. generate_function_call_reply
  3. generate_code_execution_reply
  4. generate_oai_reply Every function returns a tuple (final, reply). When a function returns final=False, the next function will be checked. So by default, termination and human reply will be checked first. If not terminating and human reply is skipped, execute function or code and return the result. AI replies are generated only when no code execution is performed.

 **Arguments** :

* `messages` \- a list of messages in the conversation history.
* `default_reply` _str or dict_ \- default reply.
* `sender` \- sender of an Agent instance.
* `exclude` \- a list of functions to exclude.

 **Returns** :

str or dict or None: reply. None if no reply is generated.

#### get_human_input​ {#gethumaninput}

    def get_human_input(prompt: str) -> str  

Get human input.

Override this method to customize the way to get human input.

 **Arguments** :

* `prompt` _str_ \- prompt for the human input.

 **Returns** :

* `str` \- human input.

#### run_code​ {#runcode}

    def run_code(code, **kwargs)  

Run the code and return the result.

Override this function to modify the way to run the code.

 **Arguments** :

* `code` _str_ \- the code to be executed.
* `**kwargs` \- other keyword arguments.

 **Returns** :

A tuple of (exitcode, logs, image).

* `exitcode` _int_ \- the exit code of the code execution.
* `logs` _str_ \- the logs of the code execution.
* `image` _str or None_ \- the docker image used for the code execution.

#### execute_code_blocks​ {#executecodeblocks}

    def execute_code_blocks(code_blocks)  

Execute the code blocks and return the result.

#### execute_function​ {#executefunction}

    def execute_function(func_call)  

Execute a function call and return the result.

Override this function to modify the way to execute a function call.

 **Arguments** :

* `func_call` \- a dictionary extracted from openai message at key "function_call" with keys "name" and "arguments".

 **Returns** :

A tuple of (is_exec_success, result_dict).

* `is_exec_success` _boolean_ \- whether the execution is successful.
* `result_dict` \- a dictionary with keys "name", "role", and "content". Value of "role" is "function".

#### generate_init_message​ {#generateinitmessage}

    def generate_init_message(**context) -> Union[str, Dict]  

Generate the initial message for the agent.

Override this function to customize the initial message based on user's
request. If not overriden, "message" needs to be provided in the context.

#### register_function​ {#registerfunction}

    def register_function(function_map: Dict[str, Callable])  

Register functions to the agent.

 **Arguments** :

* `function_map` \- a dictionary mapping function names to functions.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/autogen/agentchat/conversable_agent.md)

[Previous«
assistant_agent](.docs/reference/autogen/agentchat/assistant_agent)

[Nextgroupchat »](.docs/reference/autogen/agentchat/groupchat)

* ConversableAgent Objects

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

Recent posts

* [Use flaml.autogen for Local LLMs](.blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](.blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](.blog/2023/05/18/GPT-adaptive-humaneval)
* [Surpassing 1 Million Downloads - A Retrospective and a Look into the Future](.blog/2023/05/07/1M-milestone)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](.blog/2023/04/21/LLM-tuning-math)

# Surpassing 1 Million Downloads - A Retrospective and a Look into the Future {#surpassing-1-million-downloads-a-retrospective-and-a-look-into-the-future}

May 7, 2023 · 4 min read

[![Qingyun Wu](https://github.com/qingyun-wu.png)](<https://qingyun->
wu.github.io/)

[Qingyun Wu](https://qingyun-wu.github.io/)

Assistant Professor at the Pennsylvania State University

 **TL;DR:**

* **Celebrating FLAML 's milestone: 1 million downloads**
* **Introducing Large Language Model (LLM) support in the upcoming FLAML v2**

This week, FLAML has reached a significant milestone: 1 million downloads.
Originating as an intern research project within Microsoft Research, FLAML has
grown into an open-source library used widely across the industry and
supported by an active community. As we celebrate this milestone, we want to
recognize the passionate contributors and users who have played an essential
role in molding FLAML into the flourishing project it is today. Our heartfelt
gratitude goes out to each of you for your unwavering support, constructive
feedback, and innovative contributions that have driven FLAML to new heights.
A big shoutout to our industrial collaborators from Azure Core, Azure Machine
Learning, Azure Synapse Analytics, Microsoft 365, ML.NET, Vowpal Wabbit,
Anyscale, Databricks, and Wise; and academic collaborators from MIT, Penn
State University, Stevens Institute of Technology, Tel Aviv University, Texas
A & M University, University of Manchester, University of Washington, and The
Chinese University of Hong Kong etc.

We'd also like to take the opportunity to reflect on FLAML's past achievements
and its future roadmap, with a particular focus on large language models (LLM)
and LLMOps.

## FLAML's Journey: Past Achievements and Milestones​ {#flamls-journey-past-achievements-and-milestones}

### Bring AutoML to One's Fingertips​ {#bring-automl-to-ones-fingertips}

FLAML offers an off-the-shelf AutoML solution that enables users to quickly
discover high-quality models or configurations for common ML/AI tasks. By
automatically selecting models and hyperparameters for training or inference,
FLAML saves users time and effort. FLAML has significantly reduced development
time for developers and data scientists alike, while also providing a
convenient way to integrate new algorithms into the pipeline, enabling easy
extensions and large-scale parallel tuning. These features make FLAML a
valuable tool in R&D efforts for many enterprise users. FLAML is capable of
handling a variety of common ML tasks, such as
[classification](<https://microsoft.github.io.docs/Examples/AutoML->
Classification),
[regression](<https://microsoft.github.io.docs/Examples/AutoML->
Regression), [time series
forecasting](<https://microsoft.github.io.docs/Examples/AutoML->
Time%20series%20forecast), [NLP
tasks](https://microsoft.github.io.docs/Examples/AutoML-Rank), and
[generative tasks](https://microsoft.github.io/autogen/), providing a
comprehensive solution for various applications.

### Speed and Efficiency: The FLAML Advantage​ {#speed-and-efficiency-the-flaml-advantage}

What sets FLAML apart from other AutoML libraries is its exceptional
efficiency, thanks to the economical and efficient hyperparameter optimization
and model selection methods developed in our
[research](https://microsoft.github.io.docs/Research). FLAML is also
capable of handling large search spaces with heterogeneous evaluation costs,
complex constraints, guidance, and early stopping. The [zero-shot
AutoML](https://microsoft.github.io.docs/Use-Cases/Zero-Shot-AutoML)
option further reduces the cost of AutoML, making FLAML an even more
attractive solution for a wide range of applications with low resources.

### Easy Customization and Extensibility​ {#easy-customization-and-extensibility}

FLAML is designed for easy extensibility and customization, allowing users to
add custom learners, metrics, search space, etc. For example, the support of
hierarchical search spaces allows one to first choose an ML learner and then
sampling from the hyperparameter space specific to that learner. The level of
customization ranges from minimal (providing only training data and task type
as input) to full (tuning a user-defined function). This flexibility and
support for easy customization have led to FLAML's adoption in various
domains, including security, finance, marketing, engineering, supply chain,
insurance, and healthcare, delivering highly accurate results.

## Embracing Large Language Models in FLAML v2​ {#embracing-large-language-models-in-flaml-v2}

As large language models continue to reshape the AI ecosystem, FLAML is poised
to adapt and grow alongside these advancements. Recognizing the importance of
large language models, we have recently incorporated an autogen package into
FLAML, and are committed to focusing our collective efforts on addressing the
unique challenges that arise in LLMOps (Large Language Model Operations).

In its current iteration, FLAML offers support for model selection and
inference parameter tuning for large language models. We are actively working
on the development of new features, such as low-level inference API with
caching, templating, filtering, and higher-level components like LLM-based
coding and interactive agents, to enable more effective and economical usage
of LLM.

We are eagerly preparing for the launch of FLAML v2, where we will place
special emphasis on incorporating and enhancing features specifically tailored
for large language models (LLMs), further expanding FLAML's capabilities. We
invite contributions from anyone interested in this topic and look forward to
collaborating with the community as we shape the future of FLAML and LLMOps
together.

## For Further Reading​ {#for-further-reading}

* [Documentation about `flaml.autogen`](https://microsoft.github.io/autogen/)
* [Code Example: Tune chatGPT for Math Problem Solving with FLAML](https://github.com/microsoft.blob/main/notebook/autogen_chatgpt_gpt4.ipynb)

 _Do you have any experience to share about LLM applications? Do you like to
see more support or research of LLMOps? Please join
our[Discord](https://discord.gg/Cppx2vSPVP) server for discussion._

 **Tags:**

* [LLM](.blog/tags/llm)
* [LLMOps](.blog/tags/llm-ops)
* [FLAMLv2](.blog/tags/flam-lv-2)

[Newer Post« Achieve More, Pay Less - Use GPT-4
Smartly](.blog/2023/05/18/GPT-adaptive-humaneval)

[Older PostDoes Model and Inference Parameter Matter in LLM Applications? - A
Case Study for MATH »](.blog/2023/04/21/LLM-tuning-math)

* FLAML's Journey: Past Achievements and Milestones
  * Bring AutoML to One's Fingertips
  * Speed and Efficiency: The FLAML Advantage
  * Easy Customization and Extensibility
* Embracing Large Language Models in FLAML v2
* For Further Reading

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* [Getting Started](.docs/Getting-Started)
* [Installation](.docs/Installation)
* Use Cases
* Examples
* [Contributing](.docs/Contribute)
* [Research](.docs/Research)

# Research {#research}

For technical details, please check our research publications.

* [FLAML: A Fast and Lightweight AutoML Library](https://www.microsoft.com/en-us/research/publication/flaml-a-fast-and-lightweight-automl-library/). Chi Wang, Qingyun Wu, Markus Weimer, Erkang Zhu. MLSys 2021.

    @inproceedings{wang2021flaml,  
        title={FLAML: A Fast and Lightweight AutoML Library},  
        author={Chi Wang and Qingyun Wu and Markus Weimer and Erkang Zhu},  
        year={2021},  
        booktitle={MLSys},  
    }  

* [Frugal Optimization for Cost-related Hyperparameters](https://arxiv.org/abs/2005.01571). Qingyun Wu, Chi Wang, Silu Huang. AAAI 2021.

    @inproceedings{wu2021cfo,  
        title={Frugal Optimization for Cost-related Hyperparameters},  
        author={Qingyun Wu and Chi Wang and Silu Huang},  
        year={2021},  
        booktitle={AAAI},  
    }  

* [Economical Hyperparameter Optimization With Blended Search Strategy](https://www.microsoft.com/en-us/research/publication/economical-hyperparameter-optimization-with-blended-search-strategy/). Chi Wang, Qingyun Wu, Silu Huang, Amin Saied. ICLR 2021.

    @inproceedings{wang2021blendsearch,  
        title={Economical Hyperparameter Optimization With Blended Search Strategy},  
        author={Chi Wang and Qingyun Wu and Silu Huang and Amin Saied},  
        year={2021},  
        booktitle={ICLR},  
    }  

* [An Empirical Study on Hyperparameter Optimization for Fine-Tuning Pre-trained Language Models](https://aclanthology.org/2021.acl-long.178.pdf). Susan Xueqing Liu, Chi Wang. ACL 2021.

    @inproceedings{liuwang2021hpolm,  
        title={An Empirical Study on Hyperparameter Optimization for Fine-Tuning Pre-trained Language Models},  
        author={Susan Xueqing Liu and Chi Wang},  
        year={2021},  
        booktitle={ACL},  
    }  

* [ChaCha for Online AutoML](https://www.microsoft.com/en-us/research/publication/chacha-for-online-automl/). Qingyun Wu, Chi Wang, John Langford, Paul Mineiro and Marco Rossi. ICML 2021.

    @inproceedings{wu2021chacha,  
        title={ChaCha for Online AutoML},  
        author={Qingyun Wu and Chi Wang and John Langford and Paul Mineiro and Marco Rossi},  
        year={2021},  
        booktitle={ICML},  
    }  

* [Fair AutoML](https://arxiv.org/abs/2111.06495). Qingyun Wu, Chi Wang. ArXiv preprint arXiv:2111.06495 (2021).

    @inproceedings{wuwang2021fairautoml,  
        title={Fair AutoML},  
        author={Qingyun Wu and Chi Wang},  
        year={2021},  
        booktitle={ArXiv preprint arXiv:2111.06495},  
    }  

* [Mining Robust Default Configurations for Resource-constrained AutoML](https://arxiv.org/abs/2202.09927). Moe Kayali, Chi Wang. ArXiv preprint arXiv:2202.09927 (2022).

    @inproceedings{kayaliwang2022default,  
        title={Mining Robust Default Configurations for Resource-constrained AutoML},  
        author={Moe Kayali and Chi Wang},  
        year={2022},  
        booktitle={ArXiv preprint arXiv:2202.09927},  
    }  

* [Targeted Hyperparameter Optimization with Lexicographic Preferences Over Multiple Objectives](https://openreview.net/forum?id=0Ij9_q567Ma). Shaokun Zhang, Feiran Jia, Chi Wang, Qingyun Wu. ICLR 2023 (notable-top-5%).

    @inproceedings{zhang2023targeted,  
        title={Targeted Hyperparameter Optimization with Lexicographic Preferences Over Multiple Objectives},  
        author={Shaokun Zhang and Feiran Jia and Chi Wang and Qingyun Wu},  
        booktitle={International Conference on Learning Representations},  
        year={2023},  
        url={<https://openreview.net/forum?id=0Ij9_q567Ma}>,  
    }  

* [Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference](https://arxiv.org/abs/2303.04673). Chi Wang, Susan Xueqing Liu, Ahmed H. Awadallah. ArXiv preprint arXiv:2303.04673 (2023).

    @inproceedings{wang2023EcoOptiGen,  
        title={Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference},  
        author={Chi Wang and Susan Xueqing Liu and Ahmed H. Awadallah},  
        year={2023},  
        booktitle={ArXiv preprint arXiv:2303.04673},  
    }  

* [An Empirical Study on Challenging Math Problem Solving with GPT-4](https://arxiv.org/abs/2306.01337). Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, Chi Wang. ArXiv preprint arXiv:2306.01337 (2023).

    @inproceedings{wu2023empirical,  
        title={An Empirical Study on Challenging Math Problem Solving with GPT-4},  
        author={Yiran Wu and Feiran Jia and Shaokun Zhang and Hangyu Li and Erkang Zhu and Yue Wang and Yin Tat Lee and Richard Peng and Qingyun Wu and Chi Wang},  
        year={2023},  
        booktitle={ArXiv preprint arXiv:2306.01337},  
    }  

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/Research.md)

[Previous« Contributing](.docs/Contribute)

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
  * automl
    * automl.contrib
    * automl.nlp
    * automl.spark
    * automl.task
    * automl.time_series
      * [sklearn](.docs/reference/automl/time_series/sklearn)
      * [tft](.docs/reference/automl/time_series/tft)
      * [ts_data](.docs/reference/automl/time_series/ts_data)
      * [ts_model](.docs/reference/automl/time_series/ts_model)
    * [automl](.docs/reference/automl/automl)
    * [data](.docs/reference/automl/data)
    * [ml](.docs/reference/automl/ml)
    * [model](.docs/reference/automl/model)
    * [state](.docs/reference/automl/state)
  * default
  * onlineml
  * tune

On this page

# automl.time_series.tft {#automltimeseriestft}

## TemporalFusionTransformerEstimator Objects​ {#temporalfusiontransformerestimator-objects}

    class TemporalFusionTransformerEstimator(TimeSeriesEstimator)  

The class for tuning Temporal Fusion Transformer

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/automl/time_series/tft.md)

[Previous« sklearn](.docs/reference/automl/time_series/sklearn)

[Nextts_data »](.docs/reference/automl/time_series/ts_data)

* TemporalFusionTransformerEstimator Objects

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)
**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-
Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)

Resources

* [Ecosystem](/autogen/docs/Ecosystem)
* [Gallery](/autogen/docs/Gallery)

[GitHub](https://github.com/microsoft/autogen)

🌜

🌞

`ctrl``K`

Recent posts

* [All About Agent Descriptions](/autogen/blog/2023/12/29/AgentDescriptions)
* [AgentOptimizer - An Agentic Way to Train Your LLM Agent](/autogen/blog/2023/12/23/AgentOptimizer)
* [AutoGen Studio: Interactively Explore Multi-Agent Workflows](/autogen/blog/2023/12/01/AutoGenStudio)
* [Agent AutoBuild - Automatically Building Multi-agent Systems](/autogen/blog/2023/11/26/Agent-AutoBuild)
* [How to Assess Utility of LLM-powered Applications?](/autogen/blog/2023/11/20/AgentEval)
* [AutoGen Meets GPTs](/autogen/blog/2023/11/13/OAI-assistants)
* [EcoAssistant - Using LLM Assistants More Accurately and Affordably](/autogen/blog/2023/11/09/EcoAssistant)
* [Multimodal with GPT-4V and LLaVA](/autogen/blog/2023/11/06/LMM-Agent)
* [AutoGen's Teachable Agents](/autogen/blog/2023/10/26/TeachableAgent)
* [Retrieval-Augmented Generation (RAG) Applications with AutoGen](/autogen/blog/2023/10/18/RetrieveChat)
* [Use AutoGen for Local LLMs](/autogen/blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](/autogen/blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](/autogen/blog/2023/05/18/GPT-adaptive-humaneval)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](/autogen/blog/2023/04/21/LLM-tuning-math)

# Agent AutoBuild - Automatically Building Multi-agent Systems {#agent-autobuild-automatically-building-multiagent-systems}

November 26, 2023 · 7 min read

[![Linxin
Song](https://github.com/LinxinS97.png)](https://linxins97.github.io/)

[Linxin Song](https://linxins97.github.io/)

MS student at Waseda University

[![Jieyu Zhang](https://github.com/jieyuz2.png)](https://jieyuz2.github.io/)

[Jieyu Zhang](https://jieyuz2.github.io/)

PhD student at University of Washington

![Overall structure of
AutoBuild](/autogen/assets/images/agent_autobuild-e48543a81e85bb185c7365db1290a91a.png)

 **TL;DR:** Introducing **AutoBuild** , building multi-agent system
automatically, fast, and easily for complex tasks with minimal user prompt
required, powered by a new designed class **AgentBuilder**. AgentBuilder also
supports open-source LLMs by leveraging
[vLLM](https://docs.vllm.ai/en/latest/index.html) and
[FastChat](https://github.com/lm-sys/FastChat). Checkout example notebooks and
source code for reference:

* [AutoBuild Examples](https://github.com/microsoft/autogen/blob/main/notebook/autobuild_basic.ipynb)
* [AgentBuilder](https://github.com/microsoft/autogen/blob/main/autogen/agentchat/contrib/agent_builder.py)

## Introduction​ {#introduction}

In this blog, we introduce **AutoBuild** , a pipeline that can automatically
build multi-agent systems for complex tasks. Specifically, we design a new
class called **AgentBuilder** , which will complete the generation of
participant expert agents and the construction of group chat automatically
after the user provides descriptions of a building task and an execution task.

AgentBuilder supports open-source models on Hugging Face powered by
[vLLM](https://docs.vllm.ai/en/latest/index.html) and
[FastChat](https://github.com/lm-sys/FastChat). Once the user chooses to use
open-source LLM, AgentBuilder will set up an endpoint server automatically
without any user participation.

## Installation​ {#installation}

* AutoGen:

    pip install pyautogen[autobuild]  

* (Optional: if you want to use open-source LLMs) vLLM and FastChat

    pip install vllm fastchat  

## Basic Example​ {#basic-example}

In this section, we provide a step-by-step example of how to use AgentBuilder
to build a multi-agent system for a specific task.

### Step 1: prepare configurations​ {#step-1-prepare-configurations}

First, we need to prepare the Agent configurations. Specifically, a config
path containing the model name and API key, and a default config for each
agent, are required.

    config_file_or_env = '/home/elpis_ubuntu/LLM/autogen/OAI_CONFIG_LIST'  # modify path  
    default_llm_config = {  
        'temperature': 0  
    }  

### Step 2: create an AgentBuilder instance​ {#step-2-create-an-agentbuilder-instance}

Then, we create an AgentBuilder instance with the config path and default
config. You can also specific the builder model and agent model, which are the
LLMs used for building and agent respectively.

    from autogen.agentchat.contrib.agent_builder import AgentBuilder  
      
    builder = AgentBuilder(config_file_or_env=config_file_or_env, builder_model='gpt-4-1106-preview', agent_model='gpt-4-1106-preview')  

### Step 3: specify the building task​ {#step-3-specify-the-building-task}

Specify a building task with a general description. Building task will help
the build manager (a LLM) decide what agents should be built. Note that your
building task should have a general description of the task. Adding some
specific examples is better.

    building_task = "Find a paper on arxiv by programming, and analyze its application in some domain. For example, find a latest paper about gpt-4 on arxiv and find its potential applications in software."  

### Step 4: build group chat agents​ {#step-4-build-group-chat-agents}

Use `build()` to let the build manager (with a `builder_model` as backbone)
complete the group chat agents generation. If you think coding is necessary
for your task, you can use `coding=True` to add a user proxy (a local code
interpreter) into the agent list as:

    agent_list, agent_configs = builder.build(building_task, default_llm_config, coding=True)  

If `coding` is not specified, AgentBuilder will determine on its own whether
the user proxy should be added or not according to the task. The generated
`agent_list` is a list of `AssistantAgent` instances. If `coding` is true, a
user proxy (a `UserProxyAssistant` instance) will be added as the first
element to the `agent_list`. `agent_configs` is a list of agent configurations
including agent name, backbone LLM model, and system message. For example

    // an example of agent_configs. AgentBuilder will generate agents with the following configurations.  
    [  
        {  
            "name": "ArXiv_Data_Scraper_Developer",  
            "model": "gpt-4-1106-preview",  
            "system_message": "You are now in a group chat. You need to complete a task with other participants. As an ArXiv_Data_Scraper_Developer, your focus is to create and refine tools capable of intelligent search and data extraction from arXiv, honing in on topics within the realms of computer science and medical science. Utilize your proficiency in Python programming to design scripts that navigate, query, and parse information from the platform, generating valuable insights and datasets for analysis. \n\nDuring your mission, it\u2019s not just about formulating queries; your role encompasses the optimization and precision of the data retrieval process, ensuring relevance and accuracy of the information extracted. If you encounter an issue with a script or a discrepancy in the expected output, you are encouraged to troubleshoot and offer revisions to the code you find in the group chat.\n\nWhen you reach a point where the existing codebase does not fulfill task requirements or if the operation of provided code is unclear, you should ask for help from the group chat manager. They will facilitate your advancement by providing guidance or appointing another participant to assist you. Your ability to adapt and enhance scripts based on peer feedback is critical, as the dynamic nature of data scraping demands ongoing refinement of techniques and approaches.\n\nWrap up your participation by confirming the user's need has been satisfied with the data scraping solutions you've provided. Indicate the completion of your task by replying \"TERMINATE\" in the group chat.",  
            "description": "ArXiv_Data_Scraper_Developer is a specialized software development role requiring proficiency in Python, including familiarity with web scraping libraries such as BeautifulSoup or Scrapy, and a solid understanding of APIs and data parsing. They must possess the ability to identify and correct errors in existing scripts and confidently engage in technical discussions to improve data retrieval processes. The role also involves a critical eye for troubleshooting and optimizing code to ensure efficient data extraction from the ArXiv platform for research and analysis purposes."  
        },  
        ...  
    ]  

### Step 5: execute the task​ {#step-5-execute-the-task}

Let agents generated in `build()` complete the task collaboratively in a group
chat.

    import autogen  
      
    def start_task(execution_task: str, agent_list: list, llm_config: dict):  
        config_list = autogen.config_list_from_json(config_file_or_env, filter_dict={"model": ["gpt-4-1106-preview"]})  
      
        group_chat = autogen.GroupChat(agents=agent_list, messages=[], max_round=12)  
        manager = autogen.GroupChatManager(  
            groupchat=group_chat, llm_config={"config_list": config_list, **llm_config}  
        )  
        agent_list[0].initiate_chat(manager, message=execution_task)  
      
    start_task(  
        execution_task="Find a recent paper about gpt-4 on arxiv and find its potential applications in software.",  
        agent_list=agent_list,  
        llm_config=default_llm_config  
    )  

### Step 6 (Optional): clear all agents and prepare for the next task​ {#step-6-optional-clear-all-agents-and-prepare-for-the-next-task}

You can clear all agents generated in this task by the following code if your
task is completed or if the next task is largely different from the current
task.

    builder.clear_all_agents(recycle_endpoint=True)  

If the agent's backbone is an open-source LLM, this process will also shut
down the endpoint server. More details are in the next section. If necessary,
you can use `recycle_endpoint=False` to retain the previous open-source LLM's
endpoint server.

## Save and Load​ {#save-and-load}

You can save all necessary information of the built group chat agents by

    saved_path = builder.save()  

Configurations will be saved in JSON format with the following content:

    // FILENAME: save_config_TASK_MD5.json  
    {  
        "building_task": "Find a paper on arxiv by programming, and analysis its application in some domain. For example, find a latest paper about gpt-4 on arxiv and find its potential applications in software.",  
        "agent_configs": [  
            {  
                "name": "...",  
                "model": "...",  
                "system_message": "...",  
                "description": "..."  
            },  
            ...  
        ],  
        "manager_system_message": "...",  
        "code_execution_config": {...},  
        "default_llm_config": {...}  
    }  

You can provide a specific filename, otherwise, AgentBuilder will save config
to the current path with the generated filename `save_config_TASK_MD5.json`.

You can load the saved config and skip the building process. AgentBuilder will
create agents with those information without prompting the build manager.

    new_builder = AgentBuilder(config_file_or_env=config_file_or_env)  
    agent_list, agent_config = new_builder.load(saved_path)  
    start_task(...)  # skip build()  

## Use OpenAI Assistant​ {#use-openai-assistant}

[Assistants API](https://platform.openai.com/docs/assistants/overview) allows
you to build AI assistants within your own applications. An Assistant has
instructions and can leverage models, tools, and knowledge to respond to user
queries. AutoBuild also supports the assistant API by adding
`use_oai_assistant=True` to `build()`.

    # Transfer to the OpenAI Assistant API.  
    agent_list, agent_config = new_builder.build(building_task, default_llm_config, use_oai_assistant=True)  
    ...  

## (Experimental) Use Open-source LLM​ {#experimental-use-opensource-llm}

AutoBuild supports open-source LLM by
[vLLM](https://docs.vllm.ai/en/latest/index.html) and
[FastChat](https://github.com/lm-sys/FastChat). Check the supported model list
[here](https://docs.vllm.ai/en/latest/models/supported_models.html). After
satisfying the requirements, you can add an open-source LLM's huggingface
repository to the config file,

    // Add the LLM's huggingface repo to your config file and use EMPTY as the api_key.  
    [  
        ...  
        {  
            "model": "meta-llama/Llama-2-13b-chat-hf",  
            "api_key": "EMPTY"  
        }  
    ]  

and specify it when initializing AgentBuilder. AgentBuilder will automatically
set up an endpoint server for open-source LLM. Make sure you have sufficient
GPUs resources.

## Future work/Roadmap​ {#future-workroadmap}

* Let the builder select the best agents from a given library/database to solve the task.

## Summary​ {#summary}

We propose AutoBuild with a new class `AgentBuilder`. AutoBuild can help user
solve their complex task with an automatically built multi-agent system.
AutoBuild supports open-source LLMs and GPTs API, giving users more
flexibility to choose their favorite models. More advanced features are coming
soon.

 **Tags:**

* [LLM](/autogen/blog/tags/llm)
* [research](/autogen/blog/tags/research)

[Newer Post« AutoGen Studio: Interactively Explore Multi-Agent
Workflows](/autogen/blog/2023/12/01/AutoGenStudio)

[Older PostHow to Assess Utility of LLM-powered Applications?
»](/autogen/blog/2023/11/20/AgentEval)

* Introduction
* Installation
* Basic Example
  * Step 1: prepare configurations
  * Step 2: create an AgentBuilder instance
  * Step 3: specify the building task
  * Step 4: build group chat agents
  * Step 5: execute the task
  * Step 6 (Optional): clear all agents and prepare for the next task
* Save and Load
* Use OpenAI Assistant
* (Experimental) Use Open-source LLM
* Future work/Roadmap
* Summary

Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)

Copyright © 2024 AutoGen Authors | [Privacy and
Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
  * automl
  * default
  * onlineml
  * tune
    * tune.scheduler
    * tune.searcher
      * [blendsearch](.docs/reference/tune/searcher/blendsearch)
      * [cfo_cat](.docs/reference/tune/searcher/cfo_cat)
      * [flow2](.docs/reference/tune/searcher/flow2)
      * [online_searcher](.docs/reference/tune/searcher/online_searcher)
      * [search_thread](.docs/reference/tune/searcher/search_thread)
      * [suggestion](.docs/reference/tune/searcher/suggestion)
      * [variant_generator](.docs/reference/tune/searcher/variant_generator)
    * tune.spark
    * [analysis](.docs/reference/tune/analysis)
    * [sample](.docs/reference/tune/sample)
    * [space](.docs/reference/tune/space)
    * [trial](.docs/reference/tune/trial)
    * [trial_runner](.docs/reference/tune/trial_runner)
    * [tune](.docs/reference/tune/tune)
    * [utils](.docs/reference/tune/utils)

On this page

# tune.searcher.blendsearch {#tunesearcherblendsearch}

## BlendSearch Objects​ {#blendsearch-objects}

    class BlendSearch(Searcher)  

class for BlendSearch algorithm.

#### __init__​ {#init}

    def __init__(metric: Optional[str] = None, mode: Optional[str] = None, space: Optional[dict] = None, low_cost_partial_config: Optional[dict] = None, cat_hp_cost: Optional[dict] = None, points_to_evaluate: Optional[List[dict]] = None, evaluated_rewards: Optional[List] = None, time_budget_s: Union[int, float] = None, num_samples: Optional[int] = None, resource_attr: Optional[str] = None, min_resource: Optional[float] = None, max_resource: Optional[float] = None, reduction_factor: Optional[float] = None, global_search_alg: Optional[Searcher] = None, config_constraints: Optional[List[Tuple[Callable[[dict], float], str, float]]] = None, metric_constraints: Optional[List[Tuple[str, str, float]]] = None, seed: Optional[int] = 20, cost_attr: Optional[str] = "auto", cost_budget: Optional[float] = None, experimental: Optional[bool] = False, lexico_objectives: Optional[dict] = None, use_incumbent_result_in_evaluation=False, allow_empty_config=False)  

Constructor.

 **Arguments** :

* `metric` \- A string of the metric name to optimize for.
* `mode` \- A string in ['min', 'max'] to specify the objective as minimization or maximization.
* `space` \- A dictionary to specify the search space.
* `low_cost_partial_config` \- A dictionary from a subset of controlled dimensions to the initial low-cost values. E.g., `{'n_estimators': 4, 'max_leaves': 4}`.
* `cat_hp_cost` \- A dictionary from a subset of categorical dimensions to the relative cost of each choice. E.g., `{'tree_method': [1, 1, 2]}`. I.e., the relative cost of the three choices of 'tree_method' is 1, 1 and 2 respectively.
* `points_to_evaluate` \- Initial parameter suggestions to be run first.
* `evaluated_rewards` _list_ \- If you have previously evaluated the parameters passed in as points_to_evaluate you can avoid re-running those trials by passing in the reward attributes as a list so the optimiser can be told the results without needing to re-compute the trial. Must be the same or shorter length than points_to_evaluate. When provided, `mode` must be specified.
* `time_budget_s` \- int or float | Time budget in seconds.
* `num_samples` \- int | The number of configs to try. -1 means no limit on the number of configs to try.
* `resource_attr` \- A string to specify the resource dimension and the best performance is assumed to be at the max_resource.
* `min_resource` \- A float of the minimal resource to use for the resource_attr.
* `max_resource` \- A float of the maximal resource to use for the resource_attr.
* `reduction_factor` \- A float of the reduction factor used for incremental pruning.
* `global_search_alg` \- A Searcher instance as the global search instance. If omitted, Optuna is used. The following algos have known issues when used as global_search_alg:
  * HyperOptSearch raises exception sometimes
  * TuneBOHB has its own scheduler
* `config_constraints` \- A list of config constraints to be satisfied. E.g., `config_constraints = [(mem_size, '<=', 1024**3)]`. `mem_size` is a function which produces a float number for the bytes needed for a config. It is used to skip configs which do not fit in memory.
* `metric_constraints` \- A list of metric constraints to be satisfied. E.g., `['precision', '>=', 0.9]`. The sign can be ">=" or "<=".
* `seed` \- An integer of the random seed.
* `cost_attr` \- None or str to specify the attribute to evaluate the cost of different trials. Default is "auto", which means that we will automatically choose the cost attribute to use (depending on the nature of the resource budget). When cost_attr is set to None, cost differences between different trials will be omitted in our search algorithm. When cost_attr is set to a str different from "auto" and "time_total_s", this cost_attr must be available in the result dict of the trial.
* `cost_budget` \- A float of the cost budget. Only valid when cost_attr is a str different from "auto" and "time_total_s".
* `lexico_objectives` \- dict, default=None | It specifics information needed to perform multi-objective optimization with lexicographic preferences. This is only supported in CFO currently. When lexico_objectives is not None, the arguments metric, mode will be invalid. This dictionary shall contain the following fields of key-value pairs:
  * "metrics": a list of optimization objectives with the orders reflecting the priorities/preferences of the objectives.
  * "modes" (optional): a list of optimization modes (each mode either "min" or "max") corresponding to the objectives in the metric list. If not provided, we use "min" as the default mode for all the objectives.
  * "targets" (optional): a dictionary to specify the optimization targets on the objectives. The keys are the metric names (provided in "metric"), and the values are the numerical target values.
  * "tolerances" (optional): a dictionary to specify the optimality tolerances on objectives. The keys are the metric names (provided in "metrics"), and the values are the absolute/percentage tolerance in the form of numeric/string. E.g.,

        lexico_objectives = {  

* `"metrics"` \- ["error_rate", "pred_time"],
* `"modes"` \- ["min", "min"],
* `"tolerances"` \- {"error_rate": 0.01, "pred_time": 0.0},
* `"targets"` \- {"error_rate": 0.0}, }

        We also support percentage tolerance.  
    E.g.,  

    ```python  
    lexico_objectives = {  

* `"metrics"` \- ["error_rate", "pred_time"],
* `"modes"` \- ["min", "min"],
* `"tolerances"` \- {"error_rate": "5%", "pred_time": "0%"},
* `"targets"` \- {"error_rate": 0.0}, }

* `experimental` \- A bool of whether to use experimental features.

#### save​ {#save}

    def save(checkpoint_path: str)  

save states to a checkpoint path.

#### restore​ {#restore}

    def restore(checkpoint_path: str)  

restore states from checkpoint.

#### on_trial_complete​ {#ontrialcomplete}

    def on_trial_complete(trial_id: str, result: Optional[Dict] = None, error: bool = False)  

search thread updater and cleaner.

#### on_trial_result​ {#ontrialresult}

    def on_trial_result(trial_id: str, result: Dict)  

receive intermediate result.

#### suggest​ {#suggest}

    def suggest(trial_id: str) -> Optional[Dict]  

choose thread, suggest a valid config.

#### results​ {#results}

    @property  
    def results() -> List[Dict]  

A list of dicts of results for each evaluated configuration.

Each dict has "config" and metric names as keys. The returned dict includes
the initial results provided via `evaluated_reward`.

## BlendSearchTuner Objects​ {#blendsearchtuner-objects}

    class BlendSearchTuner(BlendSearch,  NNITuner)  

Tuner class for NNI.

#### receive_trial_result​ {#receivetrialresult}

    def receive_trial_result(parameter_id, parameters, value, **kwargs)  

Receive trial's final result.

 **Arguments** :

* `parameter_id` \- int.
* `parameters` \- object created by `generate_parameters()`.
* `value` \- final metrics of the trial, including default metric.

#### generate_parameters​ {#generateparameters}

    def generate_parameters(parameter_id, **kwargs) -> Dict  

Returns a set of trial (hyper-)parameters, as a serializable object.

 **Arguments** :

* `parameter_id` \- int.

#### update_search_space​ {#updatesearchspace}

    def update_search_space(search_space)  

Required by NNI.

Tuners are advised to support updating search space at run-time. If a tuner
can only set search space once before generating first hyper-parameters, it
should explicitly document this behaviour.

 **Arguments** :

* `search_space` \- JSON object created by experiment owner.

## CFO Objects​ {#cfo-objects}

    class CFO(BlendSearchTuner)  

class for CFO algorithm.

## RandomSearch Objects​ {#randomsearch-objects}

    class RandomSearch(CFO)  

Class for random search.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/tune/searcher/blendsearch.md)

[Previous«
trial_scheduler](.docs/reference/tune/scheduler/trial_scheduler)

[Nextcfo_cat »](.docs/reference/tune/searcher/cfo_cat)

* BlendSearch Objects
* BlendSearchTuner Objects
* CFO Objects
* RandomSearch Objects

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

* Reference
  * autogen
  * automl
  * default
  * onlineml
  * tune
    * tune.scheduler
    * tune.searcher
      * [blendsearch](.docs/reference/tune/searcher/blendsearch)
      * [cfo_cat](.docs/reference/tune/searcher/cfo_cat)
      * [flow2](.docs/reference/tune/searcher/flow2)
      * [online_searcher](.docs/reference/tune/searcher/online_searcher)
      * [search_thread](.docs/reference/tune/searcher/search_thread)
      * [suggestion](.docs/reference/tune/searcher/suggestion)
      * [variant_generator](.docs/reference/tune/searcher/variant_generator)
    * tune.spark
    * [analysis](.docs/reference/tune/analysis)
    * [sample](.docs/reference/tune/sample)
    * [space](.docs/reference/tune/space)
    * [trial](.docs/reference/tune/trial)
    * [trial_runner](.docs/reference/tune/trial_runner)
    * [tune](.docs/reference/tune/tune)
    * [utils](.docs/reference/tune/utils)

On this page

# tune.searcher.variant_generator {#tunesearchervariantgenerator}

## TuneError Objects​ {#tuneerror-objects}

    class TuneError(Exception)  

General error class raised by ray.tune.

#### generate_variants​ {#generatevariants}

    def generate_variants(unresolved_spec: Dict, constant_grid_search: bool = False, random_state: "RandomState" = None) -> Generator[Tuple[Dict, Dict], None, None]  

Generates variants from a spec (dict) with unresolved values. There are two
types of unresolved values: Grid search: These define a grid search over
values. For example, the following grid search values in a spec will produce
six distinct variants in combination: "activation": grid_search(["relu",
"tanh"]) "learning_rate": grid_search([1e-3, 1e-4, 1e-5]) Lambda functions:
These are evaluated to produce a concrete value, and can express dependencies
or conditional distributions between values. They can also be used to express
random search (e.g., by calling into the `random` or `np` module). "cpu":
lambda spec: spec.config.num_workers "batch_size": lambda spec:
random.uniform(1, 1000) Finally, to support defining specs in plain JSON /
YAML, grid search and lambda functions can also be defined alternatively as
follows: "activation": {"grid_search": ["relu", "tanh"]} "cpu": {"eval":
"spec.config.num_workers"} Use `format_vars` to format the returned dict of
hyperparameters.

 **Yields** :

(Dict of resolved variables, Spec object)

#### grid_search​ {#gridsearch}

    def grid_search(values: List) -> Dict[str, List]  

Convenience method for specifying grid search over a value.

 **Arguments** :

* `values` \- An iterable whose parameters will be gridded.

[Edit this
page](https://github.com/microsoft.edit/main/website/docs/reference/tune/searcher/variant_generator.md)

[Previous« suggestion](.docs/reference/tune/searcher/suggestion)

[Nextutils »](.docs/reference/tune/spark/utils)

* TuneError Objects

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.

Skip to main content

[![FLAML](.img/flaml_logo_fill.svg)![FLAML](.img/flaml_logo_fill.svg)
**FLAML**](.)[Docs](.docs/Getting-
Started)[SDK](.docs/reference/automl/automl)[Blog](.blog)[FAQ](.docs/FAQ)

[GitHub](https://github.com/microsoft/FLAML)

🌜

🌞

`ctrl``K`

Recent posts

* [Use flaml.autogen for Local LLMs](.blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](.blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](.blog/2023/05/18/GPT-adaptive-humaneval)
* [Surpassing 1 Million Downloads - A Retrospective and a Look into the Future](.blog/2023/05/07/1M-milestone)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](.blog/2023/04/21/LLM-tuning-math)

# Use flaml.autogen for Local LLMs {#use-flamlautogen-for-local-llms}

July 14, 2023 · 3 min read

[![Jiale
Liu](https://github.com/LeoLjl/leoljl.github.io/blob/main/profile.jpg?raw=true)](https://leoljl.github.io)

[Jiale Liu](https://leoljl.github.io)

Undergraduate student at Xidian University

 **TL;DR:** We demonstrate how to use flaml.autogen for local LLM application.
As an example, we will initiate an endpoint using
[FastChat](https://github.com/lm-sys/FastChat) and perform inference on
[ChatGLMv2-6b](https://github.com/THUDM/ChatGLM2-6B).

## Preparations​ {#preparations}

### Clone FastChat​ {#clone-fastchat}

FastChat provides OpenAI-compatible APIs for its supported models, so you can
use FastChat as a local drop-in replacement for OpenAI APIs. However, its code
needs minor modification in order to function properly.

    git clone https://github.com/lm-sys/FastChat.git  
    cd FastChat  

### Download checkpoint​ {#download-checkpoint}

ChatGLM-6B is an open bilingual language model based on General Language Model
(GLM) framework, with 6.2 billion parameters. ChatGLM2-6B is its second-
generation version.

Before downloading from HuggingFace Hub, you need to have Git LFS
[installed](<https://docs.github.com/en/repositories/working-with->
files/managing-large-files/installing-git-large-file-storage).

    git clone https://huggingface.co/THUDM/chatglm2-6b  

## Initiate server​ {#initiate-server}

First, launch the controller

    python -m fastchat.serve.controller  

Then, launch the model worker(s)

    python -m fastchat.serve.model_worker --model-path chatglm2-6b  

Finally, launch the RESTful API server

    python -m fastchat.serve.openai_api_server --host localhost --port 8000  

Normally this will work. However, if you encounter error like
[this](https://github.com/lm-sys/FastChat/issues/1641), commenting out all the
lines containing `finish_reason` in `fastchat/protocol/api_protocal.py` and
`fastchat/protocol/openai_api_protocol.py` will fix the problem. The modified
code looks like:

    class CompletionResponseChoice(BaseModel):  
        index: int  
        text: str  
        logprobs: Optional[int] = None  
        # finish_reason: Optional[Literal["stop", "length"]]  
      
    class CompletionResponseStreamChoice(BaseModel):  
        index: int  
        text: str  
        logprobs: Optional[float] = None  
        # finish_reason: Optional[Literal["stop", "length"]] = None  

## Interact with model using `oai.Completion`​ {#interact-with-model-using-oaicompletion}

Now the models can be directly accessed through openai-python library as well
as `flaml.oai.Completion` and `flaml.oai.ChatCompletion`.

    from flaml import oai  
      
    # create a text completion request  
    response = oai.Completion.create(  
        config_list=[  
            {  
                "model": "chatglm2-6b",  
                "api_base": "http://localhost:8000/v1",  
                "api_type": "open_ai",  
                "api_key": "NULL", # just a placeholder  
            }  
        ],  
        prompt="Hi",  
    )  
    print(response)  
      
    # create a chat completion request  
    response = oai.ChatCompletion.create(  
        config_list=[  
            {  
                "model": "chatglm2-6b",  
                "api_base": "http://localhost:8000/v1",  
                "api_type": "open_ai",  
                "api_key": "NULL",  
            }  
        ],  
        messages=[{"role": "user", "content": "Hi"}]  
    )  
    print(response)  

If you would like to switch to different models, download their checkpoints
and specify model path when launching model worker(s).

## interacting with multiple local LLMs​ {#interacting-with-multiple-local-llms}

If you would like to interact with multiple LLMs on your local machine,
replace the `model_worker` step above with a multi model variant:

    python -m fastchat.serve.multi_model_worker \  
        --model-path lmsys/vicuna-7b-v1.3 \  
        --model-names vicuna-7b-v1.3 \  
        --model-path chatglm2-6b \  
        --model-names chatglm2-6b  

The inference code would be:

    from flaml import oai  
      
    # create a chat completion request  
    response = oai.ChatCompletion.create(  
        config_list=[  
            {  
                "model": "chatglm2-6b",  
                "api_base": "http://localhost:8000/v1",  
                "api_type": "open_ai",  
                "api_key": "NULL",  
            },  
            {  
                "model": "vicuna-7b-v1.3",  
                "api_base": "http://localhost:8000/v1",  
                "api_type": "open_ai",  
                "api_key": "NULL",  
            }  
        ],  
        messages=[{"role": "user", "content": "Hi"}]  
    )  
    print(response)  

## For Further Reading​ {#for-further-reading}

* [Documentation](https://microsoft.github.io/autogen/) about `flaml.autogen`
* [Documentation](https://github.com/lm-sys/FastChat) about FastChat.

 **Tags:**

* [LLM](.blog/tags/llm)
* [FLAMLv2](.blog/tags/flam-lv-2)

[Older PostMathChat - An Conversational Framework to Solve Math Problems
»](.blog/2023/06/28/MathChat)

* Preparations
  * Clone FastChat
  * Download checkpoint
* Initiate server
* Interact with model using `oai.Completion`
* interacting with multiple local LLMs
* For Further Reading

Community

* [Discord](https://discord.gg/Cppx2vSPVP)

Copyright © 2023 FLAML Authors. Built with Docusaurus.
