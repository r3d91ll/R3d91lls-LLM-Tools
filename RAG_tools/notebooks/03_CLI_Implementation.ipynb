{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Tools: CLI Implementation\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "In this notebook, we'll implement a Command Line Interface (CLI) for our RAG Tools project. The CLI will allow us to interact with our Ollama instances and manage our Docker containers from the command line, providing a more efficient way to control our ML environment.\n",
    "By creating a CLI, we're adding a new layer of usability to our project. This interface will make it easier to start and stop services, check their status, and interact with our LLM without having to navigate through Docker commands or Python scripts directly.\n",
    "\n",
    "# 2. Project Structure Update\n",
    "First, let's update our project structure to include our new CLI file. We'll create a new file called cli.py in the src directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the project root directory\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Path for the new CLI file\n",
    "cli_file_path = os.path.join(project_root, 'src', 'utils', 'cli.py')\n",
    "\n",
    "# Create an empty cli.py file\n",
    "with open(cli_file_path, 'w') as f:\n",
    "    pass\n",
    "\n",
    "print(f\"Created empty cli.py file at: {cli_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CLI Implementation\n",
    "\n",
    "We'll use the `typer` library to create our CLI. Typer is built on top of Click and provides a simple, intuitive interface for creating command-line tools.\n",
    "\n",
    "### 3.1 Install Typer\n",
    "\n",
    "Let's start by installing Typer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge typer -y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Implement Basic CLI Structure\n",
    "\n",
    "Now, let's implement the basic structure of our CLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {cli_file_path}\n",
    "import typer\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = Path(__file__).resolve().parent.parent.parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from src.utils.config_utils import Config\n",
    "from src.utils.ollama_manager import OllamaManager\n",
    "from src.utils.DockerComposeManager import DockerComposeManager\n",
    "\n",
    "app = typer.Typer()\n",
    "config = Config()\n",
    "ollama_manager = OllamaManager(config)\n",
    "docker_manager = DockerComposeManager(str(project_root / \"config\" / \"docker-compose.yml\"))\n",
    "\n",
    "@app.command()\n",
    "def start():\n",
    "    \"\"\"Start the Ollama containers\"\"\"\n",
    "    typer.echo(\"Starting Ollama containers...\")\n",
    "    docker_manager.start_containers()\n",
    "\n",
    "@app.command()\n",
    "def stop():\n",
    "    \"\"\"Stop the Ollama containers\"\"\"\n",
    "    typer.echo(\"Stopping Ollama containers...\")\n",
    "    docker_manager.stop_containers()\n",
    "\n",
    "@app.command()\n",
    "def status():\n",
    "    \"\"\"Check the status of the Ollama containers\"\"\"\n",
    "    typer.echo(\"Checking container status...\")\n",
    "    docker_manager.show_container_status()\n",
    "\n",
    "@app.command()\n",
    "def chat():\n",
    "    \"\"\"Start a chat session with the LLM\"\"\"\n",
    "    typer.echo(\"Starting chat session. Type 'exit' to end the session.\")\n",
    "    while True:\n",
    "        prompt = typer.prompt(\"You\")\n",
    "        if prompt.lower() == 'exit':\n",
    "            break\n",
    "        response = ollama_manager.generate_response(prompt)\n",
    "        typer.echo(f\"LLM: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test the CLI tool\n",
    "\n",
    "Let's test the cli.py script commands for starting/stopping containers, checking status, and chatting with the LLM. The bellow commands will just tell you if the CLI help menu is working and that the docker containers are running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python {cli_file_path} --help\n",
    "!python {cli_file_path} status\n",
    "!python {cli_file_path} chat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "however given the nature of the jypter notebook we will need to do some further checking at the CLI. Run the following commands from a terminal on your desktop. Be sure to update the file path as needed in the second command.\n",
    "```bash\n",
    "conda activate ragtools\n",
    "python3 src/utils/cli.py chat\n",
    "```\n",
    "\n",
    "You should see similar output to this bellow:\n",
    "```text\n",
    "/RAG_tools$ python3 src/utils/cli.py chat\n",
    "Starting chat session. Type 'exit' to end the session.\n",
    "You: what is the capitol of Texas?                                            \n",
    "ERROR:root:Error generating response: 400 Client Error: Bad Request for url: http://localhost:11435/api/generate\n",
    "LLM: None\n",
    "You: exit\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Troubleshooting and Improving the CLI\n",
    "\n",
    "After initial testing, we encountered some issues with our CLI, particularly when trying to generate responses from the LLM. Let's analyze the problem and implement a solution.\n",
    "\n",
    "### 5.1 Problem Identification\n",
    "\n",
    "1. The Ollama service is running correctly on port 11435.\n",
    "2. The service can handle GET requests to `/api/tags` successfully.\n",
    "3. POST requests to `/api/generate` are sometimes successful, but often result in 400 Bad Request errors.\n",
    "\n",
    "### 5.2 Potential Issues\n",
    "\n",
    "1. The format of the request being sent to the Ollama API might be incorrect.\n",
    "2. There might be an issue with how we're handling the streaming response from the API.\n",
    "3. The error handling in our `generate_response` method may not be capturing all potential errors.\n",
    "\n",
    "### 5.3 Solution Implementation\n",
    "\n",
    "Let's update our `OllamaManager` class to address these issues:\n",
    "\n",
    "1. Ensure the request format matches Ollama's API expectations.\n",
    "2. Improve error handling and logging.\n",
    "3. Implement proper handling of the streaming response.\n",
    "\n",
    "Here's an updated version of the `generate_response` method:\n",
    "```python\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "from .DockerComposeManager import DockerComposeManager\n",
    "from .config_utils import Config\n",
    "\n",
    "class OllamaManager::\n",
    "    # ... (previous code remains the same)\n",
    "\n",
    "    def generate_response(self, prompt):\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f'http://localhost:{self.port}/api/generate',\n",
    "                json={'model': self.model, 'prompt': prompt},\n",
    "                stream=True\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            full_response = \"\"\n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    try:\n",
    "                        chunk = json.loads(line)\n",
    "                        if 'response' in chunk:\n",
    "                            token = chunk['response']\n",
    "                            full_response += token\n",
    "                            print(token, end='', flush=True)\n",
    "                        if chunk.get('done', False):\n",
    "                            break\n",
    "                    except json.JSONDecodeError:\n",
    "                        logging.warning(f\"Failed to decode JSON: {line}\")\n",
    "            \n",
    "            print(\"\\n\")  # New line after the response\n",
    "            return full_response.strip()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Error generating response: {str(e)}\")\n",
    "            if hasattr(e.response, 'text'):\n",
    "                logging.error(f\"Response content: {e.response.text}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error: {str(e)}\")\n",
    "            return f\"Unexpected error: {str(e)}\"\n",
    "```\n",
    "the code block below will update the ollama_manager.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/utils/ollama_manager.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/utils/ollama_manager.py\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "from .DockerComposeManager import DockerComposeManager\n",
    "from .config_utils import Config\n",
    "\n",
    "\n",
    "class OllamaManager:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.container_name = self.config.OLLAMA_LLM_CONTAINER_NAME\n",
    "        self.port = self.config.OLLAMA_LLM_PORT\n",
    "        self.model = self.config.OLLAMA_LLM_MODEL\n",
    "        self.gpu = self.config.OLLAMA_LLM_GPU\n",
    "        \n",
    "        self.models_path = self.config.OLLAMA_MODELS_PATH or os.path.expanduser('~/ollama_models')\n",
    "        self.llm_path = self.config.OLLAMA_LLM_PATH or os.path.join(self.models_path, 'llm')\n",
    "        \n",
    "        # Ensure directories exist\n",
    "        os.makedirs(self.models_path, exist_ok=True)\n",
    "        os.makedirs(self.llm_path, exist_ok=True)\n",
    "        \n",
    "        # Initialize DockerComposeManager\n",
    "        docker_compose_path = os.path.join('..', 'config', 'docker-compose.yml')\n",
    "        self.docker_manager = DockerComposeManager(docker_compose_path)\n",
    "\n",
    "        logging.info(f\"OllamaManager initialized with models_path: {self.models_path}, llm_path: {self.llm_path}\")\n",
    "        logging.info(f\"Using model: {self.model} on port: {self.port}\")\n",
    "\n",
    "    def is_model_running(self):\n",
    "        try:\n",
    "            response = requests.get(f'http://localhost:{self.port}/api/tags')\n",
    "            response.raise_for_status()\n",
    "            models = response.json()\n",
    "            return self.model in [model['name'] for model in models['models']]\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Error checking if model is running: {e}\")\n",
    "            return False\n",
    "\n",
    "    def generate_response(self, prompt):\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f'http://localhost:{self.port}/api/generate',\n",
    "                json={'model': self.model, 'prompt': prompt},\n",
    "                stream=True\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            full_response = \"\"\n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    try:\n",
    "                        chunk = json.loads(line)\n",
    "                        if 'response' in chunk:\n",
    "                            token = chunk['response']\n",
    "                            full_response += token\n",
    "                            print(token, end='', flush=True)\n",
    "                        if chunk.get('done', False):\n",
    "                            break\n",
    "                    except json.JSONDecodeError:\n",
    "                        logging.warning(f\"Failed to decode JSON: {line}\")\n",
    "            \n",
    "            print(\"\\n\")  # New line after the response\n",
    "            return full_response.strip()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Error generating response: {str(e)}\")\n",
    "            if hasattr(e.response, 'text'):\n",
    "                logging.error(f\"Response content: {e.response.text}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error: {str(e)}\")\n",
    "            return f\"Unexpected error: {str(e)}\"\n",
    "\n",
    "    def pull_model(self):\n",
    "        logging.debug(f\"Starting pull_model for model: {self.model}\")\n",
    "        logging.debug(f\"self.models_path: {self.models_path}\")\n",
    "        logging.debug(f\"self.model: {self.model}\")\n",
    "        \n",
    "        try:\n",
    "            model_path = os.path.join(self.models_path, 'models', 'manifests', 'registry.ollama.ai', 'library', self.model)\n",
    "            logging.info(f\"Checking for model at path: {model_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error constructing model path: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "        if os.path.exists(model_path):\n",
    "            logging.info(f\"Model {self.model} already exists. Skipping download.\")\n",
    "            return\n",
    "\n",
    "        logging.info(f\"Pulling model {self.model}...\")\n",
    "        try:\n",
    "            response = requests.post(f'http://localhost:{self.port}/api/pull', json={'name': self.model}, stream=True)\n",
    "            response.raise_for_status()\n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    print(line.decode())\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Error pulling model: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def start_container(self):\n",
    "        self.docker_manager.start_containers()\n",
    "        logging.info(f\"Started container: {self.container_name}\")\n",
    "\n",
    "    def stop_container(self):\n",
    "        self.docker_manager.stop_containers()\n",
    "        logging.info(f\"Stopped container: {self.container_name}\")\n",
    "\n",
    "    def wait_for_ollama(self, max_attempts=5, delay=5):\n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                response = requests.get(f'http://localhost:{self.port}/api/tags')\n",
    "                if response.status_code == 200:\n",
    "                    logging.info(f\"Successfully connected to Ollama on port {self.port}\")\n",
    "                    return True\n",
    "            except requests.exceptions.RequestException:\n",
    "                logging.warning(f\"Attempt {attempt + 1}/{max_attempts}: Ollama on port {self.port} is not ready yet. Retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "        logging.error(f\"Failed to connect to Ollama after {max_attempts} attempts\")\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This updated version includes better error handling and logging, which should help us identify any issues more easily.\n",
    "\n",
    "### 5.4 CLI Update\n",
    "\n",
    "We should also update our CLI to handle these potential errors more gracefully:\n",
    "\n",
    "```python\n",
    "@app.command()\n",
    "def chat():\n",
    "    \"\"\"Start a chat session with the LLM\"\"\"\n",
    "    typer.echo(\"Starting chat session. Type 'exit' to end the session.\")\n",
    "    logging.debug(f\"Configuration values:\")\n",
    "    logging.debug(f\"OLLAMA_LLM_CONTAINER_NAME: {config.OLLAMA_LLM_CONTAINER_NAME}\")\n",
    "    logging.debug(f\"OLLAMA_LLM_PORT: {config.OLLAMA_LLM_PORT}\")\n",
    "    logging.debug(f\"OLLAMA_LLM_MODEL: {config.OLLAMA_LLM_MODEL}\")\n",
    "    logging.debug(f\"Using model: {ollama_manager.model}\")\n",
    "    logging.debug(f\"Ollama port: {ollama_manager.port}\")\n",
    "    \n",
    "    # Check if the model is running\n",
    "    if not ollama_manager.is_model_running():\n",
    "        typer.echo(f\"Error: Model {ollama_manager.model} is not running. Please start the model first.\")\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        prompt = typer.prompt(\"You\")\n",
    "        if prompt.lower() == 'exit':\n",
    "            break\n",
    "        logging.debug(f\"Sending prompt to OllamaManager: {prompt}\")\n",
    "        response = ollama_manager.generate_response(prompt)\n",
    "        logging.debug(f\"Received response from OllamaManager: {response[:100]}...\")  # Log first 100 chars\n",
    "        if response.startswith(\"Error:\") or response.startswith(\"Unexpected error:\"):\n",
    "            typer.echo(f\"LLM Error: {response}\", err=True)\n",
    "        else:\n",
    "            typer.echo(f\"LLM: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app()\n",
    "```\n",
    "\n",
    "This update will display errors to the user more clearly and use the standard error stream for error messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/utils/cli.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/utils/cli.py\n",
    "import typer\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = Path(__file__).resolve().parent.parent.parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from src.utils.config_utils import Config\n",
    "from src.utils.ollama_manager import OllamaManager\n",
    "from src.utils.DockerComposeManager import DockerComposeManager\n",
    "\n",
    "app = typer.Typer()\n",
    "config = Config()\n",
    "ollama_manager = OllamaManager(config)\n",
    "docker_manager = DockerComposeManager(str(project_root / \"config\" / \"docker-compose.yml\"))\n",
    "\n",
    "@app.command()\n",
    "def start():\n",
    "    \"\"\"Start the Ollama containers\"\"\"\n",
    "    typer.echo(\"Starting Ollama containers...\")\n",
    "    docker_manager.start_containers()\n",
    "\n",
    "@app.command()\n",
    "def stop():\n",
    "    \"\"\"Stop the Ollama containers\"\"\"\n",
    "    typer.echo(\"Stopping Ollama containers...\")\n",
    "    docker_manager.stop_containers()\n",
    "\n",
    "@app.command()\n",
    "def status():\n",
    "    \"\"\"Check the status of the Ollama containers\"\"\"\n",
    "    typer.echo(\"Checking container status...\")\n",
    "    docker_manager.show_container_status()\n",
    "\n",
    "@app.command()\n",
    "def chat():\n",
    "    \"\"\"Start a chat session with the LLM\"\"\"\n",
    "    typer.echo(\"Starting chat session. Type 'exit' to end the session.\")\n",
    "    logging.debug(f\"Configuration values:\")\n",
    "    logging.debug(f\"OLLAMA_LLM_CONTAINER_NAME: {config.OLLAMA_LLM_CONTAINER_NAME}\")\n",
    "    logging.debug(f\"OLLAMA_LLM_PORT: {config.OLLAMA_LLM_PORT}\")\n",
    "    logging.debug(f\"OLLAMA_LLM_MODEL: {config.OLLAMA_LLM_MODEL}\")\n",
    "    logging.debug(f\"Using model: {ollama_manager.model}\")\n",
    "    logging.debug(f\"Ollama port: {ollama_manager.port}\")\n",
    "    \n",
    "    # Check if the model is running\n",
    "    if not ollama_manager.is_model_running():\n",
    "        typer.echo(f\"Error: Model {ollama_manager.model} is not running. Please start the model first.\")\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        prompt = typer.prompt(\"You\")\n",
    "        if prompt.lower() == 'exit':\n",
    "            break\n",
    "        logging.debug(f\"Sending prompt to OllamaManager: {prompt}\")\n",
    "        response = ollama_manager.generate_response(prompt)\n",
    "        logging.debug(f\"Received response from OllamaManager: {response[:100]}...\")  # Log first 100 chars\n",
    "        if response.startswith(\"Error:\") or response.startswith(\"Unexpected error:\"):\n",
    "            typer.echo(f\"LLM Error: {response}\", err=True)\n",
    "        else:\n",
    "            typer.echo(f\"LLM: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we've successfully implemented a basic CLI for our RAG Tools project. This CLI allows us to manage our Docker containers and interact with our LLM from the command line.\n",
    "\n",
    "Next steps could include:\n",
    "1. Adding more advanced commands (e.g., switching models, viewing logs)\n",
    "2. Implementing error handling and input validation\n",
    "3. Adding support for LLM configurables (which we'll address in a future notebook)\n",
    "4. Creating a user guide for the CLI\n",
    "\n",
    "In our next notebook, we'll focus on [brief description of the next topic]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragtools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
