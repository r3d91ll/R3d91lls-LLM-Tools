{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Advanced Ollama Setup\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "In this notebook, we'll set up multiple Ollama instances in Docker containers. Ollama is an open-source tool that allows us to run large language models locally. This setup will enable us to use different models for various tasks in our RAG (Retrieval-Augmented Generation) system, such as embedding generation and text generation.\n",
    "\n",
    "We'll cover the following steps:\n",
    "1. Updating our project directory structure\n",
    "2. Creating an OllamaManager class to handle Ollama operations\n",
    "3. Updating our environment variables\n",
    "4. Updating our Docker Compose configuration\n",
    "5. Testing the OllamaManager class\n",
    "\n",
    "## 2. Update Project Directory Structure\n",
    "\n",
    "First, let's update our project directory structure to accommodate the Ollama models and ensure they're shared between containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the project root directory\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Create directories for Ollama models\n",
    "ollama_models_path = os.path.join(project_root, 'db_data', 'ollama_models')\n",
    "ollama_llm_path = os.path.join(ollama_models_path, 'llm')\n",
    "\n",
    "os.makedirs(ollama_models_path, exist_ok=True)\n",
    "os.makedirs(ollama_llm_path, exist_ok=True)\n",
    "\n",
    "print(f\"Created Ollama models directory at: {ollama_models_path}\")\n",
    "print(f\"Created Ollama LLM directory at: {ollama_llm_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our updated project structure now looks like this:\n",
    "\n",
    "```\n",
    "RAG_tools/\n",
    "├── config/\n",
    "│   ├── docker-compose.yml\n",
    "│   └── .env\n",
    "├── notebooks/\n",
    "│   ├── 00_Environment_Setup.ipynb\n",
    "│   ├── 01_Database_Setup.ipynb\n",
    "│   └── 02_Ollama_setup.ipynb\n",
    "├── src/\n",
    "│   └── utils/\n",
    "│       ├── config_utils.py\n",
    "│       └── ollama_manager.py\n",
    "├── db_data/\n",
    "│   ├── postgres/\n",
    "│   ├── neo4j/\n",
    "│   └── ollama_models/\n",
    "│       └── llm/\n",
    "└── tests/\n",
    "```\n",
    "\n",
    "## 3. Create OllamaManager Class\n",
    "\n",
    "Now, let's create the OllamaManager class. This class will be used to launch and manage Ollama instances in Docker containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/utils/ollama_manager.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/utils/ollama_manager.py\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "from .DockerComposeManager import DockerComposeManager\n",
    "from .config_utils import Config\n",
    "\n",
    "class OllamaManager:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.container_name = self.config.OLLAMA_LLM_CONTAINER_NAME\n",
    "        self.port = self.config.OLLAMA_LLM_PORT\n",
    "        self.model = self.config.OLLAMA_LLM_MODEL\n",
    "        self.gpu = self.config.OLLAMA_LLM_GPU\n",
    "        \n",
    "        self.models_path = self.config.OLLAMA_MODELS_PATH or os.path.expanduser('~/ollama_models')\n",
    "        self.llm_path = self.config.OLLAMA_LLM_PATH or os.path.join(self.models_path, 'llm')\n",
    "        \n",
    "        # Ensure directories exist\n",
    "        os.makedirs(self.models_path, exist_ok=True)\n",
    "        os.makedirs(self.llm_path, exist_ok=True)\n",
    "        \n",
    "        # Initialize DockerComposeManager\n",
    "        docker_compose_path = os.path.join('..', 'config', 'docker-compose.yml')\n",
    "        self.docker_manager = DockerComposeManager(docker_compose_path)\n",
    "\n",
    "        logging.info(f\"OllamaManager initialized with models_path: {self.models_path}, llm_path: {self.llm_path}\")\n",
    "        logging.info(f\"Using model: {self.model} on port: {self.port}\")\n",
    "\n",
    "    def generate_response(self, prompt):\n",
    "        try:\n",
    "            payload = {\n",
    "                'model': self.model,\n",
    "                'prompt': prompt\n",
    "            }\n",
    "            logging.debug(f\"Sending request to Ollama API with payload: {payload}\")\n",
    "            logging.debug(f\"API URL: http://localhost:{self.port}/api/generate\")\n",
    "            \n",
    "            response = requests.post(\n",
    "                f'http://localhost:{self.port}/api/generate',\n",
    "                json=payload,\n",
    "                stream=True\n",
    "            )\n",
    "            logging.debug(f\"Response status code: {response.status_code}\")\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            full_response = \"\"\n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    try:\n",
    "                        chunk = json.loads(line)\n",
    "                        logging.debug(f\"Received chunk: {chunk}\")\n",
    "                        if 'response' in chunk:\n",
    "                            token = chunk['response']\n",
    "                            full_response += token\n",
    "                            print(token, end='', flush=True)\n",
    "                        if chunk.get('done', False):\n",
    "                            break\n",
    "                    except json.JSONDecodeError:\n",
    "                        logging.warning(f\"Failed to decode JSON: {line}\")\n",
    "            \n",
    "            print(\"\\n\")  # New line after the response\n",
    "            return full_response.strip()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Error generating response: {str(e)}\")\n",
    "            if hasattr(e, 'response') and e.response is not None:\n",
    "                logging.error(f\"Response content: {e.response.text}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error: {str(e)}\")\n",
    "            return f\"Unexpected error: {str(e)}\"\n",
    "\n",
    "    def is_model_running(self):\n",
    "        try:\n",
    "            response = requests.get(f'http://localhost:{self.port}/api/tags')\n",
    "            response.raise_for_status()\n",
    "            models = response.json()\n",
    "            logging.debug(f\"Available models: {models}\")\n",
    "            return self.model in [model['name'] for model in models['models']]\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Error checking if model is running: {e}\")\n",
    "            return False\n",
    "\n",
    "    def pull_model(self):\n",
    "        logging.debug(f\"Starting pull_model for model: {self.model}\")\n",
    "        logging.debug(f\"self.models_path: {self.models_path}\")\n",
    "        logging.debug(f\"self.model: {self.model}\")\n",
    "        \n",
    "        try:\n",
    "            model_path = os.path.join(self.models_path, 'models', 'manifests', 'registry.ollama.ai', 'library', self.model)\n",
    "            logging.info(f\"Checking for model at path: {model_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error constructing model path: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "        if os.path.exists(model_path):\n",
    "            logging.info(f\"Model {self.model} already exists. Skipping download.\")\n",
    "            return\n",
    "\n",
    "        logging.info(f\"Pulling model {self.model}...\")\n",
    "        try:\n",
    "            response = requests.post(f'http://localhost:{self.port}/api/pull', json={'name': self.model}, stream=True)\n",
    "            response.raise_for_status()\n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    print(line.decode())\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Error pulling model: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def start_container(self):\n",
    "        self.docker_manager.start_containers()\n",
    "        logging.info(f\"Started container: {self.container_name}\")\n",
    "\n",
    "    def stop_container(self):\n",
    "        self.docker_manager.stop_containers()\n",
    "        logging.info(f\"Stopped container: {self.container_name}\")\n",
    "\n",
    "    def wait_for_ollama(self, max_attempts=5, delay=5):\n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                response = requests.get(f'http://localhost:{self.port}/api/tags')\n",
    "                if response.status_code == 200:\n",
    "                    logging.info(f\"Successfully connected to Ollama on port {self.port}\")\n",
    "                    return True\n",
    "            except requests.exceptions.RequestException:\n",
    "                logging.warning(f\"Attempt {attempt + 1}/{max_attempts}: Ollama on port {self.port} is not ready yet. Retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "        logging.error(f\"Failed to connect to Ollama after {max_attempts} attempts\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Update Environment Variables\n",
    "\n",
    "Now, let's update our .env file to include the Ollama-related variables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "env_file_path = os.path.join('..', 'config', '.env')\n",
    "\n",
    "ollama_env_vars = \"\"\"\n",
    "# Ollama Configuration\n",
    "OLLAMA_EMBEDDING_CONTAINER_NAME=ragtools_ollama_embedding\n",
    "OLLAMA_EMBEDDING_PORT=11434\n",
    "OLLAMA_EMBEDDING_MODEL=bert-base-multilingual-cased\n",
    "OLLAMA_EMBEDDING_GPU=0\n",
    "\n",
    "OLLAMA_LLM_CONTAINER_NAME=ragtools_ollama_llm\n",
    "OLLAMA_LLM_PORT=11435\n",
    "OLLAMA_LLM_MODEL=tinyllama\n",
    "OLLAMA_LLM_GPU=1\n",
    "\n",
    "OLLAMA_MODELS_PATH=./db_data/ollama_models\n",
    "OLLAMA_LLM_PATH=./db_data/ollama_models/llm\n",
    "\"\"\"\n",
    "\n",
    "with open(env_file_path, 'a') as f:\n",
    "    f.write(ollama_env_vars)\n",
    "\n",
    "print(\"Updated .env file with Ollama configurations.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Update Config Class\n",
    "\n",
    "Before we proceed with the verification step, we need to update our Config class to include the new Ollama-related attributes. This is a crucial step when extending our framework with new components.\n",
    "\n",
    "This step demonstrates how to extend the Config class when new components are added to the framework. It's important to update this class whenever new environment variables or configuration options are introduced.\n",
    "\n",
    "Let's update the `config_utils.py` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "config_utils_path = os.path.join('..', 'src', 'utils', 'config_utils.py')\n",
    "\n",
    "# Read the existing content\n",
    "with open(config_utils_path, 'r') as f:\n",
    "    existing_content = f.read()\n",
    "\n",
    "# Define the new Ollama configurations\n",
    "ollama_configs = '''        # Ollama configurations\n",
    "        self.OLLAMA_EMBEDDING_CONTAINER_NAME = os.getenv('OLLAMA_EMBEDDING_CONTAINER_NAME')\n",
    "        self.OLLAMA_EMBEDDING_PORT = int(os.getenv('OLLAMA_EMBEDDING_PORT', 11434))\n",
    "        self.OLLAMA_EMBEDDING_MODEL = os.getenv('OLLAMA_EMBEDDING_MODEL')\n",
    "        self.OLLAMA_EMBEDDING_GPU = int(os.getenv('OLLAMA_EMBEDDING_GPU', 0))\n",
    "        self.OLLAMA_LLM_CONTAINER_NAME = os.getenv('OLLAMA_LLM_CONTAINER_NAME')\n",
    "        self.OLLAMA_LLM_PORT = int(os.getenv('OLLAMA_LLM_PORT', 11435))\n",
    "        self.OLLAMA_LLM_MODEL = os.getenv('OLLAMA_LLM_MODEL')\n",
    "        self.OLLAMA_LLM_GPU = int(os.getenv('OLLAMA_LLM_GPU', 1))\n",
    "        self.OLLAMA_MODELS_PATH = os.getenv('OLLAMA_MODELS_PATH')\n",
    "        self.OLLAMA_LLM_PATH = os.getenv('OLLAMA_LLM_PATH')\n",
    "'''\n",
    "\n",
    "# Find the position to insert the new configurations\n",
    "lines = existing_content.split('\\n')\n",
    "insert_line = -1\n",
    "for i, line in enumerate(lines):\n",
    "    if line.strip().startswith('def get_postgres_connection_params(self):'):\n",
    "        insert_line = i\n",
    "        break\n",
    "\n",
    "if insert_line == -1:\n",
    "    # If method not found, insert at the end of __init__\n",
    "    for i, line in enumerate(reversed(lines)):\n",
    "        if line.strip() == \"self.DOCKER_NETWORK_NAME = os.getenv('DOCKER_NETWORK_NAME')\":\n",
    "            insert_line = len(lines) - i\n",
    "            break\n",
    "\n",
    "# Insert the new configurations\n",
    "if insert_line != -1:\n",
    "    updated_lines = lines[:insert_line] + ollama_configs.split('\\n') + lines[insert_line:]\n",
    "    updated_content = '\\n'.join(updated_lines)\n",
    "else:\n",
    "    print(\"Could not find appropriate insertion point. Please update manually.\")\n",
    "    updated_content = existing_content\n",
    "\n",
    "# Write the updated content back to the file\n",
    "with open(config_utils_path, 'w') as f:\n",
    "    f.write(updated_content)\n",
    "\n",
    "print(\"Updated config_utils.py with Ollama configurations.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Update Docker Compose Configuration\n",
    "\n",
    "Let's update our docker-compose.yml file to include the Ollama services:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "docker_compose_path = os.path.join('..', 'config', 'docker-compose.yml')\n",
    "\n",
    "# Read existing docker-compose.yml\n",
    "with open(docker_compose_path, 'r') as f:\n",
    "    docker_compose = yaml.safe_load(f)\n",
    "\n",
    "# Add or update Ollama services\n",
    "docker_compose['services']['ollama_embedding'] = {\n",
    "    'image': 'ollama/ollama',\n",
    "    'container_name': '${OLLAMA_EMBEDDING_CONTAINER_NAME}',\n",
    "    'environment': ['OLLAMA_HOST=0.0.0.0:${OLLAMA_EMBEDDING_PORT}'],\n",
    "    'ports': ['${OLLAMA_EMBEDDING_PORT}:${OLLAMA_EMBEDDING_PORT}'],\n",
    "    'volumes': ['${OLLAMA_MODELS_PATH}:/root/.ollama'],\n",
    "    'deploy': {\n",
    "        'resources': {\n",
    "            'reservations': {\n",
    "                'devices': [{'driver': 'nvidia', 'count': 1, 'capabilities': ['gpu']}]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'networks': ['ragtools_network']\n",
    "}\n",
    "\n",
    "docker_compose['services']['ollama_llm'] = {\n",
    "    'image': 'ollama/ollama',\n",
    "    'container_name': '${OLLAMA_LLM_CONTAINER_NAME}',\n",
    "    'environment': ['OLLAMA_HOST=0.0.0.0:${OLLAMA_LLM_PORT}'],\n",
    "    'ports': ['${OLLAMA_LLM_PORT}:${OLLAMA_LLM_PORT}'],\n",
    "    'volumes': [\n",
    "        '${OLLAMA_MODELS_PATH}:/root/.ollama',\n",
    "        '${OLLAMA_LLM_PATH}:/root/.ollama/llm'\n",
    "    ],\n",
    "    'deploy': {\n",
    "        'resources': {\n",
    "            'reservations': {\n",
    "                'devices': [{'driver': 'nvidia', 'count': 1, 'capabilities': ['gpu']}]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'networks': ['ragtools_network']\n",
    "}\n",
    "\n",
    "# Ensure the network is defined\n",
    "if 'networks' not in docker_compose:\n",
    "    docker_compose['networks'] = {}\n",
    "docker_compose['networks']['ragtools_network'] = {'name': '${DOCKER_NETWORK_NAME}'}\n",
    "\n",
    "# Write updated docker-compose.yml\n",
    "with open(docker_compose_path, 'w') as f:\n",
    "    yaml.dump(docker_compose, f)\n",
    "\n",
    "print(\"Updated docker-compose.yml with Ollama services.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test OllamaManager Class\n",
    "\n",
    "Now, let's test our OllamaManager class by spinning up a test model and running a simple prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 10:32:39,423 - INFO - Starting all containers...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 10:32:49,481 - INFO - Checking container status:\n",
      "2024-07-09 10:32:49,539 - INFO - OllamaManager initialized with models_path: ./db_data/ollama_models, llm_path: ./db_data/ollama_models/llm\n",
      "2024-07-09 10:32:49,540 - INFO - Using model: llama3:70b on port: 11435\n",
      "2024-07-09 10:32:49,540 - INFO - Ollama LLM Port: 11435\n",
      "2024-07-09 10:32:49,540 - INFO - Ollama LLM Model: llama3:70b\n",
      "2024-07-09 10:32:49,541 - DEBUG - Starting new HTTP connection (1): localhost:11435\n",
      "2024-07-09 10:32:49,542 - DEBUG - http://localhost:11435 \"GET /api/tags HTTP/11\" 200 663\n",
      "2024-07-09 10:32:49,543 - INFO - Ollama is responsive\n",
      "2024-07-09 10:32:49,543 - INFO - Available models: ['llama3:70b', 'tinyllama:latest']\n",
      "2024-07-09 10:32:49,543 - DEBUG - Starting new HTTP connection (1): localhost:11435\n",
      "2024-07-09 10:32:49,544 - DEBUG - http://localhost:11435 \"GET /api/tags HTTP/11\" 200 663\n",
      "2024-07-09 10:32:49,545 - DEBUG - Starting new HTTP connection (1): localhost:11435\n",
      "2024-07-09 10:32:49,711 - DEBUG - http://localhost:11435 \"POST /api/generate HTTP/11\" 200 None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                        IMAGE             COMMAND                  SERVICE            CREATED        STATUS        PORTS\n",
      "ragtools_neo4j              neo4j:latest      \"tini -g -- /startup…\"   neo4j              15 hours ago   Up 15 hours   0.0.0.0:7474->7474/tcp, :::7474->7474/tcp, 7473/tcp, 0.0.0.0:7687->7687/tcp, :::7687->7687/tcp\n",
      "ragtools_ollama_embedding   ollama/ollama     \"/bin/ollama serve\"      ollama_embedding   15 hours ago   Up 15 hours   0.0.0.0:11434->11434/tcp, :::11434->11434/tcp\n",
      "ragtools_ollama_llm         ollama/ollama     \"/bin/ollama serve\"      ollama_llm         15 hours ago   Up 15 hours   11434/tcp, 0.0.0.0:11435->11435/tcp, :::11435->11435/tcp\n",
      "ragtools_postgres           ankane/pgvector   \"docker-entrypoint.s…\"   postgres           15 hours ago   Up 15 hours   0.0.0.0:5432->5432/tcp, :::5432->5432/tcp\n",
      "\n",
      "\n",
      "Prompt: What is the capital of Texas?\n",
      "Response:\n",
      "The capital of Texas is Austin."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 10:32:50,207 - DEBUG - Starting new HTTP connection (1): localhost:11435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Prompt: Where did the breakfast taco originate from, Austin or San Antonio?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 10:32:50,405 - DEBUG - http://localhost:11435 \"POST /api/generate HTTP/11\" 200 None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "The origins of the breakfast taco are a topic of debate among Texans, and both Austin and San Antonio claim to be its birthplace. However, after digging into the history, I'd argue that San Antonio has a stronger case for being the true originator of the breakfast taco.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "1. **Tejano cuisine**: San Antonio is the heart of Tejano country, where Mexican and Spanish influences merged with American and African American flavors to create a unique culinary style. The breakfast taco is a staple of Tejano cuisine, which was shaped by the city's early settlers.\n",
      "2. **Early tacos**: Tacos have been a part of San Antonio's food scene since the 18th century, when Mexican street vendors sold them as a quick, affordable meal to workers and travelers. Breakfast tacos likely evolved from these humble beginnings.\n",
      "3. **Chili queens**: In the late 19th and early 20th centuries, San Antonio's famous \"chili queens\" – women who sold chili con carne from carts in the city's markets – began offering breakfast items like eggs, tortillas, and chorizo to pair with their spicy stews. This combo laid the groundwork for the modern breakfast taco.\n",
      "4. **Taco shops**: San Antonio is home to some of the oldest continuously operating taco shops in Texas, such as Ray's Drive-In (established in 1955) and Tito's Mexican Restaurant (founded in 1956). These eateries have been serving breakfast tacos for decades, long before Austin's taco scene took off.\n",
      "5. **Cultural significance**: The breakfast taco is deeply ingrained in San Antonio's cultural identity. It's a staple at family gatherings, festivals, and everyday meals, reflecting the city's strong Hispanic heritage.\n",
      "\n",
      "Austin, on the other hand, has a thriving food truck scene that popularized tacos in the 1990s and 2000s, leading to a wider variety of taco styles and fusion flavors. While Austin is certainly a hub for innovative tacos, its breakfast taco history doesn't stretch back as far as San Antonio's.\n",
      "\n",
      "In conclusion, while both cities can claim some credit for popularizing the breakfast taco, San Antonio's rich Tejano heritage, long history of taco vendors, and cultural significance make it the more likely birthplace of this beloved dish.\n",
      "\n",
      "\n",
      "Verification complete. The containers are still running for manual testing.\n",
      "To manually test, use the following curl command:\n",
      "curl -X POST http://localhost:11435/api/generate -d '{\"model\": \"llama3:70b\", \"prompt\": \"Your question here\"}'\n",
      "\n",
      "When you're done testing, run the following to stop the containers:\n",
      "docker-compose -f ../config/docker-compose.yml down\n",
      "\n",
      "Debug Information:\n",
      "OLLAMA_LLM_PORT: 11435\n",
      "OLLAMA_LLM_MODEL: llama3:70b\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import requests\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.utils.config_utils import Config\n",
    "from src.utils.ollama_manager import OllamaManager\n",
    "from src.utils.DockerComposeManager import DockerComposeManager\n",
    "\n",
    "# Initialize the Config class\n",
    "config = Config()\n",
    "\n",
    "# Initialize DockerComposeManager\n",
    "docker_compose_path = os.path.join('..', 'config', 'docker-compose.yml')\n",
    "docker_manager = DockerComposeManager(docker_compose_path)\n",
    "\n",
    "# Start all containers\n",
    "logging.info(\"Starting all containers...\")\n",
    "docker_manager.start_containers()\n",
    "\n",
    "# Wait for containers to start\n",
    "time.sleep(10)\n",
    "\n",
    "# Check container status\n",
    "logging.info(\"Checking container status:\")\n",
    "docker_manager.show_container_status()\n",
    "\n",
    "# Initialize OllamaManager for the LLM\n",
    "llm_manager = OllamaManager(config)\n",
    "\n",
    "# Debug information\n",
    "logging.info(f\"Ollama LLM Port: {llm_manager.port}\")\n",
    "logging.info(f\"Ollama LLM Model: {llm_manager.model}\")\n",
    "\n",
    "# Check if Ollama is responsive\n",
    "try:\n",
    "    response = requests.get(f'http://localhost:{llm_manager.port}/api/tags')\n",
    "    if response.status_code == 200:\n",
    "        logging.info(\"Ollama is responsive\")\n",
    "        available_models = response.json().get('models', [])\n",
    "        logging.info(f\"Available models: {[model['name'] for model in available_models]}\")\n",
    "    else:\n",
    "        logging.error(f\"Ollama returned status code {response.status_code}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    logging.error(f\"Error connecting to Ollama: {e}\")\n",
    "\n",
    "# Check if the model is running\n",
    "if not llm_manager.is_model_running():\n",
    "    logging.warning(f\"Model '{llm_manager.model}' is not running. You may need to pull it first.\")\n",
    "    # Optionally, you could try to pull the model here\n",
    "    # llm_manager.pull_model()  # You'd need to implement this method\n",
    "\n",
    "# Test questions\n",
    "test_questions = [\n",
    "    \"What is the capital of Texas?\",\n",
    "    \"Where did the breakfast taco originate from, Austin or San Antonio?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\nPrompt: {question}\")\n",
    "    llm_manager.generate_response(question)\n",
    "\n",
    "print(\"\\nVerification complete. The containers are still running for manual testing.\")\n",
    "print(\"To manually test, use the following curl command:\")\n",
    "print(f\"curl -X POST http://localhost:{config.OLLAMA_LLM_PORT}/api/generate -d '{{\\\"model\\\": \\\"{config.OLLAMA_LLM_MODEL}\\\", \\\"prompt\\\": \\\"Your question here\\\"}}'\")\n",
    "print(\"\\nWhen you're done testing, run the following to stop the containers:\")\n",
    "print(f\"docker-compose -f {docker_compose_path} down\")\n",
    "\n",
    "# Additional debug information\n",
    "print(\"\\nDebug Information:\")\n",
    "print(f\"OLLAMA_LLM_PORT: {config.OLLAMA_LLM_PORT}\")\n",
    "print(f\"OLLAMA_LLM_MODEL: {config.OLLAMA_LLM_MODEL}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we have successfully:\n",
    "\n",
    "1. Set up Ollama instances in Docker containers\n",
    "2. Created an OllamaManager class to handle Ollama operations\n",
    "3. Implemented a method to generate responses from the LLM\n",
    "4. Demonstrated the streaming nature of the LLM's output\n",
    "5. Verified the functionality of our setup with test questions\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Our next notebook will focus on creating a CLI interface for interacting with the LLM. Before diving into the implementation, we'll need to consider:\n",
    "\n",
    "1. LLM Configurables:\n",
    "   - Context length\n",
    "   - Temperature\n",
    "   - Other relevant parameters (e.g., top_p, frequency_penalty, presence_penalty)\n",
    "\n",
    "2. CLI Interface Options:\n",
    "   - Evaluate the merits of adopting a pre-built CLI interface vs. creating our own\n",
    "   - Consider libraries like `click`, `typer`, or `argparse` for building a custom CLI\n",
    "\n",
    "3. Chat Interface Design:\n",
    "   - How to maintain conversation history\n",
    "   - Handling user input and system responses\n",
    "   - Implementing commands for adjusting LLM parameters on-the-fly\n",
    "\n",
    "4. Integration with OllamaManager:\n",
    "   - How to incorporate our existing OllamaManager class into the CLI interface\n",
    "\n",
    "By addressing these points, we'll be well-prepared to create a robust and user-friendly CLI for interacting with our LLM setup."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragtools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
