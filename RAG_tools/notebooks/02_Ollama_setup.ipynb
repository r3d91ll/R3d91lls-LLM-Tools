{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Ollama Setup\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Welcome to the third notebook in our RAG Tools series! In this notebook, we'll set up Ollama, an open-source tool that allows us to run large language models locally. We'll configure Ollama instances in Docker containers, enabling us to use different models for various tasks in our RAG (Retrieval-Augmented Generation) system, such as embedding generation and text generation.\n",
    "\n",
    "By the end of this notebook, you'll have:\n",
    "1. Updated our project directory structure for Ollama\n",
    "2. Created an OllamaManager class to handle Ollama operations\n",
    "3. Updated our environment variables and Docker Compose configuration\n",
    "4. Tested the OllamaManager class with a sample model\n",
    "\n",
    "## 2. Update Project Directory Structure\n",
    "\n",
    "First, let's update our project directory structure to accommodate the Ollama models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the project root directory\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Create directories for Ollama models\n",
    "ollama_models_path = os.path.join(project_root, 'db_data', 'ollama_models')\n",
    "ollama_llm_path = os.path.join(ollama_models_path, 'llm')\n",
    "\n",
    "os.makedirs(ollama_models_path, exist_ok=True)\n",
    "os.makedirs(ollama_llm_path, exist_ok=True)\n",
    "\n",
    "print(f\"Created Ollama models directory at: {ollama_models_path}\")\n",
    "print(f\"Created Ollama LLM directory at: {ollama_llm_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our updated project structure now looks like this:\n",
    "\n",
    "```\n",
    "RAG_tools/\n",
    "├── config/\n",
    "│   ├── docker-compose.yml\n",
    "│   └── .env\n",
    "├── notebooks/\n",
    "│   ├── 00_Environment_Setup.ipynb\n",
    "│   ├── 01_Database_Setup.ipynb\n",
    "│   └── 02_Ollama_setup.ipynb\n",
    "├── src/\n",
    "│   └── utils/\n",
    "│       ├── config_utils.py\n",
    "│       └── ollama_manager.py\n",
    "├── db_data/\n",
    "│   ├── postgres/\n",
    "│   ├── neo4j/\n",
    "│   └── ollama_models/\n",
    "│       └── llm/\n",
    "└── tests/\n",
    "```\n",
    "\n",
    "## 3. Create OllamaManager Class\n",
    "\n",
    "Now, let's create the OllamaManager class. This class will be used to launch and manage Ollama instances in Docker containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../src/utils/ollama_manager.py\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "from .DockerComposeManager import DockerComposeManager\n",
    "from .config_utils import Config\n",
    "\n",
    "class OllamaManager:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.container_name = self.config.OLLAMA_LLM_CONTAINER_NAME\n",
    "        self.port = self.config.OLLAMA_LLM_PORT\n",
    "        self.model = self.config.OLLAMA_LLM_MODEL\n",
    "        self.gpu = self.config.OLLAMA_LLM_GPU\n",
    "        \n",
    "        self.models_path = self.config.OLLAMA_MODELS_PATH\n",
    "        self.llm_path = self.config.OLLAMA_LLM_PATH\n",
    "        \n",
    "        print(\"OllamaManager initialized with:\")\n",
    "        print(f\"container_name: {self.container_name}\")\n",
    "        print(f\"port: {self.port}\")\n",
    "        print(f\"model: {self.model}\")\n",
    "        print(f\"gpu: {self.gpu}\")\n",
    "        print(f\"models_path: {self.models_path}\")\n",
    "        print(f\"llm_path: {self.llm_path}\")\n",
    "        \n",
    "        # Ensure directories exist\n",
    "        if self.models_path:\n",
    "            os.makedirs(self.models_path, exist_ok=True)\n",
    "        if self.llm_path:\n",
    "            os.makedirs(self.llm_path, exist_ok=True)\n",
    "        \n",
    "        # Initialize DockerComposeManager\n",
    "        docker_compose_path = os.path.join('..', 'config', 'docker-compose.yml')\n",
    "        self.docker_manager = DockerComposeManager(docker_compose_path)\n",
    "\n",
    "        logging.info(f\"OllamaManager initialized with models_path: {self.models_path}, llm_path: {self.llm_path}\")\n",
    "        logging.info(f\"Using model: {self.model} on port: {self.port}\")\n",
    "\n",
    "    def generate_response(self, prompt):\n",
    "        try:\n",
    "            payload = {\n",
    "                'model': self.model,\n",
    "                'prompt': prompt\n",
    "            }\n",
    "            logging.debug(f\"Sending request to Ollama API with payload: {payload}\")\n",
    "            logging.debug(f\"API URL: http://localhost:{self.port}/api/generate\")\n",
    "            \n",
    "            response = requests.post(\n",
    "                f'http://localhost:{self.port}/api/generate',\n",
    "                json=payload,\n",
    "                stream=True\n",
    "            )\n",
    "            logging.debug(f\"Response status code: {response.status_code}\")\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            full_response = \"\"\n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    try:\n",
    "                        chunk = json.loads(line)\n",
    "                        logging.debug(f\"Received chunk: {chunk}\")\n",
    "                        if 'response' in chunk:\n",
    "                            token = chunk['response']\n",
    "                            full_response += token\n",
    "                            print(token, end='', flush=True)\n",
    "                        if chunk.get('done', False):\n",
    "                            break\n",
    "                    except json.JSONDecodeError:\n",
    "                        logging.warning(f\"Failed to decode JSON: {line}\")\n",
    "            \n",
    "            print(\"\\n\")  # New line after the response\n",
    "            return full_response.strip()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Error generating response: {str(e)}\")\n",
    "            if hasattr(e, 'response') and e.response is not None:\n",
    "                logging.error(f\"Response content: {e.response.text}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error: {str(e)}\")\n",
    "            return f\"Unexpected error: {str(e)}\"\n",
    "\n",
    "    def is_model_running(self):\n",
    "        try:\n",
    "            response = requests.get(f'http://localhost:{self.port}/api/tags')\n",
    "            response.raise_for_status()\n",
    "            models = response.json()\n",
    "            logging.debug(f\"Available models: {models}\")\n",
    "            return self.model in [model['name'] for model in models['models']]\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Error checking if model is running: {e}\")\n",
    "            return False\n",
    "\n",
    "    def pull_model(self):\n",
    "        logging.info(f\"Pulling model {self.model}...\")\n",
    "        logging.debug(f\"self.models_path: {self.models_path}\")\n",
    "        logging.debug(f\"self.model: {self.model}\")\n",
    "        \n",
    "        if not self.models_path:\n",
    "            logging.error(\"models_path is not set. Cannot pull model.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            model_path = os.path.join(self.models_path, 'models', 'manifests', 'registry.ollama.ai', 'library', self.model)\n",
    "            logging.info(f\"Checking for model at path: {model_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error constructing model path: {str(e)}\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            response = requests.post(f'http://localhost:{self.port}/api/pull', json={'name': self.model}, stream=True)\n",
    "            response.raise_for_status()\n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    print(line.decode())\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Error pulling model: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def start_container(self):\n",
    "        self.docker_manager.start_containers()\n",
    "        logging.info(f\"Started container: {self.container_name}\")\n",
    "\n",
    "    def stop_container(self):\n",
    "        self.docker_manager.stop_containers()\n",
    "        logging.info(f\"Stopped container: {self.container_name}\")\n",
    "\n",
    "    def wait_for_ollama(self, max_attempts=5, delay=5):\n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                response = requests.get(f'http://localhost:{self.port}/api/tags')\n",
    "                if response.status_code == 200:\n",
    "                    logging.info(f\"Successfully connected to Ollama on port {self.port}\")\n",
    "                    return True\n",
    "            except requests.exceptions.RequestException:\n",
    "                logging.warning(f\"Attempt {attempt + 1}/{max_attempts}: Ollama on port {self.port} is not ready yet. Retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "        logging.error(f\"Failed to connect to Ollama after {max_attempts} attempts\")\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Update Environment Variables\n",
    "\n",
    "Now, let's update our .env file to include the Ollama-related variables. Add the following to your `.env` file in the `config/` directory:\n",
    "\n",
    "```\n",
    "# Ollama Configuration\n",
    "OLLAMA_LLM_CONTAINER_NAME=ragtools_ollama_llm\n",
    "OLLAMA_LLM_PORT=11435\n",
    "OLLAMA_LLM_MODEL=tinyllama\n",
    "OLLAMA_LLM_GPU=0\n",
    "\n",
    "OLLAMA_MODELS_PATH=../db_data/ollama_models\n",
    "OLLAMA_LLM_PATH=../db_data/ollama_models/llm\n",
    "```\n",
    "\n",
    "## 5. Update Config Class\n",
    "\n",
    "Before we proceed with the verification step, we need to update our Config class to include the new Ollama-related attributes. This is a crucial step when extending our framework with new components.\n",
    "\n",
    "This step demonstrates how to extend the Config class when new components are added to the framework. It's important to update this class whenever new environment variables or configuration options are introduced.\n",
    "\n",
    "Let's update the `config_utils.py` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../src/utils/config_utils.py\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        load_dotenv()\n",
    "        \n",
    "        # Database configurations\n",
    "        self.POSTGRES_DB = os.getenv('POSTGRES_DB')\n",
    "        self.POSTGRES_USER = os.getenv('POSTGRES_USER')\n",
    "        self.POSTGRES_PASSWORD = os.getenv('POSTGRES_PASSWORD')\n",
    "        self.POSTGRES_HOST = os.getenv('POSTGRES_HOST')\n",
    "        self.POSTGRES_PORT = os.getenv('POSTGRES_PORT')\n",
    "        \n",
    "        self.NEO4J_AUTH = os.getenv('NEO4J_AUTH')\n",
    "        self.NEO4J_HOST = os.getenv('NEO4J_HOST')\n",
    "        self.NEO4J_HTTP_PORT = os.getenv('NEO4J_HTTP_PORT')\n",
    "        self.NEO4J_BOLT_PORT = os.getenv('NEO4J_BOLT_PORT')\n",
    "        \n",
    "        # Docker configurations\n",
    "        self.POSTGRES_CONTAINER_NAME = os.getenv('POSTGRES_CONTAINER_NAME')\n",
    "        self.NEO4J_CONTAINER_NAME = os.getenv('NEO4J_CONTAINER_NAME')\n",
    "        self.DOCKER_NETWORK_NAME = os.getenv('DOCKER_NETWORK_NAME')\n",
    "\n",
    "        # Ollama configurations\n",
    "        self.OLLAMA_LLM_CONTAINER_NAME = os.getenv('OLLAMA_LLM_CONTAINER_NAME')\n",
    "        self.OLLAMA_LLM_PORT = int(os.getenv('OLLAMA_LLM_PORT', 11435))\n",
    "        self.OLLAMA_LLM_MODEL = os.getenv('OLLAMA_LLM_MODEL')\n",
    "        self.OLLAMA_LLM_GPU = int(os.getenv('OLLAMA_LLM_GPU', 0))\n",
    "        self.OLLAMA_MODELS_PATH = os.getenv('OLLAMA_MODELS_PATH')\n",
    "        self.OLLAMA_LLM_PATH = os.getenv('OLLAMA_LLM_PATH')\n",
    "\n",
    "    def get_postgres_connection_params(self):\n",
    "        return {\n",
    "            \"dbname\": self.POSTGRES_DB,\n",
    "            \"user\": self.POSTGRES_USER,\n",
    "            \"password\": self.POSTGRES_PASSWORD,\n",
    "            \"host\": self.POSTGRES_HOST,\n",
    "            \"port\": self.POSTGRES_PORT\n",
    "        }\n",
    "\n",
    "    def get_neo4j_connection_params(self):\n",
    "        return {\n",
    "            \"uri\": f\"bolt://{self.NEO4J_HOST}:{self.NEO4J_BOLT_PORT}\",\n",
    "            \"auth\": tuple(self.NEO4J_AUTH.split('/'))\n",
    "        }\n",
    "\n",
    "    def print_all_attributes(self):\n",
    "        print(\"All Config attributes:\")\n",
    "        for attr, value in self.__dict__.items():\n",
    "            print(f\"{attr}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Update Docker Compose Configuration\n",
    "\n",
    "Now, let's update our docker-compose.yml file to include the Ollama service:\n",
    "\n",
    "```yaml\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  postgres:\n",
    "    # ... (existing PostgreSQL configuration)\n",
    "\n",
    "  neo4j:\n",
    "    # ... (existing Neo4j configuration)\n",
    "\n",
    "  ollama:\n",
    "    image: ollama/ollama\n",
    "    container_name: ${OLLAMA_LLM_CONTAINER_NAME}\n",
    "    environment:\n",
    "      - OLLAMA_HOST=0.0.0.0:${OLLAMA_LLM_PORT}\n",
    "    ports:\n",
    "      - \"${OLLAMA_LLM_PORT}:${OLLAMA_LLM_PORT}\"\n",
    "    volumes:\n",
    "      - ${OLLAMA_MODELS_PATH}:/root/.ollama\n",
    "    deploy:\n",
    "      resources:\n",
    "        reservations:\n",
    "          devices:\n",
    "            - driver: nvidia\n",
    "              count: 1\n",
    "              capabilities: [gpu]\n",
    "    networks:\n",
    "      - ragtools_network\n",
    "\n",
    "networks:\n",
    "  ragtools_network:\n",
    "    name: ${DOCKER_NETWORK_NAME}\n",
    "\n",
    "volumes:\n",
    "  postgres_data:\n",
    "  neo4j_data:\n",
    "  ollama_data:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Config class\n",
    "config = Config()\n",
    "\n",
    "print(\"Configuration values:\")\n",
    "for attr, value in vars(config).items():\n",
    "    if attr.startswith('OLLAMA_'):\n",
    "        print(f\"{attr}: {value}\")\n",
    "\n",
    "# Initialize OllamaManager\n",
    "ollama_manager = OllamaManager(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test OllamaManager Class\n",
    "\n",
    "Now, let's test our OllamaManager class by spinning up a test model and running a simple prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 11:07:39,066 - ERROR - .env file not found\n",
      "2024-07-16 11:07:39,066 - INFO - Config initialized with OLLAMA_LLM_MODEL: None\n",
      "2024-07-16 11:07:39,068 - INFO - OllamaManager initialized with models_path: None, llm_path: None\n",
      "2024-07-16 11:07:39,068 - INFO - Using model: None on port: 11435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OllamaManager initialized with:\n",
      "container_name: None\n",
      "port: 11435\n",
      "model: None\n",
      "gpu: 0\n",
      "models_path: None\n",
      "llm_path: None\n",
      "Starting Ollama container...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 11:07:39,532 - INFO - Started container: None\n",
      "2024-07-16 11:07:39,533 - DEBUG - Starting new HTTP connection (1): localhost:11435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Waiting for Ollama to be ready...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 11:07:41,587 - DEBUG - http://localhost:11435 \"GET /api/tags HTTP/11\" 200 13\n",
      "2024-07-16 11:07:41,587 - INFO - Successfully connected to Ollama on port 11435\n",
      "2024-07-16 11:07:41,588 - INFO - Pulling model None...\n",
      "2024-07-16 11:07:41,588 - DEBUG - self.models_path: None\n",
      "2024-07-16 11:07:41,588 - DEBUG - self.model: None\n",
      "2024-07-16 11:07:41,588 - ERROR - models_path is not set. Cannot pull model.\n",
      "2024-07-16 11:07:41,589 - INFO - Generating response for prompt: Explain the concept of Retrieval-Augmented Generation in three sentences.\n",
      "2024-07-16 11:07:41,589 - INFO - Sending request to http://localhost:11435/api/generate with payload: {'model': None, 'prompt': 'Explain the concept of Retrieval-Augmented Generation in three sentences.'}\n",
      "2024-07-16 11:07:41,589 - DEBUG - Starting new HTTP connection (1): localhost:11435\n",
      "2024-07-16 11:07:41,591 - DEBUG - http://localhost:11435 \"POST /api/generate HTTP/11\" 400 29\n",
      "2024-07-16 11:07:41,591 - INFO - Response status code: 400\n",
      "2024-07-16 11:07:41,591 - ERROR - Error response: {\"error\":\"model is required\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensuring Codestral 22B model is available...\n",
      "Testing Codestral 22B model...\n",
      "Prompt: Explain the concept of Retrieval-Augmented Generation in three sentences.\n",
      "Response: Error: 400 Bad Request - {\"error\":\"model is required\"}\n",
      "Stopping Ollama container...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 11:07:42,306 - INFO - Stopped container: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from src.utils.config_utils import Config\n",
    "from src.utils.ollama_manager import OllamaManager\n",
    "from src.utils.DockerComposeManager import DockerComposeManager\n",
    "\n",
    "config = Config()\n",
    "codestral_manager = OllamaManager(config)\n",
    "\n",
    "print(\"Starting Ollama container...\")\n",
    "codestral_manager.start_container()\n",
    "\n",
    "print(\"Waiting for Ollama to be ready...\")\n",
    "codestral_manager.wait_for_ollama()\n",
    "\n",
    "print(\"Ensuring Codestral 22B model is available...\")\n",
    "codestral_manager.pull_model()\n",
    "\n",
    "print(\"Testing Codestral 22B model...\")\n",
    "test_prompt = \"Explain the concept of Retrieval-Augmented Generation in three sentences.\"\n",
    "response = codestral_manager.generate_response(test_prompt)\n",
    "print(f\"Prompt: {test_prompt}\")\n",
    "print(f\"Response: {response}\")\n",
    "\n",
    "print(\"Stopping Ollama container...\")\n",
    "codestral_manager.stop_container()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we have successfully:\n",
    "\n",
    "1. Set up Ollama instances in Docker containers\n",
    "2. Created an OllamaManager class to handle Ollama operations\n",
    "3. Implemented a method to generate responses from the LLM\n",
    "4. Demonstrated the streaming nature of the LLM's output\n",
    "5. Verified the functionality of our setup with test questions\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Our next notebook will focus on creating a CLI interface for interacting with the LLM. Before diving into the implementation, we'll need to consider:\n",
    "\n",
    "1. LLM Configurables:\n",
    "   - Context length\n",
    "   - Temperature\n",
    "   - Other relevant parameters (e.g., top_p, frequency_penalty, presence_penalty)\n",
    "\n",
    "2. CLI Interface Options:\n",
    "   - Evaluate the merits of adopting a pre-built CLI interface vs. creating our own\n",
    "   - Consider libraries like `click`, `typer`, or `argparse` for building a custom CLI\n",
    "\n",
    "3. Chat Interface Design:\n",
    "   - How to maintain conversation history\n",
    "   - Handling user input and system responses\n",
    "   - Implementing commands for adjusting LLM parameters on-the-fly\n",
    "\n",
    "4. Integration with OllamaManager:\n",
    "   - How to incorporate our existing OllamaManager class into the CLI interface\n",
    "\n",
    "By addressing these points, we'll be well-prepared to create a robust and user-friendly CLI for interacting with our LLM setup."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragtools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
